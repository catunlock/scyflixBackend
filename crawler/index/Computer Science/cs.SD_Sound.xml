<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acs.SD%26id_list%3D%26start%3D0%26max_results%3D500" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:cs.SD&amp;id_list=&amp;start=0&amp;max_results=500</title>
  <id>http://arxiv.org/api/EaNgwLfSo1Qv64wb42NvNJEuNm8</id>
  <updated>2017-10-08T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">1002</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">500</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/0902.2783v2</id>
    <updated>2010-01-08T10:28:32Z</updated>
    <published>2009-02-16T21:03:30Z</published>
    <title>New Ica-Beamforming Method to Under-Determined BSS</title>
    <summary>  This paper has been withdrawn by the author ali pourmohammad.
</summary>
    <author>
      <name>Ali Pourmohammad</name>
    </author>
    <author>
      <name>Seyed Mohammad Ahadi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn</arxiv:comment>
    <link href="http://arxiv.org/abs/0902.2783v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0902.2783v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.08982v1</id>
    <updated>2015-12-30T15:46:30Z</updated>
    <published>2015-12-30T15:46:30Z</published>
    <title>Technical Report: a tool for measuring Prosodic Accommodation</title>
    <summary>  This article has been withdrawn by arXiv administrators because the submitter
did not have the legal authority to grant the license applied to the work.
</summary>
    <author>
      <name>Sucheta Ghosh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Withdrawn by arXiv administrators</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.08982v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.08982v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.3198v1</id>
    <updated>2009-03-18T16:51:25Z</updated>
    <published>2009-03-18T16:51:25Z</published>
    <title>TR02: State dependent oracle masks for improved dynamical features</title>
    <summary>  Using the AURORA-2 digit recognition task, we show that recognition
accuracies obtained with classical, SNR based oracle masks can be substantially
improved by using a state-dependent mask estimation technique.
</summary>
    <author>
      <name>J. F. Gemmeke</name>
    </author>
    <author>
      <name>B. Cranen</name>
    </author>
    <link href="http://arxiv.org/abs/0903.3198v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.3198v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0703049v1</id>
    <updated>2007-03-10T23:59:55Z</updated>
    <published>2007-03-10T23:59:55Z</published>
    <title>Algorithm of Segment-Syllabic Synthesis in Speech Recognition Problem</title>
    <summary>  Speech recognition based on the syllable segment is discussed in this paper.
The principal search methods in space of states for the speech recognition
problem by segment-syllabic parameters trajectory synthesis are investigated.
Recognition as comparison the parameters trajectories in chosen speech units on
the sections of the segmented speech is realized. Some experimental results are
given and discussed.
</summary>
    <author>
      <name>Oleg N. Karpov</name>
    </author>
    <author>
      <name>Olga A. Savenkova</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0703049v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0703049v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0803.0197v1</id>
    <updated>2008-03-03T09:04:46Z</updated>
    <published>2008-03-03T09:04:46Z</published>
    <title>DSP Based System for Real time Voice Synthesis Applications Development</title>
    <summary>  This paper describes an experimental system designed for development of real
time voice synthesis applications. The system is composed from a DSP
coprocessor card, equipped with an TMS320C25 or TMS320C50 chip, voice
acquisition module (ADDA2),host computer (IBM-PC compatible), software specific
tools.
</summary>
    <author>
      <name>Radu Arsinte</name>
    </author>
    <author>
      <name>Attila Ferencz</name>
    </author>
    <author>
      <name>Costin Miron</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures, SPECOM' 96 Conference</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of SPECOM' 96 Conference, 1996, St. Petersburg, Russia</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0803.0197v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0803.0197v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0911.3538v1</id>
    <updated>2009-11-18T13:36:20Z</updated>
    <published>2009-11-18T13:36:20Z</published>
    <title>Noise Speech wavelet analyzing in special time ranges</title>
    <summary>  Speech analyzing in special periods of time has been presented in this paper.
One of the most important periods in signal processing is near to Zero. By this
paper, we analyze noise speech signals when these signals are near to Zero. Our
strategy is defining some subfunctions and compress histograms when a noise
speech signal is in a special period. It can be so useful for wavelet signal
processing and spoken systems analyzing.
</summary>
    <author>
      <name>Amin Daneshmand Malayeri</name>
    </author>
    <link href="http://arxiv.org/abs/0911.3538v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.3538v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1112.1368v1</id>
    <updated>2011-12-06T18:30:13Z</updated>
    <published>2011-12-06T18:30:13Z</published>
    <title>Discovering novel computer music techniques by exploring the space of
  short computer programs</title>
    <summary>  Very short computer programs, sometimes consisting of as few as three
arithmetic operations in an infinite loop, can generate data that sounds like
music when output as raw PCM audio. The space of such programs was recently
explored by dozens of individuals within various on-line communities. This
paper discusses the programs resulting from this exploratory work and
highlights some rather unusual methods they use for synthesizing sound and
generating musical structure.
</summary>
    <author>
      <name>Ville-Matias Heikkilä</name>
    </author>
    <link href="http://arxiv.org/abs/1112.1368v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1112.1368v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.6031v1</id>
    <updated>2013-02-25T10:13:09Z</updated>
    <published>2013-02-25T10:13:09Z</published>
    <title>Phoneme discrimination using KS algebra I</title>
    <summary>  In our work we define a new algebra of operators as a substitute for fuzzy
logic. Its primary purpose is for construction of binary discriminators for
phonemes based on spectral content. It is optimized for design of
non-parametric computational circuits, and makes uses of 4 operations: $\min$,
$\max$, the difference and generalized additively homogenuous means.
</summary>
    <author>
      <name>Ondrej Such</name>
    </author>
    <link href="http://arxiv.org/abs/1302.6031v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.6031v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; I.5.2; I.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.6194v1</id>
    <updated>2013-02-25T18:56:49Z</updated>
    <published>2013-02-25T18:56:49Z</published>
    <title>Phoneme discrimination using $KS$-algebra II</title>
    <summary>  $KS$-algebra consists of expressions constructed with four kinds operations,
the minimum, maximum, difference and additively homogeneous generalized means.
Five families of $Z$-classifiers are investigated on binary classification
tasks between English phonemes. It is shown that the classifiers are able to
reflect well known formant characteristics of vowels, while having very small
Kolmogoroff's complexity.
</summary>
    <author>
      <name>Ondrej Such</name>
    </author>
    <author>
      <name>Lenka Mackovicova</name>
    </author>
    <link href="http://arxiv.org/abs/1302.6194v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.6194v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; I.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.7866v1</id>
    <updated>2014-05-30T13:58:36Z</updated>
    <published>2014-05-30T13:58:36Z</published>
    <title>Vocal signal digital processing. Instrument for analog to digital
  conversion study</title>
    <summary>  The goal of this article is to present interactive didactic software for
analog to digital conversion using PCM method. After a short introduction
regarding vocal signal processing we present some method for analog to digital
conversion. The didactic software is an applet that can be direct accessed by
any interested person.
</summary>
    <author>
      <name>Ovidiu-Andrei Schipor</name>
    </author>
    <author>
      <name>Felicia-Florentina Giza</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 4 figures, in Romanian</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.7866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.7866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.0117v1</id>
    <updated>2014-08-30T14:51:33Z</updated>
    <published>2014-08-30T14:51:33Z</published>
    <title>Computerized Multi Microphone Test System</title>
    <summary>  An acoustic testing approach based on the concept of a microphone sensor
surrounding the product under test is proposed. Microphone signals are
processed simultaneously by a test system computer, according to the objective
of the test. The spatial and frequency domain selectivity features of this
method are examined. Sound-spatial visualization algorithm is observed. A test
system design based on the concept of a microphone surrounding the tested
product has the potential to improve distortion measurement accuracy.
</summary>
    <author>
      <name>A. M. Dorman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.0117v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.0117v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.07411v1</id>
    <updated>2015-09-24T15:46:55Z</updated>
    <published>2015-09-24T15:46:55Z</published>
    <title>Speech Dereverberation in the STFT Domain</title>
    <summary>  Reverberation is damaging to both the quality and the intelligibility of a
speech signal. We propose a novel single-channel method of dereverberation
based on a linear filter in the Short Time Fourier Transform domain. Each
enhanced frame is constructed from a linear sum of nearby frames based on the
channel impulse response. The results show that the method can resolve any
reverberant signal with knowledge of the impulse response to a non-reverberant
signal.
</summary>
    <author>
      <name>Richard Stanton</name>
    </author>
    <author>
      <name>Mike Brookes</name>
    </author>
    <link href="http://arxiv.org/abs/1509.07411v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.07411v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.07767v1</id>
    <updated>2016-02-25T01:41:34Z</updated>
    <published>2016-02-25T01:41:34Z</published>
    <title>Breath Activity Detection Algorithm</title>
    <summary>  This report describes the use of a support vector machines with a novel
kernel, to determine the breathing rate and inhalation duration of a fire
fighter wearing a Self-Contained Breathing Apparatus. With this information, an
incident commander can monitor the firemen in his command for exhaustion and
ensure timely rotation of personnel to ensure overall fire fighter safety
</summary>
    <author>
      <name>Eric E. Hamke</name>
    </author>
    <author>
      <name>Ramiro Jordan</name>
    </author>
    <author>
      <name>Manel Ramon-Martinez</name>
    </author>
    <link href="http://arxiv.org/abs/1602.07767v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.07767v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.03296v1</id>
    <updated>2016-09-12T07:50:41Z</updated>
    <published>2016-09-12T07:50:41Z</published>
    <title>A Neural Network Alternative to Non-Negative Audio Models</title>
    <summary>  We present a neural network that can act as an equivalent to a Non-Negative
Matrix Factorization (NMF), and further show how it can be used to perform
supervised source separation. Due to the extensibility of this approach we show
how we can achieve better source separation performance as compared to
NMF-based methods, and propose a variety of derivative architectures that can
be used for further improvements.
</summary>
    <author>
      <name>Paris Smaragdis</name>
    </author>
    <author>
      <name>Shrikant Venkataramani</name>
    </author>
    <link href="http://arxiv.org/abs/1609.03296v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.03296v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.01651v1</id>
    <updated>2017-05-03T23:19:56Z</updated>
    <published>2017-05-03T23:19:56Z</published>
    <title>Modeling temporal constraints for a system of interactive scores</title>
    <summary>  In this chapter we explain briefly the fundamentals of the interactive scores
formalism. Then we develop a solution for implementing the ECO machine by
mixing petri nets and constraints propagation. We also present another solution
for implementing the ECO machine using concurrent constraint programming.
Finally, we present an extension of interactive score with conditional
branching.
</summary>
    <author>
      <name>Mauricio Toro</name>
    </author>
    <author>
      <name>Myriam Desainte-Catherine</name>
    </author>
    <author>
      <name>Antoine Allombert</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of Book Chapter published in Constraint Programming
  in Music, 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.01651v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.01651v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.05322v1</id>
    <updated>2017-05-15T16:34:20Z</updated>
    <published>2017-05-15T16:34:20Z</published>
    <title>Understanding MIDI: A Painless Tutorial on Midi Format</title>
    <summary>  A short overview demystifying the midi audio format is presented. The goal is
to explain the file structure and how the instructions are used to produce a
music signal, both in the case of monophonic signals as for polyphonic signals.
</summary>
    <author>
      <name>H. M. de Oliveira</name>
    </author>
    <author>
      <name>R. C. de Oliveira</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.05322v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.05322v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.04486v2</id>
    <updated>2017-06-15T20:09:52Z</updated>
    <published>2017-06-14T13:35:46Z</published>
    <title>Learning and Evaluating Musical Features with Deep Autoencoders</title>
    <summary>  In this work we describe and evaluate methods to learn musical embeddings.
Each embedding is a vector that represents four contiguous beats of music and
is derived from a symbolic representation. We consider autoencoding-based
methods including denoising autoencoders, and context reconstruction, and
evaluate the resulting embeddings on a forward prediction and a classification
task.
</summary>
    <author>
      <name>Mason Bretan</name>
    </author>
    <author>
      <name>Sageev Oore</name>
    </author>
    <author>
      <name>Doug Eck</name>
    </author>
    <author>
      <name>Larry Heck</name>
    </author>
    <link href="http://arxiv.org/abs/1706.04486v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.04486v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09557v1</id>
    <updated>2017-06-29T02:52:25Z</updated>
    <published>2017-06-29T02:52:25Z</published>
    <title>Machine listening intelligence</title>
    <summary>  This manifesto paper will introduce machine listening intelligence, an
integrated research framework for acoustic and musical signals modelling, based
on signal processing, deep learning and computational musicology.
</summary>
    <author>
      <name>C. E. Cella</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the First International Conference on Deep Learning
  and Music, Anchorage, US, May, 2017 (arXiv:1706.08675v1 [cs.NE])</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the First International Workshop on Deep Learning
  and Music joint with IJCNN. Anchorage, US. 1(1). pp 50-55 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1706.09557v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09557v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3; H.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09758v1</id>
    <updated>2017-06-29T13:51:31Z</updated>
    <published>2017-06-29T13:51:31Z</published>
    <title>Using Second-Order Hidden Markov Model to Improve Speaker Identification
  Recognition Performance under Neutral Condition</title>
    <summary>  In this paper, second-order hidden Markov model (HMM2) has been used and
implemented to improve the recognition performance of text-dependent speaker
identification systems under neutral talking condition. Our results show that
HMM2 improves the recognition performance under neutral talking condition
compared to the first-order hidden Markov model (HMM1). The recognition
performance has been improved by 9%.
</summary>
    <author>
      <name>Ismail Shahin</name>
    </author>
    <link href="http://arxiv.org/abs/1706.09758v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09758v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.00149v1</id>
    <updated>2017-07-01T12:23:37Z</updated>
    <published>2017-07-01T12:23:37Z</published>
    <title>Modeling and Analyzing the Vocal Tract under Normal and Stressful
  Talking Conditions</title>
    <summary>  In this research, we model and analyze the vocal tract under normal and
stressful talking conditions. This research answers the question of the
degradation in the recognition performance of text-dependent speaker
identification under stressful talking conditions. This research can be used
(for future research) to improve the recognition performance under stressful
talking conditions.
</summary>
    <author>
      <name>Ismail Shahin</name>
    </author>
    <author>
      <name>Nazeih Botros</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE SOUTHEASTCON 2001, Clemson, South Carolina, USA, 2001</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.00149v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.00149v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.02322v1</id>
    <updated>2017-08-07T22:18:55Z</updated>
    <published>2017-08-07T22:18:55Z</published>
    <title>Automatic Raga Recognition in Hindustani Classical Music</title>
    <summary>  Raga is the central melodic concept in Hindustani Classical Music. It has a
complex structure, often characterized by pathos. In this paper, we describe a
technique for Automatic Raga Recognition, based on pitch distributions. We are
able to successfully classify ragas with a commendable accuracy on our test
dataset.
</summary>
    <author>
      <name>Sanchit Alekh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Seminar on Computer Music, RWTH Aachen,
  http://hpac.rwth-aachen.de/teaching/sem-mus-17/Reports/Alekh.pdf</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.02322v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.02322v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0005022v2</id>
    <updated>2000-06-12T06:21:40Z</updated>
    <published>2000-05-17T14:30:01Z</published>
    <title>Fractionally-addressed delay lines</title>
    <summary>  While traditional implementations of variable-length digital delay lines are
based on a circular buffer accessed by two pointers, we propose an
implementation where a single fractional pointer is used both for read and
write operations. On modern general-purpose architectures, the proposed method
is nearly as efficient as the popularinterpolated circular buffer, and it
behaves well for delay-length modulations commonly found in digital audio
effects. The physical interpretation of the new implementation shows that it is
suitable for simulating tension or density modulations in wave-propagating
media.
</summary>
    <author>
      <name>Davide Rocchesso</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/89.876310</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/89.876310" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 19 figures, to be published in IEEE Transactions on Speech
  and Audio Processing Corrected ACM-class</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Speech and Audio Processing, vol. 8, no. 6,
  november 2000, pp. 717-727</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0005022v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0005022v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0303025v1</id>
    <updated>2003-03-24T16:01:46Z</updated>
    <published>2003-03-24T16:01:46Z</published>
    <title>Algorithmic Clustering of Music</title>
    <summary>  We present a fully automatic method for music classification, based only on
compression of strings that represent the music pieces. The method uses no
background knowledge about music whatsoever: it is completely general and can,
without change, be used in different areas like linguistic classification and
genomics. It is based on an ideal theory of the information content in
individual objects (Kolmogorov complexity), information distance, and a
universal similarity metric. Experiments show that the method distinguishes
reasonably well between various musical genres and can even cluster pieces by
composer.
</summary>
    <author>
      <name>Rudi Cilibrasi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CWI</arxiv:affiliation>
    </author>
    <author>
      <name>Paul Vitanyi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CWI and University of Amsterdam</arxiv:affiliation>
    </author>
    <author>
      <name>Ronald de Wolf</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CWI</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0303025v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0303025v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.4, H.3.1, I.5.3, F.1.3, J.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0612139v1</id>
    <updated>2006-12-28T06:45:43Z</updated>
    <published>2006-12-28T06:45:43Z</published>
    <title>Alignment of Speech to Highly Imperfect Text Transcriptions</title>
    <summary>  We introduce a novel and inexpensive approach for the temporal alignment of
speech to highly imperfect transcripts from automatic speech recognition (ASR).
Transcripts are generated for extended lecture and presentation videos, which
in some cases feature more than 30 speakers with different accents, resulting
in highly varying transcription qualities. In our approach we detect a subset
of phonemes in the speech track, and align them to the sequence of phonemes
extracted from the transcript. We report on the results for 4 speech-transcript
sets ranging from 22 to 108 minutes. The alignment performance is promising,
showing a correct matching of phonemes within 10, 20, 30 second error margins
for more than 60%, 75%, 90% of text, respectively, on average.
</summary>
    <author>
      <name>Alexander Haubold</name>
    </author>
    <author>
      <name>John R. Kender</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0612139v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0612139v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.1; H.5.1; H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0701177v4</id>
    <updated>2008-08-17T19:17:05Z</updated>
    <published>2007-01-26T20:07:43Z</published>
    <title>Pitch Tracking of Acoustic Signals based on Average Squared Mean
  Difference Function</title>
    <summary>  In this paper, a method of pitch tracking based on variance minimization of
locally periodic subsamples of an acoustic signal is presented. Replicates
along the length of the periodically sampled data of the signal vector are
taken and locally averaged sample variances are minimized to estimate the
fundamental frequency. Using this method, pitch tracking of any text
independent voiced signal is possible for different speakers.
</summary>
    <author>
      <name>Roudra Chakraborty</name>
    </author>
    <author>
      <name>Debapriya Sengupta</name>
    </author>
    <author>
      <name>Sagnik Sinha</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0701177v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0701177v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0901.2416v1</id>
    <updated>2009-01-16T08:24:14Z</updated>
    <published>2009-01-16T08:24:14Z</published>
    <title>TR01: Time-continuous Sparse Imputation</title>
    <summary>  An effective way to increase the noise robustness of automatic speech
recognition is to label noisy speech features as either reliable or unreliable
(missing) prior to decoding, and to replace the missing ones by clean speech
estimates. We present a novel method to obtain such clean speech estimates.
Unlike previous imputation frameworks which work on a frame-by-frame basis, our
method focuses on exploiting information from a large time-context. Using a
sliding window approach, denoised speech representations are constructed using
a sparse representation of the reliable features in an overcomplete basis of
fixed-length exemplar fragments. We demonstrate the potential of our approach
with experiments on the AURORA-2 connected digit database.
</summary>
    <author>
      <name>J. F. Gemmeke</name>
    </author>
    <author>
      <name>B. Cranen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 5 figures, Technical Report</arxiv:comment>
    <link href="http://arxiv.org/abs/0901.2416v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0901.2416v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0901.3902v1</id>
    <updated>2009-01-25T14:57:55Z</updated>
    <published>2009-01-25T14:57:55Z</published>
    <title>iKlax: a New Musical Audio Format for Active Listening</title>
    <summary>  In this paper, we are presenting a new model for interactive music. Unlike
most interactive systems, our model is based on file organization, but does not
require digital audio treatments. This model includes a definition of a
constraints system and its solver. The products of this project are intended
for the general public, inexperienced users, as well as professional musicians,
and will be distributed commercially. We are here presenting three products of
this project. The difficulty of this project is to design a technology and
software products for interactive music which must be easy to use by the
general public and by professional composers.
</summary>
    <author>
      <name>Fabien Gallot</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LaBRI</arxiv:affiliation>
    </author>
    <author>
      <name>Owen Lagadec</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LaBRI</arxiv:affiliation>
    </author>
    <author>
      <name>Myriam Desainte-Catherine</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LaBRI</arxiv:affiliation>
    </author>
    <author>
      <name>Sylvain Marchand</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LaBRI</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Computer Music Conference (ICMC), Belfast : Irlande
  (2008)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0901.3902v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0901.3902v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.3976v2</id>
    <updated>2011-09-10T12:02:21Z</updated>
    <published>2009-09-22T12:24:19Z</published>
    <title>The Information Theory of Emotions of Musical Chords</title>
    <summary>  The paper offers a solution to the centuries-old puzzle - why the major
chords are perceived as happy and the minor chords as sad - based on the
information theory of emotions. A theory and a formula of musical emotions were
created. They define the sign and the amplitude of the utilitarian emotional
coloration of separate major and minor chords through relative pitches of
constituent sounds. Keywords: chord, major, minor, the formula of musical
emotions, the information theory of emotions.
</summary>
    <author>
      <name>Vadim R. Madgazin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 2 figures, in English, and copy in Russian</arxiv:comment>
    <link href="http://arxiv.org/abs/0909.3976v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.3976v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0911.4642v1</id>
    <updated>2009-11-24T15:07:37Z</updated>
    <published>2009-11-24T15:07:37Z</published>
    <title>G3 : GENESIS software envrionment update</title>
    <summary>  GENESIS3 is the new version of the GENESIS software environment for musical
creation by means of mass-interaction physics network modeling. It was
designed, and developed from scratch, in hindsight of more than 10 years
working on and using the previous version. We take the opportunity of this
birth to provide in this article (1) an analysis of the peculiarities in
GENESIS, aiming at highlighting its core ?software paradigm?; and (2) an update
on the features of the new version as compared to the last.
</summary>
    <author>
      <name>Nicolas Castagné</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ACROE</arxiv:affiliation>
    </author>
    <author>
      <name>Claude Cadoz</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ACROE, ICA</arxiv:affiliation>
    </author>
    <author>
      <name>Ali Allaoui</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ACROE, ICA</arxiv:affiliation>
    </author>
    <author>
      <name>Olivier Michel Tache</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ACROE</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Computer Music Conference (ICMC), Montr\'eal :
  Canada (2009)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0911.4642v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.4642v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.0745v1</id>
    <updated>2009-12-04T20:27:14Z</updated>
    <published>2009-12-04T20:27:14Z</published>
    <title>A Digital Guitar Tuner</title>
    <summary>  The objective of this paper is to understand the critical parameters that
need to be addressed while designing a guitar tuner. The focus of the design
lies in developing a suitable algorithm to accurately detect the fundamental
frequency of a plucked guitar string from its frequency spectrum. A
userfriendly graphical interface is developed using Matlab to allow any user to
easily tune his guitar using the developed program.
</summary>
    <author>
      <name>Mary Lourde R.</name>
    </author>
    <author>
      <name>Anjali Kuppayil Saji</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages IEEE format, International Journal of Computer Science and
  Information Security, IJCSIS November 2009, ISSN 1947 5500,
  http://sites.google.com/site/ijcsis/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science and Information
  Security, IJCSIS, Vol. 6, No. 2, pp. 082-088, November 2009, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0912.0745v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.0745v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.4190v1</id>
    <updated>2010-01-23T19:04:50Z</updated>
    <published>2010-01-23T19:04:50Z</published>
    <title>Speech Recognition of the letter 'zha' in Tamil Language using HMM</title>
    <summary>  Speech signals of the letter 'zha' in Tamil language of 3 males and 3 females
were coded using an improved version of Linear Predictive Coding (LPC). The
sampling frequency was at 16 kHz and the bit rate was at 15450 bits per second,
where the original bit rate was at 128000 bits per second with the help of wave
surfer audio tool. The output LPC cepstrum is implemented in first order three
state Hidden Markov Model(HMM) chain.
</summary>
    <author>
      <name>A. Srinivasan</name>
    </author>
    <author>
      <name>K. Srinivasa Rao</name>
    </author>
    <author>
      <name>K. Kannan</name>
    </author>
    <author>
      <name>D. Narasimhan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 Pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJEST Volume 1 Issue 2 2009 67-72</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1001.4190v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.4190v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.2465v2</id>
    <updated>2010-07-05T12:15:44Z</updated>
    <published>2010-05-14T07:26:49Z</published>
    <title>Dichotic harmony for the musical practice</title>
    <summary>  The dichotic method of hearing sound adapts in the region of musical harmony.
The algorithm of the separation of the being dissonant voices into several
separate groups is proposed. For an increase in the pleasantness of chords the
different groups of voices are heard out through the different channels of
headphones. Is created two demonstration program for PC. Keywords: music,
harmony, chord, dichotic listening, dissonance, consonance, headphones,
pleasantness, midi.
</summary>
    <author>
      <name>Vadim R. Madgazin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, in Russian, changed content</arxiv:comment>
    <link href="http://arxiv.org/abs/1005.2465v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.2465v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.2796v1</id>
    <updated>2010-09-14T21:36:25Z</updated>
    <published>2010-09-14T21:36:25Z</published>
    <title>Estimation of Infants' Cry Fundamental Frequency using a Modified SIFT
  algorithm</title>
    <summary>  This paper addresses the problem of infants' cry fundamental frequency
estimation. The fundamental frequency is estimated using a modified simple
inverse filtering tracking (SIFT) algorithm. The performance of the modified
SIFT is studied using a real database of infants' cry. It is shown that the
algorithm is capable of overcoming the problem of under-estimation and
over-estimation of the cry fundamental frequency, with an estimation accuracy
of 6.15% and 3.75%, for hyperphonated and phonated cry segments, respectively.
Some typical examples of the fundamental frequency contour in typical cases of
pathological and healthy cry signals are presented and discussed.
</summary>
    <author>
      <name>Dror Lederman</name>
    </author>
    <link href="http://arxiv.org/abs/1009.2796v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.2796v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.4719v1</id>
    <updated>2010-09-23T20:38:06Z</updated>
    <published>2010-09-23T20:38:06Z</published>
    <title>A Fast Audio Clustering Using Vector Quantization and Second Order
  Statistics</title>
    <summary>  This paper describes an effective unsupervised speaker indexing approach. We
suggest a two stage algorithm to speed-up the state-of-the-art algorithm based
on the Bayesian Information Criterion (BIC). In the first stage of the merging
process a computationally cheap method based on the vector quantization (VQ) is
used. Then in the second stage a more computational expensive technique based
on the BIC is applied. In the speaker indexing task a turning parameter or a
threshold is used. We suggest an on-line procedure to define the value of a
turning parameter without using development data. The results are evaluated
using 10 hours of audio data.
</summary>
    <author>
      <name>Konstantin Biatov</name>
    </author>
    <link href="http://arxiv.org/abs/1009.4719v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.4719v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.2797v1</id>
    <updated>2010-12-13T16:44:46Z</updated>
    <published>2010-12-13T16:44:46Z</published>
    <title>Should Corpora be Big, Rich, or Dense?</title>
    <summary>  In this paper, we ask what properties makes a large corpus more or less
useful. We suggest that size, by itself, should not be the ultimate goal of
building a corpus. Large-scale corpora are considered desirable because they
offer statistical stability and rich variation. But this rich variation means
more factors to control and evaluate, which can limit the advantages of size.
We discuss the use of multi-channel data to complement large-scale speech
corpora. Even though multi-channel data may limit the scale of a corpus (due to
the complex and labor-intensive nature of data collection) they can offer
information that allows us to tease apart various factors related to speech
production.
</summary>
    <author>
      <name>Greg P. Kochanski</name>
    </author>
    <author>
      <name>Chilin Shih</name>
    </author>
    <author>
      <name>Ryan Shosted</name>
    </author>
    <link href="http://arxiv.org/abs/1012.2797v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.2797v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.4118v1</id>
    <updated>2011-03-21T19:32:00Z</updated>
    <published>2011-03-21T19:32:00Z</published>
    <title>Sampling-rate-aware noise generation</title>
    <summary>  In this paper we consider the generation of discrete white noise. Despite
this seems to be a simple problem, common noise generator implementations do
not deliver comparable results at different sampling rates. First we define
what we mean with "comparable results". From this we conclude, that the
variance of the random variables shall grow proportionally to the sampling
rate. Eventually we consider how noise behaves under common signal
transformations, such as frequency filters, quantisation and impulse generation
and we explore how these signal transformations must be designed in order
generate sampling-rate-aware results when applied to white noise.
</summary>
    <author>
      <name>Henning Thielemann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1103.4118v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.4118v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.1383v1</id>
    <updated>2011-05-06T20:25:36Z</updated>
    <published>2011-05-06T20:25:36Z</published>
    <title>Topological Considerations for Tuning and Fingering Stringed Instruments</title>
    <summary>  We present a formal language for assigning pitches to strings for fingered
multi-string instruments, particularly the six-string guitar. Given the
instrument's tuning (the strings' open pitches) and the compass of the fingers
of the hand stopping the strings, the formalism yields a framework for
simultaneously optimizing three things: the mapping of pitches to strings, the
choice of instrument tuning, and the key of the composition. Final optimization
relies on heuristics idiomatic to the tuning, the particular musical style, and
the performer's proficiency.
</summary>
    <author>
      <name>Terry Allen</name>
    </author>
    <author>
      <name>Camille Goudeseune</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1105.1383v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.1383v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="14P10" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.4.0; H.5.5; G.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1107.4969v1</id>
    <updated>2011-07-25T15:09:37Z</updated>
    <published>2011-07-25T15:09:37Z</published>
    <title>An end-to-end machine learning system for harmonic analysis of music</title>
    <summary>  We present a new system for simultaneous estimation of keys, chords, and bass
notes from music audio. It makes use of a novel chromagram representation of
audio that takes perception of loudness into account. Furthermore, it is fully
based on machine learning (instead of expert knowledge), such that it is
potentially applicable to a wider range of genres as long as training data is
available. As compared to other models, the proposed system is fast and memory
efficient, while achieving state-of-the-art performance.
</summary>
    <author>
      <name>Yizhao Ni</name>
    </author>
    <author>
      <name>Matt Mcvicar</name>
    </author>
    <author>
      <name>Raul Santos-Rodriguez</name>
    </author>
    <author>
      <name>Tijl De Bie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">MIREX report and preparation of Journal submission</arxiv:comment>
    <link href="http://arxiv.org/abs/1107.4969v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.4969v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.3236v1</id>
    <updated>2012-04-15T04:49:08Z</updated>
    <published>2012-04-15T04:49:08Z</published>
    <title>Using Mimicry to Learn about Mental Representations</title>
    <summary>  Phonology typically describes speech in terms of discrete signs like
features. The field of intonational phonology uses discrete accents to describe
intonation and prosody. But, are such representations useful? The results of
mimicry experiments indicate that discrete signs are not a useful
representation of the shape of intonation contours. Human behaviour seems to be
better represented by a attractors where memory retains substantial fine detail
about an utterance. There is no evidence that discrete abstract representations
that might be formed that have an effect on the speech that is subsequently
produced. This paper also discusses conditions under which a discrete phonology
can arise from an attractor model and why - for intonation - attractors can be
inferred without the implying a discrete phonology.
</summary>
    <author>
      <name>Greg Kochanski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">36 pages, plus extra figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.3236v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.3236v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.5827v1</id>
    <updated>2012-07-24T21:08:35Z</updated>
    <published>2012-07-24T21:08:35Z</published>
    <title>Algorithm to suppress scanner noise in recorded speech during functional
  magnetic resonance imaging</title>
    <summary>  The high-intensity, repetitive noise associated with functional magnetic
resonance imaging hinders on-line monitoring of subjects' speech and/or
recording speech signals suitable for off-line analysis. The proposed algorithm
enhances the speech signal by suppressing the scanner noise in the signal
recorded by a single-channel microphone. Significant increases in
signal-to-noise ratio are achieved using an adaptive filter that combines time
and frequency domain elements. In addition to providing a recording suitable
for speech analysis, such a real-time system provides an alternative means (to,
e.g., the "panic ball") for communication between the patient and the operator
during image acquisition.
</summary>
    <author>
      <name>Satrajit S. Ghosh</name>
    </author>
    <link href="http://arxiv.org/abs/1207.5827v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.5827v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.3119v1</id>
    <updated>2012-12-13T10:39:26Z</updated>
    <published>2012-12-13T10:39:26Z</published>
    <title>A nuclear-norm based convex formulation for informed source separation</title>
    <summary>  We study the problem of separating audio sources from a single linear
mixture. The goal is to find a decomposition of the single channel spectrogram
into a sum of individual contributions associated to a certain number of
sources. In this paper, we consider an informed source separation problem in
which the input spectrogram is partly annotated. We propose a convex
formulation that relies on a nuclear norm penalty to induce low rank for the
contributions. We show experimentally that solving this model with a simple
subgradient method outperforms a previously introduced nonnegative matrix
factorization (NMF) technique, both in terms of source separation quality and
computation time.
</summary>
    <author>
      <name>Augustin Lefèvre</name>
    </author>
    <author>
      <name>François Glineur</name>
    </author>
    <author>
      <name>P. -A. Absil</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ESANN 2013 conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.3119v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.3119v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.0278v1</id>
    <updated>2013-01-02T17:50:00Z</updated>
    <published>2013-01-02T17:50:00Z</published>
    <title>Evaluation of a Multi-Resolution Dyadic Wavelet Transform Method for
  usable Speech Detection</title>
    <summary>  Many applications of speech communication and speaker identification suffer
from the problem of co-channel speech. This paper deals with a multi-resolution
dyadic wavelet transform method for usable segments of co-channel speech
detection that could be processed by a speaker identification system.
Evaluation of this method is performed on TIMIT database referring to the
Target to Interferer Ratio measure. Co-channel speech is constructed by mixing
all possible gender speakers. Results do not show much difference for different
mixtures. For the overall mixtures 95.76% of usable speech is correctly
detected with false alarms of 29.65%.
</summary>
    <author>
      <name>Wajdi Ghezaiel</name>
    </author>
    <author>
      <name>Amel Ben Slimane Rahmouni</name>
    </author>
    <author>
      <name>Ezzedine Ben Braiek</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">WASET Journal, 2011 Vol.79 P. 829-833</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1301.0278v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.0278v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.0136v1</id>
    <updated>2013-02-01T10:51:20Z</updated>
    <published>2013-02-01T10:51:20Z</published>
    <title>Maximum a posteriori estimation of piecewise arcs in tempo time-series</title>
    <summary>  In musical performances with expressive tempo modulation, the tempo variation
can be modelled as a sequence of tempo arcs. Previous authors have used this
idea to estimate series of piecewise arc segments from data. In this paper we
describe a probabilistic model for a time-series process of this nature, and
use this to perform inference of single- and multi-level arc processes from
data. We describe an efficient Viterbi-like process for MAP inference of arcs.
Our approach is score-agnostic, and together with efficient inference allows
for online analysis of performances including improvisations, and can predict
immediate future tempo trajectories.
</summary>
    <author>
      <name>Dan Stowell</name>
    </author>
    <author>
      <name>Elaine Chew</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to postprint volume for Computer Music Modeling and
  Retrieval (CMMR) 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1302.0136v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.0136v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.3462v2</id>
    <updated>2013-02-15T21:18:13Z</updated>
    <published>2013-02-14T16:44:10Z</published>
    <title>Improved multiple birdsong tracking with distribution derivative method
  and Markov renewal process clustering</title>
    <summary>  Segregating an audio mixture containing multiple simultaneous bird sounds is
a challenging task. However, birdsong often contains rapid pitch modulations,
and these modulations carry information which may be of use in automatic
recognition. In this paper we demonstrate that an improved spectrogram
representation, based on the distribution derivative method, leads to improved
performance of a segregation algorithm which uses a Markov renewal process
model to track vocalisation patterns consisting of singing and silences.
</summary>
    <author>
      <name>Dan Stowell</name>
    </author>
    <author>
      <name>Sašo Muševič</name>
    </author>
    <author>
      <name>Jordi Bonada</name>
    </author>
    <author>
      <name>Mark D. Plumbley</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICASSP.2013.6637691</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICASSP.2013.6637691" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ICASSP 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1302.3462v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.3462v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.7070v1</id>
    <updated>2013-02-28T03:43:08Z</updated>
    <published>2013-02-28T03:43:08Z</published>
    <title>Sound localization using compressive sensing</title>
    <summary>  In a sensor network with remote sensor devices, it is important to have a
method that can accurately localize a sound event with a small amount of data
transmitted from the sensors. In this paper, we propose a novel method for
localization of a sound source using compressive sensing. Instead of sampling a
large amount of data at the Nyquist sampling rate in time domain, the acoustic
sensors take compressive measurements integrated in time. The compressive
measurements can be used to accurately compute the location of a sound source.
</summary>
    <author>
      <name>Hong Jiang</name>
    </author>
    <author>
      <name>Boyd Mathews</name>
    </author>
    <author>
      <name>Paul Wilford</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 7 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. SENSORNETS, 2012, pp.159-166</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1302.7070v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.7070v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.6763v2</id>
    <updated>2014-01-10T20:35:56Z</updated>
    <published>2013-04-24T21:50:03Z</published>
    <title>Deep Scattering Spectrum</title>
    <summary>  A scattering transform defines a locally translation invariant representation
which is stable to time-warping deformations. It extends MFCC representations
by computing modulation spectrum coefficients of multiple orders, through
cascades of wavelet convolutions and modulus operators. Second-order scattering
coefficients characterize transient phenomena such as attacks and amplitude
modulation. A frequency transposition invariant representation is obtained by
applying a scattering transform along log-frequency. State-the-of-art
classification results are obtained for musical genre and phone classification
on GTZAN and TIMIT databases, respectively.
</summary>
    <author>
      <name>Joakim Andén</name>
    </author>
    <author>
      <name>Stéphane Mallat</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TSP.2014.2326991</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TSP.2014.2326991" rel="related"/>
    <link href="http://arxiv.org/abs/1304.6763v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.6763v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.2959v1</id>
    <updated>2013-05-09T08:47:47Z</updated>
    <published>2013-05-09T08:47:47Z</published>
    <title>Automatic Speech Recognition Using Template Model for Man-Machine
  Interface</title>
    <summary>  Speech is a natural form of communication for human beings, and computers
with the ability to understand speech and speak with a human voice are expected
to contribute to the development of more natural man-machine interfaces.
Computers with this kind of ability are gradually becoming a reality, through
the evolution of speech recognition technologies. Speech is being an important
mode of interaction with computers. In this paper Feature extraction is
implemented using well-known Mel-Frequency Cepstral Coefficients (MFCC).Pattern
matching is done using Dynamic time warping (DTW) algorithm.
</summary>
    <author>
      <name>Neema Mishra</name>
    </author>
    <author>
      <name>Urmila Shrawankar</name>
    </author>
    <author>
      <name>V M Thakare</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Pages: 05 Figures : 01 Tables : 03 Proceedings of the International
  Conference ICAET 2010, Chennai, India. arXiv admin note: text overlap with
  arXiv:1305.2847</arxiv:comment>
    <link href="http://arxiv.org/abs/1305.2959v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.2959v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.5275v2</id>
    <updated>2013-10-01T21:29:13Z</updated>
    <published>2013-09-20T14:12:04Z</published>
    <title>An open dataset for research on audio field recording archives:
  freefield1010</title>
    <summary>  We introduce a free and open dataset of 7690 audio clips sampled from the
field-recording tag in the Freesound audio archive. The dataset is designed for
use in research related to data mining in audio archives of field recordings /
soundscapes. Audio is standardised, and audio and metadata are Creative Commons
licensed. We describe the data preparation process, characterise the dataset
descriptively, and illustrate its use through an auto-tagging experiment.
</summary>
    <author>
      <name>Dan Stowell</name>
    </author>
    <author>
      <name>Mark D. Plumbley</name>
    </author>
    <link href="http://arxiv.org/abs/1309.5275v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.5275v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.6047v1</id>
    <updated>2013-09-24T05:14:53Z</updated>
    <published>2013-09-24T05:14:53Z</published>
    <title>Non-negative Matrix Factorization with Linear Constraints for
  Single-Channel Speech Enhancement</title>
    <summary>  This paper investigates a non-negative matrix factorization (NMF)-based
approach to the semi-supervised single-channel speech enhancement problem where
only non-stationary additive noise signals are given. The proposed method
relies on sinusoidal model of speech production which is integrated inside NMF
framework using linear constraints on dictionary atoms. This method is further
developed to regularize harmonic amplitudes. Simple multiplicative algorithms
are presented. The experimental evaluation was made on TIMIT corpus mixed with
various types of noise. It has been shown that the proposed method outperforms
some of the state-of-the-art noise suppression techniques in terms of
signal-to-noise ratio.
</summary>
    <author>
      <name>Nikolay Lyubimov</name>
    </author>
    <author>
      <name>Mikhail Kotov</name>
    </author>
    <link href="http://arxiv.org/abs/1309.6047v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.6047v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.4014v1</id>
    <updated>2013-12-14T07:55:23Z</updated>
    <published>2013-12-14T07:55:23Z</published>
    <title>A Simple Method to Produce Algorithmic MIDI Music based on Randomness,
  Simple Probabilities and Multi-Threading</title>
    <summary>  This paper introduces a simple method for producing multichannel MIDI music
that is based on randomness and simple probabilities. One distinctive feature
of the method is that it produces and sends in parallel to the sound card more
than one unsynchronized channels by exploiting the multi-threading capabilities
of general purpose programming languages. As consequence the derived sound
offers a quite ``full" and ``unpredictable" acoustic experience to the
listener. Subsequently the paper reports the results of an evaluation with
users. The results were very surprising: the majority of users responded that
they could tolerate this music in various occasions.
</summary>
    <author>
      <name>Yannis Tzitzikas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.4014v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.4014v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.3689v1</id>
    <updated>2014-02-15T13:27:01Z</updated>
    <published>2014-02-15T13:27:01Z</published>
    <title>Sound Representation and Classification Benchmark for Domestic Robots</title>
    <summary>  We address the problem of sound representation and classification and present
results of a comparative study in the context of a domestic robotic scenario. A
dataset of sounds was recorded in realistic conditions (background noise,
presence of several sound sources, reverberations, etc.) using the humanoid
robot NAO. An extended benchmark is carried out to test a variety of
representations combined with several classifiers. We provide results obtained
with the annotated dataset and we assess the methods quantitatively on the
basis of their classification scores, computation times and memory
requirements. The annotated dataset is publicly available at
https://team.inria.fr/perception/nard/.
</summary>
    <author>
      <name>Maxime Janvier</name>
    </author>
    <author>
      <name>Xavier Alameda-Pineda</name>
    </author>
    <author>
      <name>Laurent Girin</name>
    </author>
    <author>
      <name>Radu Horaud</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1402.3689v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.3689v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.1501v1</id>
    <updated>2014-03-06T17:29:39Z</updated>
    <published>2014-03-06T17:29:39Z</published>
    <title>Sparse DOA Estimation of Wideband Sound Sources Using Circular Harmonics</title>
    <summary>  Sparse signal models are in the focus of recent developments in narrowband
DOA estimation. Applying these methods to localizing audio sources, however, is
challenging due to the wideband nature of the signals. The common approach of
processing all frequency bands separately and fusing the results is costly and
can introduce errors in the solution. We show how these problems can be
overcome by decomposing the wavefield of a circular microphone array and using
circular harmonic coefficients instead of time-frequency data for sparse DOA
estimation. As a result, we present the super-resolution localization method
WASCHL (Wideband Audio Sparse Circular Harmonics Localizer) that is inherently
frequency-coherent and highly efficient from a computational point of view.
</summary>
    <author>
      <name>Clemens Hage</name>
    </author>
    <author>
      <name>Tim Habigt</name>
    </author>
    <author>
      <name>Martin Kleinsteuber</name>
    </author>
    <link href="http://arxiv.org/abs/1403.1501v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.1501v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.2180v2</id>
    <updated>2014-12-18T15:59:17Z</updated>
    <published>2014-03-10T09:18:54Z</published>
    <title>Optimal Window and Lattice in Gabor Transform Application to Audio
  Analysis</title>
    <summary>  This article deals with the use of optimal lattice and optimal window in
Discrete Gabor Transform computation. In the case of a generalized Gaussian
window, extending earlier contributions, we introduce an additional local
window adaptation technique for non-stationary signals. We illustrate our
approach and the earlier one by addressing three time-frequency analysis
problems to show the improvements achieved by the use of optimal lattice and
window: close frequencies distinction, frequency estimation and SNR estimation.
The results are presented, when possible, with real world audio signals.
</summary>
    <author>
      <name>Helene Lachambre</name>
    </author>
    <author>
      <name>Benjamin Ricaud</name>
    </author>
    <author>
      <name>Guillaume Stempfel</name>
    </author>
    <author>
      <name>Bruno Torresani</name>
    </author>
    <author>
      <name>Christoph Wiesmeyr</name>
    </author>
    <author>
      <name>Darian M. Onchis</name>
    </author>
    <link href="http://arxiv.org/abs/1403.2180v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.2180v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.6881v1</id>
    <updated>2014-04-28T07:17:18Z</updated>
    <published>2014-04-28T07:17:18Z</published>
    <title>Improving Blind Source Separation Performance By Adaptive Array
  Geometries For Humanoid Robots</title>
    <summary>  In this paper, the concept of an adaptation algorithm is proposed, which can
be used to blindly adapt the microphone array geometry of a humanoid robot such
that the performance of the underlying signal separation algorithm is improved.
As a decisive feature, an online performance measure for blind source
separation is introduced which allows a robust and reliable estimation of the
instantaneous separation performance based on currently observable data.
Experimental results from a simulated environment confirm the efficacy of the
concept.
</summary>
    <author>
      <name>Hendrik Barfuss</name>
    </author>
    <author>
      <name>Walter Kellermann</name>
    </author>
    <link href="http://arxiv.org/abs/1404.6881v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.6881v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.6945v1</id>
    <updated>2014-05-25T22:22:04Z</updated>
    <published>2014-05-25T22:22:04Z</published>
    <title>Sparsity-Aware Filtered-X Affine Projection Algorithms for Active Noise
  Control</title>
    <summary>  This paper describes a novel technique for promoting sparsity in the modified
filtered-x algorithms required for active noise control. The proposed
algorithms are based on recent techniques incorporating approximations to the
\ell_0-norm in the cost functions that are used to derive adaptive filtering
algorithms. In particular, zero-attracting and reweighted zero-attracting
filtered-x adaptive algorithms are developed and considered for active noise
control problems. The results of simulations indicate that the proposed
techniques improve the convergence of the existing modified algorithm in the
case where the primary and secondary paths exhibit a degree of sparsity.
</summary>
    <author>
      <name>A. Gully</name>
    </author>
    <author>
      <name>R. C. de Lamare</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 figures, 5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.6945v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.6945v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.3915v1</id>
    <updated>2014-06-16T06:41:54Z</updated>
    <published>2014-06-16T06:41:54Z</published>
    <title>A Bengali HMM Based Speech Synthesis System</title>
    <summary>  The paper presents the capability of an HMM-based TTS system to produce
Bengali speech. In this synthesis method, trajectories of speech parameters are
generated from the trained Hidden Markov Models. A final speech waveform is
synthesized from those speech parameters. In our experiments, spectral
properties were represented by Mel Cepstrum Coefficients. Both the training and
synthesis issues are investigated in this paper using annotated Bengali speech
database. Experimental evaluation depicts that the developed text-to-speech
system is capable of producing adequately natural speech in terms of
intelligibility and intonation for Bengali.
</summary>
    <author>
      <name>Sankar Mukherjee</name>
    </author>
    <author>
      <name>Shyamal Kumar Das Mandal</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Oriental COCOSDA 2012, pp.225 259</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1406.3915v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.3915v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.2430v1</id>
    <updated>2014-10-09T11:46:02Z</updated>
    <published>2014-10-09T11:46:02Z</published>
    <title>Phase-Optimized K-SVD for Signal Extraction from Underdetermined
  Multichannel Sparse Mixtures</title>
    <summary>  We propose a novel sparse representation for heavily underdetermined
multichannel sound mixtures, i.e., with much more sources than microphones. The
proposed approach operates in the complex Fourier domain, thus preserving
spatial characteristics carried by phase differences. We derive a
generalization of K-SVD which jointly estimates a dictionary capturing both
spectral and spatial features, a sparse activation matrix, and all
instantaneous source phases from a set of signal examples. The dictionary can
then be used to extract the learned signal from a new input mixture. The method
is applied to the challenging problem of ego-noise reduction for robot
audition. We demonstrate its superiority relative to conventional
dictionary-based techniques using recordings made in a real room.
</summary>
    <author>
      <name>Antoine Deleforge</name>
    </author>
    <author>
      <name>Walter Kellermann</name>
    </author>
    <link href="http://arxiv.org/abs/1410.2430v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.2430v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.6645v3</id>
    <updated>2015-04-20T12:35:32Z</updated>
    <published>2014-12-20T11:54:41Z</published>
    <title>Weakly Supervised Multi-Embeddings Learning of Acoustic Models</title>
    <summary>  We trained a Siamese network with multi-task same/different information on a
speech dataset, and found that it was possible to share a network for both
tasks without a loss in performance. The first task was to discriminate between
two same or different words, and the second was to discriminate between two
same or different talkers.
</summary>
    <author>
      <name>Gabriel Synnaeve</name>
    </author>
    <author>
      <name>Emmanuel Dupoux</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.6645v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.6645v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.2.7; I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.07866v1</id>
    <updated>2015-01-28T21:58:51Z</updated>
    <published>2015-01-28T21:58:51Z</published>
    <title>A Comparison of Classifiers in Performing Speaker Accent Recognition
  Using MFCCs</title>
    <summary>  An algorithm involving Mel-Frequency Cepstral Coefficients (MFCCs) is
provided to perform signal feature extraction for the task of speaker accent
recognition. Then different classifiers are compared based on the MFCC feature.
For each signal, the mean vector of MFCC matrix is used as an input vector for
pattern recognition. A sample of 330 signals, containing 165 US voice and 165
non-US voice, is analyzed. By comparison, k-nearest neighbors yield the highest
average test accuracy, after using a cross-validation of size 500, and least
time being used in the computation
</summary>
    <author>
      <name>Zichen Ma</name>
    </author>
    <author>
      <name>Ernest Fokoue</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 7 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Open Journal of Statistics, 2014, 4, 258-266</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1501.07866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.07866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62H25, 62H30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.03387v1</id>
    <updated>2015-02-11T17:30:48Z</updated>
    <published>2015-02-11T17:30:48Z</published>
    <title>A Full Frequency Masking Vocoder for Legal Eavesdropping Conversation
  Recording</title>
    <summary>  This paper presents a new approach for a vocoder design based on full
frequency masking by octaves in addition to a technique for spectral filling
via beta probability distribution. Some psycho-acoustic characteristics of
human hearing - inaudibility masking in frequency and phase - are used as a
basis for the proposed algorithm. The results confirm that this technique may
be useful to save bandwidth in applications requiring intelligibility. It is
recommended for the legal eavesdropping of long voice conversations.
</summary>
    <author>
      <name>R. F. B. Sotero Filho</name>
    </author>
    <author>
      <name>H. M. de Oliveira</name>
    </author>
    <author>
      <name>R. M. Campello de Souza</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5540/03.2015.003.01.0468</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5540/03.2015.003.01.0468" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures, 3 tables, XXXV Cong. Nac. de Matematica Aplicada
  e Computacional, Natal, RN, Brazil 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.03387v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.03387v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.05849v1</id>
    <updated>2015-03-19T17:24:16Z</updated>
    <published>2015-03-19T17:24:16Z</published>
    <title>Deep Transform: Time-Domain Audio Error Correction via Probabilistic
  Re-Synthesis</title>
    <summary>  In the process of recording, storage and transmission of time-domain audio
signals, errors may be introduced that are difficult to correct in an
unsupervised way. Here, we train a convolutional deep neural network to
re-synthesize input time-domain speech signals at its output layer. We then use
this abstract transformation, which we call a deep transform (DT), to perform
probabilistic re-synthesis on further speech (of the same speaker) which has
been degraded. Using the convolutive DT, we demonstrate the recovery of speech
audio that has been subject to extreme degradation. This approach may be useful
for correction of errors in communications devices.
</summary>
    <author>
      <name>Andrew J. R. Simpson</name>
    </author>
    <link href="http://arxiv.org/abs/1503.05849v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.05849v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.06046v1</id>
    <updated>2015-03-20T12:00:44Z</updated>
    <published>2015-03-20T12:00:44Z</published>
    <title>Deep Transform: Cocktail Party Source Separation via Probabilistic
  Re-Synthesis</title>
    <summary>  In cocktail party listening scenarios, the human brain is able to separate
competing speech signals. However, the signal processing implemented by the
brain to perform cocktail party listening is not well understood. Here, we
trained two separate convolutive autoencoder deep neural networks (DNN) to
separate monaural and binaural mixtures of two concurrent speech streams. We
then used these DNNs as convolutive deep transform (CDT) devices to perform
probabilistic re-synthesis. The CDTs operated directly in the time-domain. Our
simulations demonstrate that very simple neural networks are capable of
exploiting monaural and binaural information available in a cocktail party
listening scenario.
</summary>
    <author>
      <name>Andrew J. R. Simpson</name>
    </author>
    <link href="http://arxiv.org/abs/1503.06046v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.06046v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.06962v1</id>
    <updated>2015-03-24T09:34:51Z</updated>
    <published>2015-03-24T09:34:51Z</published>
    <title>Probabilistic Binary-Mask Cocktail-Party Source Separation in a
  Convolutional Deep Neural Network</title>
    <summary>  Separation of competing speech is a key challenge in signal processing and a
feat routinely performed by the human auditory brain. A long standing benchmark
of the spectrogram approach to source separation is known as the ideal binary
mask. Here, we train a convolutional deep neural network, on a two-speaker
cocktail party problem, to make probabilistic predictions about binary masks.
Our results approach ideal binary mask performance, illustrating that
relatively simple deep neural networks are capable of robust binary mask
prediction. We also illustrate the trade-off between prediction statistics and
separation quality.
</summary>
    <author>
      <name>Andrew J. R. Simpson</name>
    </author>
    <link href="http://arxiv.org/abs/1503.06962v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.06962v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.05268v1</id>
    <updated>2015-06-17T10:17:59Z</updated>
    <published>2015-06-17T10:17:59Z</published>
    <title>Deep Denoising Auto-encoder for Statistical Speech Synthesis</title>
    <summary>  This paper proposes a deep denoising auto-encoder technique to extract better
acoustic features for speech synthesis. The technique allows us to
automatically extract low-dimensional features from high dimensional spectral
features in a non-linear, data-driven, unsupervised way. We compared the new
stochastic feature extractor with conventional mel-cepstral analysis in
analysis-by-synthesis and text-to-speech experiments. Our results confirm that
the proposed method increases the quality of synthetic speech in both
experiments.
</summary>
    <author>
      <name>Zhenzhou Wu</name>
    </author>
    <author>
      <name>Shinji Takaki</name>
    </author>
    <author>
      <name>Junichi Yamagishi</name>
    </author>
    <link href="http://arxiv.org/abs/1506.05268v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.05268v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.05143v1</id>
    <updated>2015-07-18T03:55:50Z</updated>
    <published>2015-07-18T03:55:50Z</published>
    <title>Cover Song Identification with Timbral Shape Sequences</title>
    <summary>  We introduce a novel low level feature for identifying cover songs which
quantifies the relative changes in the smoothed frequency spectrum of a song.
Our key insight is that a sliding window representation of a chunk of audio can
be viewed as a time-ordered point cloud in high dimensions. For corresponding
chunks of audio between different versions of the same song, these point clouds
are approximately rotated, translated, and scaled copies of each other. If we
treat MFCC embeddings as point clouds and cast the problem as a relative shape
sequence, we are able to correctly identify 42/80 cover songs in the "Covers
80" dataset. By contrast, all other work to date on cover songs exclusively
relies on matching note sequences from Chroma derived features.
</summary>
    <author>
      <name>Christopher J. Tralie</name>
    </author>
    <author>
      <name>Paul Bendich</name>
    </author>
    <link href="http://arxiv.org/abs/1507.05143v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.05143v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.07348v1</id>
    <updated>2015-07-27T10:08:24Z</updated>
    <published>2015-07-27T10:08:24Z</published>
    <title>A model for the temporal evolution of the spatial coherence in decaying
  reverberant sound fields</title>
    <summary>  Reverberant sound fields are often modeled as isotropic. However, it has been
observed that spatial properties change during the decay of the sound field
energy, due to non-isotropic attenuation in non-ideal rooms. In this letter, a
model for the spatial coherence between two sensors in a decaying reverberant
sound field is developed for rectangular rooms. The modeled coherence function
depends on room dimensions, surface reflectivity and orientation of the sensor
pair, but is independent of the position of source and sensors in the room. The
model includes the spherically isotropic (diffuse) and cylindrically isotropic
sound field models as special cases.
</summary>
    <author>
      <name>Sam Nees</name>
    </author>
    <author>
      <name>Andreas Schwarz</name>
    </author>
    <author>
      <name>Walter Kellermann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1121/1.4929733</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1121/1.4929733" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for JASA Express Letters</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.07348v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.07348v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.00334v1</id>
    <updated>2015-09-01T15:04:16Z</updated>
    <published>2015-09-01T15:04:16Z</published>
    <title>Transformée en scattering sur la spirale temps-chroma-octave</title>
    <summary>  We introduce a scattering representation for the analysis and classification
of sounds. It is locally translation-invariant, stable to deformations in time
and frequency, and has the ability to capture harmonic structures. The
scattering representation can be interpreted as a convolutional neural network
which cascades a wavelet transform in time and along a harmonic spiral. We
study its application for the analysis of the deformations of the source-filter
model.
</summary>
    <author>
      <name>Vincent Lostanlen</name>
    </author>
    <author>
      <name>Stéphane Mallat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in French, 4 pages, 3 figures, presented at GRETSI 2015 in Lyon,
  France</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.00334v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.00334v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.06279v1</id>
    <updated>2015-09-18T12:47:09Z</updated>
    <published>2015-09-18T12:47:09Z</published>
    <title>Sports highlights generation based on acoustic events detection: A rugby
  case study</title>
    <summary>  We approach the challenging problem of generating highlights from sports
broadcasts utilizing audio information only. A language-independent,
multi-stage classification approach is employed for detection of key acoustic
events which then act as a platform for summarization of highlight scenes.
Objective results and human experience indicate that our system is highly
efficient.
</summary>
    <author>
      <name>Anant Baijal</name>
    </author>
    <author>
      <name>Jaeyoun Cho</name>
    </author>
    <author>
      <name>Woojung Lee</name>
    </author>
    <author>
      <name>Byeong-Seob Ko</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICCE.2015.7066303</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICCE.2015.7066303" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Conference on Consumer Electronics (IEEE ICCE
  2015)</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.06279v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.06279v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.07298v4</id>
    <updated>2017-03-13T16:27:03Z</updated>
    <published>2015-09-24T10:16:09Z</published>
    <title>An Investigation of Universal Background Sparse Coding Based Speaker
  Verification on TIMIT</title>
    <summary>  In this paper, we propose a universal background model, named universal
background sparse coding (UBSC), for speaker verification. The proposed method
trains an ensemble of clusterings by data resampling, and produces sparse codes
from the clusterings by one-nearest-neighbor optimization plus binarization.
The main advantage of UBSC is that it does not suffer from local minima and
does not make Gaussian assumptions on data distributions. We evaluated UBSC on
a clean speech corpus---TIMIT. We used the cosine similarity and inner product
similarity as the scoring methods of a trial. Experimental results show that
UBSC is comparable to Gaussian mixture model.
</summary>
    <author>
      <name>Xiao-Lei Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1509.07298v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.07298v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.00266v1</id>
    <updated>2015-10-01T14:53:29Z</updated>
    <published>2015-10-01T14:53:29Z</published>
    <title>Noise robust integration for blind and non-blind reverberation time
  estimation</title>
    <summary>  The estimation of the decay rate of a signal section is an integral component
of both blind and non-blind reverberation time estimation methods. Several
decay rate estimators have previously been proposed, based on, e.g., linear
regression and maximum-likelihood estimation. Unfortunately, most approaches
are sensitive to background noise, and/or are fairly demanding in terms of
computational complexity. This paper presents a low complexity decay rate
estimator, robust to stationary noise, for reverberation time estimation.
Simulations using artificial signals, and experiments with speech in
ventilation noise, demonstrate the performance and noise robustness of the
proposed method.
</summary>
    <author>
      <name>Christian Schüldt</name>
    </author>
    <author>
      <name>Peter Händel</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICASSP.2015.7177931</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICASSP.2015.7177931" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP), Brisbane, Australia, April 19-24, 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.00266v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.00266v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.01193v1</id>
    <updated>2015-10-05T15:42:45Z</updated>
    <published>2015-10-05T15:42:45Z</published>
    <title>Reverberation time estimation on the ACE corpus using the SDD method</title>
    <summary>  Reverberation Time (T60) is an important measure for characterizing the
properties of a room. The author's T60 estimation algorithm was previously
tested on simulated data where the noise is artificially added to the speech
after convolution with a impulse responses simulated using the image method. We
test the algorithm on speech convolved with real recorded impulse responses and
noise from the same rooms from the Acoustic Characterization of Environments
(ACE) corpus and achieve results comparable results to those using simulated
data.
</summary>
    <author>
      <name>James Eaton</name>
    </author>
    <author>
      <name>Patrick A. Naylor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the ACE Challenge Workshop - a satellite event of
  IEEE-WASPAA 2015 (arXiv:1510.00383)</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.01193v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.01193v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.08484v1</id>
    <updated>2015-10-28T20:59:04Z</updated>
    <published>2015-10-28T20:59:04Z</published>
    <title>MUSAN: A Music, Speech, and Noise Corpus</title>
    <summary>  This report introduces a new corpus of music, speech, and noise. This dataset
is suitable for training models for voice activity detection (VAD) and
music/speech discrimination. Our corpus is released under a flexible Creative
Commons license. The dataset consists of music from several genres, speech from
twelve languages, and a wide assortment of technical and non-technical noises.
We demonstrate use of this corpus for music/speech discrimination on Broadcast
news and VAD for speaker identification.
</summary>
    <author>
      <name>David Snyder</name>
    </author>
    <author>
      <name>Guoguo Chen</name>
    </author>
    <author>
      <name>Daniel Povey</name>
    </author>
    <link href="http://arxiv.org/abs/1510.08484v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.08484v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.08963v1</id>
    <updated>2015-10-30T03:45:41Z</updated>
    <published>2015-10-30T03:45:41Z</published>
    <title>PSD estimation in Beamspace for Estimating Direct-to-Reverberant Ratio
  from A Reverberant Speech Signal</title>
    <summary>  A method for estimation of direct-to-reverberant ratio (DRR) using a
microphone array is proposed. The proposed method estimates the power spectral
density (PSD) of the direct sound and the reverberation using the algorithm
\textit{PSD estimation in beamspace} with a microphone array and calculates the
DRR of the observed signal. The speech corpus of the ACE (Acoustic
Characterisation of Environments) Challenge was utilised for evaluating the
practical feasibility of the proposed method. The experimental results revealed
that the proposed method was able to effectively estimate the DRR from a
recording of a reverberant speech signal which included various environmental
noise.
</summary>
    <author>
      <name>Yusuke Hioka</name>
    </author>
    <author>
      <name>Kenta Niwa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the ACE Challenge Workshop - a satellite event of
  IEEE-WASPAA2015 (arXiv:1510.00383)</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.08963v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.08963v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.05520v1</id>
    <updated>2015-11-17T19:43:53Z</updated>
    <published>2015-11-17T19:43:53Z</published>
    <title>Automatic Instrument Recognition in Polyphonic Music Using Convolutional
  Neural Networks</title>
    <summary>  Traditional methods to tackle many music information retrieval tasks
typically follow a two-step architecture: feature engineering followed by a
simple learning algorithm. In these "shallow" architectures, feature
engineering and learning are typically disjoint and unrelated. Additionally,
feature engineering is difficult, and typically depends on extensive domain
expertise.
  In this paper, we present an application of convolutional neural networks for
the task of automatic musical instrument identification. In this model, feature
extraction and learning algorithms are trained together in an end-to-end
fashion. We show that a convolutional neural network trained on raw audio can
achieve performance surpassing traditional methods that rely on hand-crafted
features.
</summary>
    <author>
      <name>Peter Li</name>
    </author>
    <author>
      <name>Jiyuan Qian</name>
    </author>
    <author>
      <name>Tian Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1511.05520v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.05520v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.02125v1</id>
    <updated>2015-12-07T17:11:01Z</updated>
    <published>2015-12-07T17:11:01Z</published>
    <title>Joint Time-Frequency Scattering for Audio Classification</title>
    <summary>  We introduce the joint time-frequency scattering transform, a time shift
invariant descriptor of time-frequency structure for audio classification. It
is obtained by applying a two-dimensional wavelet transform in time and
log-frequency to a time-frequency wavelet scalogram. We show that this
descriptor successfully characterizes complex time-frequency phenomena such as
time-varying filters and frequency modulated excitations. State-of-the-art
results are achieved for signal reconstruction and phone segment classification
on the TIMIT dataset.
</summary>
    <author>
      <name>Joakim andén</name>
    </author>
    <author>
      <name>Vincent Lostanlen</name>
    </author>
    <author>
      <name>Stéphane Mallat</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MLSP.2015.7324385</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MLSP.2015.7324385" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures in IEEE 25th International Workshop on Machine
  Learning for Signal Processing (MLSP), 2015. Sept. 17-20. Boston, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.02125v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.02125v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.04243v1</id>
    <updated>2015-12-14T10:14:53Z</updated>
    <published>2015-12-14T10:14:53Z</published>
    <title>Trigonometric dictionary based codec for music compression with high
  quality recovery</title>
    <summary>  A codec for compression of music signals is proposed. The method belongs to
the class of transform lossy compression. It is conceived to be applied in the
high quality recovery range though. The transformation, endowing the codec with
its distinctive feature, relies on the ability to construct high quality sparse
approximation of music signals. This is achieved by a redundant trigonometric
dictionary and a dedicated pursuit strategy. The potential of the approach is
illustrated by comparison with the OGG Vorbis format, on a sample consisting of
clips of melodic music. The comparison evidences remarkable improvements in
compression performance for the identical quality of the decompressed signal.
</summary>
    <author>
      <name>Laura Rebollo-Neira</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Software implementing the codec is available on
  http://www.nonlinear-approx.info/examples/node03.html</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.04243v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.04243v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.02546v1</id>
    <updated>2016-01-11T18:14:03Z</updated>
    <published>2016-01-11T18:14:03Z</published>
    <title>Automatic Determination of Chord Roots</title>
    <summary>  Even though chord roots constitute a fundamental concept in music theory,
existing models do not explain and determine them to full satisfaction. We
present a new method which takes sequential context into account to resolve
ambiguities and detect nonharmonic tones. We extract features from chord pairs
and use a decision tree to determine chord roots. This leads to a quantitative
improvement in correctness of the predicted roots in comparison to other
models. All this raises the question how much harmonic and nonharmonic tones
actually contribute to the perception of chord roots.
</summary>
    <author>
      <name>Samuel Rupprechter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">BSc Thesis</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.02546v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.02546v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.07394v1</id>
    <updated>2016-02-24T04:33:49Z</updated>
    <published>2016-02-24T04:33:49Z</published>
    <title>Improved Accent Classification Combining Phonetic Vowels with Acoustic
  Features</title>
    <summary>  Researches have shown accent classification can be improved by integrating
semantic information into pure acoustic approach. In this work, we combine
phonetic knowledge, such as vowels, with enhanced acoustic features to build an
improved accent classification system. The classifier is based on Gaussian
Mixture Model-Universal Background Model (GMM-UBM), with normalized Perceptual
Linear Predictive (PLP) features. The features are further optimized by
Principle Component Analysis (PCA) and Hetroscedastic Linear Discriminant
Analysis (HLDA). Using 7 major types of accented speech from the Foreign
Accented English (FAE) corpus, the system achieves classification accuracy 54%
with input test data as short as 20 seconds, which is competitive to the state
of the art in this field.
</summary>
    <author>
      <name>Zhenhao Ge</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CISP.2015.7408064</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CISP.2015.7408064" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Congress on Image and Signal Processing (CISP) 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.07394v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.07394v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.08044v1</id>
    <updated>2016-02-25T19:17:19Z</updated>
    <published>2016-02-25T19:17:19Z</published>
    <title>On Adjusting the Learning Rate in Frequency Domain Echo Cancellation
  With Double-Talk</title>
    <summary>  One of the main difficulties in echo cancellation is the fact that the
learning rate needs to vary according to conditions such as double-talk and
echo path change. In this paper we propose a new method of varying the learning
rate of a frequency-domain echo canceller. This method is based on the
derivation of the optimal learning rate of the NLMS algorithm in the presence
of noise. The method is evaluated in conjunction with the multidelay block
frequency domain (MDF) adaptive filter. We demonstrate that it performs better
than current double-talk detection techniques and is simple to implement.
</summary>
    <author>
      <name>Jean-Marc Valin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TASL.2006.885935</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TASL.2006.885935" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Audio, Speech and Language Processing, Vol.
  15, No. 3, pp. 1030-1034, 2007</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1602.08044v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.08044v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.08215v1</id>
    <updated>2016-02-26T06:47:06Z</updated>
    <published>2016-02-26T06:47:06Z</published>
    <title>Bandwidth Extension of Narrowband Speech for Low Bit-Rate Wideband
  Coding</title>
    <summary>  Wireless telephone speech is usually limited to the 300-3400 Hz band, which
reduces its quality. There is thus a growing demand for wideband speech systems
that transmit from 50 Hz to 8000 Hz. This paper presents an algorithm to
generate wideband speech from narrowband speech using as low as 500 bits/s of
side information. The 50-300 Hz band is predicted from the narrowband signal. A
source-excitation model is used for the 3400-8000 Hz band, where the excitation
is extrapolated at the receiver, and the spectral envelope is transmitted.
Though some artifacts are present, the resulting wideband speech has enhanced
quality compared to narrowband speech.
</summary>
    <author>
      <name>Jean-Marc Valin</name>
    </author>
    <author>
      <name>Roch Lefebvre</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/SCFT.2000.878425</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/SCFT.2000.878425" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. IEEE Speech Coding Workshop (SCW), 2000, pp. 130-132</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1602.08215v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.08215v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.08633v1</id>
    <updated>2016-02-27T19:32:25Z</updated>
    <published>2016-02-27T19:32:25Z</published>
    <title>Perceptually-Motivated Nonlinear Channel Decorrelation For Stereo
  Acoustic Echo Cancellation</title>
    <summary>  Acoustic echo cancellation with stereo signals is generally an
under-determined problem because of the high coherence between the left and
right channels. In this paper, we present a novel method of significantly
reducing inter-channel coherence without affecting the audio quality. Our work
takes into account psychoacoustic masking and binaural auditory cues. The
proposed non-linear processing combines a shaped comb-allpass (SCAL) filter
with the injection of psychoacoustically masked noise. We show that the
proposed method performs significantly better than other known methods for
reducing inter-channel coherence.
</summary>
    <author>
      <name>Jean-Marc Valin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/HSCMA.2008.4538718</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/HSCMA.2008.4538718" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of Joint Workshop on Hands-free Speech Communication
  and Microphone Arrays (HSCMA), pp. 188-191, 2008</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1602.08633v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.08633v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.08668v1</id>
    <updated>2016-02-28T04:38:33Z</updated>
    <published>2016-02-28T04:38:33Z</published>
    <title>Speex: A Free Codec For Free Speech</title>
    <summary>  The Speex project has been started in 2002 to address the need for a free,
open-source speech codec. Speex is based on the Code Excited Linear Prediction
(CELP) algorithm and, unlike the previously existing Vorbis codec, is optimised
for transmitting speech for low latency communication over an unreliable packet
network. This paper presents an overview of Speex, the technology involved in
it and how it can be used in applications. The most recent developments in
Speex, such as the fixed-point port, acoustic echo cancellation and noise
suppression are also addressed.
</summary>
    <author>
      <name>Jean-Marc Valin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at linux.conf.au 2006, Dunedin. 8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.08668v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.08668v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.03364v1</id>
    <updated>2016-03-10T18:51:24Z</updated>
    <published>2016-03-10T18:51:24Z</published>
    <title>Channel Decorrelation For Stereo Acoustic Echo Cancellation In
  High-Quality Audio Communication</title>
    <summary>  In this paper, we address an important problem in high-quality audio
communication systems. Acoustic echo cancellation with stereo signals is
generally an under-determined problem because of the generally important
correlation that exists between the left and right channels. In this paper, we
present a novel method of significantly reducing that correlation without
affecting the audio quality. This method is perceptually motivated and combines
a shaped comb-allpass (SCAL) filter with the injection of psychoacoustically
masked noise. We show that the proposed method performs significantly better
than other known methods for channel decorrelation.
</summary>
    <author>
      <name>Jean-Marc Valin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages in Proceedings of Workshop on the Internet,
  Telecommunications and Signal Processing (WITSP), 2006. arXiv admin note:
  substantial text overlap with arXiv:1602.08633</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.03364v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.03364v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.04979v1</id>
    <updated>2016-03-16T06:59:11Z</updated>
    <published>2016-03-16T06:59:11Z</published>
    <title>Guitar Solos as Networks</title>
    <summary>  This paper presents an approach to model melodies (and music pieces in
general) as networks. Notes of a melody can be seen as nodes of a network that
are connected whenever these are played in sequence. This creates a directed
graph. By using complex network theory, it is possible to extract some main
metrics, typical of networks, that characterize the piece. Using this
framework, we provide an analysis on a set of guitar solos performed by main
musicians. The results of this study indicate that this model can have an
impact on multimedia applications such as music classification, identification,
and automatic music generation.
</summary>
    <author>
      <name>Stefano Ferretti</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICME.2016.7553001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICME.2016.7553001" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 2016 IEEE International Conference on
  Multimedia and Expo (ICME 2016), IEEE, Seattle, 2016, pp. 1-6</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1603.04979v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.04979v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.07173v1</id>
    <updated>2016-03-23T13:21:12Z</updated>
    <published>2016-03-23T13:21:12Z</published>
    <title>Deductive Refinement of Species Labelling in Weakly Labelled Birdsong
  Recordings</title>
    <summary>  Many approaches have been used in bird species classification from their
sound in order to provide labels for the whole of a recording. However, a more
precise classification of each bird vocalization would be of great importance
to the use and management of sound archives and bird monitoring. In this work,
we introduce a technique that using a two step process can first automatically
detect all bird vocalizations and then, with the use of 'weakly' labelled
recordings, classify them. Evaluations of our proposed method show that it
achieves a correct classification of 61% when used in a synthetic dataset, and
up to 89% when the synthetic dataset only consists of vocalizations larger than
1000 pixels.
</summary>
    <author>
      <name>Veronica Morfi</name>
    </author>
    <author>
      <name>Dan Stowell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.07173v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.07173v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.08740v1</id>
    <updated>2016-03-29T12:21:36Z</updated>
    <published>2016-03-29T12:21:36Z</published>
    <title>On the Impact of Localization Errors on HRTF-based Robust Least-Squares
  Beamforming</title>
    <summary>  In this work, a recently proposed Head-Related Transfer Function (HRTF)-based
Robust Least-Squares Frequency-Invariant (RLSFI) beamformer design is analyzed
with respect to its robustness against localization errors, which lead to a
mismatch between the HRTFs corresponding to the actual target source position
and the HRTFs which have been used for the beamformer design. The impact of
this mismatch on the performance of the HRTF-based RLSFI beamformer is
evaluated, including a comparison to the free-field-based beamformer design,
using signal-based measures and word error rates for an off-the-shelf speech
recognizer.
</summary>
    <author>
      <name>Hendrik Barfuss</name>
    </author>
    <author>
      <name>Walter Kellermann</name>
    </author>
    <link href="http://arxiv.org/abs/1603.08740v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.08740v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.04669v1</id>
    <updated>2016-04-16T00:26:59Z</updated>
    <published>2016-04-16T00:26:59Z</published>
    <title>Two Pairwise Iterative Schemes For High Dimensional Blind Source
  Separation</title>
    <summary>  This paper addresses the high dimensionality problem in blind source
separation (BSS), where the number of sources is greater than two. Two pairwise
iterative schemes are proposed to tackle this high dimensionality problem. The
two pairwise schemes realize nonparametric independent component analysis (ICA)
algorithms based on a new high-performance Convex CauchySchwarz Divergence
(CCSDIV). These two schemes enable fast and efficient demixing of sources in
real-world high dimensional source applications. Finally, the performance
superiority of the proposed schemes is demonstrated in metric-comparison with
FastICA, RobustICA, convex ICA (CICA), and other leading existing algorithms.
</summary>
    <author>
      <name>Zaid Albataineh</name>
    </author>
    <author>
      <name>Fathi M. Salem</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 1 figures, 6 tables. arXiv admin note: substantial text
  overlap with arXiv:1408.0192</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.04669v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.04669v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.08095v1</id>
    <updated>2016-02-24T02:50:44Z</updated>
    <published>2016-02-24T02:50:44Z</published>
    <title>Accent Classification with Phonetic Vowel Representation</title>
    <summary>  Previous accent classification research focused mainly on detecting accents
with pure acoustic information without recognizing accented speech. This work
combines phonetic knowledge such as vowels with acoustic information to build
Guassian Mixture Model (GMM) classifier with Perceptual Linear Predictive (PLP)
features, optimized by Hetroscedastic Linear Discriminant Analysis (HLDA). With
input about 20-second accented speech, this system achieves classification rate
of 51% on a 7-way classification system focusing on the major types of accents
in English, which is competitive to the state-of-the-art results in this field.
</summary>
    <author>
      <name>Zhenhao Ge</name>
    </author>
    <author>
      <name>Yingyi Tan</name>
    </author>
    <author>
      <name>Aravind Ganapathiraju</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Asian Conference on Pattern Recognition (ACPR) 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.08095v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.08095v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.08852v1</id>
    <updated>2016-04-29T14:32:03Z</updated>
    <published>2016-04-29T14:32:03Z</published>
    <title>Joint Sound Source Separation and Speaker Recognition</title>
    <summary>  Non-negative Matrix Factorization (NMF) has already been applied to learn
speaker characterizations from single or non-simultaneous speech for speaker
recognition applications. It is also known for its good performance in (blind)
source separation for simultaneous speech. This paper explains how NMF can be
used to jointly solve the two problems in a multichannel speaker recognizer for
simultaneous speech. It is shown how state-of-the-art multichannel NMF for
blind source separation can be easily extended to incorporate speaker
recognition. Experiments on the CHiME corpus show that this method outperforms
the sequential approach of first applying source separation, followed by
speaker recognition that uses state-of-the-art i-vector techniques.
</summary>
    <author>
      <name>Jeroen Zegers</name>
    </author>
    <author>
      <name>Hugo Van hamme</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to INTERSPEECH2016. 4 pages, 1 extra page for references</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.08852v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.08852v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.01755v1</id>
    <updated>2016-04-28T22:21:01Z</updated>
    <published>2016-04-28T22:21:01Z</published>
    <title>DCTNet and PCANet for acoustic signal feature extraction</title>
    <summary>  We introduce the use of DCTNet, an efficient approximation and alternative to
PCANet, for acoustic signal classification. In PCANet, the eigenfunctions of
the local sample covariance matrix (PCA) are used as filterbanks for
convolution and feature extraction. When the eigenfunctions are well
approximated by the Discrete Cosine Transform (DCT) functions, each layer of of
PCANet and DCTNet is essentially a time-frequency representation. We relate
DCTNet to spectral feature representation methods, such as the the short time
Fourier transform (STFT), spectrogram and linear frequency spectral
coefficients (LFSC). Experimental results on whale vocalization data show that
DCTNet improves classification rate, demonstrating DCTNet's applicability to
signal processing problems such as underwater acoustics.
</summary>
    <author>
      <name>Yin Xian</name>
    </author>
    <author>
      <name>Andrew Thompson</name>
    </author>
    <author>
      <name>Xiaobai Sun</name>
    </author>
    <author>
      <name>Douglas Nowacek</name>
    </author>
    <author>
      <name>Loren Nolte</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.01755v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.01755v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.02427v1</id>
    <updated>2016-05-09T06:13:37Z</updated>
    <published>2016-05-09T06:13:37Z</published>
    <title>Speech Enhancement In Multiple-Noise Conditions using Deep Neural
  Networks</title>
    <summary>  In this paper we consider the problem of speech enhancement in real-world
like conditions where multiple noises can simultaneously corrupt speech. Most
of the current literature on speech enhancement focus primarily on presence of
single noise in corrupted speech which is far from real-world environments.
Specifically, we deal with improving speech quality in office environment where
multiple stationary as well as non-stationary noises can be simultaneously
present in speech. We propose several strategies based on Deep Neural Networks
(DNN) for speech enhancement in these scenarios. We also investigate a DNN
training strategy based on psychoacoustic models from speech coding for
enhancement of noisy speech
</summary>
    <author>
      <name>Anurag Kumar</name>
    </author>
    <author>
      <name>Dinei Florencio</name>
    </author>
    <link href="http://arxiv.org/abs/1605.02427v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.02427v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02542v1</id>
    <updated>2016-06-08T13:19:01Z</updated>
    <published>2016-06-08T13:19:01Z</published>
    <title>Symbolic Music Data Version 1.0</title>
    <summary>  In this document, we introduce a new dataset designed for training machine
learning models of symbolic music data. Five datasets are provided, one of
which is from a newly collected corpus of 20K midi files. We describe our
preprocessing and cleaning pipeline, which includes the exclusion of a number
of files based on scores from a previously developed probabilistic machine
learning model. We also define training, testing and validation splits for the
new dataset, based on a clustering scheme which we also describe. Some simple
histograms are included.
</summary>
    <author>
      <name>Christian Walder</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1606.01368</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.02542v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02542v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02816v2</id>
    <updated>2016-11-11T02:45:44Z</updated>
    <published>2016-06-09T04:01:36Z</published>
    <title>Audio Content based Geotagging in Multimedia</title>
    <summary>  In this paper we propose methods to extract geographically relevant
information in a multimedia recording using its audio. Our method primarily is
based on the fact that urban acoustic environment consists of a variety of
sounds. Hence, location information can be inferred from the composition of
sound events/classes present in the audio. More specifically, we adopt matrix
factorization techniques to obtain semantic content of recording in terms of
different sound classes. These semantic information are then combined to
identify the location of recording.
</summary>
    <author>
      <name>Anurag Kumar</name>
    </author>
    <author>
      <name>Benjamin Elizalde</name>
    </author>
    <author>
      <name>Bhiksha Raj</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.02816v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02816v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03365v4</id>
    <updated>2017-06-27T09:34:28Z</updated>
    <published>2015-12-17T10:40:22Z</published>
    <title>Acoustic Characterization of Environments (ACE) Challenge Results
  Technical Report</title>
    <summary>  This document provides the results of the tests of acoustic parameter
estimation algorithms on the Acoustic Characterization of Environments (ACE)
Challenge Evaluation dataset which were subsequently submitted and written up
into papers for the Proceedings of the ACE Challenge. This document is
supporting material for a forthcoming journal paper on the ACE Challenge which
will provide further analysis of the results.
</summary>
    <author>
      <name>James Eaton</name>
    </author>
    <author>
      <name>Nikolay D. Gaubitch</name>
    </author>
    <author>
      <name>Alastair H. Moore</name>
    </author>
    <author>
      <name>Patrick A. Naylor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Supporting material for Proceedings of the ACE Challenge Workshop - a
  satellite event of IEEE-WASPAA 2015 (arXiv:1510.00383)</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.03365v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03365v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03664v1</id>
    <updated>2016-06-12T04:07:45Z</updated>
    <published>2016-06-12T04:07:45Z</published>
    <title>Weakly Supervised Scalable Audio Content Analysis</title>
    <summary>  Audio Event Detection is an important task for content analysis of multimedia
data. Most of the current works on detection of audio events is driven through
supervised learning approaches. We propose a weakly supervised learning
framework which can make use of the tremendous amount of web multimedia data
with significantly reduced annotation effort and expense. Specifically, we use
several multiple instance learning algorithms to show that audio event
detection through weak labels is feasible. We also propose a novel scalable
multiple instance learning algorithm and show that its competitive with other
multiple instance learning algorithms for audio event detection tasks.
</summary>
    <author>
      <name>Anurag Kumar</name>
    </author>
    <author>
      <name>Bhiksha Raj</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICME 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.03664v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03664v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.05765v1</id>
    <updated>2016-07-19T21:29:03Z</updated>
    <published>2016-07-19T21:29:03Z</published>
    <title>Features and Kernels for Audio Event Recognition</title>
    <summary>  One of the most important problems in audio event detection research is
absence of benchmark results for comparison with any proposed method. Different
works consider different sets of events and datasets which makes it difficult
to comprehensively analyze any novel method with an existing one. In this paper
we propose to establish results for audio event recognition on two recent
publicly-available datasets. In particular we use Gaussian Mixture model based
feature representation and combine them with linear as well as non-linear
kernel Support Vector Machines.
</summary>
    <author>
      <name>Anurag Kumar</name>
    </author>
    <author>
      <name>Bhiksha Raj</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.05765v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.05765v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.06642v2</id>
    <updated>2016-09-08T09:32:17Z</updated>
    <published>2016-07-22T11:56:52Z</published>
    <title>HRTF-based Robust Least-Squares Frequency-Invariant Polynomial
  Beamforming</title>
    <summary>  In this work, we propose a robust Head-Related Transfer Function (HRTF)-based
polynomial beamformer design which accounts for the influence of a humanoid
robot's head on the sound field. In addition, it allows for a flexible steering
of our previously proposed robust HRTF-based beamformer design. We evaluate the
HRTF-based polynomial beamformer design and compare it to the original
HRTF-based beamformer design by means of signal-independent measures as well as
word error rates of an off-the-shelf speech recognition system. Our results
confirm the effectiveness of the polynomial beamformer design, which makes it a
promising approach to robust beamforming for robot audition.
</summary>
    <author>
      <name>Hendrik Barfuss</name>
    </author>
    <author>
      <name>Marcel Mueglich</name>
    </author>
    <author>
      <name>Walter Kellermann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, accepted for IWAENC 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.06642v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.06642v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.06667v3</id>
    <updated>2017-09-17T17:15:08Z</updated>
    <published>2016-07-22T13:12:33Z</published>
    <title>Similarity graphs for the concealment of long duration data loss in
  music</title>
    <summary>  We present a novel method for the compensation of long duration data gaps in
audio signals, in particular music. The concealment of such signal defects is
based on a graph that encodes signal structure in terms of time-persistent
spectral similarity. A suitable candidate segment for the substitution of the
lost content is proposed by an intuitive optimization scheme and smoothly
inserted into the gap. Extensive listening tests show that the proposed
algorithm provides highly promising results when applied to a variety of
real-world music signals.
</summary>
    <author>
      <name>Nathanael Perraudin</name>
    </author>
    <author>
      <name>Nicki Holighaus</name>
    </author>
    <author>
      <name>Piotr Majdak</name>
    </author>
    <author>
      <name>Peter Balazs</name>
    </author>
    <link href="http://arxiv.org/abs/1607.06667v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.06667v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.06706v2</id>
    <updated>2016-08-25T15:12:31Z</updated>
    <published>2016-07-22T15:15:53Z</published>
    <title>Experiments on the DCASE Challenge 2016: Acoustic Scene Classification
  and Sound Event Detection in Real Life Recording</title>
    <summary>  In this paper we present our work on Task 1 Acoustic Scene Classi- fication
and Task 3 Sound Event Detection in Real Life Recordings. Among our experiments
we have low-level and high-level features, classifier optimization and other
heuristics specific to each task. Our performance for both tasks improved the
baseline from DCASE: for Task 1 we achieved an overall accuracy of 78.9%
compared to the baseline of 72.6% and for Task 3 we achieved a Segment-Based
Error Rate of 0.76 compared to the baseline of 0.91.
</summary>
    <author>
      <name>Benjamin Elizalde</name>
    </author>
    <author>
      <name>Anurag Kumar</name>
    </author>
    <author>
      <name>Ankit Shah</name>
    </author>
    <author>
      <name>Rohan Badlani</name>
    </author>
    <author>
      <name>Emmanuel Vincent</name>
    </author>
    <author>
      <name>Bhiksha Raj</name>
    </author>
    <author>
      <name>Ian Lane</name>
    </author>
    <link href="http://arxiv.org/abs/1607.06706v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.06706v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.07801v1</id>
    <updated>2016-06-22T05:51:40Z</updated>
    <published>2016-06-22T05:51:40Z</published>
    <title>ABROA : Audio-Based Room-Occupancy Analysis using Gaussian Mixtures and
  Hidden Markov Models</title>
    <summary>  This paper outlines preliminary steps towards the development of an audio-
based room-occupancy analysis model. Our approach borrows from speech
recognition tradition and is based on Gaussian Mixtures and Hidden Markov
Models. We analyze possible challenges encountered in the development of such a
model, and offer several solutions including feature design and prediction
strategies. We provide results obtained from experiments with audio data from a
retail store in Palo Alto, California. Model assessment is done via
leave-two-out Bootstrap and model convergence achieves good accuracy, thus
representing a contribution to multimodal people counting algorithms.
</summary>
    <author>
      <name>Rafael Valle</name>
    </author>
    <link href="http://arxiv.org/abs/1607.07801v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.07801v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.03417v1</id>
    <updated>2016-08-11T11:01:21Z</updated>
    <published>2016-08-11T11:01:21Z</published>
    <title>Bird detection in audio: a survey and a challenge</title>
    <summary>  Many biological monitoring projects rely on acoustic detection of birds.
Despite increasingly large datasets, this detection is often manual or
semi-automatic, requiring manual tuning/postprocessing. We review the state of
the art in automatic bird sound detection, and identify a widespread need for
tuning-free and species-agnostic approaches. We introduce new datasets and an
IEEE research challenge to address this need, to make possible the development
of fully automatic algorithms for bird sound detection.
</summary>
    <author>
      <name>Dan Stowell</name>
    </author>
    <author>
      <name>Mike Wood</name>
    </author>
    <author>
      <name>Yannis Stylianou</name>
    </author>
    <author>
      <name>Hervé Glotin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Slightly extended preprint of paper accepted for MLSP 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.03417v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.03417v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.07713v1</id>
    <updated>2016-08-27T14:23:51Z</updated>
    <published>2016-08-27T14:23:51Z</published>
    <title>Diffuse-field coherence of sensors with arbitrary directional responses</title>
    <summary>  Knowledge of the diffuse-field coherence between array sensors is a basic
assumption for a wide range of array processing applications. Explicit
relations previously existed only for omnidirectional and first-order
directional sensors, or a restricted arrangement of differential patterns. We
present a closed-form formulation of the theoretical coherence function between
arbitrary directionally band-limited sensors for the general cases that a) the
responses of the individual sensors are known or estimated, and the coherence
needs to be known for an arbitrary arrangement, and b) that no information on
the sensor directionality or on array geometry exists, but calibration
measurements around the array are available.
</summary>
    <author>
      <name>Archontis Politis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.07713v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.07713v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.03409v2</id>
    <updated>2016-09-13T13:09:54Z</updated>
    <published>2016-09-12T14:07:05Z</published>
    <title>Acoustic intensity, energy-density and diffuseness estimation in a
  directionally-constrained region</title>
    <summary>  This work presents a method for estimation of the acoustic intensity, the
energy density and the associated sound field diffuseness around the origin,
when the sound field is weighted with a spatial filter. The method permits
energetic DOA estimation and sound field characterization focused in a specific
angular region determined by the beam pattern of the spatial filter. The
formulation of the estimators is presented and their behavior is analyzed for
the fundamental cases useful in parametric sound field models of a single plane
wave, a uniform diffuse field and a mixture of the two.
</summary>
    <author>
      <name>Archontis Politis</name>
    </author>
    <author>
      <name>Ville Pulkki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.03409v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.03409v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.04301v3</id>
    <updated>2017-04-11T15:15:20Z</updated>
    <published>2016-09-14T14:39:36Z</published>
    <title>TristouNet: Triplet Loss for Speaker Turn Embedding</title>
    <summary>  TristouNet is a neural network architecture based on Long Short-Term Memory
recurrent networks, meant to project speech sequences into a fixed-dimensional
euclidean space. Thanks to the triplet loss paradigm used for training, the
resulting sequence embeddings can be compared directly with the euclidean
distance, for speaker comparison purposes. Experiments on short (between 500ms
and 5s) speech turn comparison and speaker change detection show that
TristouNet brings significant improvements over the current state-of-the-art
techniques for both tasks.
</summary>
    <author>
      <name>Hervé Bredin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICASSP 2017 (42nd IEEE International Conference on Acoustics, Speech
  and Signal Processing). Code available at
  http://github.com/hbredin/TristouNet</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.04301v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.04301v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.08211v1</id>
    <updated>2016-09-26T22:18:49Z</updated>
    <published>2016-09-26T22:18:49Z</published>
    <title>A Robust Diarization System for Measuring Dominance in Peer-Led Team
  Learning Groups</title>
    <summary>  Peer-Led Team Learning (PLTL) is a structured learning model where a team
leader is appointed to facilitate collaborative problem solving among students
for Science, Technology, Engineering and Mathematics (STEM) courses. This paper
presents an informed HMM-based speaker diarization system. The minimum duration
of short conversationalturns and number of participating students were fed as
side information to the HMM system. A modified form of Bayesian Information
Criterion (BIC) was used for iterative merging and re-segmentation. Finally, we
used the diarization output to compute a novel dominance score based on
unsupervised acoustic analysis.
</summary>
    <author>
      <name>Harishchandra Dubey</name>
    </author>
    <author>
      <name>Abhijeet Sangwan</name>
    </author>
    <author>
      <name>John H. L. Hansen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 1 figures, 2 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Workshop on Spoken Language Technology 2016, December, 2016,
  San Diego, California, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1609.08211v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.08211v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.08442v2</id>
    <updated>2017-05-23T09:56:54Z</updated>
    <published>2016-09-27T13:48:01Z</published>
    <title>Collaborative Learning for Language and Speaker Recognition</title>
    <summary>  This paper presents a unified model to perform language and speaker
recognition simultaneously and altogether. The model is based on a multi-task
recurrent neural network where the output of one task is fed as the input of
the other, leading to a collaborative learning framework that can improve both
language and speaker recognition by borrowing information from each other. Our
experiments demonstrated that the multi-task model outperforms the
task-specific models on both tasks.
</summary>
    <author>
      <name>Lantian Li</name>
    </author>
    <author>
      <name>Zhiyuan Tang</name>
    </author>
    <author>
      <name>Dong Wang</name>
    </author>
    <author>
      <name>Andrew Abel</name>
    </author>
    <author>
      <name>Yang Feng</name>
    </author>
    <author>
      <name>Shiyue Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1609.08442v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.08442v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.06214v1</id>
    <updated>2016-10-19T20:56:05Z</updated>
    <published>2016-10-19T20:56:05Z</published>
    <title>A model of infant speech perception and learning</title>
    <summary>  Infant speech perception and learning is modeled using Echo State Network
classification and Reinforcement Learning. Ambient speech for the modeled
infant learner is created using the speech synthesizer Vocaltractlab. An
auditory system is trained to recognize vowel sounds from a series of speakers
of different anatomies in Vocaltractlab. Having formed perceptual targets, the
infant uses Reinforcement Learning to imitate his ambient speech. A possible
way of bridging the problem of speaker normalisation is proposed, using direct
imitation but also including a caregiver who listens to the infants sounds and
imitates those that sound vowel-like.
</summary>
    <author>
      <name>Philip Zurbuchen</name>
    </author>
    <link href="http://arxiv.org/abs/1610.06214v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.06214v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.00514v1</id>
    <updated>2016-11-02T09:24:10Z</updated>
    <published>2016-11-02T09:24:10Z</published>
    <title>The Intelligent Voice 2016 Speaker Recognition System</title>
    <summary>  This paper presents the Intelligent Voice (IV) system submitted to the NIST
2016 Speaker Recognition Evaluation (SRE). The primary emphasis of SRE this
year was on developing speaker recognition technology which is robust for novel
languages that are much more heterogeneous than those used in the current
state-of-the-art, using significantly less training data, that does not contain
meta-data from those languages. The system is based on the state-of-the-art
i-vector/PLDA which is developed on the fixed training condition, and the
results are reported on the protocol defined on the development set of the
challenge.
</summary>
    <author>
      <name>Abbas Khosravani</name>
    </author>
    <author>
      <name>Cornelius Glackin</name>
    </author>
    <author>
      <name>Nazim Dugan</name>
    </author>
    <author>
      <name>Gérard Chollet</name>
    </author>
    <author>
      <name>Nigel Cannings</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures, NIST SRE 2016 Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.00514v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.00514v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.00966v1</id>
    <updated>2016-11-03T12:08:25Z</updated>
    <published>2016-11-03T12:08:25Z</published>
    <title>Frame Theory for Signal Processing in Psychoacoustics</title>
    <summary>  This review chapter aims to strengthen the link between frame theory and
signal processing tasks in psychoacoustics. On the one side, the basic concepts
of frame theory are presented and some proofs are provided to explain those
concepts in some detail. The goal is to reveal to hearing scientists how this
mathematical theory could be relevant for their research. In particular, we
focus on frame theory in a filter bank approach, which is probably the most
relevant view-point for audio signal processing. On the other side, basic
psychoacoustic concepts are presented to stimulate mathematicians to apply
their knowledge in this field.
</summary>
    <author>
      <name>Peter Balazs</name>
    </author>
    <author>
      <name>Nicki Holighaus</name>
    </author>
    <author>
      <name>Thibaud Necciari</name>
    </author>
    <author>
      <name>Diana Stoeva</name>
    </author>
    <link href="http://arxiv.org/abs/1611.00966v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.00966v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.FA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.03477v1</id>
    <updated>2016-11-10T20:35:47Z</updated>
    <published>2016-11-10T20:35:47Z</published>
    <title>Song From PI: A Musically Plausible Network for Pop Music Generation</title>
    <summary>  We present a novel framework for generating pop music. Our model is a
hierarchical Recurrent Neural Network, where the layers and the structure of
the hierarchy encode our prior knowledge about how pop music is composed. In
particular, the bottom layers generate the melody, while the higher levels
produce the drums and chords. We conduct several human studies that show strong
preference of our generated music over that produced by the recent method by
Google. We additionally show two applications of our framework: neural dancing
and karaoke, as well as neural story singing.
</summary>
    <author>
      <name>Hang Chu</name>
    </author>
    <author>
      <name>Raquel Urtasun</name>
    </author>
    <author>
      <name>Sanja Fidler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">under review at ICLR 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.03477v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.03477v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.09524v1</id>
    <updated>2016-11-29T08:33:48Z</updated>
    <published>2016-11-29T08:33:48Z</published>
    <title>Understanding Audio Pattern Using Convolutional Neural Network From Raw
  Waveforms</title>
    <summary>  One key step in audio signal processing is to transform the raw signal into
representations that are efficient for encoding the original information.
Traditionally, people transform the audio into spectral representations, as a
function of frequency, amplitude and phase transformation. In this work, we
take a purely data-driven approach to understand the temporal dynamics of audio
at the raw signal level. We maximize the information extracted from the raw
signal through a deep convolutional neural network (CNN) model. Our CNN model
is trained on the urbansound8k dataset. We discover that salient audio patterns
embedded in the raw waveforms can be efficiently extracted through a
combination of nonlinear filters learned by the CNN model.
</summary>
    <author>
      <name>Shuhui Qu</name>
    </author>
    <author>
      <name>Juncheng Li</name>
    </author>
    <author>
      <name>Wei Dai</name>
    </author>
    <author>
      <name>Samarjit Das</name>
    </author>
    <link href="http://arxiv.org/abs/1611.09524v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.09524v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.09733v1</id>
    <updated>2016-11-29T17:19:45Z</updated>
    <published>2016-11-29T17:19:45Z</published>
    <title>Getting Closer to the Essence of Music: The Con Espressione Manifesto</title>
    <summary>  This text offers a personal and very subjective view on the current situation
of Music Information Research (MIR). Motivated by the desire to build systems
with a somewhat deeper understanding of music than the ones we currently have,
I try to sketch a number of challenges for the next decade of MIR research,
grouped around six simple truths about music that are probably generally agreed
on, but often ignored in everyday research.
</summary>
    <author>
      <name>Gerhard Widmer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2899004</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2899004" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint (author's accepted manuscript) of a journal article (see
  "Journal Reference")</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Transactions on Intelligent Systems and Technology (TIST)
  8(2), Article No. 19, November 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1611.09733v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.09733v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.00876v1</id>
    <updated>2016-12-02T22:02:04Z</updated>
    <published>2016-12-02T22:02:04Z</published>
    <title>FRIDA: FRI-Based DOA Estimation for Arbitrary Array Layouts</title>
    <summary>  In this paper we present FRIDA---an algorithm for estimating directions of
arrival of multiple wideband sound sources. FRIDA combines multi-band
information coherently and achieves state-of-the-art resolution at extremely
low signal-to-noise ratios. It works for arbitrary array layouts, but unlike
the various steered response power and subspace methods, it does not require a
grid search. FRIDA leverages recent advances in sampling signals with a finite
rate of innovation. It is based on the insight that for any array layout, the
entries of the spatial covariance matrix can be linearly transformed into a
uniformly sampled sum of sinusoids.
</summary>
    <author>
      <name>Hanjie Pan</name>
    </author>
    <author>
      <name>Robin Scheibler</name>
    </author>
    <author>
      <name>Eric Bezzam</name>
    </author>
    <author>
      <name>Ivan Dokmanic</name>
    </author>
    <author>
      <name>Martin Vetterli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ICASSP2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.00876v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.00876v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.04919v1</id>
    <updated>2016-12-15T04:22:39Z</updated>
    <published>2016-12-15T04:22:39Z</published>
    <title>Combination of Linear Prediction and Phase Decomposition for Glottal
  Source Analysis on Voiced Speech</title>
    <summary>  Some glottal analysis approaches based upon linear prediction or complex
cepstrum approaches have been proved to be effective to estimate glottal source
from real speech utterances. We propose a new approach employing both an
all-pole odd-order linear prediction to provide a coarse estimation and phase
decomposition based causality/anti-causality separation to generate further
refinements. The obtained measures show that this method improved performance
in terms of reducing source-filter separation in estimation of glottal flow
pulses (GFP). No glottal model fitting is required by this method, thus it has
wide and flexible adaptation to retain fidelity of speakers's vocal features
with computationally affordable resource. The method is evaluated on real
speech utterances to validate it.
</summary>
    <author>
      <name>Yiqiao Chen</name>
    </author>
    <author>
      <name>John N. Gowdy</name>
    </author>
    <link href="http://arxiv.org/abs/1612.04919v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.04919v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.05070v1</id>
    <updated>2016-12-15T14:07:51Z</updated>
    <published>2016-12-15T14:07:51Z</published>
    <title>Towards End-to-End Audio-Sheet-Music Retrieval</title>
    <summary>  This paper demonstrates the feasibility of learning to retrieve short
snippets of sheet music (images) when given a short query excerpt of music
(audio) -- and vice versa --, without any symbolic representation of music or
scores. This would be highly useful in many content-based musical retrieval
scenarios. Our approach is based on Deep Canonical Correlation Analysis (DCCA)
and learns correlated latent spaces allowing for cross-modality retrieval in
both directions. Initial experiments with relatively simple monophonic music
show promising results.
</summary>
    <author>
      <name>Matthias Dorfer</name>
    </author>
    <author>
      <name>Andreas Arzt</name>
    </author>
    <author>
      <name>Gerhard Widmer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In NIPS 2016 End-to-end Learning for Speech and Audio Processing
  Workshop, Barcelona, Spain</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.05070v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.05070v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.05076v1</id>
    <updated>2016-12-15T14:16:56Z</updated>
    <published>2016-12-15T14:16:56Z</published>
    <title>Live Score Following on Sheet Music Images</title>
    <summary>  In this demo we show a novel approach to score following. Instead of relying
on some symbolic representation, we are using a multi-modal convolutional
neural network to match the incoming audio stream directly to sheet music
images. This approach is in an early stage and should be seen as proof of
concept. Nonetheless, the audience will have the opportunity to test our
implementation themselves via 3 simple piano pieces.
</summary>
    <author>
      <name>Matthias Dorfer</name>
    </author>
    <author>
      <name>Andreas Arzt</name>
    </author>
    <author>
      <name>Sebastian Böck</name>
    </author>
    <author>
      <name>Amaury Durand</name>
    </author>
    <author>
      <name>Gerhard Widmer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17th International Society for Music Information Retrieval Conference
  (ISMIR 2016), Late Breaking/Demo Papers, New York, NY</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.05076v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.05076v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.05168v1</id>
    <updated>2016-12-15T17:59:05Z</updated>
    <published>2016-12-15T17:59:05Z</published>
    <title>LIA system description for NIST SRE 2016</title>
    <summary>  This paper describes the LIA speaker recognition system developed for the
Speaker Recognition Evaluation (SRE) campaign. Eight sub-systems are developed,
all based on a state-of-the-art approach: i-vector/PLDA which represents the
mainstream technique in text-independent speaker recognition. These sub-systems
differ: on the acoustic feature extraction front-end (MFCC, PLP), at the
i-vector extraction stage (UBM, DNN or two-feats posteriors) and finally on the
data-shifting (IDVC, mean-shifting). The submitted system is a fusion at the
score-level of these eight sub-systems.
</summary>
    <author>
      <name>Mickael Rouvier</name>
    </author>
    <author>
      <name>Pierre-Michel Bousquet</name>
    </author>
    <author>
      <name>Moez Ajili</name>
    </author>
    <author>
      <name>Waad Ben Kheder</name>
    </author>
    <author>
      <name>Driss Matrouf</name>
    </author>
    <author>
      <name>Jean-François Bonastre</name>
    </author>
    <link href="http://arxiv.org/abs/1612.05168v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.05168v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.03834v1</id>
    <updated>2017-01-09T15:10:38Z</updated>
    <published>2017-01-09T15:10:38Z</published>
    <title>On Higher Order Positive Differential Energy Operator</title>
    <summary>  The higher order differential energy operator (DEO), denoted via
$\Upsilon_k(x)$, is an extension to the second order famous Teager-Kaiser
operator. The DEO helps measuring the higher order gauge of energy of a signal
which is useful for AM-FM demodulation. However, the energy criterion defined
by the DEO is not compliant with the presumption of positivity of energy. In
this paper we introduce a higher order operator called Positive Differential
Energy Operator (PDEO). This operator which can be obtained using alternative
recursive relations, resolves the energy sign problem. The simulations
demonstrate that the proposed operator can outperform DEOs in terms of Average
Signal to Error Ratio (ASER) in AM/FM demodulation.
</summary>
    <author>
      <name>Amirhossein Javaheri</name>
    </author>
    <author>
      <name>Mohammad Bagher Shamsollahi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.03834v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.03834v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.08156v1</id>
    <updated>2017-01-27T12:38:47Z</updated>
    <published>2017-01-27T12:38:47Z</published>
    <title>A Comprehensive Survey on Bengali Phoneme Recognition</title>
    <summary>  Hidden Markov model based various phoneme recognition methods for Bengali
language is reviewed. Automatic phoneme recognition for Bengali language using
multilayer neural network is reviewed. Usefulness of multilayer neural network
over single layer neural network is discussed. Bangla phonetic feature table
construction and enhancement for Bengali speech recognition is also discussed.
Comparison among these methods is discussed.
</summary>
    <author>
      <name>Sadia Tasnim Swarna</name>
    </author>
    <author>
      <name>Shamim Ehsan</name>
    </author>
    <author>
      <name>Md. Saiful Islam</name>
    </author>
    <author>
      <name>Marium E Jannat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.08156v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.08156v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.07713v1</id>
    <updated>2017-02-24T17:23:01Z</updated>
    <published>2017-02-24T17:23:01Z</published>
    <title>Multichannel Linear Prediction for Blind Reverberant Audio Source
  Separation</title>
    <summary>  A class of methods based on multichannel linear prediction (MCLP) can achieve
effective blind dereverberation of a source, when the source is observed with a
microphone array. We propose an inventive use of MCLP as a pre-processing step
for blind source separation with a microphone array. We show theoretically
that, under certain assumptions, such pre-processing reduces the original blind
reverberant source separation problem to a non-reverberant one, which in turn
can be effectively tackled using existing methods. We demonstrate our claims
using real recordings obtained with an eight-microphone circular array in
reverberant environments.
</summary>
    <author>
      <name>İlker Bayram</name>
    </author>
    <author>
      <name>Savaşkan Bulek</name>
    </author>
    <link href="http://arxiv.org/abs/1702.07713v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.07713v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.00009v1</id>
    <updated>2017-02-28T18:26:44Z</updated>
    <published>2017-02-28T18:26:44Z</published>
    <title>Nonlinear Model and its Inverse of an Audio System</title>
    <summary>  This computer science master thesis aims at modelling the nonlinearities of a
loudspeaker. A piecewise linear approximation is initially explored and then we
present a nonlinear Volterra model to simulate the behavior of the system. The
general theory of continuous and discrete Volterra series is summarised. A
Normalized Least Mean Square algorithm is used to determine the Volterra series
to third order. We also present as inverted system which is trained with the
same algorithm. Training data for the models were collected measuring a
physical speaker using a laser interferometer. Results indicate a decrease in
Mean Squared Error compared to the linear model with a dependency on the
particular test signal, the order and the parameters of the model.
</summary>
    <author>
      <name>Alessandro Loriga</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">113 pages, master thesis</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.00009v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.00009v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.02317v1</id>
    <updated>2017-03-07T10:36:30Z</updated>
    <published>2017-03-07T10:36:30Z</published>
    <title>Convolutional Recurrent Neural Networks for Bird Audio Detection</title>
    <summary>  Bird sounds possess distinctive spectral structure which may exhibit small
shifts in spectrum depending on the bird species and environmental conditions.
In this paper, we propose using convolutional recurrent neural networks on the
task of automated bird audio detection in real-life environments. In the
proposed method, convolutional layers extract high dimensional, local frequency
shift invariant features, while recurrent layers capture longer term
dependencies between the features extracted from short time frames. This method
achieves 88.5% Area Under ROC Curve (AUC) score on the unseen evaluation data
and obtains the second place in the Bird Audio Detection challenge.
</summary>
    <author>
      <name> EmreÇakır</name>
    </author>
    <author>
      <name>Sharath Adavanne</name>
    </author>
    <author>
      <name>Giambattista Parascandolo</name>
    </author>
    <author>
      <name>Konstantinos Drossos</name>
    </author>
    <author>
      <name>Tuomas Virtanen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to EUSIPCO 2017 Special Session on Bird Audio Signal
  Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.02317v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.02317v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.07588v2</id>
    <updated>2017-08-31T12:01:36Z</updated>
    <published>2017-03-22T10:08:51Z</published>
    <title>Gate Activation Signal Analysis for Gated Recurrent Neural Networks and
  Its Correlation with Phoneme Boundaries</title>
    <summary>  In this paper we analyze the gate activation signals inside the gated
recurrent neural networks, and find the temporal structure of such signals is
highly correlated with the phoneme boundaries. This correlation is further
verified by a set of experiments for phoneme segmentation, in which better
results compared to standard approaches were obtained.
</summary>
    <author>
      <name>Yu-Hsuan Wang</name>
    </author>
    <author>
      <name>Cheng-Tao Chung</name>
    </author>
    <author>
      <name>Hung-yi Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, The code is available at
  https://github.com/allyoushawn/timit_gas.git</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.07588v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.07588v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.08019v1</id>
    <updated>2017-03-23T11:45:10Z</updated>
    <published>2017-03-23T11:45:10Z</published>
    <title>Single Channel Audio Source Separation using Convolutional Denoising
  Autoencoders</title>
    <summary>  Deep learning techniques have been used recently to tackle the audio source
separation problem. In this work, we propose to use deep convolution denoising
auto-encoders (CDAEs) for monaural audio source separation. We use as many
CDAEs as the number of sources to be separated from the mixed signal. Each CDAE
is trained to separate one source and treats the other sources as background
noise. The main idea is to allow each CDAE to learn suitable time-frequency
filters and features to its corresponding source. Our experimental results show
that CDAEs perform source separation slightly better than the deep feedforward
neural networks (FNNs) even with a much less number of parameters than FNNs.
</summary>
    <author>
      <name>Emad M. Grais</name>
    </author>
    <author>
      <name>Mark D. Plumbley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to InterSpeech2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.08019v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.08019v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.02216v1</id>
    <updated>2017-04-07T13:15:15Z</updated>
    <published>2017-04-07T13:15:15Z</published>
    <title>OBTAIN: Real-Time Beat Tracking in Audio Signals</title>
    <summary>  In this paper, we design a system in order to perform the real-time beat
tracking for an audio signal. We use Onset Strength Signal (OSS) to detect the
onsets and estimate the tempos. Then, we form Cumulative Beat Strength Signal
(CBSS) by taking advantage of OSS and estimated tempos. Next, we perform peak
detection by extracting the periodic sequence of beats among all CBSS peaks. In
simulations, we can see that our proposed algorithm, Online Beat TrAckINg
(OBTAIN), outperforms state-of-art results in terms of prediction accuracy
while maintaining comparable and practical computational complexity. The
real-time performance is tractable visually as illustrated in the simulations.
</summary>
    <author>
      <name>Ali Mottaghi</name>
    </author>
    <author>
      <name>Kayhan Behdin</name>
    </author>
    <author>
      <name>Ashkan Esmaeili</name>
    </author>
    <author>
      <name>Mohammadreza Heydari</name>
    </author>
    <author>
      <name>Farokh Marvasti</name>
    </author>
    <link href="http://arxiv.org/abs/1704.02216v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.02216v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.04971v1</id>
    <updated>2017-05-14T14:43:10Z</updated>
    <published>2017-05-14T14:43:10Z</published>
    <title>Musical Instrument Recognition Using Their Distinctive Characteristics
  in Artificial Neural Networks</title>
    <summary>  In this study an Artificial Neural Network was trained to classify musical
instruments, using audio samples transformed to the frequency domain. Different
features of the sound, in both time and frequency domain, were analyzed and
compared in relation to how much information that could be derived from that
limited data. The study concluded that in comparison with the base experiment,
that had an accuracy of 93.5%, using the attack only resulted in 80.2% and the
initial 100 Hz in 64.2%.
</summary>
    <author>
      <name>Babak Toghiani-Rizi</name>
    </author>
    <author>
      <name>Marcus Windmark</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Results based on a study conducted during the course Machine Learning
  at Uppsala University</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.04971v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.04971v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.05458v3</id>
    <updated>2017-07-14T21:23:24Z</updated>
    <published>2017-05-15T21:21:06Z</published>
    <title>Music generation with variational recurrent autoencoder supported by
  history</title>
    <summary>  A serious problem for automated music generation is to propose the model that
could reproduce sophisticated temporal and melodic patterns that would
correspond to the style of the training input. We propose a new architecture of
an artificial neural network that helps to deal with such tasks. The proposed
approach is based on a long short-term memory language model combined with
variational recurrent autoencoder. These methods have certain advantages when
dealing with temporally rich inputs. The proposed architecture comprises this
features and helps to generate results of higher complexity and diversity.
</summary>
    <author>
      <name>Alexey Tikhonov</name>
    </author>
    <author>
      <name>Ivan P. Yamshchikov</name>
    </author>
    <link href="http://arxiv.org/abs/1705.05458v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.05458v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.10134v2</id>
    <updated>2017-05-30T13:17:47Z</updated>
    <published>2017-05-29T11:50:57Z</published>
    <title>On Residual CNN in text-dependent speaker verification task</title>
    <summary>  Deep learning approaches are still not very common in the speaker
verification field. We investigate the possibility of using deep residual
convolutional neural network with spectrograms as an input features in the
text-dependent speaker verification task. Despite the fact that we were not
able to surpass the baseline system in quality, we achieved a quite good
results for such a new approach getting an 5.23% ERR on the RSR2015 evaluation
part. Fusion of the baseline and proposed systems outperformed the best
individual system by 18% relatively.
</summary>
    <author>
      <name>Egor Malykh</name>
    </author>
    <author>
      <name>Sergey Novoselov</name>
    </author>
    <author>
      <name>Oleg Kudashev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for Specom 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.10134v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.10134v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.02047v1</id>
    <updated>2017-06-07T05:09:41Z</updated>
    <published>2017-06-07T05:09:41Z</published>
    <title>Stacked Convolutional and Recurrent Neural Networks for Bird Audio
  Detection</title>
    <summary>  This paper studies the detection of bird calls in audio segments using
stacked convolutional and recurrent neural networks. Data augmentation by
blocks mixing and domain adaptation using a novel method of test mixing are
proposed and evaluated in regard to making the method robust to unseen data.
The contributions of two kinds of acoustic features (dominant frequency and log
mel-band energy) and their combinations are studied in the context of bird
audio detection. Our best achieved AUC measure on five cross-validations of the
development data is 95.5% and 88.1% on the unseen evaluation data.
</summary>
    <author>
      <name>Sharath Adavanne</name>
    </author>
    <author>
      <name>Konstantinos Drossos</name>
    </author>
    <author>
      <name>Emre Çakır</name>
    </author>
    <author>
      <name>Tuomas Virtanen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for European Signal Processing Conference 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.02047v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.02047v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.02292v1</id>
    <updated>2017-06-07T06:06:14Z</updated>
    <published>2017-06-07T06:06:14Z</published>
    <title>Stacked Convolutional and Recurrent Neural Networks for Music Emotion
  Recognition</title>
    <summary>  This paper studies the emotion recognition from musical tracks in the
2-dimensional valence-arousal (V-A) emotional space. We propose a method based
on convolutional (CNN) and recurrent neural networks (RNN), having
significantly fewer parameters compared with the state-of-the-art method for
the same task. We utilize one CNN layer followed by two branches of RNNs
trained separately for arousal and valence. The method was evaluated using the
'MediaEval2015 emotion in music' dataset. We achieved an RMSE of 0.202 for
arousal and 0.268 for valence, which is the best result reported on this
dataset.
</summary>
    <author>
      <name>Miroslav Malik</name>
    </author>
    <author>
      <name>Sharath Adavanne</name>
    </author>
    <author>
      <name>Konstantinos Drossos</name>
    </author>
    <author>
      <name>Tuomas Virtanen</name>
    </author>
    <author>
      <name>Dasa Ticha</name>
    </author>
    <author>
      <name>Roman Jarina</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for Sound and Music Computing (SMC 2017)</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.02292v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.02292v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.05077v1</id>
    <updated>2017-06-08T11:13:32Z</updated>
    <published>2017-06-08T11:13:32Z</published>
    <title>SUT System Description for NIST SRE 2016</title>
    <summary>  This paper describes the submission to fixed condition of NIST SRE 2016 by
Sharif University of Technology (SUT) team. We provide a full description of
the systems that were included in our submission. We start with an overview of
the datasets that were used for training and development. It is followed by
describing front-ends which contain different VAD and feature types. UBM and
i-vector extractor training are the next details in this paper. As one of the
important steps in system preparation, preconditioning the i-vectors are
explained in more details. Then, we describe the classifier and score
normalization methods. And finally, some results on SRE16 evaluation dataset
are reported and analyzed.
</summary>
    <author>
      <name>Hossein Zeinali</name>
    </author>
    <author>
      <name>Hossein Sameti</name>
    </author>
    <author>
      <name>Nooshin Maghsoodi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented in NIST SRE 2016 Evaluation Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.05077v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.05077v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.05781v1</id>
    <updated>2017-06-19T04:42:14Z</updated>
    <published>2017-06-19T04:42:14Z</published>
    <title>Kapre: On-GPU Audio Preprocessing Layers for a Quick Implementation of
  Deep Neural Network Models with Keras</title>
    <summary>  We introduce Kapre, Keras layers for audio and music signal preprocessing.
Music research using deep neural networks requires a heavy and tedious
preprocessing stage, for which audio processing parameters are often ignored in
parameter optimisation. To solve this problem, Kapre implements time-frequency
conversions, normalisation, and data augmentation as Keras layers. We report
simple benchmark results, showing real-time on-GPU preprocessing adds a
reasonable amount of computation.
</summary>
    <author>
      <name>Keunwoo Choi</name>
    </author>
    <author>
      <name>Deokjin Joo</name>
    </author>
    <author>
      <name>Juho Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML 2017 machine learning for music discovery</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.05781v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.05781v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.06810v1</id>
    <updated>2017-06-21T09:57:24Z</updated>
    <published>2017-06-21T09:57:24Z</published>
    <title>Multi-Level and Multi-Scale Feature Aggregation Using Sample-level Deep
  Convolutional Neural Networks for Music Classification</title>
    <summary>  Music tag words that describe music audio by text have different levels of
abstraction. Taking this issue into account, we propose a music classification
approach that aggregates multi-level and multi-scale features using pre-trained
feature extractors. In particular, the feature extractors are trained in
sample-level deep convolutional neural networks using raw waveforms. We show
that this approach achieves state-of-the-art results on several music
classification datasets.
</summary>
    <author>
      <name>Jongpil Lee</name>
    </author>
    <author>
      <name>Juhan Nam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML Music Discovery Workshop 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.06810v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.06810v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.08818v1</id>
    <updated>2017-06-27T12:39:10Z</updated>
    <published>2017-06-27T12:39:10Z</published>
    <title>Gabor frames and deep scattering networks in audio processing</title>
    <summary>  In this paper a feature extractor based on Gabor frames and Mallat's
scattering transform, called Gabor scattering, is introduced. This feature
extractor is applied to a simple signal model for audio signals, i.e. a class
of tones consisting of fundamental frequency and its multiples and an according
envelope. Within different layers, different invariances to certain signal
features occur. In this paper we give a mathematical explanation for the first
and the second layer which are illustrated by numerical examples. Deformation
stability of this feature extractor will be shown by using a decoupling
technique, previously suggested for the scattering transform of Cartoon
functions. Here it is used to see if the feature extractor is robust to changes
in spectral shape and frequency modulation.
</summary>
    <author>
      <name>Roswitha Bammer</name>
    </author>
    <author>
      <name>Monika Dörfler</name>
    </author>
    <link href="http://arxiv.org/abs/1706.08818v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.08818v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09691v1</id>
    <updated>2017-06-29T11:43:44Z</updated>
    <published>2017-06-29T11:43:44Z</published>
    <title>Speaker Identification in the Shouted Environment Using Suprasegmental
  Hidden Markov Models</title>
    <summary>  In this paper, Suprasegmental Hidden Markov Models (SPHMMs) have been used to
enhance the recognition performance of text-dependent speaker identification in
the shouted environment. Our speech database consists of two databases: our
collected database and the Speech Under Simulated and Actual Stress (SUSAS)
database. Our results show that SPHMMs significantly enhance speaker
identification performance compared to Second-Order Circular Hidden Markov
Models (CHMM2s) in the shouted environment. Using our collected database,
speaker identification performance in this environment is 68% and 75% based on
CHMM2s and SPHMMs respectively. Using the SUSAS database, speaker
identification performance in the same environment is 71% and 79% based on
CHMM2s and SPHMMs respectively.
</summary>
    <author>
      <name>Ismail Shahin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.sigpro.2008.05.012</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.sigpro.2008.05.012" rel="related"/>
    <link href="http://arxiv.org/abs/1706.09691v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09691v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09736v1</id>
    <updated>2017-06-29T13:04:50Z</updated>
    <published>2017-06-29T13:04:50Z</published>
    <title>Speaking Style Authentication Using Suprasegmental Hidden Markov Models</title>
    <summary>  The importance of speaking style authentication from human speech is gaining
an increasing attention and concern from the engineering community. The
importance comes from the demand to enhance both the naturalness and efficiency
of spoken language human-machine interface. Our work in this research focuses
on proposing, implementing, and testing speaker-dependent and text-dependent
speaking style authentication (verification) systems that accept or reject the
identity claim of a speaking style based on suprasegmental hidden Markov models
(SPHMMs). Based on using SPHMMs, our results show that the average speaking
style authentication performance is: 99%, 37%, 85%, 60%, 61%, 59%, 41%, 61%,
and 57% belonging respectively to the speaking styles: neutral, shouted, slow,
loud, soft, fast, angry, happy, and fearful.
</summary>
    <author>
      <name>Ismail Shahin</name>
    </author>
    <link href="http://arxiv.org/abs/1706.09736v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09736v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09754v1</id>
    <updated>2017-06-29T13:45:09Z</updated>
    <published>2017-06-29T13:45:09Z</published>
    <title>Speaker Identification Investigation and Analysis in Unbiased and Biased
  Emotional Talking Environments</title>
    <summary>  This work aims at investigating and analyzing speaker identification in each
unbiased and biased emotional talking environments based on a classifier called
Suprasegmental Hidden Markov Models (SPHMMs). The first talking environment is
unbiased towards any emotion, while the second talking environment is biased
towards different emotions. Each of these talking environments is made up of
six distinct emotions. These emotions are neutral, angry, sad, happy, disgust
and fear. The investigation and analysis of this work show that speaker
identification performance in the biased talking environment is superior to
that in the unbiased talking environment. The obtained results in this work are
close to those achieved in subjective assessment by human judges.
</summary>
    <author>
      <name>Ismail Shahin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10772-012-9156-2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10772-012-9156-2" rel="related"/>
    <link href="http://arxiv.org/abs/1706.09754v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09754v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.00138v1</id>
    <updated>2017-07-01T11:15:21Z</updated>
    <published>2017-07-01T11:15:21Z</published>
    <title>Speaker Identification in Shouted Talking Environments Based on Novel
  Third-Order Hidden Markov Models</title>
    <summary>  In this work we propose, implement, and evaluate novel models called
Third-Order Hidden Markov Models (HMM3s) to enhance low performance of
text-independent speaker identification in shouted talking environments. The
proposed models have been tested on our collected speech database using
Mel-Frequency Cepstral Coefficients (MFCCs). Our results demonstrate that HMM3s
significantly improve speaker identification performance in such talking
environments by 11.3% and 166.7% compared to second-order hidden Markov models
(HMM2s) and first-order hidden Markov models (HMM1s), respectively. The
achieved results based on the proposed models are close to those obtained in
subjective assessment by human listeners.
</summary>
    <author>
      <name>Ismail Shahin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 4th International Conference on Audio, Language and Image
  Processing (ICALIP2014), Shanghai, China, 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.00138v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.00138v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.00276v1</id>
    <updated>2017-07-02T10:31:10Z</updated>
    <published>2017-07-02T10:31:10Z</published>
    <title>Emirati Speaker Verification Based on HMM1s, HMM2s, and HMM3s</title>
    <summary>  This work focuses on Emirati speaker verification systems in neutral talking
environments based on each of First-Order Hidden Markov Models (HMM1s),
Second-Order Hidden Markov Models (HMM2s), and Third-Order Hidden Markov Models
(HMM3s) as classifiers. These systems have been evaluated on our collected
Emirati speech database which is comprised of 25 male and 25 female Emirati
speakers using Mel-Frequency Cepstral Coefficients (MFCCs) as extracted
features. Our results show that HMM3s outperform each of HMM1s and HMM2s for a
text-independent Emirati speaker verification. The obtained results based on
HMM3s are close to those achieved in subjective assessment by human listeners.
</summary>
    <author>
      <name>Ismail Shahin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13th International Conference on Signal Processing, Chengdu, China,
  2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.00276v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.00276v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.00679v1</id>
    <updated>2017-07-01T10:25:21Z</updated>
    <published>2017-07-01T10:25:21Z</published>
    <title>Talking Condition Identification Using Second-Order Hidden Markov Models</title>
    <summary>  This work focuses on enhancing the performance of text-dependent and
speaker-dependent talking condition identification systems using second-order
hidden Markov models (HMM2s). Our results show that the talking condition
identification performance based on HMM2s has been improved significantly
compared to first-order hidden Markov models (HMM1s). Our talking conditions in
this work are neutral, shouted, loud, angry, happy, and fear.
</summary>
    <author>
      <name>Ismail Shahin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3rd International Conference on Information &amp; Communication
  Technologies: from Theory to Applications, Damascus, Syria, 2008. arXiv admin
  note: text overlap with arXiv:1706.09691, arXiv:1706.09716</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.00679v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.00679v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.01653v1</id>
    <updated>2017-07-06T06:40:19Z</updated>
    <published>2017-07-06T06:40:19Z</published>
    <title>pch2csd: an application for converting Nord Modular G2 patches into
  Csound code</title>
    <summary>  The paper presents the pch2csd project, focused on converting patches of
popular Clavia Nord Modular G2 synthesizer into code of Csound language. Now
discontinued, Nord Modular G2 left a lot of interesting patches for sound
synthesis and algorithmic composition. To give this heritage a new life, we
created our project with the hope for being able to simulate the original sound
and behavior of Nord Modular.
</summary>
    <author>
      <name>Gleb Rogozinsky</name>
    </author>
    <author>
      <name>Mihail Chesnokov</name>
    </author>
    <author>
      <name>Eugene Cherny</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 4 figures, for associated source code, see
  https://github.com/gleb812/pch2csd/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. 14th Sound and Music Computing Conf. (SMC 2017) (2017)
  415-421</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1707.01653v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.01653v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.03465v1</id>
    <updated>2017-08-11T08:14:19Z</updated>
    <published>2017-08-11T08:14:19Z</published>
    <title>DNN Transfer Learning based Non-linear Feature Extraction for Acoustic
  Event Classification</title>
    <summary>  Recent acoustic event classification research has focused on training
suitable filters to represent acoustic events. However, due to limited
availability of target event databases and linearity of conventional filters,
there is still room for improving performance. By exploiting the non-linear
modeling of deep neural networks (DNNs) and their ability to learn beyond
pre-trained environments, this letter proposes a DNN-based feature extraction
scheme for the classification of acoustic events. The effectiveness and
robustness to noise of the proposed method are demonstrated using a database of
indoor surveillance environments.
</summary>
    <author>
      <name>Seongkyu Mun</name>
    </author>
    <author>
      <name>Minkyu Shin</name>
    </author>
    <author>
      <name>Suwon Shon</name>
    </author>
    <author>
      <name>Wooil Kim</name>
    </author>
    <author>
      <name>David K. Han</name>
    </author>
    <author>
      <name>Hanseok Ko</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEICE TRANSACTIONS on Information and Systems, Vol.E100-D, No.9
  (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1708.03465v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.03465v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.03535v1</id>
    <updated>2017-08-11T13:24:32Z</updated>
    <published>2017-08-11T13:24:32Z</published>
    <title>Neural Translation of Musical Style</title>
    <summary>  Music is an expressive form of communication often used to convey emotion in
scenarios where "words are not enough". Part of this information lies in the
musical composition where well-defined language exists. However, a significant
amount of information is added during a performance as the musician interprets
the composition. The performer injects expressiveness into the written score
through variations of different musical properties such as dynamics and tempo.
In this paper, we describe a model that can learn to perform sheet music. Our
research concludes that the generated performances are indistinguishable from a
human performance, thereby passing a test in the spirit of a "musical Turing
test".
</summary>
    <author>
      <name>Iman Malik</name>
    </author>
    <author>
      <name>Carl Henrik Ek</name>
    </author>
    <link href="http://arxiv.org/abs/1708.03535v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.03535v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.04821v1</id>
    <updated>2017-08-16T09:23:28Z</updated>
    <published>2017-08-16T09:23:28Z</published>
    <title>Underdetermined source separation using a sparse STFT framework and
  weighted laplacian directional modelling</title>
    <summary>  The instantaneous underdetermined audio source separation problem of
K-sensors, L-sources mixing scenario (where K &lt; L) has been addressed by many
different approaches, provided the sources remain quite distinct in the virtual
positioning space spanned by the sensors. This problem can be tackled as a
directional clustering problem along the source position angles in the mixture.
The use of Generalised Directional Laplacian Densities (DLD) in the MDCT domain
for underdetermined source separation has been proposed before. Here, we derive
weighted mixtures of DLDs in a sparser representation of the data in the STFT
domain to perform separation. The proposed approach yields improved results
compared to our previous offering and compares favourably with the
state-of-the-art.
</summary>
    <author>
      <name>Thomas Sgouros</name>
    </author>
    <author>
      <name>Nikolaos Mitianoudis</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/EUSIPCO.2016.7760549</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/EUSIPCO.2016.7760549" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EUSIPCO 2016, Budapest, Hungary</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.04821v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.04821v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.05132v1</id>
    <updated>2017-08-17T04:35:00Z</updated>
    <published>2017-08-17T04:35:00Z</published>
    <title>An instrumental intelligibility metric based on information theory</title>
    <summary>  We propose a new monaural intrusive instrumental intelligibility metric
called SIIB (speech intelligibility in bits). SIIB is an estimate of the amount
of information shared between a talker and a listener in bits per second.
Unlike existing information theoretic intelligibility metrics, SIIB accounts
for talker variability and statistical dependencies between time-frequency
units. Our evaluation shows that relative to state-of-the-art intelligibility
metrics, SIIB is highly correlated with the intelligibility of speech that has
been degraded by noise and processed by speech enhancement algorithms.
</summary>
    <author>
      <name>Steven Van Kuyk</name>
    </author>
    <author>
      <name>W. Bastiaan Kleijn</name>
    </author>
    <author>
      <name>Richard C. Hendriks</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Currently under peer-review</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.05132v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.05132v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.05826v2</id>
    <updated>2017-10-03T11:14:13Z</updated>
    <published>2017-08-19T09:18:35Z</published>
    <title>Ensemble Of Deep Neural Networks For Acoustic Scene Classification</title>
    <summary>  Deep neural networks (DNNs) have recently achieved great success in a
multitude of classification tasks. Ensembles of DNNs have been shown to improve
the performance. In this paper, we explore the recent state-of-the-art DNNs
used for image classification. We modified these DNNs and applied them to the
task of acoustic scene classification. We conducted a number of experiments on
the TUT Acoustic Scenes 2017 dataset to empirically compare these methods.
Finally, we show that the best model improves the baseline score for DCASE-2017
Task 1 by 3.1% in the test set and by 10% in the development set.
</summary>
    <author>
      <name>Venkatesh Duppada</name>
    </author>
    <author>
      <name>Sushant Hiray</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Detection and Classification of Acoustic Scenes and Events 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.05826v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.05826v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.05987v1</id>
    <updated>2017-08-20T16:18:20Z</updated>
    <published>2017-08-20T16:18:20Z</published>
    <title>Perceptual audio loss function for deep learning</title>
    <summary>  PESQ and POLQA , are standards are standards for automated assessment of
voice quality of speech as experienced by human beings. The predictions of
those objective measures should come as close as possible to subjective quality
scores as obtained in subjective listening tests. Wavenet is a deep neural
network originally developed as a deep generative model of raw audio
wave-forms. Wavenet architecture is based on dilated causal convolutions, which
exhibit very large receptive fields. In this short paper we suggest using the
Wavenet architecture, in particular its large receptive filed in order to learn
PESQ algorithm. By doing so we can use it as a differentiable loss function for
speech enhancement.
</summary>
    <author>
      <name>Dan Elbaz</name>
    </author>
    <author>
      <name>Michael Zibulevsky</name>
    </author>
    <link href="http://arxiv.org/abs/1708.05987v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.05987v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.01922v1</id>
    <updated>2017-09-06T12:44:01Z</updated>
    <published>2017-09-06T12:44:01Z</published>
    <title>A Comparison on Audio Signal Preprocessing Methods for Deep Neural
  Networks on Music Tagging</title>
    <summary>  Deep neural networks (DNN) have been successfully applied for music
classification tasks including music tagging. In this paper, we investigate the
effect of audio preprocessing on music tagging with neural networks. We perform
comprehensive experiments involving audio preprocessing using different
time-frequency representations, logarithmic magnitude compression, frequency
weighting and scaling. We show that many commonly used input preprocessing
techniques are redundant except magnitude compression.
</summary>
    <author>
      <name>Keunwoo Choi</name>
    </author>
    <author>
      <name>George Fazekas</name>
    </author>
    <author>
      <name>Kyunghyun Cho</name>
    </author>
    <author>
      <name>Mark Sandler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, will be submitted to ICASSP 2017. arXiv admin note:
  substantial text overlap with arXiv:1706.02361</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.01922v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.01922v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.02076v1</id>
    <updated>2017-09-07T05:39:00Z</updated>
    <published>2017-09-07T05:39:00Z</published>
    <title>Composition by Conversation</title>
    <summary>  Most musical programming languages are developed purely for coding virtual
instruments or algorithmic compositions. Although there has been some work in
the domain of musical query languages for music information retrieval, there
has been little attempt to unify the principles of musical programming and
query languages with cognitive and natural language processing models that
would facilitate the activity of composition by conversation. We present a
prototype framework, called MusECI, that merges these domains, permitting
score-level algorithmic composition in a text editor while also supporting
connectivity to existing natural language processing frameworks.
</summary>
    <author>
      <name>Donya Quick</name>
    </author>
    <author>
      <name>Clayton T. Morrison</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 8 figures, accepted to ICMC 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.02076v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.02076v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.1; H.5.5; I.2.4; I.2.5; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.07541v1</id>
    <updated>2017-09-21T23:23:49Z</updated>
    <published>2017-09-21T23:23:49Z</published>
    <title>A fundamental frequency estimation method for tonal sounds inspired on
  bird song studies</title>
    <summary>  A fast implementation of fundamental frequency estimation is presented in
this work. The algorithm is based on a frequency-domain approach. It was mainly
develop for tonal sounds and used in Canary bird song analysis. The method was
implemented but not restricted for this kind of data. It could be easily
adapted for other proposes. Python libraries were used to develop a code with a
simple algorithm to obtain fundamental frequency. A simple open source code is
provided in the local university repository.
</summary>
    <author>
      <name>C. Jarne</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">pre-print version of fundamental frequency estimation method for
  tonal sounds</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.07541v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.07541v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.07908v1</id>
    <updated>2017-09-20T20:45:53Z</updated>
    <published>2017-09-20T20:45:53Z</published>
    <title>Neural Network Alternatives to Convolutive Audio Models for Source
  Separation</title>
    <summary>  Convolutive Non-Negative Matrix Factorization model factorizes a given audio
spectrogram using frequency templates with a temporal dimension. In this paper,
we present a convolutional auto-encoder model that acts as a neural network
alternative to convolutive NMF. Using the modeling flexibility granted by
neural networks, we also explore the idea of using a Recurrent Neural Network
in the encoder. Experimental results on speech mixtures from TIMIT dataset
indicate that the convolutive architecture provides a significant improvement
in separation performance in terms of BSSeval metrics.
</summary>
    <author>
      <name>Shrikant Venkataramani</name>
    </author>
    <author>
      <name>Y. Cem Subakan</name>
    </author>
    <author>
      <name>Paris Smaragdis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in MLSP 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.07908v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.07908v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.08243v1</id>
    <updated>2017-09-24T19:23:22Z</updated>
    <published>2017-09-24T19:23:22Z</published>
    <title>A Hybrid DSP/Deep Learning Approach to Real-Time Full-Band Speech
  Enhancement</title>
    <summary>  Despite noise suppression being a mature area in signal processing, it
remains highly dependent on fine tuning of estimator algorithms and parameters.
In this paper, we demonstrate a hybrid DSP/deep learning approach to noise
suppression. A deep neural network is used to estimate ideal critical band
gains, while a more traditional pitch filter attenuates noise between pitch
harmonics. The approach achieves significantly higher quality than a
traditional minimum mean squared error spectral estimator, while keeping the
complexity low enough for real-time operation at 48 kHz on a low-power
processor.
</summary>
    <author>
      <name>Jean-Marc Valin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.08243v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.08243v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.2654v1</id>
    <updated>2012-01-12T19:43:06Z</updated>
    <published>2012-01-12T19:43:06Z</published>
    <title>Musical Modes, Their Associated Chords and Their Musicality</title>
    <summary>  In this paper we present a mathematical way of defining musical modes and we
define the musicality of a mode as a product of three different factors. We
conclude by classifying the modes which are most musical according to our
definition.
</summary>
    <author>
      <name>Mihail Cocos</name>
    </author>
    <author>
      <name>Kent Kidman</name>
    </author>
    <link href="http://arxiv.org/abs/1201.2654v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.2654v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0707.0514v1</id>
    <updated>2007-07-03T22:41:11Z</updated>
    <published>2007-07-03T22:41:11Z</published>
    <title>Phase space methods and psychoacoustic models in lossy transform coding</title>
    <summary>  I present a method for lossy transform coding of digital audio that uses the
Weyl symbol calculus for constructing the encoding and decoding transformation.
The method establishes a direct connection between a time-frequency
representation of the signal dependent threshold of masked noise and the
encode/decode pair. The formalism also offers a time-frequency measure of
perceptual entropy.
</summary>
    <author>
      <name>Matthew Charles Cargo</name>
    </author>
    <link href="http://arxiv.org/abs/0707.0514v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0707.0514v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.1.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.4501v1</id>
    <updated>2008-09-25T20:54:29Z</updated>
    <published>2008-09-25T20:54:29Z</published>
    <title>Audio Classification from Time-Frequency Texture</title>
    <summary>  Time-frequency representations of audio signals often resemble texture
images. This paper derives a simple audio classification algorithm based on
treating sound spectrograms as texture images. The algorithm is inspired by an
earlier visual classification scheme particularly efficient at classifying
textures. While solely based on time-frequency texture features, the algorithm
achieves surprisingly good performance in musical instrument classification
experiments.
</summary>
    <author>
      <name>Guoshen Yu</name>
    </author>
    <author>
      <name>Jean-Jacques Slotine</name>
    </author>
    <link href="http://arxiv.org/abs/0809.4501v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.4501v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.4269v1</id>
    <updated>2012-02-20T09:44:47Z</updated>
    <published>2012-02-20T09:44:47Z</published>
    <title>Live-Musikprogrammierung in Haskell</title>
    <summary>  We aim to compose algorithmic music in an interactive way with multiple
participants. To this end we develop an interpreter for a sub-language of the
non-strict functional programming language Haskell that allows to modify the
program during its execution. Our system can be used both for musical
live-coding and for demonstration and education of functional programming.
</summary>
    <author>
      <name>Henning Thielemann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 2 figures, 5. Arbeitstagung Programmiersprachen 2012,
  ATPS'12</arxiv:comment>
    <link href="http://arxiv.org/abs/1202.4269v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.4269v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.5091v1</id>
    <updated>2012-12-19T17:40:07Z</updated>
    <published>2012-12-19T17:40:07Z</published>
    <title>Maximally Informative Observables and Categorical Perception</title>
    <summary>  We formulate the problem of perception in the framework of information
theory, and prove that categorical perception is equivalent to the existence of
an observable that has the maximum possible information on the target of
perception. We call such an observable maximally informative. Regardless
whether categorical perception is real, maximally informative observables can
form the basis of a theory of perception. We conclude with the implications of
such a theory for the problem of speech perception.
</summary>
    <author>
      <name>Elaine Tsiang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.5091v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.5091v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.5768v1</id>
    <updated>2013-03-22T20:02:38Z</updated>
    <published>2013-03-22T20:02:38Z</published>
    <title>Live music programming in Haskell</title>
    <summary>  We aim for composing algorithmic music in an interactive way with multiple
participants. To this end we have developed an interpreter for a sub-language
of the non-strict functional programming language Haskell that allows the
modification of a program during its execution. Our system can be used both for
musical live-coding and for demonstration and education of functional
programming.
</summary>
    <author>
      <name>Henning Thielemann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 2 figures, Linux Audio Conference 2013. This is a
  translation and update of the ATPS-2012 paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1303.5768v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.5768v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.7746v1</id>
    <updated>2014-03-30T12:22:36Z</updated>
    <published>2014-03-30T12:22:36Z</published>
    <title>Multi-label Ferns for Efficient Recognition of Musical Instruments in
  Recordings</title>
    <summary>  In this paper we introduce multi-label ferns, and apply this technique for
automatic classification of musical instruments in audio recordings. We compare
the performance of our proposed method to a set of binary random ferns, using
jazz recordings as input data. Our main result is obtaining much faster
classification and higher F-score. We also achieve substantial reduction of the
model size.
</summary>
    <author>
      <name>Miron B. Kursa</name>
    </author>
    <author>
      <name>Alicja A. Wieczorkowska</name>
    </author>
    <link href="http://arxiv.org/abs/1403.7746v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.7746v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.06095v1</id>
    <updated>2015-09-21T02:28:44Z</updated>
    <published>2015-09-21T02:28:44Z</published>
    <title>Multilayer bootstrap network for unsupervised speaker recognition</title>
    <summary>  We apply multilayer bootstrap network (MBN), a recent proposed unsupervised
learning method, to unsupervised speaker recognition. The proposed method first
extracts supervectors from an unsupervised universal background model, then
reduces the dimension of the high-dimensional supervectors by multilayer
bootstrap network, and finally conducts unsupervised speaker recognition by
clustering the low-dimensional data. The comparison results with 2 unsupervised
and 1 supervised speaker recognition techniques demonstrate the effectiveness
and robustness of the proposed method.
</summary>
    <author>
      <name>Xiao-Lei Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1509.06095v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.06095v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.01058v1</id>
    <updated>2016-12-04T03:36:51Z</updated>
    <published>2016-12-04T03:36:51Z</published>
    <title>Algorithmic Songwriting with ALYSIA</title>
    <summary>  This paper introduces ALYSIA: Automated LYrical SongwrIting Application.
ALYSIA is based on a machine learning model using Random Forests, and we
discuss its success at pitch and rhythm prediction. Next, we show how ALYSIA
was used to create original pop songs that were subsequently recorded and
produced. Finally, we discuss our vision for the future of Automated
Songwriting for both co-creative and autonomous systems.
</summary>
    <author>
      <name>Margareta Ackerman</name>
    </author>
    <author>
      <name>David Loker</name>
    </author>
    <link href="http://arxiv.org/abs/1612.01058v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.01058v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.08675v1</id>
    <updated>2017-06-27T05:28:06Z</updated>
    <published>2017-06-27T05:28:06Z</published>
    <title>Proceedings of the First International Workshop on Deep Learning and
  Music</title>
    <summary>  Proceedings of the First International Workshop on Deep Learning and Music,
joint with IJCNN, Anchorage, US, May 17-18, 2017
</summary>
    <author>
      <name>Dorien Herremans</name>
    </author>
    <author>
      <name>Ching-Hua Chuan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.13140/RG.2.2.22227.99364/1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.13140/RG.2.2.22227.99364/1" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the First International Workshop on Deep Learning
  and Music, joint with IJCNN, Anchorage, US, May 17-18, 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1706.08675v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.08675v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3; H.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.08928v1</id>
    <updated>2017-06-27T16:25:00Z</updated>
    <published>2017-06-27T16:25:00Z</published>
    <title>Classical Music Clustering Based on Acoustic Features</title>
    <summary>  In this paper we cluster 330 classical music pieces collected from MusicNet
database based on their musical note sequence. We use shingling and chord
trajectory matrices to create signature for each music piece and performed
spectral clustering to find the clusters. Based on different resolution, the
output clusters distinctively indicate composition from different classical
music era and different composing style of the musicians.
</summary>
    <author>
      <name>Xindi Wang</name>
    </author>
    <author>
      <name>Syed Arefinul Haque</name>
    </author>
    <link href="http://arxiv.org/abs/1706.08928v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.08928v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.05193v1</id>
    <updated>2017-09-14T07:24:08Z</updated>
    <published>2017-09-14T07:24:08Z</published>
    <title>Clustering of Musical Pieces through Complex Networks: an Assessment
  over Guitar Solos</title>
    <summary>  Musical pieces can be modeled as complex networks. This fosters innovative
ways to categorize music, paving the way towards novel applications in
multimedia domains, such as music didactics, multimedia entertainment and
digital music generation. Clustering these networks through their main metrics
allows grouping similar musical tracks. To show the viability of the approach,
we provide results on a dataset of guitar solos.
</summary>
    <author>
      <name>Stefano Ferretti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear in IEEE Multimedia magazine</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.05193v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.05193v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0006026v2</id>
    <updated>2000-06-21T15:36:29Z</updated>
    <published>2000-06-12T06:08:57Z</published>
    <title>Online Correction of Dispersion Error in 2D Waveguide Meshes</title>
    <summary>  An elastic ideal 2D propagation medium, i.e., a membrane, can be simulated by
models discretizing the wave equation on the time-space grid (finite difference
methods), or locally discretizing the solution of the wave equation (waveguide
meshes). The two approaches provide equivalent computational structures, and
introduce numerical dispersion that induces a misalignment of the modes from
their theoretical positions. Prior literature shows that dispersion can be
arbitrarily reduced by oversizing and oversampling the mesh, or by adpting
offline warping techniques. In this paper we propose to reduce numerical
dispersion by embedding warping elements, i.e., properly tuned allpass filters,
in the structure. The resulting model exhibits a significant reduction in
dispersion, and requires less computational resources than a regular mesh
structure having comparable accuracy.
</summary>
    <author>
      <name>Federico Fontana</name>
    </author>
    <author>
      <name>Davide Rocchesso</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 5 figures, to appear in the Proceedings of the International
  Computer Music Conference, 2000. Corrected first reference</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0006026v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0006026v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0007006v1</id>
    <updated>2000-07-05T18:20:51Z</updated>
    <published>2000-07-05T18:20:51Z</published>
    <title>DISCO: An object-oriented system for music composition and sound design</title>
    <summary>  This paper describes an object-oriented approach to music composition and
sound design. The approach unifies the processes of music making and instrument
building by using similar logic, objects, and procedures. The composition
modules use an abstract representation of musical data, which can be easily
mapped onto different synthesis languages or a traditionally notated score. An
abstract base class is used to derive classes on different time scales. Objects
can be related to act across time scales, as well as across an entire piece,
and relationships between similar objects can replicate traditional music
operations or introduce new ones. The DISCO (Digital Instrument for
Sonification and Composition) system is an open-ended work in progress.
</summary>
    <author>
      <name>Hans G. Kaper</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Argonne National Laboratory</arxiv:affiliation>
    </author>
    <author>
      <name>Sever Tipei</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Illinois at Urbana-Champaign</arxiv:affiliation>
    </author>
    <author>
      <name>Jeff M. Wright</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Illinois at Urbana-Champaign</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, no figures; to be published in Proc. Int'l Computer Music
  Conference 2000 (Berlin, August 2000)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0007006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0007006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0007007v1</id>
    <updated>2000-07-05T21:26:48Z</updated>
    <published>2000-07-05T21:26:48Z</published>
    <title>Data sonification and sound visualization</title>
    <summary>  This article describes a collaborative project between researchers in the
Mathematics and Computer Science Division at Argonne National Laboratory and
the Computer Music Project of the University of Illinois at Urbana-Champaign.
The project focuses on the use of sound for the exploration and analysis of
complex data sets in scientific computing. The article addresses digital sound
synthesis in the context of DIASS (Digital Instrument for Additive Sound
Synthesis) and sound visualization in a virtual-reality environment by means of
M4CAVE. It describes the procedures and preliminary results of some experiments
in scientific sonification and sound visualization.
</summary>
    <author>
      <name>Hans G. Kaper</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Argonne National Laboratory</arxiv:affiliation>
    </author>
    <author>
      <name>Sever Tipei</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Argonne National Laboratory</arxiv:affiliation>
    </author>
    <author>
      <name>Elizabeth Wiebel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Argonne National Laboratory</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 5 figures, 3 tables; preprint of the published paper
  (differing in details)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computing in Science and Engineering, Vol. 1 No. 4, July-August
  1999, pp. 48-58</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0007007v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0007007v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0010017v2</id>
    <updated>2001-01-08T07:48:08Z</updated>
    <published>2000-10-10T13:08:28Z</published>
    <title>Generalization of a 3-D resonator model for the simulation of spherical
  enclosures</title>
    <summary>  A rectangular enclosure has such an even distribution of resonances that it
can be accurately and efficiently modelled using a feedback delay network.
Conversely, a non rectangular shape such as a sphere has a distribution of
resonances that challenges the construction of an efficient model. This work
proposes an extension of the already known feedback delay network structure to
model the resonant properties of a sphere. A specific frequency distribution of
resonances can be approximated, up to a certain frequency, by inserting an
allpass filter of moderate order after each delay line of a feedback delay
network. The structure used for rectangular boxes is therefore augmented with a
set of allpass filters allowing parametric control over the enclosure size and
the boundary properties. This work was motivated by informal listening tests
which have shown that it is possible to identify a basic shape just from the
distribution of its audible resonances.
</summary>
    <author>
      <name>Davide Rocchesso</name>
    </author>
    <author>
      <name>Pierre Dutilleux</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1155/S1110865701000105</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1155/S1110865701000105" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">39 pages, 16 figures, 6 tables. Accepted for publication in Applied
  Signal Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0010017v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0010017v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0103005v1</id>
    <updated>2001-03-05T04:41:12Z</updated>
    <published>2001-03-05T04:41:12Z</published>
    <title>Source-Filter Decomposition of Harmonic Sounds</title>
    <summary>  This paper describes a method for decomposing steady-state instrument data
into excitation and formant filter components. The input data, taken from
several series of recordings of acoustical instruments is analyzed in the
frequency domain, and for each series a model is built, which most accurately
represents the data as a source-filter system. The source part is taken to be a
harmonic excitation system with frequency-invariant magnitudes, and the filter
part is considered to be responsible for all spectral inhomogenieties. This
method has been applied to the SHARC database of steady state instrument data
to create source-filter models for a large number of acoustical instruments.
Subsequent use of such models can have a wide variety of applications,
including improvements to wavetable and physical modeling synthesis, high
quality pitch shifting, and creation of "hybrid" instrument timbres.
</summary>
    <author>
      <name>Ilia Bisnovatyi</name>
    </author>
    <author>
      <name>Michael J. O'Donnell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preliminary results reported at DAFx-99, in "Decomposition of Steady
  State Instrument Data into Excitation System and Formant Filter Components",
  http://www.tele.ntnu.no/akustikk/meetings/DAFx99/bisnovatyi.pdf 5 pages + 2
  appendices (6 graphs, 1 table)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0103005v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0103005v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H 5.5; G 1.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0103006v1</id>
    <updated>2001-03-05T05:05:15Z</updated>
    <published>2001-03-05T05:05:15Z</published>
    <title>Flexible Software Framework for Modal Synthesis</title>
    <summary>  Modal synthesis is an important area of physical modeling whose exploration
in the past has been held back by a large number of control parameters, the
scarcity of general-purpose design tools and the difficulty of obtaining the
computational power required for real-time synthesis. This paper presents an
overview of a flexible software framework facilitating the design and control
of instruments based on modal synthesis. The framework is designed as a
hierarchy of polymorphic synthesis objects, representing modal structures of
various complexity. As a method of generalizing all interactions among the
elements of a modal system, an abstract notion of {\it energy} is introduced,
and a set of energy transfer functions is provided. Such abstraction leads to a
design where the dynamics of interactions can be largely separated from the
specifics of particular modal structures, yielding an easily configurable and
expandable system. A real-time version of the framework has been implemented as
a set of C++ classes along with an integrating shell and a GUI, and is
currently being used to design and play modal instruments, as well as to survey
fundamental properties of various modal algorithms.
</summary>
    <author>
      <name>Ilia Bisnovatyi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at DAFx00,
  http://profs.sci.univr.it/~dafx/DAFx-final-papers.html</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">in Proceedings of COST-G6 Conference on Digital Audio Effects
  (DAFx-00), Dec 7-9, 2000, Verona, Italy</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0103006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0103006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H 5.5; I 6.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0203015v1</id>
    <updated>2002-03-12T09:21:39Z</updated>
    <published>2002-03-12T09:21:39Z</published>
    <title>Towards Experimental Nanosound Using Almost Disjoint Set Theory</title>
    <summary>  Music composition using digital audio sequence editors is increasingly
performed in a visual workspace where sound complexes are built from discrete
sound objects, called gestures that are arranged in time and space to generate
a continuous composition. The visual workspace, common to most industry
standard audio loop sequencing software, is premised on the arrangement of
gestures defined with geometric shape properties. Here, one aspect of fractal
set theory was validated using audio-frequency sets to evaluate self-affine
scaling behavior when new sound complexes are built through union and
intersection operations on discrete musical gestures. Results showed that
intersection of two sets revealed lower complexity compared with the union
operator, meaning that the intersection of two sound gestures is an almost
disjoint set, and in accord with formal logic. These results are also discussed
with reference to fuzzy sets, cellular automata, nanotechnology and
self-organization to further explore the link between sequenced notation and
complexity.
</summary>
    <author>
      <name>Cameron L Jones</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 4 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0203015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0203015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0410027v1</id>
    <updated>2004-10-13T02:28:10Z</updated>
    <published>2004-10-13T02:28:10Z</published>
    <title>Detecting User Engagement in Everyday Conversations</title>
    <summary>  This paper presents a novel application of speech emotion recognition:
estimation of the level of conversational engagement between users of a voice
communication system. We begin by using machine learning techniques, such as
the support vector machine (SVM), to classify users' emotions as expressed in
individual utterances. However, this alone fails to model the temporal and
interactive aspects of conversational engagement. We therefore propose the use
of a multilevel structure based on coupled hidden Markov models (HMM) to
estimate engagement levels in continuous natural speech. The first level is
comprised of SVM-based classifiers that recognize emotional states, which could
be (e.g.) discrete emotion types or arousal/valence levels. A high-level HMM
then uses these emotional states as input, estimating users' engagement in
conversation by decoding the internal states of the HMM. We report experimental
results obtained by applying our algorithms to the LDC Emotional Prosody and
CallFriend speech corpora.
</summary>
    <author>
      <name>Chen Yu</name>
    </author>
    <author>
      <name>Paul M. Aoki</name>
    </author>
    <author>
      <name>Allison Woodruff</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages (A4), 1 figure (EPS)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. 8th Int'l Conf. on Spoken Language Processing (ICSLP) (Vol.
  2), Jeju Island, Republic of Korea, Oct. 2004, 1329-1332. ISCA.</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0410027v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0410027v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.4; I.2.7; H.5.2; H.4.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0612138v1</id>
    <updated>2006-12-28T06:39:55Z</updated>
    <published>2006-12-28T06:39:55Z</published>
    <title>Accommodating Sample Size Effect on Similarity Measures in Speaker
  Clustering</title>
    <summary>  We investigate the symmetric Kullback-Leibler (KL2) distance in speaker
clustering and its unreported effects for differently-sized feature matrices.
Speaker data is represented as Mel Frequency Cepstral Coefficient (MFCC)
vectors, and features are compared using the KL2 metric to form clusters of
speech segments for each speaker. We make two observations with respect to
clustering based on KL2: 1.) The accuracy of clustering is strongly dependent
on the absolute lengths of the speech segments and their extracted feature
vectors. 2.) The accuracy of the similarity measure strongly degrades with the
length of the shorter of the two speech segments. These effects of length can
be attributed to the measure of covariance used in KL2. We demonstrate an
empirical correction of this sample-size effect that increases clustering
accuracy. We draw parallels to two Vector Quantization-based (VQ) similarity
measures, one which exhibits an equivalent effect of sample size, and the
second being less influenced by it.
</summary>
    <author>
      <name>Alexander Haubold</name>
    </author>
    <author>
      <name>John R. Kender</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0612138v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0612138v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; H.5.1; H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0705.4654v1</id>
    <updated>2007-05-31T17:19:17Z</updated>
    <published>2007-05-31T17:19:17Z</published>
    <title>Local Area Damage Detection in Composite Structures Using Piezoelectric
  Transducers</title>
    <summary>  An integrated and automated smart structures approach for structural health
monitoring is presented, utilizing an array of piezoelectric transducers
attached to or embedded within the structure for both actuation and sensing.
The system actively interrogates the structure via broadband excitation of
multiple actuators across a desired frequency range. The structure's vibration
signature is then characterized by computing the transfer functions between
each actuator/sensor pair, and compared to the baseline signature. Experimental
results applying the system to local area damage detection in a MD Explorer
rotorcraft composite flexbeam are presented.
</summary>
    <author>
      <name>Peter F. Lichtenwalner</name>
    </author>
    <author>
      <name>Donald A. Sofge</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1117/12.310667</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1117/12.310667" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">P.F. Lichtenwalner and D. Sofge, "Local Area Damage Detection in
  Composite Structures Using Piezoelectric Transducers," In Proc. SPIE Sym. on
  Smart Structures and Materials, Vol. 3326, SPIE, pp. 509-515, 1998</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0705.4654v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0705.4654v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0804.3241v2</id>
    <updated>2013-11-23T09:08:26Z</updated>
    <published>2008-04-21T06:32:23Z</published>
    <title>A Synthesizer Based on Frequency-Phase Analysis and Square Waves</title>
    <summary>  This article introduces an effective generalization of the polar flavor of
the Fourier Theorem based on a new method of analysis. Under the premises of
the new theory an ample class of functions become viable as bases, with the
further advantage of using the same basis for analysis and reconstruction. In
fact other tools, like the wavelets, admit specially built nonorthogonal bases
but require different bases for analysis and reconstruction (biorthogonal and
dual bases) and vectorial coordinates; this renders those systems unintuitive
and computing intensive. As an example of the advantages of the new
generalization of the Fourier Theorem, this paper introduces a novel method for
the synthesis that is based on frequency-phase series of square waves (the
equivalent of the polar Fourier Theorem but for nonorthogonal bases). The
resulting synthesizer is very efficient needing only few components, frugal in
terms of computing needs, and viable for many applications.
</summary>
    <author>
      <name>Sossio Vergara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages. Digital Signal Processing Journal 22 (2012)</arxiv:comment>
    <link href="http://arxiv.org/abs/0804.3241v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0804.3241v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.3080v1</id>
    <updated>2009-03-18T03:33:48Z</updated>
    <published>2009-03-18T03:33:48Z</published>
    <title>A Unified Theory of Time-Frequency Reassignment</title>
    <summary>  Time-frequency representations such as the spectrogram are commonly used to
analyze signals having a time-varying distribution of spectral energy, but the
spectrogram is constrained by an unfortunate tradeoff between resolution in
time and frequency. A method of achieving high-resolution spectral
representations has been independently introduced by several parties. The
technique has been variously named reassignment and remapping, but while the
implementations have differed in details, they are all based on the same
theoretical and mathematical foundation. In this work, we present a brief
history of work on the method we will call the method of time-frequency
reassignment, and present a unified mathematical description of the technique
and its derivation. We will focus on the development of time-frequency
reassignment in the context of the spectrogram, and conclude with a discussion
of some current applications of the reassigned spectrogram.
</summary>
    <author>
      <name>Kelly R. Fitz</name>
    </author>
    <author>
      <name>Sean A. Fulop</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">38 pages, 13 figures, draft of paper submitted to Digital Signal
  Processing (Elsevier) in 2005, still in review</arxiv:comment>
    <link href="http://arxiv.org/abs/0903.3080v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.3080v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.3220v1</id>
    <updated>2009-07-18T13:25:59Z</updated>
    <published>2009-07-18T13:25:59Z</published>
    <title>Inter Genre Similarity Modelling For Automatic Music Genre
  Classification</title>
    <summary>  Music genre classification is an essential tool for music information
retrieval systems and it has been finding critical applications in various
media platforms. Two important problems of the automatic music genre
classification are feature extraction and classifier design. This paper
investigates inter-genre similarity modelling (IGS) to improve the performance
of automatic music genre classification. Inter-genre similarity information is
extracted over the mis-classified feature population. Once the inter-genre
similarity is modelled, elimination of the inter-genre similarity reduces the
inter-genre confusion and improves the identification rates. Inter-genre
similarity modelling is further improved with iterative IGS modelling(IIGS) and
score modelling for IGS elimination(SMIGS). Experimental results with promising
classification improvements are provided.
</summary>
    <author>
      <name>Ulas Bagci</name>
    </author>
    <author>
      <name>Engin Erzin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Dafx 2006 submission</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">9th International Conference on Digital Audio Effects (DAFx-06),
  Montreal, Canada, September 18-20, 2006</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0907.3220v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.3220v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5; I.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.0599v1</id>
    <updated>2009-09-03T08:49:54Z</updated>
    <published>2009-09-03T08:49:54Z</published>
    <title>Codebook Design Method for Noise Robust Speaker Identification based on
  Genetic Algorithm</title>
    <summary>  In this paper, a novel method of designing a codebook for noise robust
speaker identification purpose utilizing Genetic Algorithm has been proposed.
Wiener filter has been used to remove the background noises from the source
speech utterances. Speech features have been extracted using standard speech
parameterization method such as LPC, LPCC, RCC, MFCC, (delta)MFCC and
(delta)(delta) MFCC. For each of these techniques, the performance of the
proposed system has been compared. In this codebook design method, Genetic
Algorithm has the capability of getting global optimal result and hence
improves the quality of the codebook. Comparing with the NOIZEOUS speech
database, the experimental result shows that 79.62 percent accuracy has been
achieved.
</summary>
    <author>
      <name>Md. Rabiul Islam</name>
    </author>
    <author>
      <name>Md. Fayzur Rahman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 Pages IEEE format, International Journal of Computer Science and
  Information Security, IJCSIS 2009, ISSN 1947 5500, Impact factor
  0.423,http://sites.google.com/site/ijcsis/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science and Information
  Security, IJCSIS, Vol. 4, No. 1 &amp; 2, August 2009, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0909.0599v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.0599v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.2363v1</id>
    <updated>2009-09-12T20:36:35Z</updated>
    <published>2009-09-12T20:36:35Z</published>
    <title>Improvement of Text Dependent Speaker Identification System Using
  Neuro-Genetic Hybrid Algorithm in Office Environmental Conditions</title>
    <summary>  In this paper, an improved strategy for automated text dependent speaker
identification system has been proposed in noisy environment. The
identification process incorporates the Neuro- Genetic hybrid algorithm with
cepstral based features. To remove the background noise from the source
utterance, wiener filter has been used. Different speech pre-processing
techniques such as start-end point detection algorithm, pre-emphasis filtering,
frame blocking and windowing have been used to process the speech utterances.
RCC, MFCC, MFCC, MFCC, LPC and LPCC have been used to extract the features.
After feature extraction of the speech, Neuro-Genetic hybrid algorithm has been
used in the learning and identification purposes. Features are extracted by
using different techniques to optimize the performance of the identification.
According to the VALID speech database, the highest speaker identification rate
of 100.000 percent for studio environment and 82.33 percent for office
environmental conditions have been achieved in the close set text dependent
speaker identification system.
</summary>
    <author>
      <name>Md. Rabiul Islam</name>
    </author>
    <author>
      <name>Md. Fayzur Rahman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science Issues (IJCSI), Volume 1,
  pp42-48, August 2009</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">R. Islam and F. Rahman," International Journal of Computer Science
  Issues (IJCSI), Volume 1, pp42-48,August 2009"</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0909.2363v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.2363v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0911.5171v1</id>
    <updated>2009-11-26T23:59:44Z</updated>
    <published>2009-11-26T23:59:44Z</published>
    <title>Untangling Phase and Time in Monophonic Sounds</title>
    <summary>  We are looking for a mathematical model of monophonic sounds with independent
time and phase dimensions. With such a model we can resynthesise a sound with
arbitrarily modulated frequency and progress of the timbre. We propose such a
model and show that it exactly fulfils some natural properties, like a kind of
time-invariance, robustness against non-harmonic frequencies, envelope
preservation, and inclusion of plain resampling as a special case. The
resulting algorithm is efficient and allows to process data in a streaming
manner with phase and shape modulation at sample rate, what we demonstrate with
an implementation in the functional language Haskell. It allows a wide range of
applications, namely pitch shifting and time scaling, creative FM synthesis
effects, compression of monophonic sounds, generating loops for sampled sounds,
synthesise sounds similar to wavetable synthesis, or making ultrasound audible.
</summary>
    <author>
      <name>Henning Thielemann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4236/jsip.2010.11001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4236/jsip.2010.11001" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 22 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0911.5171v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.5171v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.2441v1</id>
    <updated>2010-03-11T23:00:15Z</updated>
    <published>2010-03-11T23:00:15Z</published>
    <title>Up-sampling and Natural Sample Value Computation for Digital Pulse Width
  Modulators</title>
    <summary>  Digital pulse width modulation has been considered for high-fidelity and
high-efficiency audio amplifiers for several years. It has been shown that the
distortion can be reduced and the implementation of the system can be
simplified if the switching frequency is much higher than the Nyquist rate of
the modulating waveform. Hence, the input digital source is normally upsampled
to a higher frequency. It was also proved that converting uniform samples to
natural samples will decrease the harmonic distortion. Thus, in this paper, we
examine a new approach that combines upsampling, digital interpolation and
natural sampling conversion. This approach uses poly-phase implementation of
the digital interpolation filter and digital differentiators. We will show that
the structure consists of an FIR-type linear stage and a nonlinear stage. Some
spectral simulation results of a pulse width modulation system based on this
approach will also be presented. Finally, we will discuss the improvement of
the new approach over old algorithms.
</summary>
    <author>
      <name>Kien C. Nguyen</name>
    </author>
    <author>
      <name>Dilip V. Sarwate</name>
    </author>
    <link href="http://arxiv.org/abs/1003.2441v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.2441v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.4908v1</id>
    <updated>2010-03-25T14:24:52Z</updated>
    <published>2010-03-25T14:24:52Z</published>
    <title>Perceptual analyses of action-related impact sounds</title>
    <summary>  Among environmental sounds, we have chosen to study a class of action-related
impact sounds: automobile door closure sounds. We propose to describe these
sounds using a model composed of perceptual properties. The development of the
perceptual model was derived from the evaluation of many door closure sounds
measured under controlled laboratory listening conditions. However, listening
to such sounds normally occurs within a natural context, which probably
modifies their perception. We therefore need to study differences between the
real situation and the laboratory situation by following standard practices in
order to specify the precise listening conditions and observe the influence of
previous learning, expectations, action-perception interactions, and attention
given to sounds. Our process consists in doing in situ experiments that are
compared with specific laboratory experiments in order to isolate certain
influential, context dependent components.
</summary>
    <author>
      <name>Marie-Céline Bezat</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PSA</arxiv:affiliation>
    </author>
    <author>
      <name>Vincent Roussarie</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PSA</arxiv:affiliation>
    </author>
    <author>
      <name>Kronland-Martinet Richard</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LMA</arxiv:affiliation>
    </author>
    <author>
      <name>Solvi Ystad</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LMA</arxiv:affiliation>
    </author>
    <author>
      <name>Stephen Mcadams</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Euronoise 2006, Tampere : France (2006)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1003.4908v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.4908v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.5623v1</id>
    <updated>2010-03-29T17:48:22Z</updated>
    <published>2010-03-29T17:48:22Z</published>
    <title>Spoken Language Identification Using Hybrid Feature Extraction Methods</title>
    <summary>  This paper introduces and motivates the use of hybrid robust feature
extraction technique for spoken language identification (LID) system. The
speech recognizers use a parametric form of a signal to get the most important
distinguishable features of speech signal for recognition task. In this paper
Mel-frequency cepstral coefficients (MFCC), Perceptual linear prediction
coefficients (PLP) along with two hybrid features are used for language
Identification. Two hybrid features, Bark Frequency Cepstral Coefficients
(BFCC) and Revised Perceptual Linear Prediction Coefficients (RPLP) were
obtained from combination of MFCC and PLP. Two different classifiers, Vector
Quantization (VQ) with Dynamic Time Warping (DTW) and Gaussian Mixture Model
(GMM) were used for classification. The experiment shows better identification
rate using hybrid feature extraction techniques compared to conventional
feature extraction methods.BFCC has shown better performance than MFCC with
both classifiers. RPLP along with GMM has shown best identification performance
among all feature extraction techniques.
</summary>
    <author>
      <name>Pawan Kumar</name>
    </author>
    <author>
      <name>Astik Biswas</name>
    </author>
    <author>
      <name>A . N. Mishra</name>
    </author>
    <author>
      <name>Mahesh Chandra</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Telecommunications, Volume 1, Issue 2, pp11-15, March
  2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1003.5623v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.5623v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.5627v1</id>
    <updated>2010-03-29T17:54:55Z</updated>
    <published>2010-03-29T17:54:55Z</published>
    <title>Wavelet-Based Mel-Frequency Cepstral Coefficients for Speaker
  Identification using Hidden Markov Models</title>
    <summary>  To improve the performance of speaker identification systems, an effective
and robust method is proposed to extract speech features, capable of operating
in noisy environment. Based on the time-frequency multi-resolution property of
wavelet transform, the input speech signal is decomposed into various frequency
channels. For capturing the characteristic of the signal, the Mel-Frequency
Cepstral Coefficients (MFCCs) of the wavelet channels are calculated. Hidden
Markov Models (HMMs) were used for the recognition stage as they give better
recognition for the speaker's features than Dynamic Time Warping (DTW).
Comparison of the proposed approach with the MFCCs conventional feature
extraction method shows that the proposed method not only effectively reduces
the influence of noise, but also improves recognition. A recognition rate of
99.3% was obtained using the proposed feature extraction technique compared to
98.7% using the MFCCs. When the test patterns were corrupted by additive white
Gaussian noise with 20 dB S/N ratio, the recognition rate was 97.3% using the
proposed method compared to 93.3% using the MFCCs.
</summary>
    <author>
      <name>Mahmoud I. Abdalla</name>
    </author>
    <author>
      <name>Hanaa S. Ali</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Telecommunications, Volume 1, Issue 2, pp16-21, March
  2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1003.5627v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.5627v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.0831v1</id>
    <updated>2010-06-04T09:58:27Z</updated>
    <published>2010-06-04T09:58:27Z</published>
    <title>Treatment the Effects of Studio Wall Resonance and Coincidence Phenomena
  for Recording Noisy Speech Via FPGA Digital Filter</title>
    <summary>  This work introduces an economic solution for the problems of sound
insulation of recording studios. Sound insulation at wall resonance frequency
is weak. Instead of acoustical treatment, a digital filter is used to eliminate
the effects of wall resonance and coincidence phenomena on recording of speech.
Sound insulation of studio is measured to calculate the wall resonance
frequency and the coincidence frequency. Pole /zero placement technique is used
to calculate the IIR filter coefficients. The digital filter is designed,
simulated and implemented. The proposed system is used to treat these problems
and it is shown to be effective in recording the noisy speech. In this work
digital signal processing is used instead of the acoustic treatment to
eliminate the effect of noise at the studio wall resonance. This technique is
cheap and effective in canceling the noise at the desired frequencies. Field
Programmable Gate Array (FPGA) is used for hardware implementation of the
proposed filter structure which provides fast and cheap solution for processing
real time audio signals. The implementation is carried out using Spartan chip
from Xinlinx achieving higher performance than commercially available software
solutions.
</summary>
    <author>
      <name>Mahmoud I. A. Abdalla</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Journal of Telecommunications, see
  http://sites.google.com/site/journaloftelecommunications/volume-2-issue-2-may-2010</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Telecommunications,Volume 2, Issue 2, pp42-48, May 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1006.0831v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.0831v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.0866v1</id>
    <updated>2010-06-04T11:20:08Z</updated>
    <published>2010-06-04T11:20:08Z</published>
    <title>A Study on the Interactive "HOPSCOTCH" Game for the Children Using
  Computer Music Techniques</title>
    <summary>  "Hopscotch" is a world-wide game for children to play since the times in the
ancient Roman Empire and China. Here we present a study mainly focused on the
research and discussion of the application on the children's well-know
edutainment via the physical interactive design to provide the sensing of the
times for the conventional hopscotch, which is a new type of experiment for the
technology aided edutainment. The innovated hopscotch music game involves the
sound samples of various animals and the characters of cartoon, and the
algorithmic composition via the development of the music technology based
interactive game, to gradually make the children perceive the world of digits,
sound, and music. It can guide the growing children's personality and character
from disorder into clarity. Furthermore, the traditional teaching materials can
be improved via the implementation of the electrical sensing devices,
electrical I/O module, and the computer music program Max/MSP, to integrate the
interactive computer music with the interactive and immersive soundscapes
composition, and the teaching tool with educational gaming is completely
accomplished eventually.
</summary>
    <author>
      <name>Shing-Kwei Tzeng</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Kainan University, Taiwan and</arxiv:affiliation>
    </author>
    <author>
      <name>Chih-Fang Huang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Yuan Ze University, Taiwan</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijma.2010.2203</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijma.2010.2203" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 Pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International journal of Multimedia &amp; Its Applications 2.2 (2010)
  32-44</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1006.0866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.0866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.5761v1</id>
    <updated>2010-09-29T03:20:40Z</updated>
    <published>2010-09-29T03:20:40Z</published>
    <title>Approximate Maximum A Posteriori Inference with Entropic Priors</title>
    <summary>  In certain applications it is useful to fit multinomial distributions to
observed data with a penalty term that encourages sparsity. For example, in
probabilistic latent audio source decomposition one may wish to encode the
assumption that only a few latent sources are active at any given time. The
standard heuristic of applying an L1 penalty is not an option when fitting the
parameters to a multinomial distribution, which are constrained to sum to 1. An
alternative is to use a penalty term that encourages low-entropy solutions,
which corresponds to maximum a posteriori (MAP) parameter estimation with an
entropic prior. The lack of conjugacy between the entropic prior and the
multinomial distribution complicates this approach. In this report I propose a
simple iterative algorithm for MAP estimation of multinomial distributions with
sparsity-inducing entropic priors.
</summary>
    <author>
      <name>Matthew D. Hoffman</name>
    </author>
    <link href="http://arxiv.org/abs/1009.5761v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.5761v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1101.1682v1</id>
    <updated>2011-01-09T23:02:52Z</updated>
    <published>2011-01-09T23:02:52Z</published>
    <title>Detecting gross alignment errors in the Spoken British National Corpus</title>
    <summary>  The paper presents methods for evaluating the accuracy of alignments between
transcriptions and audio recordings. The methods have been applied to the
Spoken British National Corpus, which is an extensive and varied corpus of
natural unscripted speech. Early results show good agreement with human ratings
of alignment accuracy. The methods also provide an indication of the location
of likely alignment problems; this should allow efficient manual examination of
large corpora. Automatic checking of such alignments is crucial when analysing
any very large corpus, since even the best current speech alignment systems
will occasionally make serious errors. The methods described here use a hybrid
approach based on statistics of the speech signal itself, statistics of the
labels being evaluated, and statistics linking the two.
</summary>
    <author>
      <name>Ladan Baghai-Ravary</name>
    </author>
    <author>
      <name>Sergio Grau</name>
    </author>
    <author>
      <name>Greg Kochanski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Four pages, 3 figures. Presented at "New Tools and Methods for
  Very-Large-Scale Phonetics Research", University of Pennsylvania, January
  28-31, 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1101.1682v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1101.1682v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1104.3544v2</id>
    <updated>2013-07-17T17:31:43Z</updated>
    <published>2011-03-29T22:29:39Z</published>
    <title>An automatic volume control for preserving intelligibility</title>
    <summary>  A new method has been developed to adjust volume automatically on all audio
devices equipped with at least one microphone, including mobile phones,
personal media players, headsets, and car radios, that might be used in noisy
environments, such as crowds, cars, and outdoors. The method uses a patented
set of algorithms, implemented on the chips in such devices, to preserve
constant intelligibility of speech in noisy environments, rather than constant
signal-to-noise ratio. The algorithms analyze the noise background in real time
and compensate only for fluctuating noise in the frequency domain and the time
domain that interferes with intelligibility of speech. Advantages of this
method of controlling volume include: Controlling volume without sacrificing
clarity; adjusting only for persistent speech-interference noise; smoothing
volume fluctuations; and eliminating static-like bursts caused by noise spikes.
Practical human-factors approaches to implementing these algorithms in mobile
phones are discussed.
</summary>
    <author>
      <name>Franklin Felber</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/SARNOF.2011.5876448</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/SARNOF.2011.5876448" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures, published in Proceedings of 34th IEEE Sarnoff
  Symposium, Princeton, NJ, 3-4 May 2011. Version 2 has typographical changes
  only</arxiv:comment>
    <link href="http://arxiv.org/abs/1104.3544v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.3544v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.2770v2</id>
    <updated>2011-05-16T19:29:53Z</updated>
    <published>2011-05-13T16:32:44Z</published>
    <title>Improving Performance of Speaker Identification System Using
  Complementary Information Fusion</title>
    <summary>  Feature extraction plays an important role as a front-end processing block in
speaker identification (SI) process. Most of the SI systems utilize like
Mel-Frequency Cepstral Coefficients (MFCC), Perceptual Linear Prediction (PLP),
Linear Predictive Cepstral Coefficients (LPCC), as a feature for representing
speech signal. Their derivations are based on short term processing of speech
signal and they try to capture the vocal tract information ignoring the
contribution from the vocal cord. Vocal cord cues are equally important in SI
context, as the information like pitch frequency, phase in the residual signal,
etc could convey important speaker specific attributes and are complementary to
the information contained in spectral feature sets. In this paper we propose a
novel feature set extracted from the residual signal of LP modeling.
Higher-order statistical moments are used here to find the nonlinear
relationship in residual signal. To get the advantages of complementarity vocal
cord based decision score is fused with the vocal tract based score. The
experimental results on two public databases show that fused mode system
outperforms single spectral features.
</summary>
    <author>
      <name>Md. Sahidullah</name>
    </author>
    <author>
      <name>Sandipan Chakroborty</name>
    </author>
    <author>
      <name>Goutam Saha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 Pages, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of 17th International Conference on Advanced Computing
  and Communications (ADCOM 2009) pp. 182-187 (2009)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1105.2770v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.2770v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.0760v1</id>
    <updated>2011-06-03T20:22:03Z</updated>
    <published>2011-06-03T20:22:03Z</published>
    <title>Simulating the Electroweak Phase Transition: Sonification of Bubble
  Nucleation</title>
    <summary>  As an applicaton of sonification, a simulation of the early universe was
developed to portray a phase transition that occurred shortly after the Big
Bang. The Standard Model of particle physics postulates that a hypothetical
particle, the Higgs boson, is responsible for the breaking of the symmetry
between the electromagnetic force and the weak force. This phase transition may
have been responsible for triggering Baryogenesis, the generation of an
abundance of matter over anti-matter. This hypothesis is known as Electroweak
Baryogenesis. In this simulation, aspects of bubble nucleation in Standard
Model Electroweak Baryogenesis were examined and modeled using Mathematica, and
sonified using SuperCollider3. The resulting simulation, which has been used
for pedagogical purposes by one of the authors, suggests interesting
possibilities for the integration of science and aesthetics as well as auditory
perception. The sonification component in particular also had the unexpected
benefit of being useful in debugging the Mathematica code.
</summary>
    <author>
      <name>R. Michael Winters</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">College of Wooster</arxiv:affiliation>
    </author>
    <author>
      <name>Andrew Blaikie</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">College of Wooster</arxiv:affiliation>
    </author>
    <author>
      <name>Deva O'Neil</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Bridgewater College</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 5 figures. To appear in Proc. ICAD 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1106.0760v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.0760v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2; H.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.0844v1</id>
    <updated>2011-06-04T18:10:09Z</updated>
    <published>2011-06-04T18:10:09Z</published>
    <title>A Fast Affine Projection Algorithm Based on Matching Pursuit in Adaptive
  Noise Cancellation for Speech Enhancement</title>
    <summary>  In many application of noise cancellation, the changes in signal
characteristics could be quite fast. This requires the utilization of adaptive
algorithms, which converge rapidly. Least Mean Squares (LMS) adaptive filters
have been used in a wide range of signal processing application. The Recursive
Least Squares (RLS) algorithm has established itself as the "ultimate" adaptive
filtering algorithm in the sense that it is the adaptive filter exhibiting the
best convergence behavior. Unfortunately, practical implementations of the
algorithm are often associated with high computational complexity and/or poor
numerical properties. Recently adaptive filtering was presented that was based
on Matching Pursuits, have a nice tradeoff between complexity and the
convergence speed. This paper describes a new approach for noise cancellation
in speech enhancement using the new adaptive filtering algorithm named fast
affine projection algorithm (FAPA). The simulation results demonstrate the good
performance of the FAPA in attenuating the noise.
</summary>
    <author>
      <name>Sayed A. Hadei</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Student Member, IEEE</arxiv:affiliation>
    </author>
    <author>
      <name>N. Sonbolestan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2010 International Conference on Intelligent Systems, Modelling and
  Simulation, Liverpool, UK</arxiv:comment>
    <link href="http://arxiv.org/abs/1106.0844v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.0844v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.0846v1</id>
    <updated>2011-06-04T18:25:34Z</updated>
    <published>2011-06-04T18:25:34Z</published>
    <title>A Family of Adaptive Filter Algorithms in Noise Cancellation for Speech
  Enhancement</title>
    <summary>  In many application of noise cancellation, the changes in signal
characteristics could be quite fast. This requires the utilization of adaptive
algorithms, which converge rapidly. Least Mean Squares (LMS) and Normalized
Least Mean Squares (NLMS) adaptive filters have been used in a wide range of
signal processing application because of its simplicity in computation and
implementation. The Recursive Least Squares (RLS) algorithm has established
itself as the "ultimate" adaptive filtering algorithm in the sense that it is
the adaptive filter exhibiting the best convergence behavior. Unfortunately,
practical implementations of the algorithm are often associated with high
computational complexity and/or poor numerical properties. Recently adaptive
filtering was presented, have a nice tradeoff between complexity and the
convergence speed. This paper describes a new approach for noise cancellation
in speech enhancement using the two new adaptive filtering algorithms named
fast affine projection algorithm and fast Euclidean direction search algorithms
for attenuating noise in speech signals. The simulation results demonstrate the
good performance of the two new algorithms in attenuating the noise.
</summary>
    <author>
      <name>Sayed. A. Hadei</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Student Member IEEE</arxiv:affiliation>
    </author>
    <author>
      <name>M. lotfizad</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer and Electrical Engineering, Vol. 2,
  No. 2, pp. 307-315, Apr. 2010. Singapore</arxiv:comment>
    <link href="http://arxiv.org/abs/1106.0846v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.0846v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.1199v1</id>
    <updated>2011-06-06T21:03:31Z</updated>
    <published>2011-06-06T21:03:31Z</published>
    <title>Open-loop multi-channel inversion of room impulse response</title>
    <summary>  This paper considers methods for audio display in a CAVE-type virtual reality
theater, a 3 m cube with displays covering all six rigid faces. Headphones are
possible since the user's headgear continuously measures ear positions, but
loudspeakers are preferable since they enhance the sense of total immersion.
The proposed solution consists of open-loop acoustic point control. The
transfer function, a matrix of room frequency responses from the loudspeakers
to the ears of the user, is inverted using multi-channel inversion methods, to
create exactly the desired sound field at the user's ears. The inverse transfer
function is constructed from impulse responses simulated by the image source
method. This technique is validated by measuring a 2x2 matrix transfer
function, simulating a transfer function with the same geometry, and filtering
the measured transfer function through the inverse of the simulation. Since
accuracy of the image source method decreases with time, inversion performance
is improved by windowing the simulated response prior to inversion. Parameters
of the simulation and inversion are adjusted to minimize residual reverberant
energy; the best-case dereverberation ratio is 10 dB.
</summary>
    <author>
      <name>Bowon Lee</name>
    </author>
    <author>
      <name>Camille Goudeseune</name>
    </author>
    <author>
      <name>Mark A. Hasegawa-Johnson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 24 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1106.1199v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.1199v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="76Q05" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5; G.2.3; I.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1107.4185v1</id>
    <updated>2011-07-21T07:17:34Z</updated>
    <published>2011-07-21T07:17:34Z</published>
    <title>Estimation of Severity of Speech Disability through Speech Envelope</title>
    <summary>  In this paper, envelope detection of speech is discussed to distinguish the
pathological cases of speech disabled children. The speech signal samples of
children of age between five to eight years are considered for the present
study. These speech signals are digitized and are used to determine the speech
envelope. The envelope is subjected to ratio mean analysis to estimate the
disability. This analysis is conducted on ten speech signal samples which are
related to both place of articulation and manner of articulation. Overall
speech disability of a pathological subject is estimated based on the results
of above analysis.
</summary>
    <author>
      <name>Anandthirtha B. Gudi</name>
    </author>
    <author>
      <name>H. K. Shreedhar</name>
    </author>
    <author>
      <name>H. C. Nagaraj</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/sipij.2011.2203</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/sipij.2011.2203" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages,4 Figures,Signal &amp; Image Processing Journal AIRCC</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Signal &amp; Image Processing : An International Journal (SIPIJ)
  Vol.2, No.2, June 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1107.4185v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.4185v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1107.5492v1</id>
    <updated>2011-07-27T14:42:52Z</updated>
    <published>2011-07-27T14:42:52Z</published>
    <title>Application of Gammachirp Auditory Filter as a Continuous Wavelet
  Analysis</title>
    <summary>  This paper presents a new method on the use of the gammachirp auditory filter
based on a continuous wavelet analysis. The gammachirp auditory filter is
designed to provide a spectrum reflecting the spectral properties of the
cochlea, which is responsible for frequency analysis in the human auditory
system. The impulse response of the theoretical gammachirp auditory filter that
has been developed by Irino and Patterson can be used as the kernel for wavelet
transform which approximates the frequency response of the cochlea. This study
implements the gammachirp auditory filter described by Irino as an analytical
wavelet and examines its application to a different speech signals.
  The obtained results will be compared with those obtained by two other
predefined wavelet families that are Morlet and Mexican Hat. The results show
that the gammachirp wavelet family gives results that are comparable to ones
obtained by Morlet and Mexican Hat wavelet family.
</summary>
    <author>
      <name>Lotfi Salhi</name>
    </author>
    <author>
      <name>Kais Ouni</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/sipij</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/sipij" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 17 figures;
  http://airccse.org/journal/sipij/papers/2011sipij10.pdf</arxiv:comment>
    <link href="http://arxiv.org/abs/1107.5492v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.5492v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.5876v1</id>
    <updated>2011-09-27T13:09:11Z</updated>
    <published>2011-09-27T13:09:11Z</published>
    <title>Rényi Information Measures for Spectral Change Detection</title>
    <summary>  Change detection within an audio stream is an important task in several
domains, such as classification and segmentation of a sound or of a music
piece, as well as indexing of broadcast news or surveillance applications. In
this paper we propose two novel methods for spectral change detection without
any assumption about the input sound: they are both based on the evaluation of
information measures applied to a time- frequency representation of the signal,
and in particular to the spectrogram. The class of measures we consider, the
R\'enyi entropies, are obtained by extending the Shannon entropy definition: a
biasing of the spectrogram coefficients is realized through the dependence of
such measures on a parameter, which allows refined results compared to those
obtained with standard divergences. These methods provide a low computational
cost and are well-suited as a support for higher level analysis, segmentation
and classification algorithms.
</summary>
    <author>
      <name>Marco Liuni</name>
    </author>
    <author>
      <name>Axel Röbel</name>
    </author>
    <author>
      <name>Marco Romito</name>
    </author>
    <author>
      <name>Xavier Rodet</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2011 IEEE Conference on Acoustics, Speech and Signal Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/1109.5876v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.5876v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.6270v1</id>
    <updated>2011-09-27T10:04:45Z</updated>
    <published>2011-09-27T10:04:45Z</published>
    <title>Fractal String Generation and Its Application in Music Composition</title>
    <summary>  Music is a string of some of the notes out of 12 notes (Sa, Komal_re, Re,
Komal_ga, Ga, Ma, Kari_ma, Pa, Komal_dha, Dha, Komal_ni, Ni) and their
harmonics. Each note corresponds to a particular frequency. When such strings
are encoded to form discrete sequences, different frequencies present in the
music corresponds to different amplitude levels (value) of the discrete
sequence. Initially, a class of discrete sequences has been generated using
logistic map. All these discrete sequences have at most n-different amplitude
levels (value) (depending on the particular raga). Without loss of generality,
we have chosen two discrete sequences of two types of Indian raga viz. Bhairabi
and Bhupali having same number of amplitude levels to obtain/search close
relatives from the class. The relative / closeness can be assured through
correlation coefficient.The search is unbiased, random and non-adaptive. The
obtained string is that which maximally resembles the given two sequences. The
same can be thought of as a music composition of the given two strings. It is
to be noted that all these string are fractal string which can be persuaded by
fractal dimension.
</summary>
    <author>
      <name>Avishek Ghosh</name>
    </author>
    <author>
      <name>Joydeep Banerjee</name>
    </author>
    <author>
      <name>Sk. S. Hassan</name>
    </author>
    <author>
      <name>P. Pal Choudhury</name>
    </author>
    <link href="http://arxiv.org/abs/1109.6270v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.6270v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.6313v1</id>
    <updated>2011-09-27T10:36:01Z</updated>
    <published>2011-09-27T10:36:01Z</published>
    <title>A Reduced Multiple Gabor Frame for Local Time Adaptation of the
  Spectrogram</title>
    <summary>  In this paper we propose a method for automatic local time adap- tation of
the spectrogram of an audio signal, based on its decomposition within a Gabor
multi-frame. The sparsity of the analyses within each individual frame is
evaluated through the R\'enyi entropies measures. According to the sparsity of
the decompositions, an optimal resolution and a reduced multi-frame are
determined, defining an adapted spectrogram with variable resolution and hop
size. The composition of such a reduced multi-frame allows an immediate
definition of a dual frame: re-synthesis techniques for this adapted analysis
are easily derived by the traditional phase vocoder scheme.
</summary>
    <author>
      <name>M. Liuni</name>
    </author>
    <author>
      <name>A. Röbel</name>
    </author>
    <author>
      <name>M. Romito</name>
    </author>
    <author>
      <name>X. Rodet</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. of the 13th Int. Conference on Digital Audio Effects
  (DAFx-10), Graz, Austria , September 6-10, 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1109.6313v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.6313v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.6314v1</id>
    <updated>2011-09-27T08:32:50Z</updated>
    <published>2011-09-27T08:32:50Z</published>
    <title>An Entropy Based Method for Local Time-Adaptation of the Spectrogram</title>
    <summary>  We propose a method for automatic local time-adaptation of the spectrogram of
audio signals: it is based on the decomposition of a signal within a Gabor
multi-frame through the STFT operator. The sparsity of the analysis in every
individual frame of the multi-frame is evaluated through the R\'enyi entropy
measures: the best local resolution is determined minimizing the entropy
values. The overall spectrogram of the signal we obtain thus provides local
optimal resolution adaptively evolving over time. We give examples of the
performance of our algorithm with an instrumental sound and a synthetic one,
showing the improvement in spectrogram displaying obtained with an automatic
adaptation of the resolution. The analysis operator is invertible, thus leading
to a perfect reconstruction of the original signal through the analysis
coefficients.
</summary>
    <author>
      <name>M. Liuni</name>
    </author>
    <author>
      <name>A. Röbel</name>
    </author>
    <author>
      <name>M. Romito</name>
    </author>
    <author>
      <name>X. Rodet</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">CMMR 2010, LNCS 6684, pp. 60-75, 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1109.6314v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.6314v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.6651v1</id>
    <updated>2011-09-29T14:06:15Z</updated>
    <published>2011-09-29T14:06:15Z</published>
    <title>Sound Analysis and Synthesis Adaptive in Time and Two Frequency Bands</title>
    <summary>  We present an algorithm for sound analysis and resynthesis with local
automatic adaptation of time-frequency resolution. There exists several
algorithms allowing to adapt the analysis window depending on its time or
frequency location; in what follows we propose a method which select the
optimal resolution depending on both time and frequency. We consider an
approach that we denote as analysis-weighting, from the point of view of Gabor
frame theory. We analyze in particular the case of different adaptive
time-varying resolutions within two complementary frequency bands; this is a
typical case where perfect signal reconstruction cannot in general be achieved
with fast algorithms, causing a certain error to be minimized. We provide
examples of adaptive analyses of a music sound, and outline several
possibilities that this work opens.
</summary>
    <author>
      <name>Marco Liuni</name>
    </author>
    <author>
      <name>Peter Balazs</name>
    </author>
    <author>
      <name>Axel Röbel</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. of the 14th Int. Conference on Digital Audio Effects
  (DAFx-11), Paris, France, September 19-23, 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1109.6651v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.6651v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1112.6178v1</id>
    <updated>2011-12-28T20:29:13Z</updated>
    <published>2011-12-28T20:29:13Z</published>
    <title>A general framework for online audio source separation</title>
    <summary>  We consider the problem of online audio source separation. Existing
algorithms adopt either a sliding block approach or a stochastic gradient
approach, which is faster but less accurate. Also, they rely either on spatial
cues or on spectral cues and cannot separate certain mixtures. In this paper,
we design a general online audio source separation framework that combines both
approaches and both types of cues. The model parameters are estimated in the
Maximum Likelihood (ML) sense using a Generalised Expectation Maximisation
(GEM) algorithm with multiplicative updates. The separation performance is
evaluated as a function of the block size and the step size and compared to
that of an offline algorithm.
</summary>
    <author>
      <name>Laurent S. R. Simon</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA - IRISA</arxiv:affiliation>
    </author>
    <author>
      <name>Emmanuel Vincent</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA - IRISA</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International conference on Latente Variable Analysis and Signal
  Separation (2012)</arxiv:comment>
    <link href="http://arxiv.org/abs/1112.6178v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1112.6178v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.2541v1</id>
    <updated>2012-04-11T07:01:31Z</updated>
    <published>2012-04-11T07:01:31Z</published>
    <title>Employing Subsequence Matching in Audio Data Processing</title>
    <summary>  We overview current problems of audio retrieval and time-series subsequence
matching. We discuss the usage of subsequence matching approaches in audio data
processing, especially in automatic speech recognition (ASR) area and we aim at
improving performance of the retrieval process. To overcome the problems known
from the time-series area like the occurrence of implementation bias and data
bias we present a Subsequence Matching Framework as a tool for fast
prototyping, building, and testing similarity search subsequence matching
applications. The framework is build on top of MESSIF (Metric Similarity Search
Implementation Framework) and thus the subsequence matching algorithms can
exploit advanced similarity indexes in order to significantly increase their
query processing performance. To prove our concept we provide a design of
query-by-example spoken term detection type of application with the usage of
phonetic posteriograms and subsequence matching approach.
</summary>
    <author>
      <name>Petr Volny</name>
    </author>
    <author>
      <name>David Novak</name>
    </author>
    <author>
      <name>Pavel Zezula</name>
    </author>
    <link href="http://arxiv.org/abs/1204.2541v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.2541v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.5651v1</id>
    <updated>2012-05-25T09:54:24Z</updated>
    <published>2012-05-25T09:54:24Z</published>
    <title>Measuring the evolution of contemporary western popular music</title>
    <summary>  Popular music is a key cultural expression that has captured listeners'
attention for ages. Many of the structural regularities underlying musical
discourse are yet to be discovered and, accordingly, their historical evolution
remains formally unknown. Here we unveil a number of patterns and metrics
characterizing the generic usage of primary musical facets such as pitch,
timbre, and loudness in contemporary western popular music. Many of these
patterns and metrics have been consistently stable for a period of more than
fifty years, thus pointing towards a great degree of conventionalism.
Nonetheless, we prove important changes or trends related to the restriction of
pitch transitions, the homogenization of the timbral palette, and the growing
loudness levels. This suggests that our perception of the new would be rooted
on these changing characteristics. Hence, an old tune could perfectly sound
novel and fashionable, provided that it consisted of common harmonic
progressions, changed the instrumentation, and increased the average loudness.
</summary>
    <author>
      <name>Joan Serrà</name>
    </author>
    <author>
      <name>Álvaro Corral</name>
    </author>
    <author>
      <name>Marián Boguñá</name>
    </author>
    <author>
      <name>Martín Haro</name>
    </author>
    <author>
      <name>Josep Lluis Arcos</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1038/srep00521</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1038/srep00521" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Supplementary materials not included. Please see the journal
  reference or contact the authors</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Scientific Reports 2, 521 (2012)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1205.5651v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.5651v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.1450v1</id>
    <updated>2012-06-07T11:08:25Z</updated>
    <published>2012-06-07T11:08:25Z</published>
    <title>A comparative study of performance of fpga based mel filter bank &amp; bark
  filter bank</title>
    <summary>  The sensitivity of human ear is dependent on frequency which is nonlinearly
resolved across the audio spectrum .Now to improve the recognition performance
in a similar non linear approach requires a front -end design, suggested by
empirical evidences. A popular alternative to linear prediction based analysis
is therefore filter bank analysis since this provides a much more
straightforward route to obtain the desired non-linear frequency resolution.
MEL filter bank and BARK filter bank are two popular filter bank analysis
techniques. This paper presents FPGA based implementation of MEL filter bank
and BARK filter bank with different bandwidths and different signal spectrum
ranges. The designs have been implemented using VHDL, simulated and verified
using Xilinx 11.1.For each filter bank, the basic building block is implemented
in Spartan 3E. A comparative study among these two mentioned filter banks is
also done in this paper.
</summary>
    <author>
      <name>Debalina Ghosh</name>
    </author>
    <author>
      <name>Depanwita Sarkar Debnath</name>
    </author>
    <author>
      <name>Saikat Bose</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 20 figures, 6 tables; International Journal of Artificial
  Intelligence &amp; Applications (IJAIA), Vol.3, No.3, May 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.1450v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.1450v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.5104v1</id>
    <updated>2012-07-21T05:25:27Z</updated>
    <published>2012-07-21T05:25:27Z</published>
    <title>Analysis of speech under stress using Linear techniques and Non-Linear
  techniques for emotion recognition system</title>
    <summary>  Analysis of speech for recognition of stress is important for identification
of emotional state of person. This can be done using 'Linear Techniques', which
has different parameters like pitch, vocal tract spectrum, formant frequencies,
Duration, MFCC etc. which are used for extraction of features from speech.
TEO-CB-Auto-Env is the method which is non-linear method of features
extraction. Analysis is done using TU-Berlin (Technical University of Berlin)
German database. Here emotion recognition is done for different emotions like
neutral, happy, disgust, sad, boredom and anger. Emotion recognition is used in
lie detector, database access systems, and in military for recognition of
soldiers' emotion identification during the war.
</summary>
    <author>
      <name>A. A. Khulage</name>
    </author>
    <author>
      <name>Prof. B. V. Pathak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1207.5104v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.5104v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.5560v1</id>
    <updated>2012-07-23T23:25:36Z</updated>
    <published>2012-07-23T23:25:36Z</published>
    <title>Evolving Musical Counterpoint: The Chronopoint Musical Evolution System</title>
    <summary>  Musical counterpoint, a musical technique in which two or more independent
melodies are played simultaneously with the goal of creating harmony, has been
around since the baroque era. However, to our knowledge computational
generation of aesthetically pleasing linear counterpoint based on subjective
fitness assessment has not been explored by the evolutionary computation
community (although generation using objective fitness has been attempted in
quite a few cases). The independence of contrapuntal melodies and the
subjective nature of musical aesthetics provide an excellent platform for the
application of genetic algorithms. In this paper, a genetic algorithm approach
to generating contrapuntal melodies is explained, with a description of the
various musical heuristics used and of how variable-length chromosome strings
are used to avoid generating "jerky" rhythms and melodic phrases, as well as
how subjectivity is incorporated into the algorithm's fitness measures. Next,
results from empirical testing of the algorithm are presented, with a focus on
how a user's musical sophistication influences their experience. Lastly,
further musical and compositional applications of the algorithm are discussed
along with planned future work on the algorithm.
</summary>
    <author>
      <name>Jeffrey Power Jacobs</name>
    </author>
    <author>
      <name>James Reggia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 6 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the First International Workshop on Evolutionary
  Music, 2011 IEEE Congress on Evolutionary Computation, 6-11 (2011)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1207.5560v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.5560v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.0171v1</id>
    <updated>2012-09-30T04:58:45Z</updated>
    <published>2012-09-30T04:58:45Z</published>
    <title>A novel method for obtaining a better quality speech signal for cochlear
  Implants using kalman with drnl and ssb technique</title>
    <summary>  Cochlear implant devices are known to exist since a long time. The purpose of
the present work is to develop a speech algorithm for obtaining robust speech.
In this paper, the technique of cochlear implant is first introduced, followed
by discussions of some of the existing techniques available for obtaining
speech. The next section introduces a new technique for obtaining robust
speech. The key feature of this technique lies in the use of the advantages of
an integrated approach involving the use of an estimation technique such as a
kalman filter with non linear filter bank strategy, using Dual Resonance Non
Linear(DRNL) and Single Side Band(SSB) Encoding method. A comparative study of
the proposed method with the existing method indicates that the proposed method
performs well compared to the existing method.
</summary>
    <author>
      <name>Rohini S. Hallikar</name>
    </author>
    <author>
      <name>Uttara Kumari</name>
    </author>
    <author>
      <name>K Padmaraju</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Advanced Computing: An International Journal ( ACIJ ), 3(4), 71 -
  75. 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1210.0171v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.0171v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.3778v1</id>
    <updated>2012-10-14T10:36:52Z</updated>
    <published>2012-10-14T10:36:52Z</published>
    <title>Blind speech separation based on undecimated wavelet packet-perceptual
  filterbanks and independent component analysis</title>
    <summary>  In this paper, we address the problem of blind separation of speech mixtures.
We propose a new blind speech separation system, which integrates a perceptual
filterbank and independent component analysis (ICA) and using kurtosis
criterion. The perceptual filterbank was designed by adjusting undecimated
wavelet packet decomposition (UWPD) tree in order to accord to critical band
characteristics of psycho-acoustic model. Our proposed technique consists on
transforming the observations signals into an adequate representation using
UWPD and Kurtosis maximization criterion in a new preprocessing step in order
to increase the non-Gaussianity which is a pre-requirement for ICA. Experiments
were carried out with the instantaneous mixture of two speech sources using two
sensors. The obtained results show that the proposed method gives a
considerable improvement when compared with FastICA and other techniques.
</summary>
    <author>
      <name>Ibrahim Missaoui</name>
    </author>
    <author>
      <name>Zied Lachiri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Perceptual Filter-Bank, Undecimated Wavelet Packet Decomposition,
  Independent Component Analysis, Blind speech separation. Downloaded from
  http://www.ijcsi.org/papers/IJCSI-8-3-1-265-272.pdf</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSI International Journal of Computer Science Issues, vol. 8,
  Issue 3, no. 1, May 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1210.3778v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.3778v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="92C55" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.4; H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.0451v2</id>
    <updated>2015-01-25T02:31:09Z</updated>
    <published>2012-12-03T17:06:41Z</published>
    <title>Semi-blind Source Separation via Sparse Representations and Online
  Dictionary Learning</title>
    <summary>  This work examines a semi-blind single-channel source separation problem. Our
specific aim is to separate one source whose local structure is approximately
known, from another a priori unspecified background source, given only a single
linear combination of the two sources. We propose a separation technique based
on local sparse approximations along the lines of recent efforts in sparse
representations and dictionary learning. A key feature of our procedure is the
online learning of dictionaries (using only the data itself) to sparsely model
the background source, which facilitates its separation from the
partially-known source. Our approach is applicable to source separation
problems in various application domains; here, we demonstrate the performance
of our proposed approach via simulation on a stylized audio source separation
task.
</summary>
    <author>
      <name>Sirisha Rambhatla</name>
    </author>
    <author>
      <name>Jarvis D. Haupt</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ACSSC.2013.6810587</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ACSSC.2013.6810587" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, In Proceedings of the 47th Asilomar Conference on Signals
  Systems and Computers, 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.0451v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.0451v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.6350v1</id>
    <updated>2012-12-27T11:31:16Z</updated>
    <published>2012-12-27T11:31:16Z</published>
    <title>Single-sided Real-time PESQ Score Estimation</title>
    <summary>  For several years now, the ITU-T's Perceptual Evaluation of Speech Quality
(PESQ) has been the reference for objective speech quality assessment. It is
widely deployed in commercial QoE measurement products, and it has been well
studied in the literature. While PESQ does provide reasonably good correlation
with subjective scores for VoIP applications, the algorithm itself is not
usable in a real-time context, since it requires a reference signal, which is
usually not available in normal conditions. In this paper we provide an
alternative technique for estimating PESQ scores in a single-sided fashion,
based on the Pseudo Subjective Quality Assessment (PSQA) technique.
</summary>
    <author>
      <name>Sebastián Basterrech</name>
    </author>
    <author>
      <name>Gerardo Rubino</name>
    </author>
    <author>
      <name>Martín Varela</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceeding of Measurement of Speech, Audio and Video Quality in
  Networks (MESAQIN'09), Prague, Czech Republic, June 2009, pp. 94-99</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.6350v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.6350v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="82C32, 62P30, 62M20" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4; D.4.4; I.5.1; B.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.6903v1</id>
    <updated>2012-12-31T15:01:37Z</updated>
    <published>2012-12-31T15:01:37Z</published>
    <title>About Multichannel Speech Signal Extraction and Separation Techniques</title>
    <summary>  The extraction of a desired speech signal from a noisy environment has become
a challenging issue. In the recent years, the scientific community has
particularly focused on multichannel techniques which are dealt with in this
review. In fact, this study tries to classify these multichannel techniques
into three main ones: Beamforming, Independent Com-ponent Analysis (ICA) and
Time Frequency (T-F) masking. This paper also highlights their advantages and
drawbacks. However these previously mentioned techniques could not afford
satisfactory results. This fact leads to the idea that a combination of those
techniques, which is depicted along this study, may probably provide more
efficient results. In-deed, giving the fact that those approaches are still be
considered as being not totally efficient, has led us to review these mentioned
above in the hope that further researches will provide this domain with
suitable innovations.
</summary>
    <author>
      <name>Adel Hidri</name>
    </author>
    <author>
      <name>Souad Meddeb</name>
    </author>
    <author>
      <name>Hamid Amiri</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4236/jsip.2012.32032</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4236/jsip.2012.32032" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 11 Figures. arXiv admin note: substantial text overlap with
  arXiv:1212.6080</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Signal and Information Processing, Vol. 3 No. 2, 2012,
  pp. 238-247</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1212.6903v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.6903v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.0265v1</id>
    <updated>2013-01-02T17:06:58Z</updated>
    <published>2013-01-02T17:06:58Z</published>
    <title>Usable Speech Assignment for Speaker Identification under Co-Channel
  Situation</title>
    <summary>  Usable speech criteria are proposed to extract minimally corrupted speech for
speaker identification (SID) in co-channel speech. In co-channel speech, either
speaker can randomly appear as the stronger speaker or the weaker one at a
time. Hence, the extracted usable segments are separated in time and need to be
organized into speaker streams for SID. In this paper, we focus to organize
extracted usable speech segment into a single stream for the same speaker by
speaker assignment system. For this, we develop model-based speaker assignment
method based on posterior probability and exhaustive search algorithm.
Evaluation of this method is performed on TIMIT database. The system is
evaluated on co-channel speech and results show a significant improvement.
</summary>
    <author>
      <name>Wajdi Ghezaiel</name>
    </author>
    <author>
      <name>Amel Ben Slimane</name>
    </author>
    <author>
      <name>Ezzedine Ben Braiek</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5120/9646-4381</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5120/9646-4381" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Applications (IJCA)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Applications 59(18):7-11, 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1301.0265v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.0265v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.1932v1</id>
    <updated>2013-01-09T17:42:59Z</updated>
    <published>2013-01-09T17:42:59Z</published>
    <title>An Approach for Classification of Dysfluent and Fluent Speech Using K-NN
  And SVM</title>
    <summary>  This paper presents a new approach for classification of dysfluent and fluent
speech using Mel-Frequency Cepstral Coefficient (MFCC). The speech is fluent
when person's speech flows easily and smoothly. Sounds combine into syllable,
syllables mix together into words and words link into sentences with little
effort. When someone's speech is dysfluent, it is irregular and does not flow
effortlessly. Therefore, a dysfluency is a break in the smooth, meaningful flow
of speech. Stuttering is one such disorder in which the fluent flow of speech
is disrupted by occurrences of dysfluencies such as repetitions, prolongations,
interjections and so on. In this work we have considered three types of
dysfluencies such as repetition, prolongation and interjection to characterize
dysfluent speech. After obtaining dysfluent and fluent speech, the speech
signals are analyzed in order to extract MFCC features. The k-Nearest Neighbor
(k-NN) and Support Vector Machine (SVM) classifiers are used to classify the
speech as dysfluent and fluent speech. The 80% of the data is used for training
and 20% for testing. The average accuracy of 86.67% and 93.34% is obtained for
dysfluent and fluent speech respectively.
</summary>
    <author>
      <name>P. Mahesha</name>
    </author>
    <author>
      <name>D. S. Vinod</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijcsea.2012.2603</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijcsea.2012.2603" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages,4 figures; International Journal of Computer Science,
  Engineering and Applications (IJCSEA) Vol.2, No.6, December 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.1932v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.1932v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.4382v1</id>
    <updated>2013-02-15T16:44:17Z</updated>
    <published>2013-02-15T16:44:17Z</published>
    <title>Finite element computation of elliptical vocal tract impedances using
  the two-microphone transfer function method</title>
    <summary>  The experimental two-microphone transfer function method (TMTF) is adapted to
the numerical framework to compute the radiation and input impedances of
three-dimensional vocal tracts of elliptical cross section. In its simplest
version, the TMTF method only requires measuring the acoustic pressure at two
points in an impedance duct and the postprocessing of the corresponding
transfer function. However, some considerations are to be taken into account
when using the TMTF method in the numerical context, which constitute the main
objective of this paper. In particular, the importance of including absorption
at the impedance duct walls to avoid lengthy numerical simulations is discussed
and analytical complex axial wave numbers for elliptical ducts are derived for
this purpose. It is also shown how the plane wave restriction of the TMTF
method can be circumvented to some extent by appropriate location of the
virtual microphones, thus extending the method frequency range of validity.
Virtual microphone spacing is also discussed on the basis of the so called
singularity factor. Numerical examples include the computation of the radiation
impedance of vowels /a/, /i/ and /u/ and the input impedance of vowel /a/, for
simplified vocal tracts of circular and elliptical cross sections.
</summary>
    <author>
      <name>Marc Arnela</name>
    </author>
    <author>
      <name>Oriol Guasch</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1121/1.4803889</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1121/1.4803889" rel="related"/>
    <link href="http://arxiv.org/abs/1302.4382v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.4382v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.class-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.1023v2</id>
    <updated>2013-03-13T19:30:57Z</updated>
    <published>2013-03-05T13:15:16Z</published>
    <title>Consistent Iterative Hard Thresholding For Signal Declipping</title>
    <summary>  Clipping or saturation in audio signals is a very common problem in signal
processing, for which, in the severe case, there is still no satisfactory
solution. In such case, there is a tremendous loss of information, and
traditional methods fail to appropriately recover the signal. We propose a
novel approach for this signal restoration problem based on the framework of
Iterative Hard Thresholding. This approach, which enforces the consistency of
the reconstructed signal with the clipped observations, shows superior
performance in comparison to the state-of-the-art declipping algorithms. This
is confirmed on synthetic and on actual high-dimensional audio data processing,
both on SNR and on subjective user listening evaluations.
</summary>
    <author>
      <name>Srdjan Kitić</name>
    </author>
    <author>
      <name>Laurent Jacques</name>
    </author>
    <author>
      <name>Nilesh Madhu</name>
    </author>
    <author>
      <name>Michael Peter Hopwood</name>
    </author>
    <author>
      <name>Ann Spriet</name>
    </author>
    <author>
      <name>Christophe De Vleeschouwer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICASSP2013 conference paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1303.1023v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.1023v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.1141v1</id>
    <updated>2013-05-06T10:33:06Z</updated>
    <published>2013-05-06T10:33:06Z</published>
    <title>Acoustic Echo Cancellation Postfilter Design Issues For Speech
  Recognition System</title>
    <summary>  In this paper a generalized postfilter algorithm design issues are presented.
This postfilter is used to jointly suppress late reverberation, residual echo,
and background noise. When residual echo and noise are suppressed, the best
result obtains by suppressing both interferences together after the Acoustic
echo cancellation (AEC). The main advantage of this approach is that the
residual echo and noise suppression does not suffer from the existence of a
strong acoustic echo component. Furthermore, the Acoustic echo cancellation
(AEC) does not suffer from the time-varying noise suppression. A disadvantage
is that the input signal of the Acoustic echo cancellation (AEC) has a low
signal-to-noise ratio (SNR). To overcome this problem, algorithms have been
proposed where, apart from the joint suppression, a noise-reduced signal is
used to adapt the echo canceller.
</summary>
    <author>
      <name>Urmila Shrawankar</name>
    </author>
    <author>
      <name>V M Thakare</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Pages: 6</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Science and Advanced Technology (IJSAT),
  ISSN 2221-8386, Vol 1 No 5, July 2011, pp 38-43</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1305.1141v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.1141v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.1145v1</id>
    <updated>2013-05-06T10:42:34Z</updated>
    <published>2013-05-06T10:42:34Z</published>
    <title>Techniques for Feature Extraction In Speech Recognition System : A
  Comparative Study</title>
    <summary>  The time domain waveform of a speech signal carries all of the auditory
information. From the phonological point of view, it little can be said on the
basis of the waveform itself. However, past research in mathematics, acoustics,
and speech technology have provided many methods for converting data that can
be considered as information if interpreted correctly. In order to find some
statistically relevant information from incoming data, it is important to have
mechanisms for reducing the information of each segment in the audio signal
into a relatively small number of parameters, or features. These features
should describe each segment in such a characteristic way that other similar
segments can be grouped together by comparing their features. There are
enormous interesting and exceptional ways to describe the speech signal in
terms of parameters. Though, they all have their strengths and weaknesses, we
have presented some of the most used methods with their importance.
</summary>
    <author>
      <name>Urmila Shrawankar</name>
    </author>
    <author>
      <name>V M Thakare</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Pages: 9 Figures : 3</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal Of Computer Applications In Engineering,
  Technology and Sciences (IJCAETS),ISSN 0974-3596,2010,pp 412-418</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1305.1145v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.1145v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.1426v1</id>
    <updated>2013-05-07T07:21:06Z</updated>
    <published>2013-05-07T07:21:06Z</published>
    <title>Speech Enhancement Modeling Towards Robust Speech Recognition System</title>
    <summary>  Form about four decades human beings have been dreaming of an intelligent
machine which can master the natural speech. In its simplest form, this machine
should consist of two subsystems, namely automatic speech recognition (ASR) and
speech understanding (SU). The goal of ASR is to transcribe natural speech
while SU is to understand the meaning of the transcription. Recognizing and
understanding a spoken sentence is obviously a knowledge-intensive process,
which must take into account all variable information about the speech
communication process, from acoustics to semantics and pragmatics. While
developing an Automatic Speech Recognition System, it is observed that some
adverse conditions degrade the performance of the Speech Recognition System. In
this contribution, speech enhancement system is introduced for enhancing speech
signals corrupted by additive noise and improving the performance of Automatic
Speech Recognizers in noisy conditions. Automatic speech recognition
experiments show that replacing noisy speech signals by the corresponding
enhanced speech signals leads to an improvement in the recognition accuracies.
The amount of improvement varies with the type of the corrupting noise.
</summary>
    <author>
      <name>Urmila Shrawankar</name>
    </author>
    <author>
      <name>V. M. Thakare</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Pages: 04; Conference Proceedings International Conference on Advance
  Computing (ICAC-2008), India</arxiv:comment>
    <link href="http://arxiv.org/abs/1305.1426v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.1426v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.1461v2</id>
    <updated>2013-06-07T16:57:39Z</updated>
    <published>2013-06-06T16:30:44Z</published>
    <title>The GTZAN dataset: Its contents, its faults, their effects on
  evaluation, and its future use</title>
    <summary>  The GTZAN dataset appears in at least 100 published works, and is the
most-used public dataset for evaluation in machine listening research for music
genre recognition (MGR). Our recent work, however, shows GTZAN has several
faults (repetitions, mislabelings, and distortions), which challenge the
interpretability of any result derived using it. In this article, we disprove
the claims that all MGR systems are affected in the same ways by these faults,
and that the performances of MGR systems in GTZAN are still meaningfully
comparable since they all face the same faults. We identify and analyze the
contents of GTZAN, and provide a catalog of its faults. We review how GTZAN has
been used in MGR research, and find few indications that its faults have been
known and considered. Finally, we rigorously study the effects of its faults on
evaluating five different MGR systems. The lesson is not to banish GTZAN, but
to use it with consideration of its contents.
</summary>
    <author>
      <name>Bob L. Sturm</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1080/09298215.2014.894533</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1080/09298215.2014.894533" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 7 figures, 6 tables, 128 references</arxiv:comment>
    <link href="http://arxiv.org/abs/1306.1461v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.1461v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.2593v1</id>
    <updated>2013-06-11T17:48:01Z</updated>
    <published>2013-06-11T17:48:01Z</published>
    <title>A 10-dimensional Phonetic-prosodic Space and its Stochastic Structure (A
  framework for probabilistic modeling of spoken languages and their phonology)</title>
    <summary>  We formulate a phonetic-prosodic space based on attributes as perceptual
observables, rather than articulatory specifications. We propose an alphabet as
markers in the phonetic subspace, aiming for a resolution sufficient to support
recognition of all spoken languages. The prosodic subspace is made up of
directly measurable physical variables. With the proposed alphabet, traditional
diphthongs naturally generalize to a broader class of language-neutral
phonotactic constraints, indicating a correlation structure similar to that of
the traditional sonority-based syllable. We define a stochastic structure on
the phone strings based on this diphthongal constraint, and show how a specific
spoken language can be defined as a specific set of probability distributions
of this stochastic structure. Furthermore, phonological variations within a
spoken language can be modeled as varying probability distributions restricted
to the phonetic subspace, conditioned on different values in the prosodic
subspace.
</summary>
    <author>
      <name>Elaine Tsiang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 10 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1306.2593v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.2593v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.6458v5</id>
    <updated>2016-10-07T12:53:08Z</updated>
    <published>2013-06-27T10:27:22Z</published>
    <title>Harmony Perception by Periodicity Detection</title>
    <summary>  The perception of consonance/dissonance of musical harmonies is strongly
correlated with their periodicity. This is shown in this article by
consistently applying recent results from psychophysics and neuroacoustics,
namely that the just noticeable difference between pitches for humans is about
1% for the musically important low frequency range and that periodicities of
complex chords can be detected in the human brain. Based thereon, the concepts
of relative and logarithmic periodicity with smoothing are introduced as
powerful measures of harmoniousness. The presented results correlate
significantly with empirical investigations on the perception of chords. Even
for scales, plausible results are obtained. For example, all classical church
modes appear in the front ranks of all theoretically possible seven-tone
scales.
</summary>
    <author>
      <name>Frieder Stolzenburg</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1080/17459737.2015.1033024</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1080/17459737.2015.1033024" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">extended version, 32 pages, 8 figures, 9 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Mathematics and Music, 9(3):215-238, 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1306.6458v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.6458v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.0819v1</id>
    <updated>2013-11-04T19:22:39Z</updated>
    <published>2013-11-04T19:22:39Z</published>
    <title>Phoneme discrimination using neurons with symmetric nonlinear response
  over a spectral range</title>
    <summary>  We consider the ability of a very simple feed-forward neural network to
discriminate phonemes based on just relative power spectrum. The network
consists of two neurons with symmetric nonlinear response over a spectral
range. The output of the neurons is subsequently fed to a comparator. We show
that often this is enough to achieve complete separation of data. We compare
the performance of found discriminants with that of more general neurons. Our
conclusion is that not much is gained in passing to real-valued weights. More
likely higher number of neurons and preprocessing of input will yield better
discrimination results. The networks considered are directly amenable to
hardware (neuromorphic) designs. Other advantages include interpretability,
guarantees of performance on unseen data and low Kolmogorov complexity.
</summary>
    <author>
      <name>Ondrej Such</name>
    </author>
    <author>
      <name>Ondrej Skvarek</name>
    </author>
    <author>
      <name>Martin Klimo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, ICASSP 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.0819v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.0819v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.0842v1</id>
    <updated>2013-11-04T20:43:47Z</updated>
    <published>2013-11-04T20:43:47Z</published>
    <title>An Intuitive Design Approach For Implementing Real Time Audio Effects</title>
    <summary>  Audio effect implementation on random musical signal is a basic application
of digital signal processors. In this paper, the compatibility features of
MATLAB R2008a with Code Composer Studio 3.3 has been exploited to develop
Simulink models which when emulated on TMS320C6713 DSK generate real time audio
effects. Each design has been done by two different asynchronous scheduling
techniques: (i) Idle task Scheduling and (ii) DSP/BIOS task Scheduling. A basic
COCOMO analysis has been done for the generated code to justify the industrial
viability of this design approach.
  KEYWORDS: Musical signal processing, Real time Audio effects, Echo, Stress
Generation, Reverberation, Reverberated Chorus, Real Time Scheduling.
</summary>
    <author>
      <name>Mayukh Mukhopadhyay</name>
    </author>
    <author>
      <name>Om Ranjan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.7323/ijaet/v6_iss5</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.7323/ijaet/v6_iss5" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages,11 figures,4 tables,24 equations,9 references</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Advances in Engineering &amp; Technology
  (IJAET), Volume 6 Issue 5(29), pp. 2216-2227, Nov. 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1311.0842v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.0842v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.2795v2</id>
    <updated>2013-12-11T07:27:01Z</updated>
    <published>2013-12-10T13:45:33Z</published>
    <title>Reverberant Audio Source Separation via Sparse and Low-Rank Modeling</title>
    <summary>  The performance of audio source separation from underdetermined convolutive
mixture assuming known mixing filters can be significantly improved by using an
analysis sparse prior optimized by a reweighting l1 scheme and a wideband
datafidelity term, as demonstrated by a recent article. In this letter, we show
that the performance can be improved even more significantly by exploiting a
low-rank prior on the source spectrograms.We present a new algorithm to
estimate the sources based on i) an analysis sparse prior, ii) a reweighting
scheme so as to increase the sparsity, iii) a wideband data-fidelity term in a
constrained form, and iv) a low-rank constraint on the source spectrograms.
Evaluation on reverberant music mixtures shows that the resulting algorithm
improves state-of-the-art methods by more than 2 dB of signal-to-distortion
ratio.
</summary>
    <author>
      <name>Simon Arberet</name>
    </author>
    <author>
      <name>Pierre Vandergheynst</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/LSP.2014.2303135</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/LSP.2014.2303135" rel="related"/>
    <link href="http://arxiv.org/abs/1312.2795v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.2795v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.4127v1</id>
    <updated>2013-12-15T09:40:37Z</updated>
    <published>2013-12-15T09:40:37Z</published>
    <title>A Hybrid Approach for Co-Channel Speech Segregation based on CASA, HMM
  Multipitch Tracking, and Medium Frame Harmonic Model</title>
    <summary>  This paper proposes a hybrid approach for co-channel speech segregation. HMM
(hidden Markov model) is used to track the pitches of 2 talkers. The resulting
pitch tracks are then enriched with the prominent pitch. The enriched tracks
are correctly grouped using pitch continuity. Medium frame harmonics are used
to extract the second pitch for frames with only one pitch deduced using the
previous steps. Finally, the pitch tracks are input to CASA (computational
auditory scene analysis) to segregate the mixed speech. The center frequency
range of the gamma tone filter banks is maximized to reduce the overlap between
the channels filtered for better segregation. Experiments were conducted using
this hybrid approach on the speech separation challenge database and compared
to the single (non-hybrid) approaches, i.e. signal processing and CASA. Results
show that using the hybrid approach outperforms the single approaches.
</summary>
    <author>
      <name>Ashraf M. Mohy Eldin</name>
    </author>
    <author>
      <name>Aliaa A. A. Youssif</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.14569/IJACSA.2013.040721</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.14569/IJACSA.2013.040721" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Keywords: CASA (computational auditory scene analysis); co-channel
  speech segregation; HMM (hidden Markov model) tracking; hybrid speech
  segregation approach; medium frame harmonic model; multipitch tracking;
  prominent pitch</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Advanced Computer Science and
  Applications (IJACSA)Volume 4 Issue 7, 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1312.4127v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.4127v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.1530v1</id>
    <updated>2014-02-06T23:43:28Z</updated>
    <published>2014-02-06T23:43:28Z</published>
    <title>TDOA--based localization in two dimensions: the bifurcation curve</title>
    <summary>  In this paper, we complete the study of the geometry of the TDOA map that
encodes the noiseless model for the localization of a source from the range
differences between three receivers in a plane, by computing the Cartesian
equation of the bifurcation curve in terms of the positions of the receivers.
From that equation, we can compute its real asymptotic lines. The present
manuscript completes the analysis of [Inverse Problems, Vol. 30, Number 3,
Pages 035004]. Our result is useful to check if a source belongs or is closed
to the bifurcation curve, where the localization in a noisy scenario is
ambiguous.
</summary>
    <author>
      <name>Marco Compagnoni</name>
    </author>
    <author>
      <name>Roberto Notari</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3233/FI-2014-1118</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3233/FI-2014-1118" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 3 figures, to appear in Fundamenta Informaticae</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Fundamenta Informaticae XXI (2014) 1001-1012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1402.1530v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.1530v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.AC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.4160v1</id>
    <updated>2014-02-14T03:11:53Z</updated>
    <published>2014-02-14T03:11:53Z</published>
    <title>Maximizing the Signal-to-Alias Ratio in Non-Uniform Filter Banks for
  Acoustic Echo Cancellation</title>
    <summary>  A new method for designing non-uniform filter-banks for acoustic echo
cancellation is proposed. In the method, the analysis prototype filter design
is framed as a convex optimization problem that maximizes the signal-to-alias
ratio (SAR) in the analysis banks. Since each sub-band has a different
bandwidth, the contribution to the overall SAR from each analysis bank is taken
into account during optimization. To increase the degrees of freedom during
optimization, no constraints are imposed on the phase or group delay of the
filters; at the same time, low delay is achieved by ensuring that the resulting
filters are minimum phase. Experimental results show that the filter bank
designed using the proposed method results in a sub-band adaptive filter with a
much better echo return loss enhancement (ERLE) when compared with existing
design methods.
</summary>
    <author>
      <name>R. C. Nongpiur</name>
    </author>
    <author>
      <name>D. J. Shpak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Circuits and Systems I: Regular Paper, vol. 59,
  no. 10, Oct. 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1402.4160v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.4160v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.6901v1</id>
    <updated>2014-03-27T01:32:09Z</updated>
    <published>2014-03-27T01:32:09Z</published>
    <title>Automatic Segmentation of Broadcast News Audio using Self Similarity
  Matrix</title>
    <summary>  Generally audio news broadcast on radio is com- posed of music, commercials,
news from correspondents and recorded statements in addition to the actual news
read by the newsreader. When news transcripts are available, automatic
segmentation of audio news broadcast to time align the audio with the text
transcription to build frugal speech corpora is essential. We address the
problem of identifying segmentation in the audio news broadcast corresponding
to the news read by the newsreader so that they can be mapped to the text
transcripts. The existing techniques produce sub-optimal solutions when used to
extract newsreader read segments. In this paper, we propose a new technique
which is able to identify the acoustic change points reliably using an acoustic
Self Similarity Matrix (SSM). We describe the two pass technique in detail and
verify its performance on real audio news broadcast of All India Radio for
different languages.
</summary>
    <author>
      <name>Sapna Soni</name>
    </author>
    <author>
      <name>Ahmed Imran</name>
    </author>
    <author>
      <name>Sunil Kumar Kopparapu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 5 images</arxiv:comment>
    <link href="http://arxiv.org/abs/1403.6901v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.6901v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.0400v1</id>
    <updated>2014-04-01T21:15:32Z</updated>
    <published>2014-04-01T21:15:32Z</published>
    <title>A Deep Representation for Invariance And Music Classification</title>
    <summary>  Representations in the auditory cortex might be based on mechanisms similar
to the visual ventral stream; modules for building invariance to
transformations and multiple layers for compositionality and selectivity. In
this paper we propose the use of such computational modules for extracting
invariant and discriminative audio representations. Building on a theory of
invariance in hierarchical architectures, we propose a novel, mid-level
representation for acoustical signals, using the empirical distributions of
projections on a set of templates and their transformations. Under the
assumption that, by construction, this dictionary of templates is composed from
similar classes, and samples the orbit of variance-inducing signal
transformations (such as shift and scale), the resulting signature is
theoretically guaranteed to be unique, invariant to transformations and stable
to deformations. Modules of projection and pooling can then constitute layers
of deep networks, for learning composite representations. We present the main
theoretical and computational aspects of a framework for unsupervised learning
of invariant audio representations, empirically evaluated on music genre
classification.
</summary>
    <author>
      <name>Chiyuan Zhang</name>
    </author>
    <author>
      <name>Georgios Evangelopoulos</name>
    </author>
    <author>
      <name>Stephen Voinea</name>
    </author>
    <author>
      <name>Lorenzo Rosasco</name>
    </author>
    <author>
      <name>Tomaso Poggio</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICASSP.2014.6854954</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICASSP.2014.6854954" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, CBMM Memo No. 002, (to appear) IEEE 2014 International
  Conference on Acoustics, Speech, and Signal Processing (ICASSP 2014)</arxiv:comment>
    <link href="http://arxiv.org/abs/1404.0400v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.0400v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.1468v1</id>
    <updated>2014-04-05T11:51:35Z</updated>
    <published>2014-04-05T11:51:35Z</published>
    <title>High Throughput and Less Area AMP Architecture for Audio Signal
  Restoration</title>
    <summary>  Audio restoration is effectively achieved by using low complexity algorithm
called AMP. This algorithm has fast convergence and has lower computation
intensity making it suitable for audio recovery problems. This paper focuses on
restoring an audio signal by using VLSI architecture called AMP-M that
implements AMP algorithm. This architecture employs MAC unit with fixed bit
Wallace tree multiplier, FFT-MUX and various memory units (RAM) for audio
restoration. VLSI and FPGA implementation results shows that reduced area, high
throughput, low power is achieved making it suitable for real time audio
recovery problems. Prominent examples are Magnetic Resonance Imaging (MRI),
Radar and Wireless Communications.
</summary>
    <author>
      <name>Swetha. R</name>
    </author>
    <author>
      <name>Rukmani Devi. D</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.14445/22312803/IJCTT-V9P123</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.14445/22312803/IJCTT-V9P123" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages,6 figures,"Published with International Journal of Computer
  Trends and Technology (IJCTT)","National Conference on Modern Electronics and
  Signal Processing(2014)-Velammal Engineering College","Recent Trends in
  Information Technology(2014)-R.M.K College of Engineering and Technology"</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Swetha.R,Rukmani Devi.D Article: High Throughput and Less Area AMP
  Architecture for Audio Signal Restoration.International Journal of Computer
  Trends and Technology (IJCTT) 9(3):107-111,Mar 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1404.1468v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.1468v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.1379v1</id>
    <updated>2014-05-05T14:37:47Z</updated>
    <published>2014-05-05T14:37:47Z</published>
    <title>Design and Optimization of a Speech Recognition Front-End for
  Distant-Talking Control of a Music Playback Device</title>
    <summary>  This paper addresses the challenging scenario for the distant-talking control
of a music playback device, a common portable speaker with four small
loudspeakers in close proximity to one microphone. The user controls the device
through voice, where the speech-to-music ratio can be as low as -30 dB during
music playback. We propose a speech enhancement front-end that relies on known
robust methods for echo cancellation, double-talk detection, and noise
suppression, as well as a novel adaptive quasi-binary mask that is well suited
for speech recognition. The optimization of the system is then formulated as a
large scale nonlinear programming problem where the recognition rate is
maximized and the optimal values for the system parameters are found through a
genetic algorithm. We validate our methodology by testing over the TIMIT
database for different music playback levels and noise types. Finally, we show
that the proposed front-end allows a natural interaction with the device for
limited-vocabulary voice commands.
</summary>
    <author>
      <name>Ramin Pichevar</name>
    </author>
    <author>
      <name>Jason Wung</name>
    </author>
    <author>
      <name>Daniele Giacobello</name>
    </author>
    <author>
      <name>Joshua Atkins</name>
    </author>
    <link href="http://arxiv.org/abs/1405.1379v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.1379v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.4843v1</id>
    <updated>2014-05-19T19:10:15Z</updated>
    <published>2014-05-19T19:10:15Z</published>
    <title>Trends and Perspectives for Signal Processing in Consumer Audio</title>
    <summary>  The trend in media consumption towards streaming and portability offers new
challenges and opportunities for signal processing in audio and acoustics. The
most significant embodiment of this trend is that most music consumption now
happens on-the-go which has recently led to an explosion in headphone sales and
small portable speakers. In particular, premium headphones offer a gateway for
a younger generation to experience high quality sound. Additionally, through
technologies incorporating head-related transfer functions headphones can also
offer unique new experiences in gaming, augmented reality, and surround sound
listening. Home audio has also seen a transition to smaller sound systems in
the form of sound bars. This speaker configuration offers many exciting
challenges for surround sound reproduction which has traditionally used five
speakers surrounding the listener. Furthermore, modern home entertainment
systems offer more than just content delivery; users now expect wireless and
connected smart devices with video conferencing, gaming, and other interactive
capabilities. With this comes challenges for voice interaction at a distance
and in demanding conditions, e.g., during content playback, and opportunities
for new smart interactive experiences based on awareness of environment and
user biometrics.
</summary>
    <author>
      <name>Joshua Atkins</name>
    </author>
    <author>
      <name>Daniele Giacobello</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Audio and Acoustic Signal Processing Technical Committee
  Newsletter, May 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.4843v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.4843v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.2464v1</id>
    <updated>2014-06-10T08:29:58Z</updated>
    <published>2014-06-10T08:29:58Z</published>
    <title>Music and Vocal Separation Using Multi-Band Modulation Based Features</title>
    <summary>  The potential use of non-linear speech features has not been investigated for
music analysis although other commonly used speech features like Mel Frequency
Ceptral Coefficients (MFCC) and pitch have been used extensively. In this
paper, we assume an audio signal to be a sum of modulated sinusoidal and then
use the energy separation algorithm to decompose the audio into amplitude and
frequency modulation components using the non-linear Teager-Kaiser energy
operator. We first identify the distribution of these non-linear features for
music only and voice only segments in the audio signal in different Mel spaced
frequency bands and show that they have the ability to discriminate. The
proposed method based on Kullback-Leibler divergence measure is evaluated using
a set of Indian classical songs from three different artists. Experimental
results show that the discrimination ability is evident in certain low and mid
frequency bands (200 - 1500 Hz).
</summary>
    <author>
      <name>Sunil Kumar Kopparapu</name>
    </author>
    <author>
      <name>Meghna Pandharipande</name>
    </author>
    <author>
      <name>G Sita</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ISIEA.2010.5679370</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ISIEA.2010.5679370" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures, 2010 IEEE Symposium on Industrial Electronics
  Applications (ISIEA)</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.2464v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.2464v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.4447v1</id>
    <updated>2014-06-17T17:44:45Z</updated>
    <published>2014-06-17T17:44:45Z</published>
    <title>Automatic Fado Music Classification</title>
    <summary>  In late 2011, Fado was elevated to the oral and intangible heritage of
humanity by UNESCO. This study aims to develop a tool for automatic detection
of Fado music based on the audio signal. To do this, frequency spectrum-related
characteristics were captured form the audio signal: in addition to the Mel
Frequency Cepstral Coefficients (MFCCs) and the energy of the signal, the
signal was further analysed in two frequency ranges, providing additional
information. Tests were run both in a 10-fold cross-validation setup (97.6%
accuracy), and in a traditional train/test setup (95.8% accuracy). The good
results reflect the fact that Fado is a very distinctive musical style.
</summary>
    <author>
      <name>Pedro Girão Antunes</name>
    </author>
    <author>
      <name>David Martins de Matos</name>
    </author>
    <author>
      <name>Ricardo Ribeiro</name>
    </author>
    <author>
      <name>Isabel Trancoso</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 1 figure, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.4447v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.4447v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.0380v1</id>
    <updated>2014-06-27T20:34:05Z</updated>
    <published>2014-06-27T20:34:05Z</published>
    <title>A Multi Level Data Fusion Approach for Speaker Identification on
  Telephone Speech</title>
    <summary>  Several speaker identification systems are giving good performance with clean
speech but are affected by the degradations introduced by noisy audio
conditions. To deal with this problem, we investigate the use of complementary
information at different levels for computing a combined match score for the
unknown speaker. In this work, we observe the effect of two supervised machine
learning approaches including support vectors machines (SVM) and na\"ive bayes
(NB). We define two feature vector sets based on mel frequency cepstral
coefficients (MFCC) and relative spectral perceptual linear predictive
coefficients (RASTA-PLP). Each feature is modeled using the Gaussian Mixture
Model (GMM). Several ways of combining these information sources give
significant improvements in a text-independent speaker identification task
using a very large telephone degraded NTIMIT database.
</summary>
    <author>
      <name>Imen Trabelsi</name>
    </author>
    <author>
      <name>Dorra Ben Ayed</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 figures, International Journal of Signal Processing,
  Image Processing and Pattern Recognition Vol. 6, No. 2, April, 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.0380v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.0380v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.2351v3</id>
    <updated>2015-02-16T04:30:42Z</updated>
    <published>2014-07-09T04:42:25Z</published>
    <title>Efficient Steered-Response Power Methods for Sound Source Localization
  Using Microphone Arrays</title>
    <summary>  This paper proposes an efficient method based on the steered-response power
(SRP) technique for sound source localization using microphone arrays: the
volumetric SRP (V-SRP). As compared to the SRP, by deploying a sparser
volumetric grid, the V-SRP achieves a significant reduction of the
computational complexity without sacrificing the accuracy of the location
estimates. By appending a fine search step to the V-SRP, its refined version
(RV-SRP) improves on the compromise between complexity and accuracy.
Experiments conducted in both simulated- and real-data scenarios demonstrate
the benefits of the proposed approaches. Specifically, the RV-SRP is shown to
outperform the SRP in accuracy at a computational cost of about ten times
lower.
</summary>
    <author>
      <name>Markus V. S. Lima</name>
    </author>
    <author>
      <name>Wallace A. Martins</name>
    </author>
    <author>
      <name>Leonardo O. Nunes</name>
    </author>
    <author>
      <name>Luiz W. P. Biscainho</name>
    </author>
    <author>
      <name>Tadeu N. Ferreira</name>
    </author>
    <author>
      <name>Maurício V. M. Costa</name>
    </author>
    <author>
      <name>Bowon Lee</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/LSP.2014.2385864</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/LSP.2014.2385864" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 9 figures, 5 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Signal Processing Letters (Volume:22 , Issue: 8 ), Aug. 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1407.2351v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.2351v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.3398v2</id>
    <updated>2014-07-17T05:23:38Z</updated>
    <published>2014-07-12T16:30:30Z</published>
    <title>Speech Polarity Detection Using Hilbert Phase Information</title>
    <summary>  The objective of the present work is to propose a method to automatically
detect polarity of the speech signals by estimating instants of significant
excitation of the vocaltract and the cosine phase of the analytic signal
representation. The phase changes in the analytic signal around the Hilbert
envelope (HE) peaks are found to vary according to the polarity of the given
speech signal. The relevant HE peaks for the Hilbert phase analysis are
selected by estimating the instants of significant excitation in speech. The
speech polarity identification rate obtained for the proposed method is almost
equal to the state of the art residual skewness method for speech polarity
detection. The proposed method also provides the same results for the polarity
detection in electro-glottogram signals. Finally, the robustness of the
proposed method is confirmed from the reduced detection error rates obtained in
noisy environments with various signal to noise ratios (SNRs). The MATLAB codes
used for implementing the proposed method are available for download from the
following link: http://nlp.amrita.edu:8080/TTS/polarityprograms.zip
</summary>
    <author>
      <name>D. Govind</name>
    </author>
    <author>
      <name>Anju Susan Biju</name>
    </author>
    <author>
      <name>Aguthu Smily</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 2 figures,
  http://nlp.amrita.edu:8080/TTS/polarityprograms.zip</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.3398v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.3398v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.0203v1</id>
    <updated>2014-08-31T10:33:45Z</updated>
    <published>2014-08-31T10:33:45Z</published>
    <title>Ad Hoc Microphone Array Calibration: Euclidean Distance Matrix
  Completion Algorithm and Theoretical Guarantees</title>
    <summary>  This paper addresses the problem of ad hoc microphone array calibration where
only partial information about the distances between microphones is available.
We construct a matrix consisting of the pairwise distances and propose to
estimate the missing entries based on a novel Euclidean distance matrix
completion algorithm by alternative low-rank matrix completion and projection
onto the Euclidean distance space. This approach confines the recovered matrix
to the EDM cone at each iteration of the matrix completion algorithm. The
theoretical guarantees of the calibration performance are obtained considering
the random and locally structured missing entries as well as the measurement
noise on the known distances. This study elucidates the links between the
calibration error and the number of microphones along with the noise level and
the ratio of missing distances. Thorough experiments on real data recordings
and simulated setups are conducted to demonstrate these theoretical insights. A
significant improvement is achieved by the proposed Euclidean distance matrix
completion algorithm over the state-of-the-art techniques for ad hoc microphone
array calibration.
</summary>
    <author>
      <name>Mohammad J. Taghizadeh</name>
    </author>
    <author>
      <name>Reza Parhizkar</name>
    </author>
    <author>
      <name>Philip N. Garner</name>
    </author>
    <author>
      <name>Herve Bourlard</name>
    </author>
    <author>
      <name>Afsaneh Asaei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Press, available online, August 1, 2014.
  http://www.sciencedirect.com/science/article/pii/S0165168414003508, Signal
  Processing, 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.0203v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.0203v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.7787v1</id>
    <updated>2014-09-27T09:47:16Z</updated>
    <published>2014-09-27T09:47:16Z</published>
    <title>Audio Surveillance: a Systematic Review</title>
    <summary>  Despite surveillance systems are becoming increasingly ubiquitous in our
living environment, automated surveillance, currently based on video sensory
modality and machine intelligence, lacks most of the time the robustness and
reliability required in several real applications. To tackle this issue, audio
sensory devices have been taken into account, both alone or in combination with
video, giving birth, in the last decade, to a considerable amount of research.
In this paper audio-based automated surveillance methods are organized into a
comprehensive survey: a general taxonomy, inspired by the more widespread video
surveillance field, is proposed in order to systematically describe the methods
covering background subtraction, event classification, object tracking and
situation analysis. For each of these tasks, all the significant works are
reviewed, detailing their pros and cons and the context for which they have
been proposed. Moreover, a specific section is devoted to audio features,
discussing their expressiveness and their employment in the above described
tasks. Differently, from other surveys on audio processing and analysis, the
present one is specifically targeted to automated surveillance, highlighting
the target applications of each described methods and providing the reader
tables and schemes useful to retrieve the most suited algorithms for a specific
requirement.
</summary>
    <author>
      <name>Marco Crocco</name>
    </author>
    <author>
      <name>Marco Cristani</name>
    </author>
    <author>
      <name>Andrea Trucco</name>
    </author>
    <author>
      <name>Vittorio Murino</name>
    </author>
    <link href="http://arxiv.org/abs/1409.7787v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.7787v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.6903v1</id>
    <updated>2014-10-25T09:40:46Z</updated>
    <published>2014-10-25T09:40:46Z</published>
    <title>Choice of Mel Filter Bank in Computing MFCC of a Resampled Speech</title>
    <summary>  Mel Frequency Cepstral Coefficients (MFCCs) are the most popularly used
speech features in most speech and speaker recognition applications. In this
paper, we study the effect of resampling a speech signal on these speech
features. We first derive a relationship between the MFCC param- eters of the
resampled speech and the MFCC parameters of the original speech. We propose six
methods of calculating the MFCC parameters of downsampled speech by
transforming the Mel filter bank used to com- pute MFCC of the original speech.
We then experimentally compute the MFCC parameters of the down sampled speech
using the proposed meth- ods and compute the Pearson coefficient between the
MFCC parameters of the downsampled speech and that of the original speech to
identify the most effective choice of Mel-filter band that enables the computed
MFCC of the resampled speech to be as close as possible to the original speech
sample MFCC.
</summary>
    <author>
      <name>Laxmi Narayana M.</name>
    </author>
    <author>
      <name>Sunil Kumar Kopparapu</name>
    </author>
    <link href="http://arxiv.org/abs/1410.6903v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.6903v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.6905v1</id>
    <updated>2014-10-25T09:53:39Z</updated>
    <published>2014-10-25T09:53:39Z</published>
    <title>On the use of Stress information in Speech for Speaker Recognition</title>
    <summary>  The performance of a speaker recognition system decreases when the speaker is
under stress or emotion. In this paper we explore and identify a mechanism that
enables use of inherent stress-in-speech or speaking style information present
in speech of a person as additional cues for speaker recognition. We quantify
the the inherent stress present in the speech of a speaker mainly using 3
features, namely, pitch, amplitude and duration (together called PAD) We
experimentally observe that the PAD vectors of similar phones in different
words of a speaker are close to each other in the three dimensional (PAD) space
confirming that the way a speaker stresses different syllables in their speech
is unique to them, thus we propose the use of PAD based speaking style of a
speaker as an additional feature for speaker recognition applications.
</summary>
    <author>
      <name>Laxmi Narayana M.</name>
    </author>
    <author>
      <name>Sunil Kumar Kopparapu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TENCON.2009.5396003</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TENCON.2009.5396003" rel="related"/>
    <link href="http://arxiv.org/abs/1410.6905v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.6905v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.0370v1</id>
    <updated>2014-11-03T06:27:13Z</updated>
    <published>2014-11-03T06:27:13Z</published>
    <title>Detection of transitions between broad phonetic classes in a speech
  signal</title>
    <summary>  Detection of transitions between broad phonetic classes in a speech signal is
an important problem which has applications such as landmark detection and
segmentation. The proposed hierarchical method detects silence to non-silence
transitions, high amplitude (mostly sonorants) to low ampli- tude (mostly
fricatives/affricates/stop bursts) transitions and vice-versa. A subset of the
extremum (minimum or maximum) samples between every pair of successive
zero-crossings is selected above a second pass threshold, from each bandpass
filtered speech signal frame. Relative to the mid-point (reference) of a frame,
locations of the first and the last extrema lie on either side, if the speech
signal belongs to a homogeneous segment; else, both these locations lie on the
left or the right side of the reference, indicating a transition frame. When
tested on the entire TIMIT database, of the transitions detected, 93.6% are
within a tolerance of 20 ms from the hand labeled boundaries. Sonorant,
unvoiced non-sonorant and silence classes and their respective onsets are
detected with an accuracy of about 83.5% for the same tolerance. The results
are as good as, and in some respects better than the state-of-the-art methods
for similar tasks.
</summary>
    <author>
      <name>T V Ananthapadmanabha</name>
    </author>
    <author>
      <name>K V Vijay Girish</name>
    </author>
    <author>
      <name>A G Ramakrishnan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.0370v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.0370v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.1267v1</id>
    <updated>2014-11-05T13:20:56Z</updated>
    <published>2014-11-05T13:20:56Z</published>
    <title>An Interesting Property of LPCs for Sonorant Vs Fricative Discrimination</title>
    <summary>  Linear prediction (LP) technique estimates an optimum all-pole filter of a
given order for a frame of speech signal. The coefficients of the all-pole
filter, 1/A(z) are referred to as LP coefficients (LPCs). The gain of the
inverse of the all-pole filter, A(z) at z = 1, i.e, at frequency = 0, A(1)
corresponds to the sum of LPCs, which has the property of being lower (higher)
than a threshold for the sonorants (fricatives). When the inverse-tan of A(1),
denoted as T(1), is used a feature and tested on the sonorant and fricative
frames of the entire TIMIT database, an accuracy of 99.07% is obtained. Hence,
we refer to T(1) as sonorant-fricative discrimination index (SFDI). This
property has also been tested for its robustness for additive white noise and
on the telephone quality speech of the NTIMIT database. These results are
comparable to, or in some respects, better than the state-of-the-art methods
proposed for a similar task. Such a property may be used for segmenting a
speech signal or for non-uniform frame-rate analysis.
</summary>
    <author>
      <name>T. V. Ananthapadmanabha</name>
    </author>
    <author>
      <name>A. G. Ramakrishnan</name>
    </author>
    <author>
      <name>Pradeep Balachandran</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages including references</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.1267v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.1267v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.1898v1</id>
    <updated>2014-11-07T13:09:39Z</updated>
    <published>2014-11-07T13:09:39Z</published>
    <title>A Novel Uncertainty Parameter SR (Signal To Residual Spectrum Ratio)
  Evaluation Approach For Speech Enhancement</title>
    <summary>  Usually, hearing impaired people use hearing aids which are implemented with
speech enhancement algorithms. Estimation of speech and estimation of nose are
the components in single channel speech enhancement system. The main objective
of any speech enhancement algorithm is estimation of noise power spectrum for
non stationary environment. VAD (Voice Activity Detector) is used to identify
speech pauses and during these pauses only estimation of noise. MMSE (Minimum
Mean Square Error) speech enhancement algorithm did not enhance the
intelligibility, quality and listener fatigues are the perceptual aspects of
speech. Novel evaluation approach SR (Signal to Residual spectrum ratio) based
on uncertainty parameter introduced for the benefits of hearing impaired people
in non stationary environments to control distortions. By estimation and
updating of noise based on division of original pure signal into three parts
such as pure speech, quasi speech and non speech frames based on multiple
threshold conditions. Different values of SR and LLR demonstrate the amount of
attenuation and amplification distortions. The proposed method will compared
with any one method WAT(Weighted Average Technique) Hence by using parameters
SR (signal to residual spectrum ratio) and LLR (log like hood ratio), MMSE
(Minim Mean Square Error) in terms of segmented SNR and LLR.
</summary>
    <author>
      <name>M. Ravichandra Kumar</name>
    </author>
    <author>
      <name>B. Ravi Teja</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.1898v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.1898v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.3715v1</id>
    <updated>2014-11-13T16:03:09Z</updated>
    <published>2014-11-13T16:03:09Z</published>
    <title>Acoustic Scene Classification</title>
    <summary>  In this article we present an account of the state-of-the-art in acoustic
scene classification (ASC), the task of classifying environments from the
sounds they produce. Starting from a historical review of previous research in
this area, we define a general framework for ASC and present different imple-
mentations of its components. We then describe a range of different algorithms
submitted for a data challenge that was held to provide a general and fair
benchmark for ASC techniques. The dataset recorded for this purpose is
presented, along with the performance metrics that are used to evaluate the
algorithms and statistical significance tests to compare the submitted methods.
We use a baseline method that employs MFCCS, GMMS and a maximum likelihood
criterion as a benchmark, and only find sufficient evidence to conclude that
three algorithms significantly outperform it. We also evaluate the human
classification accuracy in performing a similar classification task. The best
performing algorithm achieves a mean accuracy that matches the median accuracy
obtained by humans, and common pairs of classes are misclassified by both
computers and humans. However, all acoustic scenes are correctly classified by
at least some individuals, while there are scenes that are misclassified by all
algorithms.
</summary>
    <author>
      <name>Daniele Barchiesi</name>
    </author>
    <author>
      <name>Dimitrios Giannoulis</name>
    </author>
    <author>
      <name>Dan Stowell</name>
    </author>
    <author>
      <name>Mark D. Plumbley</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MSP.2014.2326181</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MSP.2014.2326181" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Signal Processing Magazine 32(3) (May 2015) 16-34</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1411.3715v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.3715v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.4890v1</id>
    <updated>2014-11-17T05:06:50Z</updated>
    <published>2014-11-17T05:06:50Z</published>
    <title>Which Are You In A Photo?</title>
    <summary>  Automatic image tagging has been a long standing problem, it mainly relies on
image recognition techniques of which the accuracy is still not satisfying.
This paper attempts to explore out-of-band sensing base on the mobile phone to
sense the people in a picture while the picture is being taken and create name
tags on-the-fly. The major challenges pertain to two aspects - "Who" and
"Which". (1) "Who": discriminating people who are in the picture from those
that are not; (2) "Which": correlating each name tag with its corresponding
people in the picture. We propose an accurate acoustic scheme applying on the
mobile phones, which leverages the Doppler effect of sound wave to address
these two challenges. As a proof of concept, we implement the scheme on 7
android phones and take pictures in various real-life scenarios with people
positioning in different ways. Extensive experiments show that the accuracy of
tag correlation is above 85% within 3m for picturing.
</summary>
    <author>
      <name>Xing Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1411.4890v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.4890v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.6741v1</id>
    <updated>2014-11-25T06:18:45Z</updated>
    <published>2014-11-25T06:18:45Z</published>
    <title>A Complex Matrix Factorization approach to Joint Modeling of Magnitude
  and Phase for Source Separation</title>
    <summary>  Conventional NMF methods for source separation factorize the matrix of
spectral magnitudes. Spectral Phase is not included in the decomposition
process of these methods. However, phase of the speech mixture is generally
used in reconstructing the target speech signal. This results in undesired
traces of interfering sources in the target signal. In this paper the spectral
phase is incorporated in the decomposition process itself. Additionally, the
complex matrix factorization problem is reduced to an NMF problem using simple
transformations. This results in effective separation of speech mixtures since
both magnitude and phase are utilized jointly in the separation process.
Improvement in source separation results are demonstrated using objective
quality evaluations on the GRID corpus.
</summary>
    <author>
      <name>Chaitanya Ahuja</name>
    </author>
    <author>
      <name>Karan Nathwani</name>
    </author>
    <author>
      <name>Rajesh M. Hegde</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.6741v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.6741v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.4052v2</id>
    <updated>2016-06-12T08:08:41Z</updated>
    <published>2014-12-11T05:11:54Z</published>
    <title>The bag-of-frames approach: a not so sufficient model for urban
  soundscapes</title>
    <summary>  The "bag-of-frames" approach (BOF), which encodes audio signals as the
long-term statistical distribution of short-term spectral features, is commonly
regarded as an effective and sufficient way to represent environmental sound
recordings (soundscapes) since its introduction in an influential 2007 article.
The present paper describes a concep-tual replication of this seminal article
using several new soundscape datasets, with results strongly questioning the
adequacy of the BOF approach for the task. We show that the good accuracy
originally re-ported with BOF likely result from a particularly thankful
dataset with low within-class variability, and that for more realistic
datasets, BOF in fact does not perform significantly better than a mere
one-point av-erage of the signal's features. Soundscape modeling, therefore,
may not be the closed case it was once thought to be. Progress, we ar-gue,
could lie in reconsidering the problem of considering individual acoustical
events within each soundscape.
</summary>
    <author>
      <name>Mathieu Lagrange</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRCCyN</arxiv:affiliation>
    </author>
    <author>
      <name>Grégoire Lafay</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRCCyN</arxiv:affiliation>
    </author>
    <author>
      <name>Boris Defreville</name>
    </author>
    <author>
      <name>Jean-Julien Aucouturier</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">JASA Express Letters, 2015, 138 (5), pp.487-492</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1412.4052v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.4052v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.7022v3</id>
    <updated>2015-04-28T02:24:14Z</updated>
    <published>2014-12-22T15:15:44Z</published>
    <title>Audio Source Separation with Discriminative Scattering Networks</title>
    <summary>  In this report we describe an ongoing line of research for solving
single-channel source separation problems. Many monaural signal decomposition
techniques proposed in the literature operate on a feature space consisting of
a time-frequency representation of the input data. A challenge faced by these
approaches is to effectively exploit the temporal dependencies of the signals
at scales larger than the duration of a time-frame. In this work we propose to
tackle this problem by modeling the signals using a time-frequency
representation with multiple temporal resolutions. The proposed representation
consists of a pyramid of wavelet scattering operators, which generalizes
Constant Q Transforms (CQT) with extra layers of convolution and complex
modulus. We first show that learning standard models with this multi-resolution
setting improves source separation results over fixed-resolution methods. As
study case, we use Non-Negative Matrix Factorizations (NMF) that has been
widely considered in many audio application. Then, we investigate the inclusion
of the proposed multi-resolution setting into a discriminative training regime.
We discuss several alternatives using different deep neural network
architectures.
</summary>
    <author>
      <name>Pablo Sprechmann</name>
    </author>
    <author>
      <name>Joan Bruna</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
    <link href="http://arxiv.org/abs/1412.7022v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.7022v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.7193v1</id>
    <updated>2014-12-22T22:38:06Z</updated>
    <published>2014-12-22T22:38:06Z</published>
    <title>Audio Source Separation Using a Deep Autoencoder</title>
    <summary>  This paper proposes a novel framework for unsupervised audio source
separation using a deep autoencoder. The characteristics of unknown source
signals mixed in the mixed input is automatically by properly configured
autoencoders implemented by a network with many layers, and separated by
clustering the coefficient vectors in the code layer. By investigating the
weight vectors to the final target, representation layer, the primitive
components of the audio signals in the frequency domain are observed. By
clustering the activation coefficients in the code layer, the previously
unknown source signals are segregated. The original source sounds are then
separated and reconstructed by using code vectors which belong to different
clusters. The restored sounds are not perfect but yield promising results for
the possibility in the success of many practical applications.
</summary>
    <author>
      <name>Giljin Jang</name>
    </author>
    <author>
      <name>Han-Gyu Kim</name>
    </author>
    <author>
      <name>Yung-Hwan Oh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 4 figures, ICLR 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.7193v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.7193v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.07496v1</id>
    <updated>2015-01-29T16:09:17Z</updated>
    <published>2015-01-29T16:09:17Z</published>
    <title>Implementation of an Automatic Syllabic Division Algorithm from Speech
  Files in Portuguese Language</title>
    <summary>  A new algorithm for voice automatic syllabic splitting in the Portuguese
language is proposed, which is based on the envelope of the speech signal of
the input audio file. A computational implementation in MatlabTM is presented
and made available at the URL
http://www2.ee.ufpe.br/codec/divisao_silabica.html. Due to its
straightforwardness, the proposed method is very attractive for embedded
systems (e.g. i-phones). It can also be used as a screen to assist more
sophisticated methods. Voice excerpts containing more than one syllable and
identified by the same envelope are named as super-syllables and they are
subsequently separated. The results indicate which samples corresponds to the
beginning and end of each detected syllable. Preliminary tests were performed
to fifty words at an identification rate circa 70% (further improvements may be
incorporated to treat particular phonemes). This algorithm is also useful in
voice command systems, as a tool in the teaching of Portuguese language or even
for patients with speech pathology.
</summary>
    <author>
      <name>E. L. F. Da Silva</name>
    </author>
    <author>
      <name>H. M. de Oliveira</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 7 figures, 4 tables, conference: XIX Congresso Brasileiro de
  Automatica CBA, Campina Grande, Setembro, 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1501.07496v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.07496v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.00524v2</id>
    <updated>2015-10-23T14:37:45Z</updated>
    <published>2015-02-02T15:45:38Z</published>
    <title>Unsupervised Incremental Learning and Prediction of Music Signals</title>
    <summary>  A system is presented that segments, clusters and predicts musical audio in
an unsupervised manner, adjusting the number of (timbre) clusters
instantaneously to the audio input. A sequence learning algorithm adapts its
structure to a dynamically changing clustering tree. The flow of the system is
as follows: 1) segmentation by onset detection, 2) timbre representation of
each segment by Mel frequency cepstrum coefficients, 3) discretization by
incremental clustering, yielding a tree of different sound classes (e.g.
instruments) that can grow or shrink on the fly driven by the instantaneous
sound events, resulting in a discrete symbol sequence, 4) extraction of
statistical regularities of the symbol sequence, using hierarchical N-grams and
the newly introduced conceptual Boltzmann machine, and 5) prediction of the
next sound event in the sequence. The system's robustness is assessed with
respect to complexity and noisiness of the signal. Clustering in isolation
yields an adjusted Rand index (ARI) of 82.7% / 85.7% for data sets of singing
voice and drums. Onset detection jointly with clustering achieve an ARI of
81.3% / 76.3% and the prediction of the entire system yields an ARI of 27.2% /
39.2%.
</summary>
    <author>
      <name>Ricard Marxer</name>
    </author>
    <author>
      <name>Hendrik Purwins</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.00524v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.00524v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T05" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.01707v1</id>
    <updated>2015-02-05T20:40:41Z</updated>
    <published>2015-02-05T20:40:41Z</published>
    <title>CS reconstruction of the speech and musical signals</title>
    <summary>  The application of Compressive sensing approach to the speech and musical
signals is considered in this paper. Compressive sensing (CS) is a new approach
to the signal sampling that allows signal reconstruction from a small set of
randomly acquired samples. This method is developed for the signals that
exhibit the sparsity in a certain domain. Here we have observed two sparsity
domains: discrete Fourier and discrete cosine transform domain. Furthermore,
two different types of audio signals are analyzed in terms of sparsity and CS
performance - musical and speech signals. Comparative analysis of the CS
reconstruction using different number of signal samples is performed in the two
domains of sparsity. It is shown that the CS can be successfully applied to
both, musical and speech signals, but the speech signals are more demanding in
terms of the number of observations. Also, our results show that discrete
cosine transform domain allows better reconstruction using lower number of
observations, compared to the Fourier transform domain, for both types of
signals.
</summary>
    <author>
      <name>Trifun Savic</name>
    </author>
    <author>
      <name>Radoje Albijanic</name>
    </author>
    <link href="http://arxiv.org/abs/1502.01707v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.01707v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.03162v1</id>
    <updated>2015-02-11T00:47:22Z</updated>
    <published>2015-02-11T00:47:22Z</published>
    <title>Sparse Head-Related Impulse Response for Efficient Direct Convolution</title>
    <summary>  Head-related impulse responses (HRIRs) are subject-dependent and
direction-dependent filters used in spatial audio synthesis. They describe the
scattering response of the head, torso, and pinnae of the subject. We propose a
structural factorization of the HRIRs into a product of non-negative and
Toeplitz matrices; the factorization is based on a novel extension of a
non-negative matrix factorization algorithm. As a result, the HRIR becomes
expressible as a convolution between a direction-independent \emph{resonance}
filter and a direction-dependent \emph{reflection} filter. Further, the
reflection filter can be made \emph{sparse} with minimal HRIR distortion. The
described factorization is shown to be applicable to the arbitrary source
signal case and allows one to employ time-domain convolution at a computational
cost lower than using convolution in the frequency domain.
</summary>
    <author>
      <name>Yuancheng Luo</name>
    </author>
    <author>
      <name>Dmitry N. Zotkin</name>
    </author>
    <author>
      <name>Ramani Duraiswami</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 Pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.03162v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.03162v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="15B05, 65F50, 42A85" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.03784v2</id>
    <updated>2015-02-13T14:05:12Z</updated>
    <published>2015-02-12T19:38:50Z</published>
    <title>Coherent-to-Diffuse Power Ratio Estimation for Dereverberation</title>
    <summary>  The estimation of the time- and frequency-dependent coherent-to-diffuse power
ratio (CDR) from the measured spatial coherence between two omnidirectional
microphones is investigated. Known CDR estimators are formulated in a common
framework, illustrated using a geometric interpretation in the complex plane,
and investigated with respect to bias and robustness towards model errors.
Several novel unbiased CDR estimators are proposed, and it is shown that
knowledge of either the direction of arrival (DOA) of the target source or the
coherence of the noise field is sufficient for unbiased CDR estimation. The
validity of the model for the application of CDR estimates to dereverberation
is investigated using measured and simulated impulse responses. A CDR-based
dereverberation system is presented and evaluated using signal-based quality
measures as well as automatic speech recognition accuracy. The results show
that the proposed unbiased estimators have a practical advantage over existing
estimators, and that the proposed DOA-independent estimator can be used for
effective blind dereverberation.
</summary>
    <author>
      <name>Andreas Schwarz</name>
    </author>
    <author>
      <name>Walter Kellermann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TASLP.2015.2418571</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TASLP.2015.2418571" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to IEEE/ACM Transactions on Audio, Speech, and Language
  Processing, 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.03784v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.03784v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.04300v1</id>
    <updated>2015-02-15T10:22:47Z</updated>
    <published>2015-02-15T10:22:47Z</published>
    <title>Mandarin Singing Voice Synthesis Based on Harmonic Plus Noise Model and
  Singing Expression Analysis</title>
    <summary>  The purpose of this study is to investigate how humans interpret musical
scores expressively, and then design machines that sing like humans. We
consider six factors that have a strong influence on the expression of human
singing. The factors are related to the acoustic, phonetic, and musical
features of a real singing signal. Given real singing voices recorded following
the MIDI scores and lyrics, our analysis module can extract the expression
parameters from the real singing signals semi-automatically. The expression
parameters are used to control the singing voice synthesis (SVS) system for
Mandarin Chinese, which is based on the harmonic plus noise model (HNM). The
results of perceptual experiments show that integrating the expression factors
into the SVS system yields a notable improvement in perceptual naturalness,
clearness, and expressiveness. By one-to-one mapping of the real singing signal
and expression controls to the synthesizer, our SVS system can simulate the
interpretation of a real singer with the timbre of a speaker.
</summary>
    <author>
      <name>Ju-Chiang Wang</name>
    </author>
    <author>
      <name>Hung-Yan Gu</name>
    </author>
    <author>
      <name>Hsin-Min Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, technical report</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.04300v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.04300v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.05751v2</id>
    <updated>2015-07-09T14:27:05Z</updated>
    <published>2015-02-19T23:58:36Z</published>
    <title>Efficient Synthesis of Room Acoustics via Scattering Delay Networks</title>
    <summary>  An acoustic reverberator consisting of a network of delay lines connected via
scattering junctions is proposed. All parameters of the reverberator are
derived from physical properties of the enclosure it simulates. It allows for
simulation of unequal and frequency-dependent wall absorption, as well as
directional sources and microphones. The reverberator renders the first-order
reflections exactly, while making progressively coarser approximations of
higher-order reflections. The rate of energy decay is close to that obtained
with the image method (IM) and consistent with the predictions of Sabine and
Eyring equations. The time evolution of the normalized echo density, which was
previously shown to be correlated with the perceived texture of reverberation,
is also close to that of IM. However, its computational complexity is one to
two orders of magnitude lower, comparable to the computational complexity of a
feedback delay network (FDN), and its memory requirements are negligible.
</summary>
    <author>
      <name>Enzo De Sena</name>
    </author>
    <author>
      <name>Huseyin Hacihabiboglu</name>
    </author>
    <author>
      <name>Zoran Cvetkovic</name>
    </author>
    <author>
      <name>Julius O. Smith III</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TASLP.2015.2438547</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TASLP.2015.2438547" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE/ACM Transactions on Audio, Speech, and Language Processing,
  Vol. 23, No. 9, September 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1502.05751v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.05751v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.06811v1</id>
    <updated>2015-02-24T13:47:45Z</updated>
    <published>2015-02-24T13:47:45Z</published>
    <title>A Review of Audio Features and Statistical Models Exploited for Voice
  Pattern Design</title>
    <summary>  Audio fingerprinting, also named as audio hashing, has been well-known as a
powerful technique to perform audio identification and synchronization. It
basically involves two major steps: fingerprint (voice pattern) design and
matching search. While the first step concerns the derivation of a robust and
compact audio signature, the second step usually requires knowledge about
database and quick-search algorithms. Though this technique offers a wide range
of real-world applications, to the best of the authors' knowledge, a
comprehensive survey of existing algorithms appeared more than eight years ago.
Thus, in this paper, we present a more up-to-date review and, for emphasizing
on the audio signal processing aspect, we focus our state-of-the-art survey on
the fingerprint design step for which various audio features and their
tractable statistical models are discussed.
</summary>
    <author>
      <name>Ngoc Q. K. Duong</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">HUMG</arxiv:affiliation>
    </author>
    <author>
      <name>Hien-Thanh Duong</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">HUMG</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">http://www.iaria.org/conferences2015/PATTERNS15.html ; Seventh
  International Conferences on Pervasive Patterns and Applications (PATTERNS
  2015), Mar 2015, Nice, France</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.06811v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.06811v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.00022v1</id>
    <updated>2015-02-27T21:57:16Z</updated>
    <published>2015-02-27T21:57:16Z</published>
    <title>Plagiarism Detection in Polyphonic Music using Monaural Signal
  Separation</title>
    <summary>  Given the large number of new musical tracks released each year, automated
approaches to plagiarism detection are essential to help us track potential
violations of copyright. Most current approaches to plagiarism detection are
based on musical similarity measures, which typically ignore the issue of
polyphony in music. We present a novel feature space for audio derived from
compositional modelling techniques, commonly used in signal separation, that
provides a mechanism to account for polyphony without incurring an inordinate
amount of computational overhead. We employ this feature representation in
conjunction with traditional audio feature representations in a classification
framework which uses an ensemble of distance features to characterize pairs of
songs as being plagiarized or not. Our experiments on a database of about 3000
musical track pairs show that the new feature space characterization produces
significant improvements over standard baselines.
</summary>
    <author>
      <name>Soham De</name>
    </author>
    <author>
      <name>Indradyumna Roy</name>
    </author>
    <author>
      <name>Tarunima Prabhakar</name>
    </author>
    <author>
      <name>Kriti Suneja</name>
    </author>
    <author>
      <name>Sourish Chaudhuri</name>
    </author>
    <author>
      <name>Rita Singh</name>
    </author>
    <author>
      <name>Bhiksha Raj</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint version</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">INTERSPEECH-2012, 1744-1747 (2012)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1503.00022v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.00022v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.07150v2</id>
    <updated>2015-07-09T17:51:02Z</updated>
    <published>2015-03-24T19:45:02Z</published>
    <title>Acoustic event detection for multiple overlapping similar sources</title>
    <summary>  Many current paradigms for acoustic event detection (AED) are not adapted to
the organic variability of natural sounds, and/or they assume a limit on the
number of simultaneous sources: often only one source, or one source of each
type, may be active. These aspects are highly undesirable for applications such
as bird population monitoring. We introduce a simple method modelling the
onsets, durations and offsets of acoustic events to avoid intrinsic limits on
polyphony or on inter-event temporal patterns. We evaluate the method in a case
study with over 3000 zebra finch calls. In comparison against a HMM-based
method we find it more accurate at recovering acoustic events, and more robust
for estimating calling rates.
</summary>
    <author>
      <name>Dan Stowell</name>
    </author>
    <author>
      <name>David Clayton</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for WASPAA 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.07150v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.07150v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.02945v1</id>
    <updated>2015-04-12T08:44:56Z</updated>
    <published>2015-04-12T08:44:56Z</published>
    <title>Deep Transform: Cocktail Party Source Separation via Complex Convolution
  in a Deep Neural Network</title>
    <summary>  Convolutional deep neural networks (DNN) are state of the art in many
engineering problems but have not yet addressed the issue of how to deal with
complex spectrograms. Here, we use circular statistics to provide a convenient
probabilistic estimate of spectrogram phase in a complex convolutional DNN. In
a typical cocktail party source separation scenario, we trained a convolutional
DNN to re-synthesize the complex spectrograms of two source speech signals
given a complex spectrogram of the monaural mixture - a discriminative deep
transform (DT). We then used this complex convolutional DT to obtain
probabilistic estimates of the magnitude and phase components of the source
spectrograms. Our separation results are on a par with equivalent binary-mask
based non-complex separation approaches.
</summary>
    <author>
      <name>Andrew J. R. Simpson</name>
    </author>
    <link href="http://arxiv.org/abs/1504.02945v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.02945v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.03128v1</id>
    <updated>2015-04-13T11:08:00Z</updated>
    <published>2015-04-13T11:08:00Z</published>
    <title>Absolute Geometry Calibration of Distributed Microphone Arrays in an
  Audio-Visual Sensor Network</title>
    <summary>  Joint audio-visual speaker tracking requires that the locations of
microphones and cameras are known and that they are given in a common
coordinate system. Sensor self-localization algorithms, however, are usually
separately developed for either the acoustic or the visual modality and return
their positions in a modality specific coordinate system, often with an unknown
rotation, scaling and translation between the two. In this paper we propose two
techniques to determine the positions of acoustic sensors in a common
coordinate system, based on audio-visual correlates, i.e., events that are
localized by both, microphones and cameras separately. The first approach maps
the output of an acoustic self-calibration algorithm by estimating rotation,
scale and translation to the visual coordinate system, while the second solves
a joint system of equations with acoustic and visual directions of arrival as
input. The evaluation of the two strategies reveals that joint calibration
outperforms the mapping approach and achieves an overall calibration error of
0.20m even in reverberant environments.
</summary>
    <author>
      <name>Florian Jacob</name>
    </author>
    <author>
      <name>Reinhold Haeb-Umbach</name>
    </author>
    <link href="http://arxiv.org/abs/1504.03128v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.03128v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.04658v1</id>
    <updated>2015-04-17T23:07:17Z</updated>
    <published>2015-04-17T23:07:17Z</published>
    <title>Deep Karaoke: Extracting Vocals from Musical Mixtures Using a
  Convolutional Deep Neural Network</title>
    <summary>  Identification and extraction of singing voice from within musical mixtures
is a key challenge in source separation and machine audition. Recently, deep
neural networks (DNN) have been used to estimate 'ideal' binary masks for
carefully controlled cocktail party speech separation problems. However, it is
not yet known whether these methods are capable of generalizing to the
discrimination of voice and non-voice in the context of musical mixtures. Here,
we trained a convolutional DNN (of around a billion parameters) to provide
probabilistic estimates of the ideal binary mask for separation of vocal sounds
from real-world musical mixtures. We contrast our DNN results with more
traditional linear methods. Our approach may be useful for automatic removal of
vocal sounds from musical mixtures for 'karaoke' type applications.
</summary>
    <author>
      <name>Andrew J. R. Simpson</name>
    </author>
    <author>
      <name>Gerard Roma</name>
    </author>
    <author>
      <name>Mark D. Plumbley</name>
    </author>
    <link href="http://arxiv.org/abs/1504.04658v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.04658v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.07372v1</id>
    <updated>2015-04-28T08:18:55Z</updated>
    <published>2015-04-28T08:18:55Z</published>
    <title>Time-Frequency Trade-offs for Audio Source Separation with Binary Masks</title>
    <summary>  The short-time Fourier transform (STFT) provides the foundation of
binary-mask based audio source separation approaches. In computing a
spectrogram, the STFT window size parameterizes the trade-off between time and
frequency resolution. However, it is not yet known how this parameter affects
the operation of the binary mask in terms of separation quality for real-world
signals such as speech or music. Here, we demonstrate that the trade-off
between time and frequency in the STFT, used to perform ideal binary mask
separation, depends upon the types of source that are to be separated. In
particular, we demonstrate that different window sizes are optimal for
separating different combinations of speech and musical signals. Our findings
have broad implications for machine audition and machine learning in general.
</summary>
    <author>
      <name>Andrew J. R. Simpson</name>
    </author>
    <link href="http://arxiv.org/abs/1504.07372v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.07372v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65Txx" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.08021v1</id>
    <updated>2015-04-29T20:56:42Z</updated>
    <published>2015-04-29T20:56:42Z</published>
    <title>Who Spoke What? A Latent Variable Framework for the Joint Decoding of
  Multiple Speakers and their Keywords</title>
    <summary>  In this paper, we present a latent variable (LV) framework to identify all
the speakers and their keywords given a multi-speaker mixture signal. We
introduce two separate LVs to denote active speakers and the keywords uttered.
The dependency of a spoken keyword on the speaker is modeled through a
conditional probability mass function. The distribution of the mixture signal
is expressed in terms of the LV mass functions and speaker-specific-keyword
models. The proposed framework admits stochastic models, representing the
probability density function of the observation vectors given that a particular
speaker uttered a specific keyword, as speaker-specific-keyword models. The LV
mass functions are estimated in a Maximum Likelihood framework using the
Expectation Maximization (EM) algorithm. The active speakers and their keywords
are detected as modes of the joint distribution of the two LVs. In mixture
signals, containing two speakers uttering the keywords simultaneously, the
proposed framework achieves an accuracy of 82% for detecting both the speakers
and their respective keywords, using Student's-t mixture models as
speaker-specific-keyword models.
</summary>
    <author>
      <name>Harshavardhan Sundar</name>
    </author>
    <author>
      <name>Thippur V. Sreenivas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures Submitted to : IEEE Signal Processing Letters</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.08021v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.08021v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.08177v2</id>
    <updated>2015-05-30T02:21:43Z</updated>
    <published>2015-04-30T11:49:38Z</published>
    <title>Noise Sensitivity of Teager-Kaiser Energy Operators and Their Ratios</title>
    <summary>  The Teager-Kaiser energy operator (TKO) belongs to a class of autocorrelators
and their linear combination that can track the instantaneous energy of a
nonstationary sinusoidal signal source. TKO-based monocomponent AM-FM
demodulation algorithms work under the basic assumption that the operator
outputs are always positive. In the absence of noise, this is assured for pure
sinusoidal inputs and the instantaneous property is also guaranteed. Noise
invalidates both of these, particularly under small signal conditions.
Post-detection filtering and thresholding are of use to reestablish these at
the cost of some time to acquire. Key questions are: (a) how many samples must
one use and (b) how much noise power at the detector input can one tolerate.
Results of study of the role of delay and the limits imposed by additive
Gaussian noise are presented along with the computation of the cumulants and
probability density functions of the individual quadratic forms and their
ratios.
</summary>
    <author>
      <name>Pradeep Kr. Banerjee</name>
    </author>
    <author>
      <name>Nirmal B. Chakrabarti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.08177v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.08177v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.00289v1</id>
    <updated>2015-05-01T22:10:58Z</updated>
    <published>2015-05-01T22:10:58Z</published>
    <title>Deep Remix: Remixing Musical Mixtures Using a Convolutional Deep Neural
  Network</title>
    <summary>  Audio source separation is a difficult machine learning problem and
performance is measured by comparing extracted signals with the component
source signals. However, if separation is motivated by the ultimate goal of
re-mixing then complete separation is not necessary and hence separation
difficulty and separation quality are dependent on the nature of the re-mix.
Here, we use a convolutional deep neural network (DNN), trained to estimate
'ideal' binary masks for separating voice from music, to perform re-mixing of
the vocal balance by operating directly on the individual magnitude components
of the musical mixture spectrogram. Our results demonstrate that small changes
in vocal gain may be applied with very little distortion to the ultimate
re-mix. Our method may be useful for re-mixing existing mixes.
</summary>
    <author>
      <name>Andrew J. R Simpson</name>
    </author>
    <author>
      <name>Gerard Roma</name>
    </author>
    <author>
      <name>Mark D. Plumbley</name>
    </author>
    <link href="http://arxiv.org/abs/1505.00289v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.00289v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.04385v1</id>
    <updated>2015-05-17T12:09:48Z</updated>
    <published>2015-05-17T12:09:48Z</published>
    <title>An Efficient Parameterization of the Room Transfer Function</title>
    <summary>  This paper proposes an efficient parameterization of the Room Transfer
Function (RTF). Typically, the RTF rapidly varies with varying source and
receiver positions, hence requires an impractical number of point to point
measurements to characterize a given room. Therefore, we derive a novel RTF
parameterization that is robust to both receiver and source variations with the
following salient features: (i) The parameterization is given in terms of a
modal expansion of 3D basis functions. (ii) The aforementioned modal expansion
can be truncated at a finite number of modes given that the source and receiver
locations are from two sizeable spatial regions, which are arbitrarily
distributed. (iii) The parameter weights/coefficients are independent of the
source/receiver positions. Therefore, a finite set of coefficients is shown to
be capable of accurately calculating the RTF between any two arbitrary points
from a predefined spatial region where the source(s) lie and a pre-defined
spatial region where the receiver(s) lie. A practical method to measure the RTF
coefficients is also provided, which only requires a single microphone unit and
a single loudspeaker unit, given that the room characteristics remain
stationary over time. The accuracy of the above parameterization is verified
using appropriate simulation examples.
</summary>
    <author>
      <name>Prasanga Samarasinghe</name>
    </author>
    <author>
      <name>Thushara Abhayapala</name>
    </author>
    <author>
      <name>Mark Poletti</name>
    </author>
    <author>
      <name>Terence Betlehem</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.04385v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.04385v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.01830v2</id>
    <updated>2015-06-09T12:35:51Z</updated>
    <published>2015-06-05T09:20:00Z</published>
    <title>Sparsity and cosparsity for audio declipping: a flexible non-convex
  approach</title>
    <summary>  This work investigates the empirical performance of the sparse synthesis
versus sparse analysis regularization for the ill-posed inverse problem of
audio declipping. We develop a versatile non-convex heuristics which can be
readily used with both data models. Based on this algorithm, we report that, in
most cases, the two models perform almost similarly in terms of signal
enhancement. However, the analysis version is shown to be amenable for real
time audio processing, when certain analysis operators are considered. Both
versions outperform state-of-the-art methods in the field, especially for the
severely saturated signals.
</summary>
    <author>
      <name>Srđan Kitić</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PANAMA</arxiv:affiliation>
    </author>
    <author>
      <name>Nancy Bertin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PANAMA</arxiv:affiliation>
    </author>
    <author>
      <name>Rémi Gribonval</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PANAMA</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">LVA/ICA 2015 - The 12th International Conference on Latent
  Variable Analysis and Signal Separation, Aug 2015, Liberec, Czech Republic</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1506.01830v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.01830v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.02170v1</id>
    <updated>2015-06-06T17:05:00Z</updated>
    <published>2015-06-06T17:05:00Z</published>
    <title>Hybridized Feature Extraction and Acoustic Modelling Approach for
  Dysarthric Speech Recognition</title>
    <summary>  Dysarthria is malfunctioning of motor speech caused by faintness in the human
nervous system. It is characterized by the slurred speech along with physical
impairment which restricts their communication and creates the lack of
confidence and affects the lifestyle. This paper attempt to increase the
efficiency of Automatic Speech Recognition (ASR) system for unimpaired speech
signal. It describes state of art of research into improving ASR for speakers
with dysarthria by means of incorporated knowledge of their speech production.
Hybridized approach for feature extraction and acoustic modelling technique
along with evolutionary algorithm is proposed for increasing the efficiency of
the overall system. Here number of feature vectors are varied and tested the
system performance. It is observed that system performance is boosted by
genetic algorithm. System with 16 acoustic features optimized with genetic
algorithm has obtained highest recognition rate of 98.28% with training time of
5:30:17.
</summary>
    <author>
      <name>Megha Rughani</name>
    </author>
    <author>
      <name>D. Shivakrishna</name>
    </author>
    <link href="http://arxiv.org/abs/1506.02170v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.02170v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.03604v1</id>
    <updated>2015-06-11T09:55:59Z</updated>
    <published>2015-06-11T09:55:59Z</published>
    <title>Binaural coherent-to-diffuse-ratio estimation for dereverberation using
  an ITD model</title>
    <summary>  Most previously proposed dual-channel coherent-to-diffuse-ratio (CDR)
estimators are based on a free-field model. When used for binaural signals,
e.g., for dereverberation in binaural hearing aids, their performance may
degrade due to the influence of the head, even when the direction-of-arrival of
the desired speaker is exactly known. In this paper, the head shadowing effect
is taken into account for CDR estimation by using a simplified model for the
frequency-dependent interaural time difference and a model for the binaural
coherence of the diffuse noise field. Evaluation of CDR-based dereverberation
with measured binaural impulse responses indicates that the proposed binaural
CDR estimators can improve PESQ scores.
</summary>
    <author>
      <name>Chengshi Zheng</name>
    </author>
    <author>
      <name>Andreas Schwarz</name>
    </author>
    <author>
      <name>Walter Kellermann</name>
    </author>
    <author>
      <name>Xiaodong Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted for EUSIPCO 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.03604v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.03604v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.06832v1</id>
    <updated>2015-06-23T00:28:08Z</updated>
    <published>2015-06-23T00:28:08Z</published>
    <title>Detection and Analysis of Emotion From Speech Signals</title>
    <summary>  Recognizing emotion from speech has become one the active research themes in
speech processing and in applications based on human-computer interaction. This
paper conducts an experimental study on recognizing emotions from human speech.
The emotions considered for the experiments include neutral, anger, joy and
sadness. The distinuishability of emotional features in speech were studied
first followed by emotion classification performed on a custom dataset. The
classification was performed for different classifiers. One of the main feature
attribute considered in the prepared dataset was the peak-to-peak distance
obtained from the graphical representation of the speech signals. After
performing the classification tests on a dataset formed from 30 different
subjects, it was found that for getting better accuracy, one should consider
the data collected from one person rather than considering the data from a
group of people.
</summary>
    <author>
      <name>Assel Davletcharova</name>
    </author>
    <author>
      <name>Sherin Sugathan</name>
    </author>
    <author>
      <name>Bibia Abraham</name>
    </author>
    <author>
      <name>Alex Pappachen James</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2nd International Symposium on Computer Vision and the Internet,
  2015; to appear in Procedia Computer Science Journal, Elsevier, 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.06832v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.06832v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.00201v1</id>
    <updated>2015-07-01T12:13:10Z</updated>
    <published>2015-07-01T12:13:10Z</published>
    <title>Towards a Generalization of Relative Transfer Functions to More Than One
  Source</title>
    <summary>  We propose a natural way to generalize relative transfer functions (RTFs) to
more than one source. We first prove that such a generalization is not possible
using a single multichannel spectro-temporal observation, regardless of the
number of microphones. We then introduce a new transform for multichannel
multi-frame spectrograms, i.e., containing several channels and time frames in
each time-frequency bin. This transform allows a natural generalization which
satisfies the three key properties of RTFs, namely, they can be directly
estimated from observed signals, they capture spatial properties of the sources
and they do not depend on emitted signals. Through simulated experiments, we
show how this new method can localize multiple simultaneously active sound
sources using short spectro-temporal windows, without relying on source
separation.
</summary>
    <author>
      <name>Antoine Deleforge</name>
    </author>
    <author>
      <name>Sharon Gannot</name>
    </author>
    <author>
      <name>Walter Kellermann</name>
    </author>
    <link href="http://arxiv.org/abs/1507.00201v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.00201v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.05546v1</id>
    <updated>2015-07-20T16:06:23Z</updated>
    <published>2015-07-20T16:06:23Z</published>
    <title>Automatic Identification of Animal Breeds and Species Using Bioacoustics
  and Artificial Neural Networks</title>
    <summary>  In this research endeavor, it was hypothesized that the sound produced by
animals during their vocalizations can be used as identifiers of the animal
breed or species even if they sound the same to unaided human ear. To test this
hypothesis, three artificial neural networks (ANNs) were developed using
bioacoustics properties as inputs for the respective automatic identification
of 13 bird species, eight dog breeds, and 11 frog species. Recorded
vocalizations of these animals were collected and processed using several known
signal processing techniques to convert the respective sounds into computable
bioacoustics values. The converted values of the vocalizations, together with
the breed or species identifications, were used to train the ANNs following a
ten-fold cross validation technique. Tests show that the respective ANNs can
correctly identify 71.43\% of the birds, 94.44\% of the dogs, and 90.91\% of
the frogs. This result show that bioacoustics and ANN can be used to
automatically determine animal breeds and species, which together could be a
promising automated tool for animal identification, biodiversity determination,
animal conservation, and other animal welfare efforts.
</summary>
    <author>
      <name>Jaderick P. Pabico</name>
    </author>
    <author>
      <name>Anne Muriel V. Gonzales</name>
    </author>
    <author>
      <name>Mariann Jocel S. Villanueva</name>
    </author>
    <author>
      <name>Arlene A. Mendoza</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.05546v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.05546v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.08074v1</id>
    <updated>2015-07-29T09:22:58Z</updated>
    <published>2015-07-29T09:22:58Z</published>
    <title>STC Anti-spoofing Systems for the ASVspoof 2015 Challenge</title>
    <summary>  This paper presents the Speech Technology Center (STC) systems submitted to
Automatic Speaker Verification Spoofing and Countermeasures (ASVspoof)
Challenge 2015. In this work we investigate different acoustic feature spaces
to determine reliable and robust countermeasures against spoofing attacks. In
addition to the commonly used front-end MFCC features we explored features
derived from phase spectrum and features based on applying the multiresolution
wavelet transform. Similar to state-of-the-art ASV systems, we used the
standard TV-JFA approach for probability modelling in spoofing detection
systems. Experiments performed on the development and evaluation datasets of
the Challenge demonstrate that the use of phase-related and wavelet-based
features provides a substantial input into the efficiency of the resulting STC
systems. In our research we also focused on the comparison of the linear (SVM)
and nonlinear (DBN) classifiers.
</summary>
    <author>
      <name>Sergey Novoselov</name>
    </author>
    <author>
      <name>Alexandr Kozlov</name>
    </author>
    <author>
      <name>Galina Lavrentyeva</name>
    </author>
    <author>
      <name>Konstantin Simonchik</name>
    </author>
    <author>
      <name>Vadim Shchemelinin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 8 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.08074v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.08074v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.00354v1</id>
    <updated>2015-08-03T09:28:22Z</updated>
    <published>2015-08-03T09:28:22Z</published>
    <title>Significance of Maximum Spectral Amplitude in Sub-bands for Spectral
  Envelope Estimation and Its Application to Statistical Parametric Speech
  Synthesis</title>
    <summary>  In this paper we propose a technique for spectral envelope estimation using
maximum values in the sub-bands of Fourier magnitude spectrum (MSASB). Most
other methods in the literature parametrize spectral envelope in cepstral
domain such as Mel-generalized cepstrum etc. Such cepstral domain
representations, although compact, are not readily interpretable. This
difficulty is overcome by our method which parametrizes in the spectral domain
itself. In our experiments, spectral envelope estimated using MSASB method was
incorporated in the STRAIGHT vocoder. Both objective and subjective results of
analysis-by-synthesis indicate that the proposed method is comparable to
STRAIGHT. We also evaluate the effectiveness of the proposed parametrization in
a statistical parametric speech synthesis framework using deep neural networks.
</summary>
    <author>
      <name>Sivanand Achanta</name>
    </author>
    <author>
      <name>Anandaswarup Vadapalli</name>
    </author>
    <author>
      <name>Sai Krishna R.</name>
    </author>
    <author>
      <name>Suryakanth V. Gangashetty</name>
    </author>
    <link href="http://arxiv.org/abs/1508.00354v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.00354v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.01746v2</id>
    <updated>2016-01-19T16:27:49Z</updated>
    <published>2015-08-07T16:20:52Z</published>
    <title>Using Deep Learning for Detecting Spoofing Attacks on Speech Signals</title>
    <summary>  It is well known that speaker verification systems are subject to spoofing
attacks. The Automatic Speaker Verification Spoofing and Countermeasures
Challenge -- ASVSpoof2015 -- provides a standard spoofing database, containing
attacks based on synthetic speech, along with a protocol for experiments. This
paper describes CPqD's systems submitted to the ASVSpoof2015 Challenge, based
on deep neural networks, working both as a classifier and as a feature
extraction module for a GMM and a SVM classifier. Results show the validity of
this approach, achieving less than 0.5\% EER for known attacks.
</summary>
    <author>
      <name>Alan Godoy</name>
    </author>
    <author>
      <name>Flávio Simões</name>
    </author>
    <author>
      <name>José Augusto Stuchi</name>
    </author>
    <author>
      <name>Marcus de Assis Angeloni</name>
    </author>
    <author>
      <name>Mário Uliani</name>
    </author>
    <author>
      <name>Ricardo Violato</name>
    </author>
    <link href="http://arxiv.org/abs/1508.01746v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.01746v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.04909v1</id>
    <updated>2015-08-20T08:07:10Z</updated>
    <published>2015-08-20T08:07:10Z</published>
    <title>Histogram of gradients of Time-Frequency Representations for Audio scene
  detection</title>
    <summary>  This paper addresses the problem of audio scenes classification and
contributes to the state of the art by proposing a novel feature. We build this
feature by considering histogram of gradients (HOG) of time-frequency
representation of an audio scene. Contrarily to classical audio features like
MFCC, we make the hypothesis that histogram of gradients are able to encode
some relevant informations in a time-frequency {representation:} namely, the
local direction of variation (in time and frequency) of the signal spectral
power. In addition, in order to gain more invariance and robustness, histogram
of gradients are locally pooled. We have evaluated the relevance of {the novel
feature} by comparing its performances with state-of-the-art competitors, on
several datasets, including a novel one that we provide, as part of our
contribution. This dataset, that we make publicly available, involves $19$
classes and contains about $900$ minutes of audio scene recording. We thus
believe that it may be the next standard dataset for evaluating audio scene
classification algorithms. Our comparison results clearly show that our
HOG-based features outperform its competitors
</summary>
    <author>
      <name>Alain Rakotomamonjy</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LITIS</arxiv:affiliation>
    </author>
    <author>
      <name>Gilles Gasso</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LITIS</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1508.04909v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.04909v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.02380v2</id>
    <updated>2016-04-06T15:03:41Z</updated>
    <published>2015-09-08T14:19:22Z</published>
    <title>Source localization and denoising: a perspective from the TDOA space</title>
    <summary>  In this manuscript, we formulate the problem of denoising Time Differences of
Arrival (TDOAs) in the TDOA space, i.e. the Euclidean space spanned by TDOA
measurements. The method consists of pre-processing the TDOAs with the purpose
of reducing the measurement noise. The complete set of TDOAs (i.e., TDOAs
computed at all microphone pairs) is known to form a redundant set, which lies
on a linear subspace in the TDOA space. Noise, however, prevents TDOAs from
lying exactly on this subspace. We therefore show that TDOA denoising can be
seen as a projection operation that suppresses the component of the noise that
is orthogonal to that linear subspace. We then generalize the projection
operator also to the cases where the set of TDOAs is incomplete. We
analytically show that this operator improves the localization accuracy, and we
further confirm that via simulation.
</summary>
    <author>
      <name>Marco Compagnoni</name>
    </author>
    <author>
      <name>Antonio Canclini</name>
    </author>
    <author>
      <name>Paolo Bestagini</name>
    </author>
    <author>
      <name>Fabio Antonacci</name>
    </author>
    <author>
      <name>Augusto Sarti</name>
    </author>
    <author>
      <name>Stefano Tubaro</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11045-016-0400-9</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11045-016-0400-9" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 9 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Multidimensional Systems and Signal Processing 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1509.02380v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.02380v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.04934v1</id>
    <updated>2015-09-16T14:28:49Z</updated>
    <published>2015-09-16T14:28:49Z</published>
    <title>Background-tracking Acoustic Features for Genre Identification of
  Broadcast Shows</title>
    <summary>  This paper presents a novel method for extracting acoustic features that
characterise the background environment in audio recordings. These features are
based on the output of an alignment that fits multiple parallel
background--based Constrained Maximum Likelihood Linear Regression
transformations asynchronously to the input audio signal. With this setup, the
resulting features can track changes in the audio background like appearance
and disappearance of music, applause or laughter, independently of the speakers
in the foreground of the audio. The ability to provide this type of acoustic
description in audiovisual data has many potential applications, including
automatic classification of broadcast archives or improving automatic
transcription and subtitling. In this paper, the performance of these features
in a genre identification task in a set of 332 BBC shows is explored. The
proposed background--tracking features outperform short--term Perceptual Linear
Prediction features in this task using Gaussian Mixture Model classifiers (62%
vs 72% accuracy). The use of more complex classifiers, Hidden Markov Models and
Support Vector Machines, increases the performance of the system with the novel
background--tracking features to 79% and 81% in accuracy respectively.
</summary>
    <author>
      <name>Oscar Saz</name>
    </author>
    <author>
      <name>Mortaza Doulaty</name>
    </author>
    <author>
      <name>Thomas Hain</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/SLT.2014.7078560</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/SLT.2014.7078560" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Spoken Language Technology Workshop (SLT 2014), pp118-123,
  7-10 Dec 2014, Lake Tahoe, NV, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1509.04934v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.04934v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.04956v1</id>
    <updated>2015-09-16T15:56:22Z</updated>
    <published>2015-09-16T15:56:22Z</published>
    <title>Melodic Contour and Mid-Level Global Features Applied to the Analysis of
  Flamenco Cantes</title>
    <summary>  This work focuses on the topic of melodic characterization and similarity in
a specific musical repertoire: a cappella flamenco singing, more specifically
in debla and martinete styles. We propose the combination of manual and
automatic description. First, we use a state-of-the-art automatic transcription
method to account for general melodic similarity from music recordings. Second,
we define a specific set of representative mid-level melodic features, which
are manually labeled by flamenco experts. Both approaches are then contrasted
and combined into a global similarity measure. This similarity measure is
assessed by inspecting the clusters obtained through phylogenetic algorithms
algorithms and by relating similarity to categorization in terms of style.
Finally, we discuss the advantage of combining automatic and expert annotations
as well as the need to include repertoire-specific descriptions for meaningful
melodic characterization in traditional music collections.
</summary>
    <author>
      <name>Francisco Gómez</name>
    </author>
    <author>
      <name>Joaquín Mora</name>
    </author>
    <author>
      <name>Emilia Gómez</name>
    </author>
    <author>
      <name>José Miguel Díaz-Báñez</name>
    </author>
    <link href="http://arxiv.org/abs/1509.04956v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.04956v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.06103v1</id>
    <updated>2015-09-21T03:37:11Z</updated>
    <published>2015-09-21T03:37:11Z</published>
    <title>Noise Robust IOA/CAS Speech Separation and Recognition System For The
  Third 'CHIME' Challenge</title>
    <summary>  This paper presents the contribution to the third 'CHiME' speech separation
and recognition challenge including both front-end signal processing and
back-end speech recognition. In the front-end, Multi-channel Wiener filter
(MWF) is designed to achieve background noise reduction. Different from
traditional MWF, optimized parameter for the tradeoff between noise reduction
and target signal distortion is built according to the desired noise reduction
level. In the back-end, several techniques are taken advantage to improve the
noisy Automatic Speech Recognition (ASR) performance including Deep Neural
Network (DNN), Convolutional Neural Network (CNN) and Long short-term memory
(LSTM) using medium vocabulary, Lattice rescoring with a big vocabulary
language model finite state transducer, and ROVER scheme. Experimental results
show the proposed system combining front-end and back-end is effective to
improve the ASR performance.
</summary>
    <author>
      <name>Xiaofei Wang</name>
    </author>
    <author>
      <name>Chao Wu</name>
    </author>
    <author>
      <name>Pengyuan Zhang</name>
    </author>
    <author>
      <name>Ziteng Wang</name>
    </author>
    <author>
      <name>Yong Liu</name>
    </author>
    <author>
      <name>Xu Li</name>
    </author>
    <author>
      <name>Qiang Fu</name>
    </author>
    <author>
      <name>Yonghong Yan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.06103v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.06103v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.06882v1</id>
    <updated>2015-09-23T08:34:18Z</updated>
    <published>2015-09-23T08:34:18Z</published>
    <title>Robust coherence-based spectral enhancement for distant speech
  recognition</title>
    <summary>  In this contribution to the 3rd CHiME Speech Separation and Recognition
Challenge (CHiME-3) we extend the acoustic front-end of the CHiME-3 baseline
speech recognition system by a coherence-based Wiener filter which is applied
to the output signal of the baseline beamformer. To compute the time- and
frequency-dependent postfilter gains the ratio between direct and diffuse
signal components at the output of the baseline beamformer is estimated and
used as approximation of the short-time signal-to-noise ratio. The proposed
spectral enhancement technique is evaluated with respect to word error rates of
the CHiME-3 challenge baseline speech recognition system using real speech
recorded in public environments. Results confirm the effectiveness of the
coherence-based postfilter when integrated into the front-end signal
enhancement.
</summary>
    <author>
      <name>Hendrik Barfuss</name>
    </author>
    <author>
      <name>Christian Huemmer</name>
    </author>
    <author>
      <name>Andreas Schwarz</name>
    </author>
    <author>
      <name>Walter Kellermann</name>
    </author>
    <link href="http://arxiv.org/abs/1509.06882v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.06882v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.07211v1</id>
    <updated>2015-09-24T02:16:11Z</updated>
    <published>2015-09-24T02:16:11Z</published>
    <title>Noise-Robust ASR for the third 'CHiME' Challenge Exploiting
  Time-Frequency Masking based Multi-Channel Speech Enhancement and Recurrent
  Neural Network</title>
    <summary>  In this paper, the Lingban entry to the third 'CHiME' speech separation and
recognition challenge is presented. A time-frequency masking based speech
enhancement front-end is proposed to suppress the environmental noise utilizing
multi-channel coherence and spatial cues. The state-of-the-art speech
recognition techniques, namely recurrent neural network based acoustic and
language modeling, state space minimum Bayes risk based discriminative acoustic
modeling, and i-vector based acoustic condition modeling, are carefully
integrated into the speech recognition back-end. To further improve the system
performance by fully exploiting the advantages of different technologies, the
final recognition results are obtained by lattice combination and rescoring.
Evaluations carried out on the official dataset prove the effectiveness of the
proposed systems. Comparing with the best baseline result, the proposed system
obtains consistent improvements with over 57% relative word error rate
reduction on the real-data test set.
</summary>
    <author>
      <name>Zaihu Pang</name>
    </author>
    <author>
      <name>Fengyun Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 3rd 'CHiME' Speech Separation and Recognition Challenge, 5 pages,
  1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.07211v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.07211v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.09113v1</id>
    <updated>2015-09-30T10:33:41Z</updated>
    <published>2015-09-30T10:33:41Z</published>
    <title>Processing of acoustical signals via a wavelet-based analysis</title>
    <summary>  In the present paper, details are given on the implementation of a
wavelet-based analysis tailored to the processing of acoustical signals. The
family of the suitable wavelets (`Reimann wavelets') are obtained in the time
domain from a Fourier transform, extracted in Ref.~\cite{r1} after invoking
theoretical principles and time-frequency localisation constraints. A scheme is
set forth to determine the optimal values of the parameters of this type of
wavelet on the basis of the goodness of the reproduction of a $30$-s audio file
containing harmonic signals corresponding to six successive $A$ notes of the
chromatic musical scale, from $A_2$ to $A_7$. The quality of the reproduction
over about six and a half octaves is investigated. Finally, details are given
on the incorporation of the re-assignment method in the analysis framework, as
the means a) to determine the important contributions of the wavelet transforms
and b) to suppress noise present in the signal.
</summary>
    <author>
      <name>Evangelos Matsinos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 8 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.09113v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.09113v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.00268v1</id>
    <updated>2015-10-01T14:57:08Z</updated>
    <published>2015-10-01T14:57:08Z</published>
    <title>The ICSTM+TUM+UP Approach to the 3rd CHIME Challenge: Single-Channel
  LSTM Speech Enhancement with Multi-Channel Correlation Shaping
  Dereverberation and LSTM Language Models</title>
    <summary>  This paper presents our contribution to the 3rd CHiME Speech Separation and
Recognition Challenge. Our system uses Bidirectional Long Short-Term Memory
(BLSTM) Recurrent Neural Networks (RNNs) for Single-channel Speech Enhancement
(SSE). Networks are trained to predict clean speech as well as noise features
from noisy speech features. In addition, the system applies two methods of
dereverberation on the 6-channel recordings of the challenge. The first is the
Phase-Error based Filtering (PEF) that uses time-varying phase-error filters
based on estimated time-difference of arrival of the speech source and the
phases of the microphone signals. The second is the Correlation Shaping (CS)
that applies a reduction of the long-term correlation energy in reverberant
speech. The Linear Prediction (LP) residual is processed to suppress the
long-term correlation. Furthermore, the system employs a LSTM Language Model
(LM) to perform N-best rescoring of recognition hypotheses. Using the proposed
methods, an improved Word Error Rate (WER) of 24.38% is achieved over the real
eval test set. This is around 25% relative improvement over the challenge
baseline.
</summary>
    <author>
      <name>Amr El-Desoky Mousa</name>
    </author>
    <author>
      <name>Erik Marchi</name>
    </author>
    <author>
      <name>Björn Schuller</name>
    </author>
    <link href="http://arxiv.org/abs/1510.00268v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.00268v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.01443v1</id>
    <updated>2015-10-06T06:12:31Z</updated>
    <published>2015-10-06T06:12:31Z</published>
    <title>A Waveform Representation Framework for High-quality Statistical
  Parametric Speech Synthesis</title>
    <summary>  State-of-the-art statistical parametric speech synthesis (SPSS) generally
uses a vocoder to represent speech signals and parameterize them into features
for subsequent modeling. Magnitude spectrum has been a dominant feature over
the years. Although perceptual studies have shown that phase spectrum is
essential to the quality of synthesized speech, it is often ignored by using a
minimum phase filter during synthesis and the speech quality suffers. To bypass
this bottleneck in vocoded speech, this paper proposes a phase-embedded
waveform representation framework and establishes a magnitude-phase joint
modeling platform for high-quality SPSS. Our experiments on waveform
reconstruction show that the performance is better than that of the widely-used
STRAIGHT. Furthermore, the proposed modeling and synthesis platform outperforms
a leading-edge, vocoded, deep bidirectional long short-term memory recurrent
neural network (DBLSTM-RNN)-based baseline system in various objective
evaluation metrics conducted.
</summary>
    <author>
      <name>Bo Fan</name>
    </author>
    <author>
      <name>Siu Wa Lee</name>
    </author>
    <author>
      <name>Xiaohai Tian</name>
    </author>
    <author>
      <name>Lei Xie</name>
    </author>
    <author>
      <name>Minghui Dong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted and will appear in APSIPA2015; keywords: speech synthesis,
  LSTM-RNN, vocoder, phase, waveform, modeling</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.01443v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.01443v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.01806v3</id>
    <updated>2017-01-12T01:45:08Z</updated>
    <published>2015-10-07T03:00:52Z</published>
    <title>Music Viewed by its Entropy Content: A Novel Window for Comparative
  Analysis</title>
    <summary>  Polyphonic music files were analyzed using the set of symbols that produced
the Minimal Entropy Description which we call the Fundamental Scale. This
allowed us to create a novel space to represent music pieces by developing: a)
a method to adjust a description from its original scale of observation to a
general scale, b) the concept of higher order entropy as the entropy associated
to the deviations of a frequency ranked symbol profile from a perfect Zipf
profile. We called this diversity index the "2nd Order Entropy". Applying these
methods to a variety of musical pieces showed how the space of "symbolic
specific diversity-entropy" and that of "2nd order entropy" captures
characteristics that are unique to each music type, style, composer and genre.
Some clustering of these properties around each musical category is shown. This
method allows to visualize a historic trajectory of academic music across this
space, from medieval to contemporary academic music. We show that description
of musical structures using entropy and symbolic diversity allows to
characterize traditional and popular expressions of music. These classification
techniques promise to be useful in other disciplines for pattern recognition
and machine learning, for example.
</summary>
    <author>
      <name>Gerardo Febres</name>
    </author>
    <author>
      <name>Klaus Jaffe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">41 pages, 15 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.01806v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.01806v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.03602v1</id>
    <updated>2015-10-13T09:51:23Z</updated>
    <published>2015-10-13T09:51:23Z</published>
    <title>A language model based approach towards large scale and lightweight
  language identification systems</title>
    <summary>  Multilingual spoken dialogue systems have gained prominence in the recent
past necessitating the requirement for a front-end Language Identification
(LID) system. Most of the existing LID systems rely on modeling the language
discriminative information from low-level acoustic features. Due to the
variabilities of speech (speaker and emotional variabilities, etc.),
large-scale LID systems developed using low-level acoustic features suffer from
a degradation in the performance. In this approach, we have attempted to model
the higher level language discriminative phonotactic information for developing
an LID system. In this paper, the input speech signal is tokenized to phone
sequences by using a language independent phone recognizer. The language
discriminative phonotactic information in the obtained phone sequences are
modeled using statistical and recurrent neural network based language modeling
approaches. As this approach, relies on higher level phonotactical information
it is more robust to variabilities of speech. Proposed approach is
computationally light weight, highly scalable and it can be used in complement
with the existing LID systems.
</summary>
    <author>
      <name>Brij Mohan Lal Srivastava</name>
    </author>
    <author>
      <name>Hari Krishna Vydana</name>
    </author>
    <author>
      <name>Anil Kumar Vuppala</name>
    </author>
    <author>
      <name>Manish Shrivastava</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Under review at ICASSP 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.03602v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.03602v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.04029v1</id>
    <updated>2015-10-14T10:14:29Z</updated>
    <published>2015-10-14T10:14:29Z</published>
    <title>Corpus COFLA: A research corpus for the Computational study of Flamenco
  Music</title>
    <summary>  Flamenco is a music tradition from Southern Spain which attracts a growing
community of enthusiasts around the world. Its unique melodic and rhythmic
elements, the typically spontaneous and improvised interpretation and its
diversity regarding styles make this still largely undocumented art form a
particularly interesting material for musicological studies. In prior works it
has already been demonstrated that research on computational analysis of
flamenco music, despite it being a relatively new field, can provide powerful
tools for the discovery and diffusion of this genre. In this paper we present
corpusCOFLA, a data framework for the development of such computational tools.
The proposed collection of audio recordings and meta-data serves as a pool for
creating annotated subsets which can be used in development and evaluation of
algorithms for specific music information retrieval tasks. First, we describe
the design criteria for the corpus creation and then provide various examples
of subsets drawn from the corpus. We showcase possible research applications in
the context of computational study of flamenco music and give perspectives
regarding further development of the corpus.
</summary>
    <author>
      <name>Nadine Kroher</name>
    </author>
    <author>
      <name>José-Miguel Díaz-Báñez</name>
    </author>
    <author>
      <name>Joaquin Mora</name>
    </author>
    <author>
      <name>Emilia Gómez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, submitted to the ACM Journal of Computing and Cultural
  Heritage</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.04029v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.04029v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.04039v1</id>
    <updated>2015-10-14T10:53:00Z</updated>
    <published>2015-10-14T10:53:00Z</published>
    <title>Automatic Transcription of Flamenco Singing from Polyphonic Music
  Recordings</title>
    <summary>  Automatic note-level transcription is considered one of the most challenging
tasks in music information retrieval. The specific case of flamenco singing
transcription poses a particular challenge due to its complex melodic
progressions, intonation inaccuracies, the use of a high degree of
ornamentation and the presence of guitar accompaniment. In this study, we
explore the limitations of existing state of the art transcription systems for
the case of flamenco singing and propose a specific solution for this genre: We
first extract the predominant melody and apply a novel contour filtering
process to eliminate segments of the pitch contour which originate from the
guitar accompaniment. We formulate a set of onset detection functions based on
volume and pitch characteristics to segment the resulting vocal pitch contour
into discrete note events. A quantised pitch label is assigned to each note
event by combining global pitch class probabilities with local pitch contour
statistics. The proposed system outperforms state of the art singing
transcription systems with respect to voicing accuracy, onset detection and
overall performance when evaluated on flamenco singing datasets.
</summary>
    <author>
      <name>Nadine Kroher</name>
    </author>
    <author>
      <name>Emilia Gómez</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TASLP.2016.2531284</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TASLP.2016.2531284" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to the IEEE Transactions on Audio, Speech and Language
  Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.04039v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.04039v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.04205v1</id>
    <updated>2015-10-14T17:17:33Z</updated>
    <published>2015-10-14T17:17:33Z</published>
    <title>Reducing one-to-many problem in Voice Conversion by equalizing the
  formant locations using dynamic frequency warping</title>
    <summary>  In this study, we investigate a solution to reduce the effect of one-to-many
problem in voice conversion. One-to-many problem in VC happens when two very
similar speech segments in source speaker have corresponding speech segments in
target speaker that are not similar to each other. As a result, the mapper
function usually over-smoothes the generated features in order to be similar to
both target speech segments. In this study, we propose to equalize the formant
location of source-target frame pairs using dynamic frequency warping in order
to reduce the complexity. After the conversion, another dynamic frequency
warping is further applied to reverse the effect of formant location
equalization during the training. The subjective experiments showed that the
proposed approach improves the speech quality significantly.
</summary>
    <author>
      <name>Seyed Hamidreza Mohammadi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.04205v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.04205v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.04595v3</id>
    <updated>2016-04-15T10:10:30Z</updated>
    <published>2015-10-15T15:48:19Z</published>
    <title>A Variational EM Algorithm for the Separation of Time-Varying
  Convolutive Audio Mixtures</title>
    <summary>  This paper addresses the problem of separating audio sources from
time-varying convolutive mixtures. We propose a probabilistic framework based
on the local complex-Gaussian model combined with non-negative matrix
factorization. The time-varying mixing filters are modeled by a continuous
temporal stochastic process. We present a variational expectation-maximization
(VEM) algorithm that employs a Kalman smoother to estimate the time-varying
mixing matrix, and that jointly estimate the source parameters. The sound
sources are then separated by Wiener filters constructed with the estimators
provided by the VEM algorithm. Extensive experiments on simulated data show
that the proposed method outperforms a block-wise version of a state-of-the-art
baseline method.
</summary>
    <author>
      <name>Dionyssos Kounades-Bastian</name>
    </author>
    <author>
      <name>Laurent Girin</name>
    </author>
    <author>
      <name>Xavier Alameda-Pineda</name>
    </author>
    <author>
      <name>Sharon Gannot</name>
    </author>
    <author>
      <name>Radu Horaud</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TASLP.2016.2554286</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TASLP.2016.2554286" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 4 figures, 2 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE/ACM Transactions on Audio, Speech and Language Processing,
  24(8), 1408-1423, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1510.04595v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.04595v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.04616v1</id>
    <updated>2015-10-15T16:42:16Z</updated>
    <published>2015-10-15T16:42:16Z</published>
    <title>Evaluating the Non-Intrusive Room Acoustics Algorithm with the ACE
  Challenge</title>
    <summary>  We present a single channel data driven method for non-intrusive estimation
of full-band reverberation time and full-band direct-to-reverberant ratio. The
method extracts a number of features from reverberant speech and builds a model
using a recurrent neural network to estimate the reverberant acoustic
parameters. We explore three configurations by including different data and
also by combining the recurrent neural network estimates using a support vector
machine. Our best method to estimate DRR provides a Root Mean Square Deviation
(RMSD) of 3.84 dB and a RMSD of 43.19 % for T60 estimation.
</summary>
    <author>
      <name>Pablo Peso Parada</name>
    </author>
    <author>
      <name>Dushyant Sharma</name>
    </author>
    <author>
      <name>Toon van Waterschoot</name>
    </author>
    <author>
      <name>Patrick A. Naylor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the ACE Challenge Workshop - a satellite event of
  IEEE-WASPAA 2015 (arXiv:1510.00383)</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.04616v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.04616v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.04620v1</id>
    <updated>2015-10-15T16:48:48Z</updated>
    <published>2015-10-15T16:48:48Z</published>
    <title>Joint Estimation of Reverberation Time and Direct-to-Reverberation Ratio
  from Speech using Auditory-Inspired Features</title>
    <summary>  Blind estimation of acoustic room parameters such as the reverberation time
$T_\mathrm{60}$ and the direct-to-reverberation ratio ($\mathrm{DRR}$) is still
a challenging task, especially in case of blind estimation from reverberant
speech signals. In this work, a novel approach is proposed for joint estimation
of $T_\mathrm{60}$ and $\mathrm{DRR}$ from wideband speech in noisy conditions.
2D Gabor filters arranged in a filterbank are exploited for extracting
features, which are then used as input to a multi-layer perceptron (MLP). The
MLP output neurons correspond to specific pairs of $(T_\mathrm{60},
\mathrm{DRR})$ estimates; the output is integrated over time, and a simple
decision rule results in our estimate. The approach is applied to
single-microphone fullband speech signals provided by the Acoustic
Characterization of Environments (ACE) Challenge. Our approach outperforms the
baseline systems with median errors of close-to-zero and -1.5 dB for the
$T_\mathrm{60}$ and $\mathrm{DRR}$ estimates, respectively, while the
calculation of estimates is 5.8 times faster compared to the baseline.
</summary>
    <author>
      <name>Feifei Xiong</name>
    </author>
    <author>
      <name>Stefan Goetze</name>
    </author>
    <author>
      <name>Bernd T. Meyer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the ACE Challenge Workshop - a satellite event of
  IEEE-WASPAA 2015 (arXiv:1510.00383)</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.04620v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.04620v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.04707v1</id>
    <updated>2015-10-15T20:18:29Z</updated>
    <published>2015-10-15T20:18:29Z</published>
    <title>SRMR variants for improved blind room acoustics characterization</title>
    <summary>  Reverberation, especially in large rooms, severely degrades speech
recognition performance and speech intelligibility. Since direct measurement of
room characteristics is usually not possible, blind estimation of
reverberation-related metrics such as the reverberation time (RT) and the
direct-to-reverberant energy ratio (DRR) can be valuable information to speech
recognition and enhancement algorithms operating in enclosed environments. The
objective of this work is to evaluate the performance of five variants of blind
RT and DRR estimators based on a modulation spectrum representation of
reverberant speech with single- and multi-channel speech data. These models are
all based on variants of the so-called Speech-to-Reverberation Modulation
Energy Ratio (SRMR). We show that these measures outperform a state-of-the-art
baseline based on maximum-likelihood estimation of sound decay rates in terms
of root-mean square error (RMSE), as well as Pearson correlation. Compared to
the baseline, the best proposed measure, called NSRMR_k , achieves a 23%
relative improvement in terms of RMSE and allows for relative correlation
improvements ranging from 13% to 47% for RT prediction.
</summary>
    <author>
      <name>M. Senoussaoui</name>
    </author>
    <author>
      <name>J. F. Santos</name>
    </author>
    <author>
      <name>T. H. Falk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the ACE Chal- lenge Workshop - a satellite event of
  IEEE-WASPAA 2015 (arXiv:1510.00383)</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.04707v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.04707v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.04880v1</id>
    <updated>2015-10-15T18:42:04Z</updated>
    <published>2015-10-15T18:42:04Z</published>
    <title>Harmonic and Timbre Analysis of Tabla Strokes</title>
    <summary>  Indian twin drums mainly bayan and dayan (tabla) are the most important
percussion instruments in India popularly used for keeping rhythm. It is a twin
percussion/drum instrument of which the right hand drum is called dayan and the
left hand drum is called bayan. Tabla strokes are commonly called as `bol',
constitutes a series of syllables. In this study we have studied the timbre
characteristics of nine strokes from each of five different tablas. Timbre
parameters were calculated from LTAS of each stroke signals. Study of timbre
characteristics is one of the most important deterministic approach for
analyzing tabla and its stroke characteristics. Statistical correlations among
timbre parameters were measured and also through factor analysis we get to know
about the parameters of timbre analysis which are closely related. Tabla
strokes have unique harmonic and timbral characteristics at mid frequency range
and have no uniqueness at low frequency ranges.
</summary>
    <author>
      <name>Anirban Patranabis</name>
    </author>
    <author>
      <name>Kaushik Banerjee</name>
    </author>
    <author>
      <name>Vishal Midya</name>
    </author>
    <author>
      <name>Sneha Chakraborty</name>
    </author>
    <author>
      <name>Shankha Sanyal</name>
    </author>
    <author>
      <name>Archi Banerjee</name>
    </author>
    <author>
      <name>Ranjan Sengupta</name>
    </author>
    <author>
      <name>Dipak Ghosh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.04880v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.04880v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.class-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.05937v2</id>
    <updated>2016-03-31T05:33:49Z</updated>
    <published>2015-10-20T15:49:59Z</published>
    <title>Binary Speaker Embedding</title>
    <summary>  The popular i-vector model represents speakers as low-dimensional continuous
vectors (i-vectors), and hence it is a way of continuous speaker embedding. In
this paper, we investigate binary speaker embedding, which transforms i-vectors
to binary vectors (codes) by a hash function. We start from locality sensitive
hashing (LSH), a simple binarization approach where binary codes are derived
from a set of random hash functions. A potential problem of LSH is that the
randomly sampled hash functions might be suboptimal. We therefore propose an
improved Hamming distance learning approach, where the hash function is learned
by a variable-sized block training that projects each dimension of the original
i-vectors to variable-sized binary codes independently. Our experiments show
that binary speaker embedding can deliver competitive or even better results on
both speaker verification and identification tasks, while the memory usage and
the computation cost are significantly reduced.
</summary>
    <author>
      <name>Lantian Li</name>
    </author>
    <author>
      <name>Dong Wang</name>
    </author>
    <author>
      <name>Chao Xing</name>
    </author>
    <author>
      <name>Kaimin Yu</name>
    </author>
    <author>
      <name>Thomas Fang Zheng</name>
    </author>
    <link href="http://arxiv.org/abs/1510.05937v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.05937v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.05940v2</id>
    <updated>2016-03-31T05:27:17Z</updated>
    <published>2015-10-20T16:01:05Z</published>
    <title>Max-margin Metric Learning for Speaker Recognition</title>
    <summary>  Probabilistic linear discriminant analysis (PLDA) is a popular normalization
approach for the i-vector model, and has delivered state-of-the-art performance
in speaker recognition. A potential problem of the PLDA model, however, is that
it essentially assumes Gaussian distributions over speaker vectors, which is
not always true in practice. Additionally, the objective function is not
directly related to the goal of the task, e.g., discriminating true speakers
and imposters. In this paper, we propose a max-margin metric learning approach
to solve the problems. It learns a linear transform with a criterion that the
margin between target and imposter trials are maximized. Experiments conducted
on the SRE08 core test show that compared to PLDA, the new approach can obtain
comparable or even better performance, though the scoring is simply a cosine
computation.
</summary>
    <author>
      <name>Lantian Li</name>
    </author>
    <author>
      <name>Dong Wang</name>
    </author>
    <author>
      <name>Chao Xing</name>
    </author>
    <author>
      <name>Thomas Fang Zheng</name>
    </author>
    <link href="http://arxiv.org/abs/1510.05940v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.05940v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.07315v1</id>
    <updated>2015-10-25T22:24:37Z</updated>
    <published>2015-10-25T22:24:37Z</published>
    <title>A Hybrid Approach for Speech Enhancement Using MoG Model and Neural
  Network Phoneme Classifier</title>
    <summary>  In this paper we present a single-microphone speech enhancement algorithm. A
hybrid approach is proposed merging the generative mixture of Gaussians (MoG)
model and the discriminative neural network (NN). The proposed algorithm is
executed in two phases, the training phase, which does not recur, and the test
phase. First, the noise-free speech power spectral density (PSD) is modeled as
a MoG, representing the phoneme based diversity in the speech signal. An NN is
then trained with phoneme labeled database for phoneme classification with
mel-frequency cepstral coefficients (MFCC) as the input features. Given the
phoneme classification results, a speech presence probability (SPP) is obtained
using both the generative and discriminative models. Soft spectral subtraction
is then executed while simultaneously, the noise estimation is updated. The
discriminative NN maintain the continuity of the speech and the generative
phoneme-based MoG preserves the speech spectral structure. Extensive
experimental study using real speech and noise signals is provided. We also
compare the proposed algorithm with alternative speech enhancement algorithms.
We show that we obtain a significant improvement over previous methods in terms
of both speech quality measures and speech recognition results.
</summary>
    <author>
      <name>Shlomo E. Chazan</name>
    </author>
    <author>
      <name>Jacob Goldberger</name>
    </author>
    <author>
      <name>Sharon Gannot</name>
    </author>
    <link href="http://arxiv.org/abs/1510.07315v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.07315v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.07546v1</id>
    <updated>2015-10-26T16:50:26Z</updated>
    <published>2015-10-26T16:50:26Z</published>
    <title>Direct-to-Reverberant Ratio Estimation on the ACE Corpus Using a
  Two-channel Beamformer</title>
    <summary>  Direct-to-Reverberant Ratio (DRR) is an important measure for characterizing
the properties of a room. The recently proposed DRR Estimation using a
Null-Steered Beamformer (DENBE) algorithm was originally tested on simulated
data where noise was artificially added to the speech after convolution with
impulse responses simulated using the image-source method. This paper evaluates
the performance of this algorithm on speech convolved with measured impulse
responses and noise using the Acoustic Characterization of Environments (ACE)
Evaluation corpus. The fullband DRR estimation performance of the DENBE
algorithm exceeds that of the baselines in all Signal-to-Noise Ratios (SNRs)
and noise types. In addition, estimation of the DRR in one third-octave ISO
frequency bands is demonstrated.
</summary>
    <author>
      <name>James Eaton</name>
    </author>
    <author>
      <name>Patrick A. Naylor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the ACE Challenge Workshop - a satellite event of
  IEEE-WASPAA 2015 (arXiv:1510.00383). arXiv admin note: text overlap with
  arXiv:1510.01193</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.07546v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.07546v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.07774v1</id>
    <updated>2015-10-27T05:25:11Z</updated>
    <published>2015-10-27T05:25:11Z</published>
    <title>A dictionary learning and source recovery based approach to classify
  diverse audio sources</title>
    <summary>  A dictionary learning based audio source classification algorithm is proposed
to classify a sample audio signal as one amongst a finite set of different
audio sources. Cosine similarity measure is used to select the atoms during
dictionary learning. Based on three objective measures proposed, namely, signal
to distortion ratio (SDR), the number of non-zero weights and the sum of
weights, a frame-wise source classification accuracy of 98.2% is obtained for
twelve different sources. Cent percent accuracy has been obtained using moving
SDR accumulated over six successive frames for ten of the audio sources tested,
while the two other sources require accumulation of 10 and 14 frames.
</summary>
    <author>
      <name>K V Vijay Girish</name>
    </author>
    <author>
      <name>T V Ananthapadmanabha</name>
    </author>
    <author>
      <name>A G Ramakrishnan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.07774v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.07774v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.08950v1</id>
    <updated>2015-10-30T01:52:31Z</updated>
    <published>2015-10-30T01:52:31Z</published>
    <title>Estimation of the direct-to-reverberant Energy Ratio using a spherical
  microphone array</title>
    <summary>  This paper proposes a practical approach to estimate the
direct-to-reverberant energy ratio (DRR) using a spherical microphone array
without having knowledge of the source signal. We base our estimation on a
theoretical relationship between the DRR and the coherence estimation function
between coincident pressure and particle velocity. We discuss the proposed
method's ability to estimate the DRR in a wide variety of room sizes,
reverberation times and source receiver distances with appropriate examples.
Test results show that the method can estimate the room DRR for frequencies
between 199 - 2511 Hz, with $\pm$ 3 dB accuracy.
</summary>
    <author>
      <name>Hanchi Chen</name>
    </author>
    <author>
      <name>Prasanga N. Samarasinghe</name>
    </author>
    <author>
      <name>Thushara D. Abhayapala</name>
    </author>
    <author>
      <name>Wen Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the ACE Challenge Workshop - a satellite event of
  IEEE-WASPAA 2015 (arXiv:1510.00383)</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.08950v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.08950v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.00067v1</id>
    <updated>2015-10-31T03:48:31Z</updated>
    <published>2015-10-31T03:48:31Z</published>
    <title>Sparsity-based Algorithm for Detecting Faults in Rotating Machines</title>
    <summary>  This paper addresses the detection of periodic transients in vibration
signals for detecting faults in rotating machines. For this purpose, we present
a method to estimate periodic-group-sparse signals in noise. The method is
based on the formulation of a convex optimization problem. A fast iterative
algorithm is given for its solution. A simulated signal is formulated to verify
the performance of the proposed approach for periodic feature extraction. The
detection performance of comparative methods is compared with that of the
proposed approach via RMSE values and receiver operating characteristic (ROC)
curves. Finally, the proposed approach is applied to compound faults diagnosis
of motor bearings. The non-stationary vibration data were acquired from a
SpectraQuest's machinery fault simulator. The processed results show the
proposed approach can effectively detect and extract the useful features of
bearing outer race and inner race defect.
</summary>
    <author>
      <name>Wangpeng He</name>
    </author>
    <author>
      <name>Yin Ding</name>
    </author>
    <author>
      <name>Yanyang Zi</name>
    </author>
    <author>
      <name>Ivan W. Selesnick</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.ymssp.2015.11.027</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.ymssp.2015.11.027" rel="related"/>
    <link href="http://arxiv.org/abs/1511.00067v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.00067v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.00393v2</id>
    <updated>2016-07-30T21:13:12Z</updated>
    <published>2015-11-02T06:08:24Z</published>
    <title>Detection of Faults in Rotating Machinery Using Periodic Time-Frequency
  Sparsity</title>
    <summary>  This paper addresses the problem of extracting periodic oscillatory features
in vibration sig- nals for detecting faults in rotating machinery. To extract
the feature, we propose an approach in the short-time Fourier transform (STFT)
domain where the periodic oscillatory feature man- ifests itself as a
relatively sparse grid. To estimate the sparse grid, we formulate an
optimization problem using customized binary weights in the regularizer, where
the weights are formulated to promote periodicity. In order to solve the
proposed optimization problem, we develop an algorithm called augmented
Lagrangian majorization-minimization algorithm, which combines the split
augmented Lagrangian shrinkage algorithm (SALSA) with majorization-minimization
(MM), and is guaranteed to converge for both convex and non-convex formulation.
As examples, the proposed approach is applied to simulated data, and used as a
tool for diagnosing faults in bearings and gearboxes for real data, and
compared to some state-of-the-art methods. The results show the proposed
approach can effectively detect and extract the periodical oscillatory
features.
</summary>
    <author>
      <name>Yin Ding</name>
    </author>
    <author>
      <name>Wangpeng He</name>
    </author>
    <author>
      <name>Binqiang Chen</name>
    </author>
    <author>
      <name>Yanyang Zi</name>
    </author>
    <author>
      <name>Ivan W. Selesnick</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.jsv.2016.07.004</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.jsv.2016.07.004" rel="related"/>
    <link href="http://arxiv.org/abs/1511.00393v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.00393v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.03174v3</id>
    <updated>2016-12-12T13:19:16Z</updated>
    <published>2015-11-10T16:39:01Z</published>
    <title>Fault Diagnosis of Rolling Element Bearings with a Spectrum Searching
  Method</title>
    <summary>  Rolling element bearing faults in rotating systems are observed as impulses
in the vibration signals, which are usually buried in noises. In order to
effectively detect the fault of bearings, a novel spectrum searching method is
proposed. The structural information of spectrum (SIOS) on a predefined basis
is constructed through a searching algorithm, such that the harmonics of
impulses generated by faults can be clearly identified and analyzed. Local
peaks of the spectrum are located on a certain bin of the basis, and then the
SIOS can interpret the spectrum via the number and energy of harmonics related
to frequency bins of the basis. Finally bearings can be diagnosed based on the
SIOS by identifying its dominant components. Mathematical formulation is
developed to guarantee the correct construction of the SISO through searching.
The effectiveness of the proposed method is verified with a simulation signal
and a benchmark study of bearings.
</summary>
    <author>
      <name>Wei Li</name>
    </author>
    <author>
      <name>Mingquan Qiu</name>
    </author>
    <author>
      <name>Zhencai Zhu</name>
    </author>
    <author>
      <name>Fan Jiang</name>
    </author>
    <author>
      <name>Gongbo Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/1511.03174v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.03174v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.04063v1</id>
    <updated>2015-11-12T20:41:40Z</updated>
    <published>2015-11-12T20:41:40Z</published>
    <title>Single-Channel Maximum-Likelihood T60 Estimation Exploiting Subband
  Information</title>
    <summary>  This contribution presents four algorithms developed by the authors for
single-channel fullband and subband T60 estimation within the ACE challenge.
The blind estimation of the fullband reverberation time (RT) by
maximum-likelihood (ML) estimation based on [15] is considered as baseline
approach. An improvement of this algorithm is devised where an energy-weighted
averaging of the upper subband RT estimates is performed using either a DCT or
1/3-octave filter-bank. The evaluation results show that this approach leads to
a lower variance for the estimation error in comparison to the baseline
approach at the price of an increased computational complexity. Moreover, a new
algorithm to estimate the subband RT is devised, where the RT estimates for the
lower octave subbands are extrapolated from the RT estimates of the upper
subbands by means of a simple model for the frequency-dependency of the subband
RT. The evaluation results of the ACE challenge reveal that this approach
allows to estimate the subband RT with an estimation error which is in a
similar range as for the presented fullband RT estimators.
</summary>
    <author>
      <name>Heinrich Loellmann</name>
    </author>
    <author>
      <name>Andreas Brendel</name>
    </author>
    <author>
      <name>Peter Vary</name>
    </author>
    <author>
      <name>Walter Kellermann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the ACE Challenge Workshop - a satellite event of
  IEEE-WASPAA 2015 (arXiv:1510.00383)</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.04063v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.04063v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.04867v2</id>
    <updated>2015-11-23T09:48:17Z</updated>
    <published>2015-11-16T08:53:37Z</published>
    <title>Quality assessment of voice converted speech using articulatory features</title>
    <summary>  We propose a novel application based on acoustic-to-articulatory inversion
towards quality assessment of voice converted speech. The ability of humans to
speak effortlessly requires coordinated movements of various articulators,
muscles, etc. This effortless movement contributes towards naturalness,
intelligibility and speakers identity which is partially present in voice
converted speech. Hence, during voice conversion, the information related to
speech production is lost. In this paper, this loss is quantified for male
voice, by showing increase in RMSE error for voice converted speech followed by
showing decrease in mutual information. Similar results are obtained in case of
female voice. This observation is extended by showing that articulatory
features can be used as an objective measure. The effectiveness of proposed
measure over MCD is illustrated by comparing their correlation with Mean
Opinion Score.
</summary>
    <author>
      <name>Avni Rajpal</name>
    </author>
    <author>
      <name>Nirmesh J. Shah</name>
    </author>
    <author>
      <name>Mohammadi Zaki</name>
    </author>
    <author>
      <name>Hemant A. Patil</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The paper is withdrawn from the arxiv. Author doesnot want
  circulation of unpublished unverified results</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.04867v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.04867v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.07008v1</id>
    <updated>2015-11-22T13:05:05Z</updated>
    <published>2015-11-22T13:05:05Z</published>
    <title>Real Time Vowel Tremolo Detection Using Low Level Audio Descriptors</title>
    <summary>  This paper resumes the results of a research conducted in a music production
situation Therefore, it is more a final lab report, a prospective methodology
then a scientific experience. The methodology we are presenting was developed
as an answer to a musical problem raised by the Italian composer Marta
Gentilucci. The problem was "how to extract a temporal structure from a vowel
tremolo, on a tenuto (steady state) pitch." The musical goal was to apply, in a
compositional context the vowel tremolo time structure on a tenuto pitch chord,
as a transposition control.In this context we decide to follow, to explore the
potential of low-level MPEG7 audio descriptors to build event detection
functions. One of the main problems using low-level audio descriptors in audio
analysis is the redundancy of information among them. We describe an "ad hoc"
interactive methodology, based on side effect use of dimensionality reduction
by PCA, to choose a feature from a set of low-level audio descriptors, to be
used to detect a vowel tremolo rhythm. This methodology is supposed to be
interactive and easy enough to be used in a live creative context.
</summary>
    <author>
      <name>Mikhail Malt</name>
    </author>
    <author>
      <name>Marta Gentilucci</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 6 figures, 2 tables, lab report</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.07008v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.07008v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.02560v1</id>
    <updated>2015-12-08T17:34:49Z</updated>
    <published>2015-12-08T17:34:49Z</published>
    <title>Deep Learning for Single and Multi-Session i-Vector Speaker Recognition</title>
    <summary>  The promising performance of Deep Learning (DL) in speech recognition has
motivated the use of DL in other speech technology applications such as speaker
recognition. Given i-vectors as inputs, the authors proposed an impostor
selection algorithm and a universal model adaptation process in a hybrid system
based on Deep Belief Networks (DBN) and Deep Neural Networks (DNN) to
discriminatively model each target speaker. In order to have more insight into
the behavior of DL techniques in both single and multi-session speaker
enrollment tasks, some experiments have been carried out in this paper in both
scenarios. Additionally, the parameters of the global model, referred to as
universal DBN (UDBN), are normalized before adaptation. UDBN normalization
facilitates training DNNs specifically with more than one hidden layer.
Experiments are performed on the NIST SRE 2006 corpus. It is shown that the
proposed impostor selection algorithm and UDBN adaptation process enhance the
performance of conventional DNNs 8-20 % and 16-20 % in terms of EER for the
single and multi-session tasks, respectively. In both scenarios, the proposed
architectures outperform the baseline systems obtaining up to 17 % reduction in
EER.
</summary>
    <author>
      <name>Omid Ghahabi</name>
    </author>
    <author>
      <name>Javier Hernando</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TASLP.2017.2661705</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TASLP.2017.2661705" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE/ACM Transactions on Audio, Speech, and Language Processing,
  Volume: 25, Issue: 4, April 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1512.02560v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.02560v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.05811v1</id>
    <updated>2015-12-17T22:06:51Z</updated>
    <published>2015-12-17T22:06:51Z</published>
    <title>Spectral Study of the Vocal Tract in Vowel Synthesis: A Comparison
  between 1D and 3D Acoustic Analysis</title>
    <summary>  A state-of-the-art 1D acoustic synthesizer has been previously developed, and
coupled to speaker-specific biomechanical models of oropharynx in ArtiSynth. As
expected, the formant frequencies of the synthesized vowel sounds were shown to
be different from those of the recorded audio. Such discrepancy was
hypothesized to be due to the simplified geometry of the vocal tract model as
well as the one dimensional implementation of Navier-Stokes equations. In this
paper, we calculate Helmholtz resonances of our vocal tract geometries using 3D
finite element method (FEM), and compare them with the formant frequencies
obtained from the 1D method and audio. We hope such comparison helps with
clarifying the limitations of our current models and/or speech synthesizer.
</summary>
    <author>
      <name>Negar M. Harandi</name>
    </author>
    <author>
      <name>Daniel Aalto</name>
    </author>
    <author>
      <name>Antti Hannukainen</name>
    </author>
    <author>
      <name>Jarmo Malinen</name>
    </author>
    <author>
      <name>Sidney Fels</name>
    </author>
    <link href="http://arxiv.org/abs/1512.05811v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.05811v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.06222v1</id>
    <updated>2015-12-19T10:27:59Z</updated>
    <published>2015-12-19T10:27:59Z</published>
    <title>A new robust adaptive algorithm for underwater acoustic channel
  equalization</title>
    <summary>  We introduce a novel family of adaptive robust equalizers for highly
challenging underwater acoustic (UWA) channel equalization. Since the
underwater environment is highly non-stationary and subjected to impulsive
noise, we use adaptive filtering techniques based on a relative logarithmic
cost function inspired by the competitive methods from the online learning
literature. To improve the convergence performance of the conventional linear
equalization methods, while mitigating the stability issues, we intrinsically
combine different norms of the error in the cost function, using logarithmic
functions. Hence, we achieve a comparable convergence performance to least mean
fourth (LMF) equalizer, while significantly enhancing the stability performance
in such an adverse communication medium. We demonstrate the performance of our
algorithms through highly realistic experiments performed on accurately
simulated underwater acoustic channels.
</summary>
    <author>
      <name>Dariush Kari</name>
    </author>
    <author>
      <name>Muhammed Omer Sayin</name>
    </author>
    <author>
      <name>Suleyman Serdar Kozat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.06222v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.06222v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.07370v1</id>
    <updated>2015-12-23T07:09:13Z</updated>
    <published>2015-12-23T07:09:13Z</published>
    <title>Musical instrument sound classification with deep convolutional neural
  network using feature fusion approach</title>
    <summary>  A new musical instrument classification method using convolutional neural
networks (CNNs) is presented in this paper. Unlike the traditional methods, we
investigated a scheme for classifying musical instruments using the learned
features from CNNs. To create the learned features from CNNs, we not only used
a conventional spectrogram image, but also proposed multiresolution recurrence
plots (MRPs) that contain the phase information of a raw input signal.
Consequently, we fed the characteristic timbre of the particular instrument
into a neural network, which cannot be extracted using a phase-blinded
representations such as a spectrogram. By combining our proposed MRPs and
spectrogram images with a multi-column network, the performance of our proposed
classifier system improves over a system that uses only a spectrogram.
Furthermore, the proposed classifier also outperforms the baseline result from
traditional handcrafted features and classifiers.
</summary>
    <author>
      <name>Taejin Park</name>
    </author>
    <author>
      <name>Taejin Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 5 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.07370v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.07370v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.07748v1</id>
    <updated>2015-12-24T08:21:48Z</updated>
    <published>2015-12-24T08:21:48Z</published>
    <title>Real-Time Audio-to-Score Alignment of Music Performances Containing
  Errors and Arbitrary Repeats and Skips</title>
    <summary>  This paper discusses real-time alignment of audio signals of music
performance to the corresponding score (a.k.a. score following) which can
handle tempo changes, errors and arbitrary repeats and/or skips (repeats/skips)
in performances. This type of score following is particularly useful in
automatic accompaniment for practices and rehearsals, where errors and
repeats/skips are often made. Simple extensions of the algorithms previously
proposed in the literature are not applicable in these situations for scores of
practical length due to the problem of large computational complexity. To cope
with this problem, we present two hidden Markov models of monophonic
performance with errors and arbitrary repeats/skips, and derive efficient
score-following algorithms with an assumption that the prior probability
distributions of score positions before and after repeats/skips are independent
from each other. We confirmed real-time operation of the algorithms with music
scores of practical length (around 10000 notes) on a modern laptop and their
tracking ability to the input performance within 0.7 s on average after
repeats/skips in clarinet performance data. Further improvements and extension
for polyphonic signals are also discussed.
</summary>
    <author>
      <name>Tomohiko Nakamura</name>
    </author>
    <author>
      <name>Eita Nakamura</name>
    </author>
    <author>
      <name>Shigeki Sagayama</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TASLP.2015.2507862</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TASLP.2015.2507862" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 8 figures, version accepted in IEEE/ACM Transactions on
  Audio, Speech, and Language Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.07748v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.07748v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.08075v1</id>
    <updated>2015-12-26T05:25:00Z</updated>
    <published>2015-12-26T05:25:00Z</published>
    <title>Multichannel audio signal source separation based on an Interchannel
  Loudness Vector Sum</title>
    <summary>  In this paper, a Blind Source Separation (BSS) algorithm for multichannel
audio contents is proposed. Unlike common BSS algorithms targeting stereo audio
contents or microphone array signals, our technique is targeted at multichannel
audio such as 5.1 and 7.1ch audio. Since most multichannel audio object sources
are panned using the Inter-channel Loudness Difference (ILD), we employ the
ILVS (Inter-channel Loudness Vector Sum) concept to cluster common signals
(such as background music) from each channel. After separating the common
signals from each channel, we employ an Expectation Maximization (EM) algorithm
with a von-Mises distribution to successfully classify the clustering of sound
source objects and separate the audio signals from the original mixture. Our
proposed method can therefore separate common audio signals and object source
signals from multiple channels with reasonable quality. Our multichannel audio
content separation technique can be applied to an upmix system or a cinema
audio system requiring multichannel audio source separation.
</summary>
    <author>
      <name>Taejin Park</name>
    </author>
    <author>
      <name>Taejin Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures and 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.08075v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.08075v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.00287v1</id>
    <updated>2016-01-03T12:30:38Z</updated>
    <published>2016-01-03T12:30:38Z</published>
    <title>Wavelet Scattering on the Pitch Spiral</title>
    <summary>  We present a new representation of harmonic sounds that linearizes the
dynamics of pitch and spectral envelope, while remaining stable to deformations
in the time-frequency plane. It is an instance of the scattering transform, a
generic operator which cascades wavelet convolutions and modulus
nonlinearities. It is derived from the pitch spiral, in that convolutions are
successively performed in time, log-frequency, and octave index. We give a
closed-form approximation of spiral scattering coefficients for a nonstationary
generalization of the harmonic source-filter model.
</summary>
    <author>
      <name>Vincent Lostanlen</name>
    </author>
    <author>
      <name>Stéphane Mallat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 18th International Conference on Digital Audio
  Effects (DAFx-15), Trondheim, Norway, Nov 30 - Dec 3, 2015, pp. 429--432. 4
  pages, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 18th International Conference on Digital Audio
  Effects (DAFx-15), Trondheim, Norway, Nov 30 - Dec 3, 2015, pp. 429--432</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1601.00287v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.00287v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65T60" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.00833v1</id>
    <updated>2016-01-05T14:13:28Z</updated>
    <published>2016-01-05T14:13:28Z</published>
    <title>An Analysis of Rhythmic Staccato-Vocalization Based on Frequency
  Demodulation for Laughter Detection in Conversational Meetings</title>
    <summary>  Human laugh is able to convey various kinds of meanings in human
communications. There exists various kinds of human laugh signal, for example:
vocalized laugh and non vocalized laugh. Following the theories of psychology,
among all the vocalized laugh type, rhythmic staccato-vocalization
significantly evokes the positive responses in the interactions. In this paper
we attempt to exploit this observation to detect human laugh occurrences, i.e.,
the laughter, in multiparty conversations from the AMI meeting corpus. First,
we separate the high energy frames from speech, leaving out the low energy
frames through power spectral density estimation. We borrow the algorithm of
rhythm detection from the area of music analysis to use that on the high energy
frames. Finally, we detect rhythmic laugh frames, analyzing the candidate
rhythmic frames using statistics. This novel approach for detection of
`positive' rhythmic human laughter performs better than the standard laughter
classification baseline.
</summary>
    <author>
      <name>Sucheta Ghosh</name>
    </author>
    <author>
      <name>Milos Cernak</name>
    </author>
    <author>
      <name>Sarbani Palit</name>
    </author>
    <author>
      <name>B. B. Chaudhuri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure, conference paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.00833v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.00833v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.01577v1</id>
    <updated>2016-01-07T15:57:45Z</updated>
    <published>2016-01-07T15:57:45Z</published>
    <title>Gender Identification using MFCC for Telephone Applications - A
  Comparative Study</title>
    <summary>  Gender recognition is an essential component of automatic speech recognition
and interactive voice response systems. Determining gender of the speaker
reduces the computational burden of such systems for any further processing.
Typical methods for gender recognition from speech largely depend on features
extraction and classification processes. The purpose of this study is to
evaluate the performance of various state-of-the-art classification methods
along with tuning their parameters for helping selection of the optimal
classification methods for gender recognition tasks. Five classification
schemes including k-nearest neighbor, na\"ive Bayes, multilayer perceptron,
random forest, and support vector machine are comprehensively evaluated for
determination of gender from telephonic speech using the Mel-frequency cepstral
coefficients. Different experiments were performed to determine the effects of
training data sizes, length of the speech streams, and parameter tuning on
classification performance. Results suggest that SVM is the best classifier
among all the five schemes for gender recognition.
</summary>
    <author>
      <name>Jamil Ahmad</name>
    </author>
    <author>
      <name>Mustansar Fiaz</name>
    </author>
    <author>
      <name>Soon-il Kwon</name>
    </author>
    <author>
      <name>Maleerat Sodanil</name>
    </author>
    <author>
      <name>Bay Vo</name>
    </author>
    <author>
      <name>Sung Wook Baik</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science and Electronics
  Engineering 3.5 (2015): 351-355</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1601.01577v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.01577v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.02069v1</id>
    <updated>2016-01-09T03:06:18Z</updated>
    <published>2016-01-09T03:06:18Z</published>
    <title>Dynamic Transposition of Melodic Sequences on Digital Devices</title>
    <summary>  A method is proposed which enables one to produce musical compositions by
using transposition in place of harmonic progression. A transposition scale is
introduced to provide a set of intervals commensurate with the musical scale,
such as chromatic or just intonation scales. A sequence of intervals selected
from the transposition scale is used to shift instrument frequency at
predefined times during the composition which serves as a harmonic sequence of
a composition. A transposition sequence constructed in such a way can be
extended to a hierarchy of sequences. The fundamental sound frequency of an
instrument is obtained as a product of the base frequency, instrument key
factor, and a cumulative product of respective factors from all the harmonic
sequences. The multiplication factors are selected from subsets of rational
numbers, which form instrument scales and transposition scales of different
levels. Each harmonic sequence can be related to its own transposition scale,
or a single scale can be used for all levels. When composing for an orchestra
of instruments, harmonic sequences and instrument scales can be assigned
independently to each musical instrument. The method solves the problem of
using just intonation scale across multiple octaves as well as simplifies
writing of instrument scores.
</summary>
    <author>
      <name>Andrei V Smirnov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 5 figures, 3 music samples</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.02069v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.02069v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="11N25, 70J40, 11K70, 11J70, 42A45" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.02339v2</id>
    <updated>2016-08-06T20:12:40Z</updated>
    <published>2016-01-11T06:45:58Z</published>
    <title>Repetitive Transients Extraction Algorithm for Detecting Bearing Faults</title>
    <summary>  This paper addresses the problem of noise reduction with simultaneous
components extrac- tion in vibration signals for faults diagnosis of bearing.
The observed vibration signal is modeled as a summation of two components
contaminated by noise, and each component composes of repetitive transients. To
extract the two components simultaneously, an approach by solving an
optimization problem is proposed in this paper. The problem adopts convex
sparsity-based regularization scheme for decomposition, and non-convex
regularization is used to further promote the sparsity but preserving the
global convexity. A synthetic example is presented to illustrate the
performance of the proposed approach for repetitive feature extraction. The
performance and effectiveness of the proposed method are further demonstrated
by applying to compound faults and single fault diagnosis of a locomotive
bearing. The results show the proposed approach can effectively extract the
features of outer and inner race defects.
</summary>
    <author>
      <name>Wangpeng He</name>
    </author>
    <author>
      <name>Yin Ding</name>
    </author>
    <author>
      <name>Yanyang Zi</name>
    </author>
    <author>
      <name>Ivan W. Selesnick</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.ymssp.2016.06.035</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.ymssp.2016.06.035" rel="related"/>
    <link href="http://arxiv.org/abs/1601.02339v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.02339v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.02489v1</id>
    <updated>2016-01-03T06:24:56Z</updated>
    <published>2016-01-03T06:24:56Z</published>
    <title>Categorization of Tablas by Wavelet Analysis</title>
    <summary>  Tabla, a percussion instrument, mainly used to accompany vocalists,
instrumentalists and dancers in every style of music from classical to light in
India, mainly used for keeping rhythm. This percussion instrument consists of
two drums played by two hands, structurally different and produces different
harmonic sounds. Earlier work has done labeling tabla strokes from real time
performances by testing neural networks and tree based classification methods.
The current work extends previous work by C. V. Raman and S. Kumar in 1920 on
spectrum modeling of tabla strokes. In this paper we have studied spectral
characteristics (by wavelet analysis by sub band coding method and using
torrence wavelet tool) of nine strokes from each of five tablas using Wavelet
transform. Wavelet analysis is now a common tool for analyzing localized
variations of power within a time series and to find the frequency distribution
in time frequency space. Statistically, we will look into the patterns depicted
by harmonics of different sub bands and the tablas. Distribution of dominant
frequencies at different sub-band of stroke signals, distribution of power and
behavior of harmonics are the important features, leads to categorization of
tabla.
</summary>
    <author>
      <name>Anirban Patranabis</name>
    </author>
    <author>
      <name>Kaushik Banerjee</name>
    </author>
    <author>
      <name>Vishal Midya</name>
    </author>
    <author>
      <name>Shankha Sanyal</name>
    </author>
    <author>
      <name>Archi Banerjee</name>
    </author>
    <author>
      <name>Ranjan Sengupta</name>
    </author>
    <author>
      <name>Dipak Ghosh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.02489v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.02489v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.06008v1</id>
    <updated>2016-01-22T14:13:53Z</updated>
    <published>2016-01-22T14:13:53Z</published>
    <title>A Robust Frame-based Nonlinear Prediction System for Automatic Speech
  Coding</title>
    <summary>  In this paper, we propose a neural-based coding scheme in which an artificial
neural network is exploited to automatically compress and decompress speech
signals by a trainable approach. Having a two-stage training phase, the system
can be fully specified to each speech frame and have robust performance across
different speakers and wide range of spoken utterances. Indeed, Frame-based
nonlinear predictive coding (FNPC) would code a frame in the procedure of
training to predict the frame samples. The motivating objective is to analyze
the system behavior in regenerating not only the envelope of spectra, but also
the spectra phase. This scheme has been evaluated in time and discrete cosine
transform (DCT) domains and the output of predicted phonemes show the
potentiality of the FNPC to reconstruct complicated signals. The experiments
were conducted on three voiced plosive phonemes, b/d/g/ in time and DCT domains
versus the number of neurons in the hidden layer. Experiments approve the FNPC
capability as an automatic coding system by which /b/d/g/ phonemes have been
reproduced with a good accuracy. Evaluations revealed that the performance of
FNPC system, trained to predict DCT coefficients is more desirable,
particularly for frames with the wider distribution of energy, compared to time
samples.
</summary>
    <author>
      <name>Mahmood Yousefi-Azar</name>
    </author>
    <author>
      <name>Farbod Razzazi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.06008v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.06008v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.06652v1</id>
    <updated>2016-01-25T16:13:37Z</updated>
    <published>2016-01-25T16:13:37Z</published>
    <title>A Perceptually Motivated Filter Bank with Perfect Reconstruction for
  Audio Signal Processing</title>
    <summary>  Many audio applications rely on filter banks (FBs) to analyze, process, and
re-synthesize sounds. To approximate the auditory frequency resolution in the
signal chain, some applications rely on perceptually motivated FBs, the
gammatone FB being a popular example. However, most perceptually motivated FBs
only allow partial signal reconstruction at high redundancies and/or do not
have good resistance to sub-channel processing. This paper introduces an
oversampled perceptually motivated FB enabling perfect reconstruction,
efficient FB design, and adaptable redundancy. The filters are directly
constructed in the frequency domain and linearly distributed on a perceptual
frequency scale (e.g. ERB, Bark, or Mel scale). The proposed design allows for
various filter shapes, uniform or non-uniform FB setting, and large
down-sampling factors. For redundancies $\geq$ 3 perfect reconstruction is
achieved by computing the canonical dual FB analytically. For lower
redundancies perfect reconstruction is achieved using an iterative method.
Experiments show performance improvements of the proposed approach when
compared to the gammatone FB in terms of reconstruction error and resistance to
sub-channel processing, especially at low redundancies.
</summary>
    <author>
      <name>Thibaud Necciari</name>
    </author>
    <author>
      <name>Nicki Holighaus</name>
    </author>
    <author>
      <name>Peter Balazs</name>
    </author>
    <author>
      <name>Zdenek Prusa</name>
    </author>
    <link href="http://arxiv.org/abs/1601.06652v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.06652v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.00739v1</id>
    <updated>2016-02-01T23:00:45Z</updated>
    <published>2016-02-01T23:00:45Z</published>
    <title>Towards a topological fingerprint of music</title>
    <summary>  Can music be represented as a meaningful geometric and topological object? In
this paper, we propose a strategy to describe some music features as a
polyhedral surface obtained by a simplicial interpretation of the
\textit{Tonnetz}. The \textit{Tonnetz} is a graph largely used in computational
musicology to describe the harmonic relationships of notes in equal tuning. In
particular, we use persistent homology in order to describe the
\textit{persistent} properties of music encoded in the aforementioned model.
Both the relevance and the characteristics of this approach are discussed by
analyzing some paradigmatic compositional styles. Eventually, the task of
automatic music style classification is addressed by computing the hierarchical
clustering of the topological fingerprints associated with some collections of
compositions.
</summary>
    <author>
      <name>Mattia G. Bergomi</name>
    </author>
    <author>
      <name>Adriano Baraté</name>
    </author>
    <author>
      <name>Barbara Di Fabio</name>
    </author>
    <link href="http://arxiv.org/abs/1602.00739v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.00739v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.AT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.02656v1</id>
    <updated>2016-02-08T17:25:22Z</updated>
    <published>2016-02-08T17:25:22Z</published>
    <title>LSTM Deep Neural Networks Postfiltering for Improving the Quality of
  Synthetic Voices</title>
    <summary>  Recent developments in speech synthesis have produced systems capable of
outcome intelligible speech, but now researchers strive to create models that
more accurately mimic human voices. One such development is the incorporation
of multiple linguistic styles in various languages and accents.
  HMM-based Speech Synthesis is of great interest to many researchers, due to
its ability to produce sophisticated features with small footprint. Despite
such progress, its quality has not yet reached the level of the predominant
unit-selection approaches that choose and concatenate recordings of real
speech. Recent efforts have been made in the direction of improving these
systems.
  In this paper we present the application of Long-Short Term Memory Deep
Neural Networks as a Postfiltering step of HMM-based speech synthesis, in order
to obtain closer spectral characteristics to those of natural speech. The
results show how HMM-voices could be improved using this approach.
</summary>
    <author>
      <name>Marvin Coto-Jiménez</name>
    </author>
    <author>
      <name>John Goddard-Close</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.02656v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.02656v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.05526v1</id>
    <updated>2016-02-17T18:41:16Z</updated>
    <published>2016-02-17T18:41:16Z</published>
    <title>A High-Quality Speech and Audio Codec With Less Than 10 ms Delay</title>
    <summary>  With increasing quality requirements for multimedia communications, audio
codecs must maintain both high quality and low delay. Typically, audio codecs
offer either low delay or high quality, but rarely both. We propose a codec
that simultaneously addresses both these requirements, with a delay of only 8.7
ms at 44.1 kHz. It uses gain-shape algebraic vector quantisation in the
frequency domain with time-domain pitch prediction. We demonstrate that the
proposed codec operating at 48 kbit/s and 64 kbit/s out-performs both G.722.1C
and MP3 and has quality comparable to AAC-LD, despite having less than one
fourth of the algorithmic delay of these codecs.
</summary>
    <author>
      <name>Jean-Marc Valin</name>
    </author>
    <author>
      <name>Timothy B. Terriberry</name>
    </author>
    <author>
      <name>Christopher Montgomery</name>
    </author>
    <author>
      <name>Gregory Maxwell</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TASL.2009.2023186</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TASL.2009.2023186" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Audio, Speech and Language Processing, Vol.
  18, No. 1, pp. 58-67, 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1602.05526v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.05526v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.05682v2</id>
    <updated>2016-04-27T02:32:38Z</updated>
    <published>2016-02-18T05:49:37Z</published>
    <title>Audio Recording Device Identification Based on Deep Learning</title>
    <summary>  In this paper we present a research on identification of audio recording
devices from background noise, thus providing a method for forensics. The audio
signal is the sum of speech signal and noise signal. Usually, people pay more
attention to speech signal, because it carries the information to deliver. So a
great amount of researches have been dedicated to getting higher
Signal-Noise-Ratio (SNR). There are many speech enhancement algorithms to
improve the quality of the speech, which can be seen as reducing the noise.
However, noises can be regarded as the intrinsic fingerprint traces of an audio
recording device. These digital traces can be characterized and identified by
new machine learning techniques. Therefore, in our research, we use the noise
as the intrinsic features. As for the identification, multiple classifiers of
deep learning methods are used and compared. The identification result shows
that the method of getting feature vector from the noise of each device and
identifying them with deep learning techniques is viable, and well-preformed.
</summary>
    <author>
      <name>Simeng Qi</name>
    </author>
    <author>
      <name>Zheng Huang</name>
    </author>
    <author>
      <name>Yan Li</name>
    </author>
    <author>
      <name>Shaopei Shi</name>
    </author>
    <link href="http://arxiv.org/abs/1602.05682v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.05682v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.05900v1</id>
    <updated>2016-02-17T18:15:36Z</updated>
    <published>2016-02-17T18:15:36Z</published>
    <title>An Iterative Linearised Solution to the Sinusoidal Parameter Estimation
  Problem</title>
    <summary>  Signal processing applications use sinusoidal modelling for speech synthesis,
speech coding, and audio coding. Estimation of the model parameters involves
non-linear optimisation methods, which can be very costly for real-time
applications. We propose a low-complexity iterative method that starts from
initial frequency estimates and converges rapidly. We show that for N sinusoids
in a frame of length L, the proposed method has a complexity of O(LN), which is
significantly less than the matching pursuits method. Furthermore, the proposed
method is shown to be more accurate than the matching pursuits and
time-frequency reassignment methods in our experiments.
</summary>
    <author>
      <name>Jean-Marc Valin</name>
    </author>
    <author>
      <name>Daniel V. Smith</name>
    </author>
    <author>
      <name>Christopher Montgomery</name>
    </author>
    <author>
      <name>Timothy B. Terriberry</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.compeleceng.2008.11.005</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.compeleceng.2008.11.005" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computers and Electrical Engineering (Elsevier), Vol. 36, No. 4,
  pp. 603-616, 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1602.05900v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.05900v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.06727v3</id>
    <updated>2016-04-05T11:31:02Z</updated>
    <published>2016-02-22T11:11:04Z</published>
    <title>Improving Trajectory Modelling for DNN-based Speech Synthesis by using
  Stacked Bottleneck Features and Minimum Generation Error Training</title>
    <summary>  We propose two novel techniques --- stacking bottleneck features and minimum
generation error training criterion --- to improve the performance of deep
neural network (DNN)-based speech synthesis. The techniques address the related
issues of frame-by-frame independence and ignorance of the relationship between
static and dynamic features, within current typical DNN-based synthesis
frameworks. Stacking bottleneck features, which are an acoustically--informed
linguistic representation, provides an efficient way to include more detailed
linguistic context at the input. The minimum generation error training
criterion minimises overall output trajectory error across an utterance, rather
than minimising the error per frame independently, and thus takes into account
the interaction between static and dynamic features. The two techniques can be
easily combined to further improve performance. We present both objective and
subjective results that demonstrate the effectiveness of the proposed
techniques. The subjective results show that combining the two techniques leads
to significantly more natural synthetic speech than from conventional DNN or
long short-term memory (LSTM) recurrent neural network (RNN) systems.
</summary>
    <author>
      <name>Zhizheng Wu</name>
    </author>
    <author>
      <name>Simon King</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TASLP.2016.2551865</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TASLP.2016.2551865" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to IEEE/ACM Transactions on Audio, Speech and Language
  Processing 2016 (AQ)</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.06727v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.06727v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.08045v1</id>
    <updated>2016-02-25T19:18:06Z</updated>
    <published>2016-02-25T19:18:06Z</published>
    <title>PCA/LDA Approach for Text-Independent Speaker Recognition</title>
    <summary>  Various algorithms for text-independent speaker recognition have been
developed through the decades, aiming to improve both accuracy and efficiency.
This paper presents a novel PCA/LDA-based approach that is faster than
traditional statistical model-based methods and achieves competitive results.
First, the performance based on only PCA and only LDA is measured; then a mixed
model, taking advantages of both methods, is introduced. A subset of the TIMIT
corpus composed of 200 male speakers, is used for enrollment, validation and
testing. The best results achieve 100%; 96% and 95% classification rate at
population level 50; 100 and 200, using 39-dimensional MFCC features with delta
and double delta. These results are based on 12-second text-independent speech
for training and 4-second data for test. These are comparable to the
conventional MFCC-GMM methods, but require significantly less time to train and
operate.
</summary>
    <author>
      <name>Zhenhao Ge</name>
    </author>
    <author>
      <name>Sudhendu R. Sharma</name>
    </author>
    <author>
      <name>Mark J. T. Smith</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1117/12.919235</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1117/12.919235" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Society of Photo-Optical Instrumentation Engineers (SPIE) Conference
  Series</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.08045v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.08045v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.08128v1</id>
    <updated>2016-02-25T21:48:56Z</updated>
    <published>2016-02-25T21:48:56Z</published>
    <title>PCA Method for Automated Detection of Mispronounced Words</title>
    <summary>  This paper presents a method for detecting mispronunciations with the aim of
improving Computer Assisted Language Learning (CALL) tools used by foreign
language learners. The algorithm is based on Principle Component Analysis
(PCA). It is hierarchical with each successive step refining the estimate to
classify the test word as being either mispronounced or correct. Preprocessing
before detection, like normalization and time-scale modification, is
implemented to guarantee uniformity of the feature vectors input to the
detection system. The performance using various features including spectrograms
and Mel-Frequency Cepstral Coefficients (MFCCs) are compared and evaluated.
Best results were obtained using MFCCs, achieving up to 99% accuracy in word
verification and 93% in native/non-native classification. Compared with Hidden
Markov Models (HMMs) which are used pervasively in recognition application,
this particular approach is computational efficient and effective when training
data is limited.
</summary>
    <author>
      <name>Zhenhao Ge</name>
    </author>
    <author>
      <name>Sudhendu R. Sharma</name>
    </author>
    <author>
      <name>Mark J. T. Smith</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1117/12.884155</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1117/12.884155" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SPIE Defense, Security, and Sensing</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.08128v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.08128v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.08132v1</id>
    <updated>2016-02-25T22:17:31Z</updated>
    <published>2016-02-25T22:17:31Z</published>
    <title>Adaptive Frequency Cepstral Coefficients for Word Mispronunciation
  Detection</title>
    <summary>  Systems based on automatic speech recognition (ASR) technology can provide
important functionality in computer assisted language learning applications.
This is a young but growing area of research motivated by the large number of
students studying foreign languages. Here we propose a Hidden Markov Model
(HMM)-based method to detect mispronunciations. Exploiting the specific dialog
scripting employed in language learning software, HMMs are trained for
different pronunciations. New adaptive features have been developed and
obtained through an adaptive warping of the frequency scale prior to computing
the cepstral coefficients. The optimization criterion used for the warping
function is to maximize separation of two major groups of pronunciations
(native and non-native) in terms of classification rate. Experimental results
show that the adaptive frequency scale yields a better coefficient
representation leading to higher classification rates in comparison with
conventional HMMs using Mel-frequency cepstral coefficients.
</summary>
    <author>
      <name>Zhenhao Ge</name>
    </author>
    <author>
      <name>Sudhendu R. Sharma</name>
    </author>
    <author>
      <name>Mark J. T. Smith</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CISP.2011.6100685</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CISP.2011.6100685" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4th International Congress on Image and Signal Processing (CISP) 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.08132v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.08132v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.08507v1</id>
    <updated>2016-02-24T04:52:01Z</updated>
    <published>2016-02-24T04:52:01Z</published>
    <title>Occupancy Estimation in Smart Buildings using Audio-Processing
  Techniques</title>
    <summary>  In the past few years, several case studies have illustrated that the use of
occupancy information in buildings leads to energy-efficient and low-cost HVAC
operation. The widely presented techniques for occupancy estimation include
temperature, humidity, CO2 concentration, image camera, motion sensor and
passive infrared (PIR) sensor. So far little studies have been reported in
literature to utilize audio and speech processing as indoor occupancy
prediction technique. With rapid advances of audio and speech processing
technologies, nowadays it is more feasible and attractive to integrate
audio-based signal processing component into smart buildings. In this work, we
propose to utilize audio processing techniques (i.e., speaker recognition and
background audio energy estimation) to estimate room occupancy (i.e., the
number of people inside a room). Theoretical analysis and simulation results
demonstrate the accuracy and effectiveness of this proposed occupancy
estimation technique. Based on the occupancy estimation, smart buildings will
adjust the thermostat setups and HVAC operations, thus, achieving greater
quality of service and drastic cost savings.
</summary>
    <author>
      <name>Qian Huang</name>
    </author>
    <author>
      <name>Zhenhao Ge</name>
    </author>
    <author>
      <name>Chao Lu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Computing in Civil and Building
  Engineering (ICCCBE) 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.08507v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.08507v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.08609v1</id>
    <updated>2016-02-27T16:17:06Z</updated>
    <published>2016-02-27T16:17:06Z</published>
    <title>A New Robust Frequency Domain Echo Canceller With Closed-Loop Learning
  Rate Adaptation</title>
    <summary>  One of the main difficulties in echo cancellation is the fact that the
learning rate needs to vary according to conditions such as double-talk and
echo path change. Several methods have been proposed to vary the learning. In
this paper we propose a new closed-loop method where the learning rate is
proportional to a misalignment parameter, which is in turn estimated based on a
gradient adaptive approach. The method is presented in the context of a
multidelay block frequency domain (MDF) echo canceller. We demonstrate that the
proposed algorithm outperforms current popular double-talk detection techniques
by up to 6 dB.
</summary>
    <author>
      <name>Jean-Marc Valin</name>
    </author>
    <author>
      <name>Iain B. Collings</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICASSP.2007.366624</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICASSP.2007.366624" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, Proc. International Conference on Acoustics, Speech, and
  Signal Processing (ICASSP), 2007</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.08609v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.08609v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.01770v1</id>
    <updated>2016-03-06T00:06:09Z</updated>
    <published>2016-03-06T00:06:09Z</published>
    <title>An Argument-based Creative Assistant for Harmonic Blending</title>
    <summary>  Conceptual blending is a powerful tool for computational creativity where,
for example, the properties of two harmonic spaces may be combined in a
consistent manner to produce a novel harmonic space. However, deciding about
the importance of property features in the input spaces and evaluating the
results of conceptual blending is a nontrivial task. In the specific case of
musical harmony, defining the salient features of chord transitions and
evaluating invented harmonic spaces requires deep musicological background
knowledge. In this paper, we propose a creative tool that helps musicologists
to evaluate and to enhance harmonic innovation. This tool allows a music expert
to specify arguments over given transition properties. These arguments are then
considered by the system when defining combinations of features in an
idiom-blending process. A music expert can assess whether the new harmonic
idiom makes musicological sense and re-adjust the arguments (selection of
features) to explore alternative blends that can potentially produce better
harmonic spaces. We conclude with a discussion of future work that would
further automate the harmonisation process.
</summary>
    <author>
      <name>Maximos Kaliakatsos-Papakostas</name>
    </author>
    <author>
      <name>Roberto Confalonieri</name>
    </author>
    <author>
      <name>Joseph Corneli</name>
    </author>
    <author>
      <name>Asterios Zacharakis</name>
    </author>
    <author>
      <name>Emilios Cambouropoulos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pp; submitted to 7th International Conference on Computational
  Creativity</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.01770v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.01770v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.01824v1</id>
    <updated>2016-03-06T13:17:27Z</updated>
    <published>2016-03-06T13:17:27Z</published>
    <title>Low-Complexity Iterative Sinusoidal Parameter Estimation</title>
    <summary>  Sinusoidal parameter estimation is a computationally-intensive task, which
can pose problems for real-time implementations. In this paper, we propose a
low-complexity iterative method for estimating sinusoidal parameters that is
based on the linearisation of the model around an initial frequency estimate.
We show that for N sinusoids in a frame of length L, the proposed method has a
complexity of O(LN), which is significantly less than the matching pursuits
method. Furthermore, the proposed method is shown to be more accurate than the
matching pursuits and time frequency reassignment methods in our experiments.
</summary>
    <author>
      <name>Jean-Marc Valin</name>
    </author>
    <author>
      <name>Daniel V. Smith</name>
    </author>
    <author>
      <name>Christopher Montgomery</name>
    </author>
    <author>
      <name>Timothy B. Terriberry</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages. arXiv admin note: substantial text overlap with
  arXiv:1602.05900</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of International Conference on Signal Processing and
  Communication Systems (ICSPCS), pp. 276-283, 2007</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1603.01824v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.01824v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.01863v1</id>
    <updated>2016-03-06T19:19:22Z</updated>
    <published>2016-03-06T19:19:22Z</published>
    <title>Improved Noise Weighting in CELP Coding of Speech - Applying the Vorbis
  Psychoacoustic Model To Speex</title>
    <summary>  One key aspect of the CELP algorithm is that it shapes the coding noise using
a simple, yet effective, weighting filter. In this paper, we improve the noise
shaping of CELP using a more modern psychoacoustic model. This has the
significant advantage of improving the quality of an existing codec without the
need to change the bit-stream. More specifically, we improve the Speex CELP
codec by using the psychoacoustic model used in the Vorbis audio codec. The
results show a significant increase in quality, especially at high bit-rates,
where the improvement is equivalent to a 20% reduction in bit-rate. The
technique itself is not specific to Speex and could be applied to other CELP
codecs.
</summary>
    <author>
      <name>Jean-Marc Valin</name>
    </author>
    <author>
      <name>Christopher Montgomery</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, Proceedings of the 120th Audio Engineering Society (AES)
  Convention, Paris, 2006</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.01863v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.01863v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.03215v1</id>
    <updated>2016-03-10T10:42:47Z</updated>
    <published>2016-03-10T10:42:47Z</published>
    <title>Microphone array post-filter for separation of simultaneous
  non-stationary sources</title>
    <summary>  Microphone array post-filters have demonstrated their ability to greatly
reduce noise at the output of a beamformer. However, current techniques only
consider a single source of interest, most of the time assuming stationary
background noise. We propose a microphone array post-filter that enhances the
signals produced by the separation of simultaneous sources using common source
separation algorithms. Our method is based on a loudness-domain optimal
spectral estimator and on the assumption that the noise can be described as the
sum of a stationary component and of a transient component that is due to
leakage between the channels of the initial source separation algorithm. The
system is evaluated in the context of mobile robotics and is shown to produce
better results than current post-filtering techniques, greatly reducing
interference while causing little distortion to the signal of interest, even at
very low SNR.
</summary>
    <author>
      <name>Jean-Marc Valin</name>
    </author>
    <author>
      <name>Jean Rouat</name>
    </author>
    <author>
      <name>François Michaud</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICASSP.2004.1325962</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICASSP.2004.1325962" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages. arXiv admin note: substantial text overlap with
  arXiv:1603.02341</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of IEEE International Conference on Acoustics, Speech
  and Signal Processing (ICASSP), pp. 221-224, 2004</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1603.03215v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.03215v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.04179v3</id>
    <updated>2017-05-22T10:43:16Z</updated>
    <published>2016-03-14T09:42:28Z</published>
    <title>Performance Analysis of Source Image Estimators in Blind Source
  Separation</title>
    <summary>  Blind methods often separate or identify signals or signal subspaces up to an
unknown scaling factor. Sometimes it is necessary to cope with the scaling
ambiguity, which can be done through reconstructing signals as they are
received by sensors, because scales of the sensor responses (images) have known
physical interpretations. In this paper, we analyze two approaches that are
widely used for computing the sensor responses, especially, in Frequency-Domain
Independent Component Analysis. One approach is the least-squares projection,
while the other one assumes a regular mixing matrix and computes its inverse.
Both estimators are invariant to the unknown scaling. Although frequently used,
their differences were not studied yet. A goal of this work is to fill this
gap. The estimators are compared through a theoretical study, perturbation
analysis and simulations. We point to the fact that the estimators are
equivalent when the separated signal subspaces are orthogonal, and vice versa.
Two applications are shown, one of which demonstrates a case where the
estimators yield substantially different results.
</summary>
    <author>
      <name>Zbyněk Koldovský</name>
    </author>
    <author>
      <name>Francesco Nesta</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TSP.2017.2709269</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TSP.2017.2709269" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.04179v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.04179v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="15-04" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.04264v1</id>
    <updated>2016-03-14T13:49:18Z</updated>
    <published>2016-03-14T13:49:18Z</published>
    <title>Novel Speech Features for Improved Detection of Spoofing Attacks</title>
    <summary>  Now-a-days, speech-based biometric systems such as automatic speaker
verification (ASV) are highly prone to spoofing attacks by an imposture. With
recent development in various voice conversion (VC) and speech synthesis (SS)
algorithms, these spoofing attacks can pose a serious potential threat to the
current state-of-the-art ASV systems. To impede such attacks and enhance the
security of the ASV systems, the development of efficient anti-spoofing
algorithms is essential that can differentiate synthetic or converted speech
from natural or human speech. In this paper, we propose a set of novel speech
features for detecting spoofing attacks. The proposed features are computed
using alternative frequency-warping technique and formant-specific block
transformation of filter bank log energies. We have evaluated existing and
proposed features against several kinds of synthetic speech data from ASVspoof
2015 corpora. The results show that the proposed techniques outperform existing
approaches for various spoofing attack detection task. The techniques
investigated in this paper can also accurately classify natural and synthetic
speech as equal error rates (EERs) of 0% have been achieved.
</summary>
    <author>
      <name>Dipjyoti Paul</name>
    </author>
    <author>
      <name>Monisankha Pal</name>
    </author>
    <author>
      <name>Goutam Saha</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/INDICON.2015.7443805</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/INDICON.2015.7443805" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented in IEEE 2015 Annual IEEE India Conference (INDICON)</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.04264v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.04264v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.05435v1</id>
    <updated>2016-03-17T11:35:09Z</updated>
    <published>2016-03-17T11:35:09Z</published>
    <title>Modified Group Delay Based MultiPitch Estimation in Co-Channel Speech</title>
    <summary>  Phase processing has been replaced by group delay processing for the
extraction of source and system parameters from speech. Group delay functions
are ill-behaved when the transfer function has zeros that are close to unit
circle in the z-domain. The modified group delay function addresses this
problem and has been successfully used for formant and monopitch estimation. In
this paper, modified group delay functions are used for multipitch estimation
in concurrent speech. The power spectrum of the speech is first flattened in
order to annihilate the system characteristics, while retaining the source
characteristics. Group delay analysis on this flattened spectrum picks the
predominant pitch in the first pass and a comb filter is used to filter out the
estimated pitch along with its harmonics. The residual spectrum is again
analyzed for the next candidate pitch estimate in the second pass. The final
pitch trajectories of the constituent speech utterances are formed using pitch
grouping and post processing techniques. The performance of the proposed
algorithm was evaluated on standard datasets using two metrics; pitch accuracy
and standard deviation of fine pitch error. Our results show that the proposed
algorithm is a promising pitch detection method in multipitch environment for
real speech recordings.
</summary>
    <author>
      <name>Rajeev Rajan</name>
    </author>
    <author>
      <name>Hema A. Murthy</name>
    </author>
    <link href="http://arxiv.org/abs/1603.05435v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.05435v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.06065v1</id>
    <updated>2016-03-19T08:45:21Z</updated>
    <published>2016-03-19T08:45:21Z</published>
    <title>A pairwise approach to simultaneous onset/offset detection for singing
  voice using correntropy</title>
    <summary>  In this paper, we propose a novelmethod to search for precise locations of
paired note onset and offset in a singing voice signal. In comparison with the
existing onset detection algorithms,our approach differs in two key respects.
First, we employ Correntropy, a generalized correlation function inspired from
Reyni's entropy, as a detection function to capture the instantaneous flux
while preserving insensitiveness to outliers. Next, a novel peak picking
algorithm is specially designed for this detection function. By calculating the
fitness of a pre-defined inverse hyperbolic kernel to a detection function, it
is possible to find an onset and its corresponding offset simultaneously.
Experimental results show that the proposed method achieves performance
significantly better than or comparable to other state-of-the-art techniques
for onset detection in singing voice.
</summary>
    <author>
      <name>Sungkyun Chang</name>
    </author>
    <author>
      <name>Kyogu Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2014 IEEE International Conference on Acoustics, Speech and Signal
  Processing, 5 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.06065v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.06065v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.07236v2</id>
    <updated>2016-04-26T16:32:24Z</updated>
    <published>2016-03-23T15:29:39Z</published>
    <title>Individual identity in songbirds: signal representations and metric
  learning for locating the information in complex corvid calls</title>
    <summary>  Bird calls range from simple tones to rich dynamic multi-harmonic structures.
The more complex calls are very poorly understood at present, such as those of
the scientifically important corvid family (jackdaws, crows, ravens, etc.).
Individual birds can recognise familiar individuals from calls, but where in
the signal is this identity encoded? We studied the question by applying a
combination of feature representations to a dataset of jackdaw calls, including
linear predictive coding (LPC) and adaptive discrete Fourier transform (aDFT).
We demonstrate through a classification paradigm that we can strongly
outperform a standard spectrogram representation for identifying individuals,
and we apply metric learning to determine which time-frequency regions
contribute most strongly to robust individual identification. Computational
methods can help to direct our search for understanding of these complex
biological signals.
</summary>
    <author>
      <name>Dan Stowell</name>
    </author>
    <author>
      <name>Veronica Morfi</name>
    </author>
    <author>
      <name>Lisa F. Gill</name>
    </author>
    <link href="http://arxiv.org/abs/1603.07236v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.07236v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.00192v1</id>
    <updated>2016-04-01T10:28:51Z</updated>
    <published>2016-04-01T10:28:51Z</published>
    <title>Singing Voice Separation and Vocal F0 Estimation based on Mutual
  Combination of Robust Principal Component Analysis and Subharmonic Summation</title>
    <summary>  This paper presents a new method of singing voice analysis that performs
mutually-dependent singing voice separation and vocal fundamental frequency
(F0) estimation. Vocal F0 estimation is considered to become easier if singing
voices can be separated from a music audio signal, and vocal F0 contours are
useful for singing voice separation. This calls for an approach that improves
the performance of each of these tasks by using the results of the other. The
proposed method first performs robust principal component analysis (RPCA) for
roughly extracting singing voices from a target music audio signal. The F0
contour of the main melody is then estimated from the separated singing voices
by finding the optimal temporal path over an F0 saliency spectrogram. Finally,
the singing voices are separated again more accurately by combining a
conventional time-frequency mask given by RPCA with another mask that passes
only the harmonic structures of the estimated F0s. Experimental results showed
that the proposed method significantly improved the performances of both
singing voice separation and vocal F0 estimation. The proposed method also
outperformed all the other methods of singing voice separation submitted to an
international music analysis competition called MIREX 2014.
</summary>
    <author>
      <name>Yukara Ikemiya</name>
    </author>
    <author>
      <name>Katsutoshi Itoyama</name>
    </author>
    <author>
      <name>Kazuyoshi Yoshii</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TASLP.2016.2577879</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TASLP.2016.2577879" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.00192v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.00192v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.00861v1</id>
    <updated>2016-04-04T13:54:09Z</updated>
    <published>2016-04-04T13:54:09Z</published>
    <title>Recurrent Neural Networks for Polyphonic Sound Event Detection in Real
  Life Recordings</title>
    <summary>  In this paper we present an approach to polyphonic sound event detection in
real life recordings based on bi-directional long short term memory (BLSTM)
recurrent neural networks (RNNs). A single multilabel BLSTM RNN is trained to
map acoustic features of a mixture signal consisting of sounds from multiple
classes, to binary activity indicators of each event class. Our method is
tested on a large database of real-life recordings, with 61 classes (e.g.
music, car, speech) from 10 different everyday contexts. The proposed method
outperforms previous approaches by a large margin, and the results are further
improved using data augmentation techniques. Overall, our system reports an
average F1-score of 65.5% on 1 second blocks and 64.7% on single frames, a
relative improvement over previous state-of-the-art approach of 6.8% and 15.1%
respectively.
</summary>
    <author>
      <name>Giambattista Parascandolo</name>
    </author>
    <author>
      <name>Heikki Huttunen</name>
    </author>
    <author>
      <name>Tuomas Virtanen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICASSP.2016.7472917</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICASSP.2016.7472917" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appean in Proceedings of IEEE ICASSP 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.00861v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.00861v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.03276v1</id>
    <updated>2016-04-12T07:52:27Z</updated>
    <published>2016-04-12T07:52:27Z</published>
    <title>Noise Robust Speech Recognition Using Multi-Channel Based Channel
  Selection And ChannelWeighting</title>
    <summary>  In this paper, we study several microphone channel selection and weighting
methods for robust automatic speech recognition (ASR) in noisy conditions. For
channel selection, we investigate two methods based on the maximum likelihood
(ML) criterion and minimum autoencoder reconstruction criterion, respectively.
For channel weighting, we produce enhanced log Mel filterbank coefficients as a
weighted sum of the coefficients of all channels. The weights of the channels
are estimated by using the ML criterion with constraints. We evaluate the
proposed methods on the CHiME-3 noisy ASR task. Experiments show that channel
weighting significantly outperforms channel selection due to its higher
flexibility. Furthermore, on real test data in which different channels have
different gains of the target signal, the channel weighting method performs
equally well or better than the MVDR beamforming, despite the fact that the
channel weighting does not make use of the phase delay information which is
normally used in beamforming.
</summary>
    <author>
      <name>Zhaofeng Zhang</name>
    </author>
    <author>
      <name>Xiong Xiao</name>
    </author>
    <author>
      <name>Longbiao Wang</name>
    </author>
    <author>
      <name>EngSiong Chng</name>
    </author>
    <author>
      <name>Haizhou Li</name>
    </author>
    <link href="http://arxiv.org/abs/1604.03276v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.03276v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07160v2</id>
    <updated>2016-12-08T04:28:16Z</updated>
    <published>2016-04-25T08:25:03Z</published>
    <title>Deep Convolutional Neural Networks and Data Augmentation for Acoustic
  Event Detection</title>
    <summary>  We propose a novel method for Acoustic Event Detection (AED). In contrast to
speech, sounds coming from acoustic events may be produced by a wide variety of
sources. Furthermore, distinguishing them often requires analyzing an extended
time period due to the lack of a clear sub-word unit. In order to incorporate
the long-time frequency structure for AED, we introduce a convolutional neural
network (CNN) with a large input field. In contrast to previous works, this
enables to train audio event detection end-to-end. Our architecture is inspired
by the success of VGGNet and uses small, 3x3 convolutions, but more depth than
previous methods in AED. In order to prevent over-fitting and to take full
advantage of the modeling capabilities of our network, we further propose a
novel data augmentation method to introduce data variation. Experimental
results show that our CNN significantly outperforms state of the art methods
including Bag of Audio Words (BoAW) and classical CNNs, achieving a 16%
absolute improvement.
</summary>
    <author>
      <name>Naoya Takahashi</name>
    </author>
    <author>
      <name>Michael Gygli</name>
    </author>
    <author>
      <name>Beat Pfister</name>
    </author>
    <author>
      <name>Luc Van Gool</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented in INTERSPEECH 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.07160v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07160v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.08716v1</id>
    <updated>2016-04-29T07:46:59Z</updated>
    <published>2016-04-29T07:46:59Z</published>
    <title>Learning Compact Structural Representations for Audio Events Using
  Regressor Banks</title>
    <summary>  We introduce a new learned descriptor for audio signals which is efficient
for event representation. The entries of the descriptor are produced by
evaluating a set of regressors on the input signal. The regressors are
class-specific and trained using the random regression forests framework. Given
an input signal, each regressor estimates the onset and offset positions of the
target event. The estimation confidence scores output by a regressor are then
used to quantify how the target event aligns with the temporal structure of the
corresponding category. Our proposed descriptor has two advantages. First, it
is compact, i.e. the dimensionality of the descriptor is equal to the number of
event classes. Second, we show that even simple linear classification models,
trained on our descriptor, yield better accuracies on audio event
classification task than not only the nonlinear baselines but also the
state-of-the-art results.
</summary>
    <author>
      <name>Huy Phan</name>
    </author>
    <author>
      <name>Marco Maass</name>
    </author>
    <author>
      <name>Lars Hertel</name>
    </author>
    <author>
      <name>Radoslaw Mazur</name>
    </author>
    <author>
      <name>Ian McLoughlin</name>
    </author>
    <author>
      <name>Alfred Mertins</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICASSP.2016.7471667</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICASSP.2016.7471667" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Proceedings of IEEE ICASSP 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.08716v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.08716v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.08723v1</id>
    <updated>2016-04-29T08:03:00Z</updated>
    <published>2016-04-29T08:03:00Z</published>
    <title>Music transcription modelling and composition using deep learning</title>
    <summary>  We apply deep learning methods, specifically long short-term memory (LSTM)
networks, to music transcription modelling and composition. We build and train
LSTM networks using approximately 23,000 music transcriptions expressed with a
high-level vocabulary (ABC notation), and use them to generate new
transcriptions. Our practical aim is to create music transcription models
useful in particular contexts of music composition. We present results from
three perspectives: 1) at the population level, comparing descriptive
statistics of the set of training transcriptions and generated transcriptions;
2) at the individual level, examining how a generated transcription reflects
the conventions of a music practice in the training transcriptions (Celtic
folk); 3) at the application level, using the system for idea generation in
music composition. We make our datasets, software and sound examples open and
available: \url{https://github.com/IraKorshunova/folk-rnn}.
</summary>
    <author>
      <name>Bob L. Sturm</name>
    </author>
    <author>
      <name>João Felipe Santos</name>
    </author>
    <author>
      <name>Oded Ben-Tal</name>
    </author>
    <author>
      <name>Iryna Korshunova</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 4 figures, contribution to 1st Conference on Computer
  Simulation of Musical Creativity</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.08723v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.08723v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.00810v1</id>
    <updated>2016-05-03T09:31:09Z</updated>
    <published>2016-05-03T09:31:09Z</published>
    <title>Diagonal Unloading Beamforming for Source Localization</title>
    <summary>  In sensor array beamforming methods, a class of algorithms commonly used to
estimate the position of a radiating source, the diagonal loading of the
beamformer covariance matrix is generally used to improve computational
accuracy and localization robustness. This paper proposes a diagonal unloading
(DU) method which extends the conventional response power beamforming method by
imposing an additional constraint to the covariance matrix of the array output
vector. The regularization is obtained by subtracting a given amount of white
noise from the main diagonal of the covariance matrix. Specifically, the DU
beamformer aims at subtracting the signal subspace from the noisy signal space
and it is computed by constraining the regularized covariance matrix to be
negative definite. It is hence a data-dependent covariance matrix conditioning
method. We show how to calculate precisely the unloading parameter, and we
present an eigenvalue analysis for comparing the proposed DU beamforming, the
minimum variance distortionless response (MVDR) filter and the multiple signal
classification (MUSIC) method. Theoretical analysis and experiments with
acoustic sources demonstrate that the DU beamformer localization performance is
comparable to that of MVDR and MUSIC. Since the DU beamformer computational
cost is comparable to that of a conventional beamformer, the proposed method
can be attractive in array processing due to its simplicity, effectiveness and
computational efficiency.
</summary>
    <author>
      <name>Daniele Salvati</name>
    </author>
    <author>
      <name>Carlo Drioli</name>
    </author>
    <author>
      <name>Gian Luca Foresti</name>
    </author>
    <link href="http://arxiv.org/abs/1605.00810v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.00810v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.01329v1</id>
    <updated>2016-05-04T16:16:12Z</updated>
    <published>2016-05-04T16:16:12Z</published>
    <title>Single Channel Speech Enhancement Using Outlier Detection</title>
    <summary>  Distortion of the underlying speech is a common problem for single-channel
speech enhancement algorithms, and hinders such methods from being used more
extensively. A dictionary based speech enhancement method that emphasizes
preserving the underlying speech is proposed. Spectral patches of clean speech
are sampled and clustered to train a dictionary. Given a noisy speech spectral
patch, the best matching dictionary entry is selected and used to estimate the
noise power at each time-frequency bin. The noise estimation step is formulated
as an outlier detection problem, where the noise at each bin is assumed present
only if it is an outlier to the corresponding bin of the best matching
dictionary entry. This framework assigns higher priority in removing spectral
elements that strongly deviate from a typical spoken unit stored in the trained
dictionary. Even without the aid of a separate noise model, this method can
achieve significant noise reduction for various non-stationary noises, while
effectively preserving the underlying speech in more challenging noisy
environments.
</summary>
    <author>
      <name>Eunjoon Cho</name>
    </author>
    <author>
      <name>Bowon Lee</name>
    </author>
    <author>
      <name>Ronald Schafer</name>
    </author>
    <author>
      <name>Bernard Widrow</name>
    </author>
    <link href="http://arxiv.org/abs/1605.01329v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.01329v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.02401v3</id>
    <updated>2016-07-06T05:46:56Z</updated>
    <published>2016-05-09T02:17:12Z</published>
    <title>Audio Event Detection using Weakly Labeled Data</title>
    <summary>  Acoustic event detection is essential for content analysis and description of
multimedia recordings. The majority of current literature on the topic learns
the detectors through fully-supervised techniques employing strongly labeled
data. However, the labels available for majority of multimedia data are
generally weak and do not provide sufficient detail for such methods to be
employed. In this paper we propose a framework for learning acoustic event
detectors using only weakly labeled data. We first show that audio event
detection using weak labels can be formulated as an Multiple Instance Learning
problem. We then suggest two frameworks for solving multiple-instance learning,
one based on support vector machines, and the other on neural networks. The
proposed methods can help in removing the time consuming and expensive process
of manually annotating data to facilitate fully supervised learning. Moreover,
it can not only detect events in a recording but can also provide temporal
locations of events in the recording. This helps in obtaining a complete
description of the recording and is notable since temporal information was
never known in the first place in weakly labeled data.
</summary>
    <author>
      <name>Anurag Kumar</name>
    </author>
    <author>
      <name>Bhiksha Raj</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2964284.2964310</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2964284.2964310" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Multimedia 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.02401v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.02401v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.06644v3</id>
    <updated>2017-01-10T14:29:13Z</updated>
    <published>2016-05-21T13:49:33Z</published>
    <title>Deep convolutional networks on the pitch spiral for musical instrument
  recognition</title>
    <summary>  Musical performance combines a wide range of pitches, nuances, and expressive
techniques. Audio-based classification of musical instruments thus requires to
build signal representations that are invariant to such transformations. This
article investigates the construction of learned convolutional architectures
for instrument recognition, given a limited amount of annotated training data.
In this context, we benchmark three different weight sharing strategies for
deep convolutional networks in the time-frequency domain: temporal kernels;
time-frequency kernels; and a linear combination of time-frequency kernels
which are one octave apart, akin to a Shepard pitch spiral. We provide an
acoustical interpretation of these strategies within the source-filter
framework of quasi-harmonic sounds with a fixed spectral envelope, which are
archetypal of musical notes. The best classification accuracy is obtained by
hybridizing all three convolutional layers into a single deep learning
architecture.
</summary>
    <author>
      <name>Vincent Lostanlen</name>
    </author>
    <author>
      <name>Carmine-Emanuele Cella</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures. Accepted at the International Society for Music
  Information Retrieval Conference (ISMIR) conference in New York City, NY,
  USA, August 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.06644v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.06644v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07008v1</id>
    <updated>2016-05-23T13:29:09Z</updated>
    <published>2016-05-23T13:29:09Z</published>
    <title>madmom: a new Python Audio and Music Signal Processing Library</title>
    <summary>  In this paper, we present madmom, an open-source audio processing and music
information retrieval (MIR) library written in Python. madmom features a
concise, NumPy-compatible, object oriented design with simple calling
conventions and sensible default values for all parameters, which facilitates
fast prototyping of MIR applications. Prototypes can be seamlessly converted
into callable processing pipelines through madmom's concept of Processors,
callable objects that run transparently on multiple cores. Processors can also
be serialised, saved, and re-run to allow results to be easily reproduced
anywhere. Apart from low-level audio processing, madmom puts emphasis on
musically meaningful high-level features. Many of these incorporate machine
learning techniques and madmom provides a module that implements some in MIR
commonly used methods such as hidden Markov models and neural networks.
Additionally, madmom comes with several state-of-the-art MIR algorithms for
onset detection, beat, downbeat and meter tracking, tempo estimation, and piano
transcription. These can easily be incorporated into bigger MIR systems or run
as stand-alone programs.
</summary>
    <author>
      <name>Sebastian Böck</name>
    </author>
    <author>
      <name>Filip Korzeniowski</name>
    </author>
    <author>
      <name>Jan Schlüter</name>
    </author>
    <author>
      <name>Florian Krebs</name>
    </author>
    <author>
      <name>Gerhard Widmer</name>
    </author>
    <link href="http://arxiv.org/abs/1605.07008v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07008v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07466v1</id>
    <updated>2016-05-24T14:04:48Z</updated>
    <published>2016-05-24T14:04:48Z</published>
    <title>Complex NMF under phase constraints based on signal modeling:
  application to audio source separation</title>
    <summary>  Nonnegative Matrix Factorization (NMF) is a powerful tool for decomposing
mixtures of audio signals in the Time-Frequency (TF) domain. In the source
separation framework, the phase recovery for each extracted component is
necessary for synthesizing time-domain signals. The Complex NMF (CNMF) model
aims to jointly estimate the spectrogram and the phase of the sources, but
requires to constrain the phase in order to produce satisfactory sounding
results. We propose to incorporate phase constraints based on signal models
within the CNMF framework: a \textit{phase unwrapping} constraint that enforces
a form of temporal coherence, and a constraint based on the \textit{repetition}
of audio events, which models the phases of the sources within onset frames. We
also provide an algorithm for estimating the model parameters. The experimental
results highlight the interest of including such constraints in the CNMF
framework for separating overlapping components in complex audio mixtures.
</summary>
    <author>
      <name>Paul Magron</name>
    </author>
    <author>
      <name>Roland Badeau</name>
    </author>
    <author>
      <name>Bertrand David</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICASSP.2016.7471634</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICASSP.2016.7471634" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP) 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.07466v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07466v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07467v1</id>
    <updated>2016-05-24T14:05:14Z</updated>
    <published>2016-05-24T14:05:14Z</published>
    <title>Phase reconstruction of spectrograms with linear unwrapping: application
  to audio signal restoration</title>
    <summary>  This paper introduces a novel technique for reconstructing the phase of
modified spectrograms of audio signals. From the analysis of mixtures of
sinusoids we obtain relationships between phases of successive time frames in
the Time-Frequency (TF) domain. To obtain similar relationships over
frequencies, in particular within onset frames, we study an impulse model.
Instantaneous frequencies and attack times are estimated locally to encompass
the class of non-stationary signals such as vibratos. These techniques ensure
both the vertical coherence of partials (over frequencies) and the horizontal
coherence (over time). The method is tested on a variety of data and
demonstrates better performance than traditional consistency-based approaches.
We also introduce an audio restoration framework and observe that our technique
outperforms traditional methods.
</summary>
    <author>
      <name>Paul Magron</name>
    </author>
    <author>
      <name>Roland Badeau</name>
    </author>
    <author>
      <name>Bertrand David</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">European Signal Processing Conference (EUSIPCO) 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.07467v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07467v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07468v1</id>
    <updated>2016-05-24T14:05:35Z</updated>
    <published>2016-05-24T14:05:35Z</published>
    <title>Phase reconstruction of spectrograms based on a model of repeated audio
  events</title>
    <summary>  Phase recovery of modified spectrograms is a major issue in audio signal
processing applications, such as source separation. This paper introduces a
novel technique for estimating the phases of components in complex mixtures
within onset frames in the Time-Frequency (TF) domain. We propose to exploit
the phase repetitions from one onset frame to another. We introduce a reference
phase which characterizes a component independently of its activation times.
The onset phases of a component are then modeled as the sum of this reference
and an offset which is linearly dependent on the frequency. We derive a complex
mixture model within onset frames and we provide two algorithms for the
estimation of the model phase parameters. The model is estimated on
experimental data and this technique is integrated into an audio source
separation framework. The results demonstrate that this model is a promising
tool for exploiting phase repetitions, and point out its potential for
separating overlapping components in complex mixtures.
</summary>
    <author>
      <name>Paul Magron</name>
    </author>
    <author>
      <name>Roland Badeau</name>
    </author>
    <author>
      <name>Bertrand David</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/WASPAA.2015.7336935</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/WASPAA.2015.7336935" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Workshop on Applications of Signal Processing to Audio and
  Acoustics (WASPAA) 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.07468v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07468v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07469v1</id>
    <updated>2016-05-24T14:05:51Z</updated>
    <published>2016-05-24T14:05:51Z</published>
    <title>Phase recovery in NMF for audio source separation: an insightful
  benchmark</title>
    <summary>  Nonnegative Matrix Factorization (NMF) is a powerful tool for decomposing
mixtures of audio signals in the Time-Frequency (TF) domain. In applications
such as source separation, the phase recovery for each extracted component is a
major issue since it often leads to audible artifacts. In this paper, we
present a methodology for evaluating various NMF-based source separation
techniques involving phase reconstruction. For each model considered, a
comparison between two approaches (blind separation without prior information
and oracle separation with supervised model learning) is performed, in order to
inquire about the room for improvement for the estimation methods. Experimental
results show that the High Resolution NMF (HRNMF) model is particularly
promising, because it is able to take phases and correlations over time into
account with a great expressive power.
</summary>
    <author>
      <name>Paul Magron</name>
    </author>
    <author>
      <name>Roland Badeau</name>
    </author>
    <author>
      <name>Bertrand David</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICASSP.2015.7177936</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICASSP.2015.7177936" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP) 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.07469v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07469v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07809v2</id>
    <updated>2016-07-22T20:56:20Z</updated>
    <published>2016-05-25T10:20:07Z</published>
    <title>Using instantaneous frequency and aperiodicity detection to estimate F0
  for high-quality speech synthesis</title>
    <summary>  This paper introduces a general and flexible framework for F0 and
aperiodicity (additive non periodic component) analysis, specifically intended
for high-quality speech synthesis and modification applications. The proposed
framework consists of three subsystems: instantaneous frequency estimator and
initial aperiodicity detector, F0 trajectory tracker, and F0 refinement and
aperiodicity extractor. A preliminary implementation of the proposed framework
substantially outperformed (by a factor of 10 in terms of RMS F0 estimation
error) existing F0 extractors in tracking ability of temporally varying F0
trajectories. The front end aperiodicity detector consists of a complex-valued
wavelet analysis filter with a highly selective temporal and spectral envelope.
This front end aperiodicity detector uses a new measure that quantifies the
deviation from periodicity. The measure is less sensitive to slow FM and AM and
closely correlates with the signal to noise ratio.
</summary>
    <author>
      <name>Hideki Kawahara</name>
    </author>
    <author>
      <name>Yannis Agiomyrgiannakis</name>
    </author>
    <author>
      <name>Heiga Zen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for presentation in ISCA workshop SSW9</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.07809v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07809v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.08396v1</id>
    <updated>2016-05-26T18:27:56Z</updated>
    <published>2016-05-26T18:27:56Z</published>
    <title>Robust Downbeat Tracking Using an Ensemble of Convolutional Networks</title>
    <summary>  In this paper, we present a novel state of the art system for automatic
downbeat tracking from music signals. The audio signal is first segmented in
frames which are synchronized at the tatum level of the music. We then extract
different kind of features based on harmony, melody, rhythm and bass content to
feed convolutional neural networks that are adapted to take advantage of each
feature characteristics. This ensemble of neural networks is combined to obtain
one downbeat likelihood per tatum. The downbeat sequence is finally decoded
with a flexible and efficient temporal model which takes advantage of the
metrical continuity of a song. We then perform an evaluation of our system on a
large base of 9 datasets, compare its performance to 4 other published
algorithms and obtain a significant increase of 16.8 percent points compared to
the second best system, for altogether a moderate cost in test and training.
The influence of each step of the method is studied to show its strengths and
shortcomings.
</summary>
    <author>
      <name>S. Durand</name>
    </author>
    <author>
      <name>J. P. Bello</name>
    </author>
    <author>
      <name>B. David</name>
    </author>
    <author>
      <name>G. Richard</name>
    </author>
    <link href="http://arxiv.org/abs/1605.08396v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.08396v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.00298v1</id>
    <updated>2016-06-01T14:18:08Z</updated>
    <published>2016-06-01T14:18:08Z</published>
    <title>Automatic tagging using deep convolutional neural networks</title>
    <summary>  We present a content-based automatic music tagging algorithm using fully
convolutional neural networks (FCNs). We evaluate different architectures
consisting of 2D convolutional layers and subsampling layers only. In the
experiments, we measure the AUC-ROC scores of the architectures with different
complexities and input types using the MagnaTagATune dataset, where a 4-layer
architecture shows state-of-the-art performance with mel-spectrogram input.
Furthermore, we evaluated the performances of the architectures with varying
the number of layers on a larger dataset (Million Song Dataset), and found that
deeper models outperformed the 4-layer architecture. The experiments show that
mel-spectrogram is an effective time-frequency representation for automatic
tagging and that more complex models benefit from more training data.
</summary>
    <author>
      <name>Keunwoo Choi</name>
    </author>
    <author>
      <name>George Fazekas</name>
    </author>
    <author>
      <name>Mark Sandler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ISMIR (International Society of Music Information
  Retrieval) Conference 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.00298v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.00298v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.01368v1</id>
    <updated>2016-06-04T10:51:24Z</updated>
    <published>2016-06-04T10:51:24Z</published>
    <title>Modelling Symbolic Music: Beyond the Piano Roll</title>
    <summary>  In this paper, we consider the problem of probabilistically modelling
symbolic music data. We introduce a representation which reduces polyphonic
music to a univariate categorical sequence. In this way, we are able to apply
state of the art natural language processing techniques, namely the long
short-term memory sequence model. The representation we employ permits
arbitrary rhythmic structure, which we assume to be given. We show that our
model is effective on four out of four piano roll based benchmark datasets. We
further improve our model by augmenting our training data set with
transpositions of the original pieces through all musical keys, thereby
convincingly advancing the state of the art on these benchmark problems. We
also fit models to music which is unconstrained in its rhythmic structure,
discuss the properties of this model, and provide musical samples which are
more sophisticated than previously possible with this class of recurrent neural
network sequence models. We also provide our newly preprocessed data set of non
piano-roll music data.
</summary>
    <author>
      <name>Christian Walder</name>
    </author>
    <link href="http://arxiv.org/abs/1606.01368v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.01368v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.05844v1</id>
    <updated>2016-06-19T08:38:26Z</updated>
    <published>2016-06-19T08:38:26Z</published>
    <title>Statistical Parametric Speech Synthesis Using Bottleneck Representation
  From Sequence Auto-encoder</title>
    <summary>  In this paper, we describe a statistical parametric speech synthesis approach
with unit-level acoustic representation. In conventional deep neural network
based speech synthesis, the input text features are repeated for the entire
duration of phoneme for mapping text and speech parameters. This mapping is
learnt at the frame-level which is the de-facto acoustic representation.
However much of this computational requirement can be drastically reduced if
every unit can be represented with a fixed-dimensional representation. Using
recurrent neural network based auto-encoder, we show that it is indeed possible
to map units of varying duration to a single vector. We then use this acoustic
representation at unit-level to synthesize speech using deep neural network
based statistical parametric speech synthesis technique. Results show that the
proposed approach is able to synthesize at the same quality as the conventional
frame based approach at a highly reduced computational cost.
</summary>
    <author>
      <name>Sivanand Achanta</name>
    </author>
    <author>
      <name>KNRK Raju Alluri</name>
    </author>
    <author>
      <name>Suryakanth V Gangashetty</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages (with references)</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.05844v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.05844v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.06061v2</id>
    <updated>2016-06-22T15:11:30Z</updated>
    <published>2016-06-20T10:54:51Z</published>
    <title>Fast, Compact, and High Quality LSTM-RNN Based Statistical Parametric
  Speech Synthesizers for Mobile Devices</title>
    <summary>  Acoustic models based on long short-term memory recurrent neural networks
(LSTM-RNNs) were applied to statistical parametric speech synthesis (SPSS) and
showed significant improvements in naturalness and latency over those based on
hidden Markov models (HMMs). This paper describes further optimizations of
LSTM-RNN-based SPSS for deployment on mobile devices; weight quantization,
multi-frame inference, and robust inference using an {\epsilon}-contaminated
Gaussian loss function. Experimental results in subjective listening tests show
that these optimizations can make LSTM-RNN-based SPSS comparable to HMM-based
SPSS in runtime speed while maintaining naturalness. Evaluations between
LSTM-RNN- based SPSS and HMM-driven unit selection speech synthesis are also
presented.
</summary>
    <author>
      <name>Heiga Zen</name>
    </author>
    <author>
      <name>Yannis Agiomyrgiannakis</name>
    </author>
    <author>
      <name>Niels Egberts</name>
    </author>
    <author>
      <name>Fergus Henderson</name>
    </author>
    <author>
      <name>Przemysław Szczepaniak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 3 figures, Interspeech 2016 (accepted)</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.06061v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.06061v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.06197v2</id>
    <updated>2016-06-21T11:43:11Z</updated>
    <published>2016-06-20T16:27:12Z</published>
    <title>Polymetric Rhythmic Feel for a Cognitive Drum Computer</title>
    <summary>  This paper addresses a question about music cognition: how do we derive
polymetric structures. A preference rule system is presented which is
implemented into a drum computer. The preference rule system allows inferring
local polymetric structures, like two-over-three and three-over-two. By
analyzing the micro-timing of West African percussion music a timing pattern
consisting of six pulses was discovered. It integrates binary and ternary
rhythmic feels. The presented drum computer integrates the discovered
superimposed polymetric swing (timing and velocity) appropriate to the rhythmic
sequence the user inputs. For binary sequences, the amount of binary swing is
increased and for ternary sequences, the ternary swing is increased.
</summary>
    <author>
      <name>Oliver Weede</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. 14th Int Conf on Culture and Computer Science, Berlin,
  Germany, May 26-27, 2016, pp. 281-295</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1606.06197v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.06197v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.06258v1</id>
    <updated>2016-06-20T19:21:46Z</updated>
    <published>2016-06-20T19:21:46Z</published>
    <title>Uncalibrated 3D Room Reconstruction from Sound</title>
    <summary>  This paper presents a method to reconstruct the 3D structure of generic
convex rooms from sound signals. Differently from most of the previous
approaches, the method is fully uncalibrated in the sense that no knowledge
about the microphones and sources position is needed. Moreover, we demonstrate
that it is possible to bypass the well known echo labeling problem, allowing to
reconstruct the room shape in a reasonable computation time without the need of
additional hypotheses on the echoes order of arrival. Finally, the method is
intrinsically robust to outliers and missing data in the echoes detection,
allowing to work also in low SNR conditions. The proposed pipeline formalises
the problem in different steps such as time of arrival estimation, microphones
and sources localization and walls estimation. After providing a solution to
these different problems we present a global optimization approach that links
together all the problems in a single optimization function. The accuracy and
robustness of the method is assessed on a wide set of simulated setups and in a
challenging real scenario. Moreover we make freely available for a challenging
dataset for 3D room reconstruction with accurate ground truth in a real
scenario.
</summary>
    <author>
      <name>Marco Crocco</name>
    </author>
    <author>
      <name>Andrea Trucco</name>
    </author>
    <author>
      <name>Alessio Del Bue</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The present work has been submitted to IEEE/ACM Transactions on Audio
  Speech and Language Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.06258v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.06258v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07598v1</id>
    <updated>2016-06-24T08:28:41Z</updated>
    <published>2016-06-24T08:28:41Z</published>
    <title>An Active Machine Hearing System for Auditory Stream Segregation</title>
    <summary>  This study describes a binaural machine hearing system that is capable of
performing auditory stream segregation in scenarios where multiple sound
sources are present. The process of stream segregation refers to the capability
of human listeners to group acoustic signals into sets of distinct auditory
streams, corresponding to individual sound sources. The proposed computational
framework mimics this ability via a probabilistic clustering scheme for joint
localization and segregation. This scheme is based on mixtures of von Mises
distributions to model the angular positions of the sound sources surrounding
the listener. The distribution parameters are estimated using block-wise
processing of auditory cues extracted from binaural signals. Additionally, the
proposed system can conduct rotational head movements to improve localization
and stream segregation performance. Evaluation of the system is conducted in
scenarios containing multiple simultaneously active speech and non-speech
sounds placed at different positions relative to the listener.
</summary>
    <author>
      <name>Christopher Schymura</name>
    </author>
    <author>
      <name>Thomas Walther</name>
    </author>
    <author>
      <name>Dorothea Kolossa</name>
    </author>
    <link href="http://arxiv.org/abs/1606.07598v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07598v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.00211v1</id>
    <updated>2016-07-01T11:49:40Z</updated>
    <published>2016-07-01T11:49:40Z</published>
    <title>Spherical Harmonic Signal Covariance and Sound Field Diffuseness</title>
    <summary>  Characterizing sound field diffuseness has many practical applications, from
room acoustics analysis to speech enhancement and sound field reproduction. In
this paper we investigate how spherical microphone arrays (SMAs) can be used to
characterize diffuseness. Due to their specific geometry, SMAs are particularly
well suited for analyzing the spatial properties of sound fields. In
particular, the signals recorded by an SMA can be analyzed in the spherical
harmonic (SH) domain, which has special and desirable mathematical properties
when it comes to analyzing diffuse sound fields. We present a new measure of
diffuseness, the COMEDIE diffuseness estimate, which is based on the analysis
of the SH signal covariance matrix. This algorithm is suited for the estimation
of diffuseness arising either from the presence of multiple sources distributed
around the SMA or from the presence of a diffuse noise background. As well, we
introduce the concept of a diffuseness profile, which consists in measuring the
diffuseness for several SH orders simultaneously. Experimental results indicate
that diffuseness profiles better describe the properties of the sound field
than a single diffuseness measurement.
</summary>
    <author>
      <name>Nicolas Epain</name>
    </author>
    <author>
      <name>Craig T. Jin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to IEEE/ACM Transactions on Audio, Speech, and Language
  Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.00211v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.00211v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.02306v2</id>
    <updated>2016-08-15T18:02:09Z</updated>
    <published>2016-07-08T10:42:43Z</published>
    <title>CaR-FOREST: Joint Classification-Regression Decision Forests for
  Overlapping Audio Event Detection</title>
    <summary>  This report describes our submissions to Task2 and Task3 of the DCASE 2016
challenge. The systems aim at dealing with the detection of overlapping audio
events in continuous streams, where the detectors are based on random decision
forests. The proposed forests are jointly trained for classification and
regression simultaneously. Initially, the training is classification-oriented
to encourage the trees to select discriminative features from overlapping
mixtures to separate positive audio segments from the negative ones. The
regression phase is then carried out to let the positive audio segments vote
for the event onsets and offsets, and therefore model the temporal structure of
audio events. One random decision forest is specifically trained for each event
category of interest. Experimental results on the development data show that
our systems significantly outperform the baseline on the Task2 evaluation while
they are inferior to the baseline in the Task3 evaluation.
</summary>
    <author>
      <name>Huy Phan</name>
    </author>
    <author>
      <name>Lars Hertel</name>
    </author>
    <author>
      <name>Marco Maass</name>
    </author>
    <author>
      <name>Philipp Koch</name>
    </author>
    <author>
      <name>Alfred Mertins</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Task2 and Task3 technical report for the DCASE2016 challenge</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.02306v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.02306v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.03682v3</id>
    <updated>2016-08-13T10:37:53Z</updated>
    <published>2016-07-13T11:31:25Z</published>
    <title>Hierarchical learning for DNN-based acoustic scene classification</title>
    <summary>  In this paper, we present a deep neural network (DNN)-based acoustic scene
classification framework. Two hierarchical learning methods are proposed to
improve the DNN baseline performance by incorporating the hierarchical taxonomy
information of environmental sounds. Firstly, the parameters of the DNN are
initialized by the proposed hierarchical pre-training. Multi-level objective
function is then adopted to add more constraint on the cross-entropy based loss
function. A series of experiments were conducted on the Task1 of the Detection
and Classification of Acoustic Scenes and Events (DCASE) 2016 challenge. The
final DNN-based system achieved a 22.9% relative improvement on average scene
classification error as compared with the Gaussian Mixture Model (GMM)-based
benchmark system across four standard folds.
</summary>
    <author>
      <name>Yong Xu</name>
    </author>
    <author>
      <name>Qiang Huang</name>
    </author>
    <author>
      <name>Wenwu Wang</name>
    </author>
    <author>
      <name>Mark D. Plumbley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, DCASE 2016 challenge workshop paper, poster</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.03682v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.03682v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.04589v1</id>
    <updated>2016-07-15T17:29:26Z</updated>
    <published>2016-07-15T17:29:26Z</published>
    <title>Automatic Environmental Sound Recognition: Performance versus
  Computational Cost</title>
    <summary>  In the context of the Internet of Things (IoT), sound sensing applications
are required to run on embedded platforms where notions of product pricing and
form factor impose hard constraints on the available computing power. Whereas
Automatic Environmental Sound Recognition (AESR) algorithms are most often
developed with limited consideration for computational cost, this article seeks
which AESR algorithm can make the most of a limited amount of computing power
by comparing the sound classification performance em as a function of its
computational cost. Results suggest that Deep Neural Networks yield the best
ratio of sound classification accuracy across a range of computational costs,
while Gaussian Mixture Models offer a reasonable accuracy at a consistently
small cost, and Support Vector Machines stand between both in terms of
compromise between accuracy and computational cost.
</summary>
    <author>
      <name>Siddharth Sigtia</name>
    </author>
    <author>
      <name>Adam M. Stark</name>
    </author>
    <author>
      <name>Sacha Krstulovic</name>
    </author>
    <author>
      <name>Mark D. Plumbley</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TASLP.2016.2592698</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TASLP.2016.2592698" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE/ACM Transactions on Audio, Speech and Language Processing
  24(11): 2096-2107, Nov 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1607.04589v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.04589v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.01953v2</id>
    <updated>2017-04-11T09:04:30Z</updated>
    <published>2016-08-05T17:45:05Z</published>
    <title>Model-based STFT phase recovery for audio source separation</title>
    <summary>  For audio source separation applications, it is common to estimate the
magnitude of the Time-Frequency (TF) representation of each source. In order to
recover a time-domain signal from a spectrogram for instance, it then becomes
necessary to recover the phase of the corresponding complex-valued Short-Time
Fourier Transform (STFT). Most authors in this field choose a Wiener-like
filtering approach which boils down to using the phase of the original mixture.
In this paper, a different standpoint is adopted. Many music events are
partially composed of slowly varying sinusoids and the STFT phase increment of
those frequency components takes a specific form. This allows phase recovery by
an unwrapping technique once a short-term frequency estimate has been obtained.
Herein, a whole iterative source separation procedure is proposed which builds
upon these results. It is tested on a variety of data, both synthetic and
realistic, and also with different source separation scenarios, oracle or non
oracle. In terms of SIR, SAR and SDR, the method achieves better performance
than consistency-based approaches. To complete the experimental analysis, sound
examples are provided which allow the reader to assess the interest of the
method regarding the improvement of sound quality.
</summary>
    <author>
      <name>Paul Magron</name>
    </author>
    <author>
      <name>Roland Badeau</name>
    </author>
    <author>
      <name>Bertrand David</name>
    </author>
    <link href="http://arxiv.org/abs/1608.01953v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.01953v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.02272v1</id>
    <updated>2016-08-07T21:43:08Z</updated>
    <published>2016-08-07T21:43:08Z</published>
    <title>Incorporation of Speech Duration Information in Score Fusion of Speaker
  Recognition Systems</title>
    <summary>  In recent years identity-vector (i-vector) based speaker verification (SV)
systems have become very successful. Nevertheless, environmental noise and
speech duration variability still have a significant effect on degrading the
performance of these systems. In many real-life applications, duration of
recordings are very short; as a result, extracted i-vectors cannot reliably
represent the attributes of the speaker. Here, we investigate the effect of
speech duration on the performance of three state-of-the-art speaker
recognition systems. In addition, using a variety of available score fusion
methods, we investigate the effect of score fusion for those speaker
verification techniques to benefit from the performance difference of different
methods under different enrollment and test speech duration conditions. This
technique performed significantly better than the baseline score fusion
methods.
</summary>
    <author>
      <name>Ali Khodabakhsh</name>
    </author>
    <author>
      <name>Seyyed Saeed Sarfjoo</name>
    </author>
    <author>
      <name>Umut Uludag</name>
    </author>
    <author>
      <name>Osman Soyyigit</name>
    </author>
    <author>
      <name>Cenk Demiroglu</name>
    </author>
    <link href="http://arxiv.org/abs/1608.02272v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.02272v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.03720v1</id>
    <updated>2016-08-12T08:52:23Z</updated>
    <published>2016-08-12T08:52:23Z</published>
    <title>Speech Signal Analysis for the Estimation of Heart Rates Under Different
  Emotional States</title>
    <summary>  A non-invasive method for the monitoring of heart activity can help to reduce
the deaths caused by heart disorders such as stroke, arrhythmia and heart
attack. The human voice can be considered as a biometric data that can be used
for estimation of heart rate. In this paper, we propose a method for estimating
the heart rate from human speech dynamically using voice signal analysis and by
the development of an empirical linear predictor model. The correlation between
the voice signal and heart rate are established by classifiers and prediction
of the heart rates with or without emotions are done using linear models. The
prediction accuracy was tested using the data collected from 15 subjects, it is
about 4050 samples of speech signals and corresponding electrocardiogram
samples. The proposed approach can use for early non-invasive detection of
heart rate changes that can be correlated to an emotional state of the
individual and also can be used as a tool for diagnosis of heart conditions in
real-time situations.
</summary>
    <author>
      <name>Aibek Ryskaliyev</name>
    </author>
    <author>
      <name>Sanzhar Askaruly</name>
    </author>
    <author>
      <name>Alex Pappachen James</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear in Proceedings of International Conference on Advances in
  Computing, Communications and Informatics, IEEE, 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.03720v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.03720v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.04069v1</id>
    <updated>2016-08-14T07:39:51Z</updated>
    <published>2016-08-14T07:39:51Z</published>
    <title>Design of Variable Bandpass Filters Using First Order Allpass
  Transformation And Coefficient Decimation</title>
    <summary>  In this paper, the design of a computationally efficient variable bandpass
digital filter is presented. The center frequency and bandwidth of this filter
can be changed online without updating the filter coefficients. The warped
filters, obtained by replacing each unit delay of a digital filter with an
allpass filter, are widely used for various audio processing applications.
However, warped filters fail to provide variable bandwidth bandpass responses
for a given center frequency using first order allpass transformation. To
overcome this drawback, our design is accomplished by combining warped filter
with the coefficient decimation technique. The design example shows that the
proposed variable digital filter is simple to design and offers a total gate
count reduction of 36% and 65% over the warped filters compared to the designs
presented in [3] and [1] respectively
</summary>
    <author>
      <name>S. J. Darak</name>
    </author>
    <author>
      <name>A. P. Vinod</name>
    </author>
    <author>
      <name>E. M-K. Lai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18th Electronics New Zealand Conference (ENZCON)</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.04069v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.04069v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.05179v2</id>
    <updated>2016-08-29T14:31:34Z</updated>
    <published>2016-08-18T05:16:38Z</published>
    <title>Improving the Efficiency of DAMAS for Sound Source Localization via
  Wavelet Compression Computational Grid</title>
    <summary>  Phased microphone arrays are used widely in the applications for acoustic
source localization. Deconvolution approaches such as DAMAS successfully
overcome the spatial resolution limit of the conventional delay-and-sum (DAS)
beamforming method. However deconvolution approaches require high computational
effort compared to conventional DAS beamforming method. This paper presents a
novel method that serves to improve the efficiency of DAMAS via wavelet
compression computational grid rather than via optimizing DAMAS algorithm. In
this method, the efficiency of DAMAS increases with compression ratio. This
method can thus save lots of run time in industrial applications for sound
source localization, particularly when sound sources are just located in a
small extent compared with scanning plane and a band of angular frequency needs
to be calculated. In addition, this method largely retains the spatial
resolution of DAMAS on original computational grid, although with a minor
deficiency that the occurrence probability of aliasing increasing slightly for
complicated sound source.
</summary>
    <author>
      <name>Wei Ma</name>
    </author>
    <author>
      <name>Xun Liu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.jsv.2017.02.005</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.jsv.2017.02.005" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 6 figures, 2 tables, 23 conferences</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.05179v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.05179v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.00291v1</id>
    <updated>2016-09-01T15:52:58Z</updated>
    <published>2016-09-01T15:52:58Z</published>
    <title>A Non-iterative Method for (Re)Construction of Phase from STFT Magnitude</title>
    <summary>  A non-iterative method for the construction of the Short-Time Fourier
Transform (STFT) phase from the magnitude is presented. The method is based on
the direct relationship between the partial derivatives of the phase and the
logarithm of the magnitude of the un-sampled STFT with respect to the Gaussian
window. Although the theory holds in the continuous setting only, the
experiments show that the algorithm performs well even in the discretized
setting (Discrete Gabor transform) with low redundancy using the sampled
Gaussian window, the truncated Gaussian window and even other compactly
supported windows like the Hann window.
  Due to the non-iterative nature, the algorithm is very fast and it is
suitable for long audio signals. Moreover, solutions of iterative phase
reconstruction algorithms can be improved considerably by initializing them
with the phase estimate provided by the present algorithm.
  We present an extensive comparison with the state-of-the-art algorithms in a
reproducible manner.
</summary>
    <author>
      <name>Zdeněk Průša</name>
    </author>
    <author>
      <name>Peter Balazs</name>
    </author>
    <author>
      <name>Peter L. Søndergaard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.00291v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.00291v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.01678v2</id>
    <updated>2016-12-20T12:13:56Z</updated>
    <published>2016-09-06T18:03:33Z</published>
    <title>Discriminative Enhancement for Single Channel Audio Source Separation
  using Deep Neural Networks</title>
    <summary>  The sources separated by most single channel audio source separation
techniques are usually distorted and each separated source contains residual
signals from the other sources. To tackle this problem, we propose to enhance
the separated sources to decrease the distortion and interference between the
separated sources using deep neural networks (DNNs). Two different DNNs are
used in this work. The first DNN is used to separate the sources from the mixed
signal. The second DNN is used to enhance the separated signals. To consider
the interactions between the separated sources, we propose to use a single DNN
to enhance all the separated sources together. To reduce the residual signals
of one source from the other separated sources (interference), we train the DNN
for enhancement discriminatively to maximize the dissimilarity between the
predicted sources. The experimental results show that using discriminative
enhancement decreases the distortion and interference between the separated
sources.
</summary>
    <author>
      <name>Emad M. Grais</name>
    </author>
    <author>
      <name>Gerard Roma</name>
    </author>
    <author>
      <name>Andrew J. R. Simpson</name>
    </author>
    <author>
      <name>Mark D. Plumbley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13th International Conference on Latent Variable Analysis and Signal
  Separation (LVA/ICA 2017)</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.01678v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.01678v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.03213v1</id>
    <updated>2016-09-11T20:36:16Z</updated>
    <published>2016-09-11T20:36:16Z</published>
    <title>Relaxed Binaural LCMV Beamforming</title>
    <summary>  In this paper we propose a new binaural beamforming technique which can be
seen as a relaxation of the linearly constrained minimum variance (LCMV)
framework. The proposed method can achieve simultaneous noise reduction and
exact binaural cue preservation of the target source, similar to the binaural
minimum variance distortionless response (BMVDR) method. However, unlike BMVDR,
the proposed method is also able to preserve the binaural cues of multiple
interferers to a certain predefined accuracy. Specifically, it is able to
control the trade-off between noise reduction and binaural cue preservation of
the interferers by using a separate trade-off parameter per interferer.
Moreover, we provide a robust way of selecting these trade-off parameters in
such a way that the preservation accuracy for the binaural cues of the
interferers is always better than the corresponding ones of the BMVDR. The
relaxation of the constraints in the proposed method achieves approximate
binaural cue preservation of more interferers than other previously presented
LCMV-based binaural beamforming methods that use strict equality constraints.
</summary>
    <author>
      <name>Andreas I. Koutrouvelis</name>
    </author>
    <author>
      <name>Richard C. Hendriks</name>
    </author>
    <author>
      <name>Richard Heusdens</name>
    </author>
    <author>
      <name>Jesper Jensen</name>
    </author>
    <link href="http://arxiv.org/abs/1609.03213v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.03213v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.03499v2</id>
    <updated>2016-09-19T18:04:35Z</updated>
    <published>2016-09-12T17:29:40Z</published>
    <title>WaveNet: A Generative Model for Raw Audio</title>
    <summary>  This paper introduces WaveNet, a deep neural network for generating raw audio
waveforms. The model is fully probabilistic and autoregressive, with the
predictive distribution for each audio sample conditioned on all previous ones;
nonetheless we show that it can be efficiently trained on data with tens of
thousands of samples per second of audio. When applied to text-to-speech, it
yields state-of-the-art performance, with human listeners rating it as
significantly more natural sounding than the best parametric and concatenative
systems for both English and Mandarin. A single WaveNet can capture the
characteristics of many different speakers with equal fidelity, and can switch
between them by conditioning on the speaker identity. When trained to model
music, we find that it generates novel and often highly realistic musical
fragments. We also show that it can be employed as a discriminative model,
returning promising results for phoneme recognition.
</summary>
    <author>
      <name>Aaron van den Oord</name>
    </author>
    <author>
      <name>Sander Dieleman</name>
    </author>
    <author>
      <name>Heiga Zen</name>
    </author>
    <author>
      <name>Karen Simonyan</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Alex Graves</name>
    </author>
    <author>
      <name>Nal Kalchbrenner</name>
    </author>
    <author>
      <name>Andrew Senior</name>
    </author>
    <author>
      <name>Koray Kavukcuoglu</name>
    </author>
    <link href="http://arxiv.org/abs/1609.03499v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.03499v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.05104v2</id>
    <updated>2016-12-10T10:20:01Z</updated>
    <published>2016-09-16T15:20:32Z</published>
    <title>Intrinsic normalization and extrinsic denormalization of formant data of
  vowels</title>
    <summary>  Using a known speaker-intrinsic normalization procedure, formant data are
scaled by the reciprocal of the geometric mean of the first three formant
frequencies. This reduces the influence of the talker but results in a
distorted vowel space. The proposed speaker-extrinsic procedure re-scales the
normalized values by the mean formant values of vowels. When tested on the
formant data of vowels published by Peterson and Barney, the combined approach
leads to well separated clusters by reducing the spread due to talkers. The
proposed procedure performs better than two top-ranked normalization procedures
based on the accuracy of vowel classification as the objective measure.
</summary>
    <author>
      <name>T. V. Ananthapadmanabha</name>
    </author>
    <author>
      <name>A. G. Ramakrishnan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 8 figures. Title has been revised. Appendix has been added
  to include more figures and to clarify 'hypothesize-test' procedure, JASA-EL,
  2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.05104v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.05104v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.06026v3</id>
    <updated>2017-06-27T17:09:59Z</updated>
    <published>2016-09-20T05:52:06Z</published>
    <title>An Approach for Self-Training Audio Event Detectors Using Web Data</title>
    <summary>  Audio Event Detection (AED) aims to recognize sounds within audio and video
recordings. AED employs machine learning algorithms commonly trained and tested
on annotated datasets. However, available datasets are limited in number of
samples and hence it is difficult to model acoustic diversity. Therefore, we
propose combining labeled audio from a dataset and unlabeled audio from the web
to improve the sound models. The audio event detectors are trained on the
labeled audio and ran on the unlabeled audio downloaded from YouTube. Whenever
the detectors recognized any of the known sounds with high confidence, the
unlabeled audio was use to re-train the detectors. The performance of the
re-trained detectors is compared to the one from the original detectors using
the annotated test set. Results showed an improvement of the AED, and uncovered
challenges of using web audio from videos.
</summary>
    <author>
      <name>Benjamin Elizalde</name>
    </author>
    <author>
      <name>Ankit Shah</name>
    </author>
    <author>
      <name>Siddharth Dalmia</name>
    </author>
    <author>
      <name>Min Hun Lee</name>
    </author>
    <author>
      <name>Rohan Badlani</name>
    </author>
    <author>
      <name>Anurag Kumar</name>
    </author>
    <author>
      <name>Bhiksha Raj</name>
    </author>
    <author>
      <name>Ian Lane</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.06026v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.06026v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.06210v2</id>
    <updated>2017-02-08T13:00:52Z</updated>
    <published>2016-09-20T15:09:36Z</published>
    <title>Interference Reduction in Music Recordings Combining Kernel Additive
  Modelling and Non-Negative Matrix Factorization</title>
    <summary>  In live and studio recordings unexpected sound events often lead to
interferences in the signal. For non-stationary interferences, sound source
separation techniques can be used to reduce the interference level in the
recording. In this context, we present a novel approach combining the strengths
of two algorithmic families: NMF and KAM. The recent KAM approach applies
robust statistics on frames selected by a source-specific kernel to perform
source separation. Based on semi-supervised NMF, we extend this approach in two
ways. First, we locate the interference in the recording based on detected NMF
activity. Second, we improve the kernel-based frame selection by incorporating
an NMF-based estimate of the clean music signal. Further, we introduce a
temporal context in the kernel, taking some musical structure into account. Our
experiments show improved separation quality for our proposed method over a
state-of-the-art approach for interference reduction.
</summary>
    <author>
      <name>Delia Fano Yela</name>
    </author>
    <author>
      <name>Sebastian Ewert</name>
    </author>
    <author>
      <name>Derry FitzGerald</name>
    </author>
    <author>
      <name>Mark Sandler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Acoustics, Speech and Signal Processing
  (ICASSP)</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.06210v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.06210v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.06404v1</id>
    <updated>2016-09-21T02:14:23Z</updated>
    <published>2016-09-21T02:14:23Z</published>
    <title>KU-ISPL Language Recognition System for NIST 2015 i-Vector Machine
  Learning Challenge</title>
    <summary>  In language recognition, the task of rejecting/differentiating closely spaced
versus acoustically far spaced languages remains a major challenge. For
confusable closely spaced languages, the system needs longer input test
duration material to obtain sufficient information to distinguish between
languages. Alternatively, if languages are distinct and not
acoustically/linguistically similar to others, duration is not a sufficient
remedy. The solution proposed here is to explore duration distribution analysis
for near/far languages based on the Language Recognition i-Vector Machine
Learning Challenge 2015 (LRiMLC15) database. Using this knowledge, we propose a
likelihood ratio based fusion approach that leveraged both score and duration
information. The experimental results show that the use of duration and score
fusion improves language recognition performance by 5% relative in LRiMLC15
cost.
</summary>
    <author>
      <name>Suwon Shon</name>
    </author>
    <author>
      <name>Seongkyu Mun</name>
    </author>
    <author>
      <name>John H. L. Hansen</name>
    </author>
    <author>
      <name>Hanseok Ko</name>
    </author>
    <link href="http://arxiv.org/abs/1609.06404v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.06404v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.07245v2</id>
    <updated>2016-12-21T08:57:51Z</updated>
    <published>2016-09-23T07:03:32Z</published>
    <title>A New Statistic Feature of the Short-Time Amplitude Spectrum Values for
  Human's Unvoiced Pronunciation</title>
    <summary>  In this paper, a new statistic feature of the discrete short-time amplitude
spectrum is discovered by experiments for the signals of unvoiced
pronunciation. For the random-varying short-time spectrum, this feature reveals
the relationship between the amplitude's average and its standard for every
frequency component. On the other hand, the association between the amplitude
distributions for different frequency components is also studied. A new model
representing such association is inspired by the normalized histogram of
amplitude. By mathematical analysis, the new statistic feature discovered is
proved to be necessary evidence which supports the proposed model, and also can
be direct evidence for the widely used hypothesis of "identical distribution of
amplitude for all frequencies".
</summary>
    <author>
      <name>Xiaodong Zhuang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures, original work</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">WSEAS Transactions on Signal Processing, Volume 12, pp. 265-269,
  2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1609.07245v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.07245v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.07384v2</id>
    <updated>2017-02-13T01:09:27Z</updated>
    <published>2016-09-23T14:35:17Z</published>
    <title>Discovering Sound Concepts and Acoustic Relations In Text</title>
    <summary>  In this paper we describe approaches for discovering acoustic concepts and
relations in text. The first major goal is to be able to identify text phrases
which contain a notion of audibility and can be termed as a sound or an
acoustic concept. We also propose a method to define an acoustic scene through
a set of sound concepts. We use pattern matching and parts of speech tags to
generate sound concepts from large scale text corpora. We use dependency
parsing and LSTM recurrent neural network to predict a set of sound concepts
for a given acoustic scene. These methods are not only helpful in creating an
acoustic knowledge base but in the future can also directly help acoustic event
and scene detection research.
</summary>
    <author>
      <name>Anurag Kumar</name>
    </author>
    <author>
      <name>Bhiksha Raj</name>
    </author>
    <author>
      <name>Ndapandula Nakashole</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICASSP 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.07384v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.07384v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.07498v1</id>
    <updated>2016-09-23T20:03:14Z</updated>
    <published>2016-09-23T20:03:14Z</published>
    <title>Speaker Recognition for Children's Speech</title>
    <summary>  This paper presents results on Speaker Recognition (SR) for children's
speech, using the OGI Kids corpus and GMM-UBM and GMM-SVM SR systems. Regions
of the spectrum containing important speaker information for children are
identified by conducting SR experiments over 21 frequency bands. As for adults,
the spectrum can be split into four regions, with the first (containing primary
vocal tract resonance information) and third (corresponding to high frequency
speech sounds) being most useful for SR. However, the frequencies at which
these regions occur are from 11% to 38% higher for children. It is also noted
that subband SR rates are lower for younger children. Finally results are
presented of SR experiments to identify a child in a class (30 children,
similar age) and school (288 children, varying ages). Class performance depends
on age, with accuracy varying from 90% for young children to 99% for older
children. The identification rate achieved for a child in a school is 81%.
</summary>
    <author>
      <name>Saeid Safavi</name>
    </author>
    <author>
      <name>Maryam Najafian</name>
    </author>
    <author>
      <name>Abualsoud Hanani</name>
    </author>
    <author>
      <name>Martin J Russell</name>
    </author>
    <author>
      <name>Peter Jancovic</name>
    </author>
    <author>
      <name>Michael J Carey</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">INTERSPEECH 2012, Pages 1836-1839</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.07498v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.07498v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.08408v1</id>
    <updated>2016-09-25T15:56:06Z</updated>
    <published>2016-09-25T15:56:06Z</published>
    <title>Deep learning for detection of bird vocalisations</title>
    <summary>  This work focuses on reliable detection of bird sound emissions as recorded
in the open field. Acoustic detection of avian sounds can be used for the
automatized monitoring of multiple bird taxa and querying in long-term
recordings for species of interest for researchers, conservation practitioners,
and decision makers. Recordings in the wild can be very noisy due to the
exposure of the microphones to a large number of audio sources originating from
all distances and directions, the number and identity of which cannot be known
a-priori. The co-existence of the target vocalizations with abiotic
interferences in an unconstrained environment is inefficiently treated by
current approaches of audio signal enhancement. A technique that would spot
only bird vocalization while ignoring other audio sources is of prime
importance. These difficulties are tackled in this work, presenting a deep
autoencoder that maps the audio spectrogram of bird vocalizations to its
corresponding binary mask that encircles the spectral blobs of vocalizations
while suppressing other audio sources. The procedure requires minimum human
attendance, it is very fast during execution, thus suitable to scan massive
volumes of data, in order to analyze them, evaluate insights and hypotheses,
identify patterns of bird activity that, hopefully, finally lead to design
policies on biodiversity issues.
</summary>
    <author>
      <name>Ilyas Potamitis</name>
    </author>
    <link href="http://arxiv.org/abs/1609.08408v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.08408v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.08419v1</id>
    <updated>2016-09-27T13:29:12Z</updated>
    <published>2016-09-27T13:29:12Z</published>
    <title>Decision Making Based on Cohort Scores for Speaker Verification</title>
    <summary>  Decision making is an important component in a speaker verification system.
For the conventional GMM-UBM architecture, the decision is usually conducted
based on the log likelihood ratio of the test utterance against the GMM of the
claimed speaker and the UBM. This single-score decision is simple but tends to
be sensitive to the complex variations in speech signals (e.g. text content,
channel, speaking style, etc.). In this paper, we propose a decision making
approach based on multiple scores derived from a set of cohort GMMs (cohort
scores). Importantly, these cohort scores are not simply averaged as in
conventional cohort methods; instead, we employ a powerful discriminative model
as the decision maker. Experimental results show that the proposed method
delivers substantial performance improvement over the baseline system,
especially when a deep neural network (DNN) is used as the decision maker, and
the DNN input involves some statistical features derived from the cohort
scores.
</summary>
    <author>
      <name>Lantian Li</name>
    </author>
    <author>
      <name>Renyu Wang</name>
    </author>
    <author>
      <name>Gang Wang</name>
    </author>
    <author>
      <name>Caixia Wang</name>
    </author>
    <author>
      <name>Thomas Fang Zheng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">APSIPA ASC 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.08419v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.08419v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.08433v1</id>
    <updated>2016-09-27T13:37:13Z</updated>
    <published>2016-09-27T13:37:13Z</published>
    <title>Local Training for PLDA in Speaker Verification</title>
    <summary>  PLDA is a popular normalization approach for the i-vector model, and it has
delivered state-of-the-art performance in speaker verification. However, PLDA
training requires a large amount of labeled development data, which is highly
expensive in most cases. A possible approach to mitigate the problem is various
unsupervised adaptation methods, which use unlabeled data to adapt the PLDA
scattering matrices to the target domain.
  In this paper, we present a new `local training' approach that utilizes
inaccurate but much cheaper local labels to train the PLDA model. These local
labels discriminate speakers within a single conversion only, and so are much
easier to obtain compared to the normal `global labels'. Our experiments show
that the proposed approach can deliver significant performance improvement,
particularly with limited globally-labeled data.
</summary>
    <author>
      <name>Chenghui Zhao</name>
    </author>
    <author>
      <name>Lantian Li</name>
    </author>
    <author>
      <name>Dong Wang</name>
    </author>
    <author>
      <name>April Pu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">O-COCOSDA 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.08433v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.08433v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.09231v1</id>
    <updated>2016-09-29T07:24:27Z</updated>
    <published>2016-09-29T07:24:27Z</published>
    <title>Low Rank and Sparsity Analysis Applied to Speech Enhancement via Online
  Estimated Dictionary</title>
    <summary>  We propose an online estimated dictionary based single channel speech
enhancement algorithm, which focuses on low rank and sparse matrix
decomposition. In this proposed algorithm, a noisy speech spectral matrix is
considered as the summation of low rank background noise components and an
activation of the online speech dictionary, on which both low rank and sparsity
constraints are imposed. This decomposition takes the advantage of local
estimated dictionary high expressiveness on speech components. The local
dictionary can be obtained through estimating the speech presence probability
by applying Expectation Maximal algorithm, in which a generalized Gamma prior
for speech magnitude spectrum is used. The evaluation results show that the
proposed algorithm achieves significant improvements when compared to four
other speech enhancement algorithms.
</summary>
    <author>
      <name>Pengfei Sun</name>
    </author>
    <author>
      <name>Jun Qin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/LSP.2016.2627029</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/LSP.2016.2627029" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 figures, journal in IEEE signal processing letter 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.09231v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.09231v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.09390v1</id>
    <updated>2016-09-29T15:33:09Z</updated>
    <published>2016-09-29T15:33:09Z</published>
    <title>Measurement of Sound Fields Using Moving Microphones</title>
    <summary>  The sampling of sound fields involves the measurement of spatially dependent
room impulse responses, where the Nyquist-Shannon sampling theorem applies in
both the temporal and spatial domain. Therefore, sampling inside a volume of
interest requires a huge number of sampling points in space, which comes along
with further difficulties such as exact microphone positioning and calibration
of multiple microphones. In this paper, we present a method for measuring sound
fields using moving microphones whose trajectories are known to the algorithm.
At that, the number of microphones is customizable by trading measurement
effort against sampling time. Through spatial interpolation of the dynamic
measurements, a system of linear equations is set up which allows for the
reconstruction of the entire sound field inside the volume of interest.
</summary>
    <author>
      <name>Fabrice Katzberg</name>
    </author>
    <author>
      <name>Radoslaw Mazur</name>
    </author>
    <author>
      <name>Marco Maass</name>
    </author>
    <author>
      <name>Philipp Koch</name>
    </author>
    <author>
      <name>Alfred Mertins</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to ICASSP 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.09390v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.09390v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.09443v2</id>
    <updated>2017-02-23T05:20:54Z</updated>
    <published>2016-09-29T17:54:51Z</published>
    <title>Semi-supervised Speech Enhancement in Envelop and Details Subspaces</title>
    <summary>  In this study, we propose a modulation decoupling based single channel speech
enhancement subspace framework, in which the spectrogram of noisy speech is
decoupled as the product of a spectral envelop subspace and a spectral details
subspace. This decoupling approach provides a method to specifically work on
elimination of those noises that greatly affect the intelligibility. Two
supervised low-rank and sparse decomposition schemes are developed in the
spectral envelop subspace to obtain a robust recovery of speech components. A
Bayesian formulation of non-negative factorization is used to learn the speech
dictionary from the spectral envelop subspace of clean speech samples. In the
spectral details subspace, a standard robust principal component analysis is
implemented to extract the speech components. The validation results show that
compared with four speech enhancement algorithms, including MMSE-SPP, NMF-RPCA,
RPCA, and LARC, the proposed MS based algorithms achieve satisfactory
performance on improving perceptual quality, and especially speech
intelligibility.
</summary>
    <author>
      <name>Pengfei Sun</name>
    </author>
    <author>
      <name>Jun Qin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.09443v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.09443v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.09743v1</id>
    <updated>2016-09-30T14:15:46Z</updated>
    <published>2016-09-30T14:15:46Z</published>
    <title>Rectified binaural ratio: A complex T-distributed feature for robust
  sound localization</title>
    <summary>  Most existing methods in binaural sound source localization rely on some kind
of aggregation of phase-and level-difference cues in the time-frequency plane.
While different ag-gregation schemes exist, they are often heuristic and suffer
in adverse noise conditions. In this paper, we introduce the rectified binaural
ratio as a new feature for sound source local-ization. We show that for
Gaussian-process point source signals corrupted by stationary Gaussian noise,
this ratio follows a complex t-distribution with explicit parameters. This new
formulation provides a principled and statistically sound way to aggregate
binaural features in the presence of noise. We subsequently derive two simple
and efficient methods for robust relative transfer function and time-delay
estimation. Experiments on heavily corrupted simulated and speech signals
demonstrate the robustness of the proposed scheme.
</summary>
    <author>
      <name>Antoine Deleforge</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PANAMA</arxiv:affiliation>
    </author>
    <author>
      <name>Florence Forbes</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MISTIS</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">European Signal Processing Conference, Aug 2016, Budapest, Hungary.
  Proceedings of the 24th European Signal Processing Conference (EUSIPCO),
  2016, 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.09743v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.09743v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.09744v2</id>
    <updated>2017-03-20T13:36:08Z</updated>
    <published>2016-09-30T14:17:32Z</published>
    <title>Phase Unmixing : Multichannel Source Separation with Magnitude
  Constraints</title>
    <summary>  We consider the problem of estimating the phases of K mixed complex signals
from a multichannel observation, when the mixing matrix and signal magnitudes
are known. This problem can be cast as a non-convex quadratically constrained
quadratic program which is known to be NP-hard in general. We propose three
approaches to tackle it: a heuristic method, an alternate minimization method,
and a convex relaxation into a semi-definite program. The last two approaches
are showed to outperform the oracle multichannel Wiener filter in
under-determined informed source separation tasks, using simulated and speech
signals. The convex relaxation approach yields best results, including the
potential for exact source separation in under-determined settings.
</summary>
    <author>
      <name>Antoine Deleforge</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PANAMA</arxiv:affiliation>
    </author>
    <author>
      <name>Yann Traonmilin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PANAMA</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2017 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP), Mar 2017, New Orleans, United States</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.09744v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.09744v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.09747v2</id>
    <updated>2017-03-20T13:39:49Z</updated>
    <published>2016-09-30T14:20:56Z</published>
    <title>Hearing in a shoe-box : binaural source position and wall absorption
  estimation using virtually supervised learning</title>
    <summary>  This paper introduces a new framework for supervised sound source
localization referred to as virtually-supervised learning. An acoustic shoe-box
room simulator is used to generate a large number of binaural single-source
audio scenes. These scenes are used to build a dataset of spatial binaural
features annotated with acoustic properties such as the 3D source position and
the walls' absorption coefficients. A probabilistic high- to low-dimensional
regression framework is used to learn a mapping from these features to the
acoustic properties. Results indicate that this mapping successfully estimates
the azimuth and elevation of new sources, but also their range and even the
walls' absorption coefficients solely based on binaural signals. Results also
reveal that incorporating random-diffusion effects in the data significantly
improves the estimation of all parameters.
</summary>
    <author>
      <name>Saurabh Kataria</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IIT Kanpur, Panama</arxiv:affiliation>
    </author>
    <author>
      <name>Clément Gaultier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Panama</arxiv:affiliation>
    </author>
    <author>
      <name>Antoine Deleforge</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Panama</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2017 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP), Mar 2017, New-Orleans, United States</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.09747v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.09747v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.00468v1</id>
    <updated>2016-10-03T09:56:43Z</updated>
    <published>2016-10-03T09:56:43Z</published>
    <title>On the Modeling of Musical Solos as Complex Networks</title>
    <summary>  Notes in a musical piece are building blocks employed in non-random ways to
create melodies. It is the "interaction" among a limited amount of notes that
allows constructing the variety of musical compositions that have been written
in centuries and within different cultures. Networks are a modeling tool that
is commonly employed to represent a set of entities interacting in some way.
Thus, notes composing a melody can be seen as nodes of a network that are
connected whenever these are played in sequence. The outcome of such a process
results in a directed graph. By using complex network theory, some main metrics
of musical graphs can be measured, which characterize the related musical
pieces. In this paper, we define a framework to represent melodies as networks.
Then, we provide an analysis on a set of guitar solos performed by main
musicians. Results of this study indicate that the presented model can have an
impact on audio and multimedia applications such as music classification,
identification, e-learning, automatic music generation, multimedia
entertainment.
</summary>
    <author>
      <name>Stefano Ferretti</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.ins.2016.10.007</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.ins.2016.10.007" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear in Information Science, Elsevier. Please cite the paper
  including such information. arXiv admin note: text overlap with
  arXiv:1603.04979</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.00468v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.00468v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.00644v2</id>
    <updated>2016-10-04T02:16:08Z</updated>
    <published>2016-10-03T17:39:01Z</published>
    <title>Speech Enhancement via Two-Stage Dual Tree Complex Wavelet Packet
  Transform with a Speech Presence Probability Estimator</title>
    <summary>  In this paper, a two-stage dual tree complex wavelet packet transform
(DTCWPT) based speech enhancement algorithm has been proposed, in which a
speech presence probability (SPP) estimator and a generalized minimum mean
squared error (MMSE) estimator are developed. To overcome the drawback of
signal distortions caused by down sampling of WPT, a two-stage analytic
decomposition concatenating undecimated WPT (UWPT) and decimated WPT is
employed. An SPP estimator in the DTCWPT domain is derived based on a
generalized Gamma distribution of speech, and Gaussian noise assumption. The
validation results show that the proposed algorithm can obtain enhanced
perceptual evaluation of speech quality (PESQ), and segmental signal-to-noise
ratio (SegSNR) at low SNR nonstationary noise, compared with other four
state-of-the-art speech enhancement algorithms, including optimally modified
LSA (OM-LSA), soft masking using a posteriori SNR uncertainty (SMPO), a
posteriori SPP based MMSE estimation (MMSE-SPP), and adaptive Bayesian wavelet
thresholding (BWT).
</summary>
    <author>
      <name>Pengfei Sun</name>
    </author>
    <author>
      <name>Jun Qin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1121/1.4976049</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1121/1.4976049" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, Journal of the Acoustical Society of America 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.00644v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.00644v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.01382v1</id>
    <updated>2016-10-05T12:16:35Z</updated>
    <published>2016-10-05T12:16:35Z</published>
    <title>Divide-and-Conquer based Ensemble to Spot Emotions in Speech using MFCC
  and Random Forest</title>
    <summary>  Besides spoken words, speech signals also carry information about speaker
gender, age, and emotional state which can be used in a variety of speech
analysis applications. In this paper, a divide and conquer strategy for
ensemble classification has been proposed to recognize emotions in speech.
Intrinsic hierarchy in emotions has been utilized to construct an emotions
tree, which assisted in breaking down the emotion recognition task into smaller
sub tasks. The proposed framework generates predictions in three phases.
Firstly, emotions are detected in the input speech signal by classifying it as
neutral or emotional. If the speech is classified as emotional, then in the
second phase, it is further classified into positive and negative classes.
Finally, individual positive or negative emotions are identified based on the
outcomes of the previous stages. Several experiments have been performed on a
widely used benchmark dataset. The proposed method was able to achieve improved
recognition rates as compared to several other approaches.
</summary>
    <author>
      <name>Abdul Malik Badshah</name>
    </author>
    <author>
      <name>Jamil Ahmad</name>
    </author>
    <author>
      <name>Mi Young Lee</name>
    </author>
    <author>
      <name>Sung Wook Baik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, conference paper, The 2nd International Integrated
  Conference &amp; Concert on Convergence (2016)</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.01382v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.01382v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.01797v1</id>
    <updated>2016-10-06T09:51:12Z</updated>
    <published>2016-10-06T09:51:12Z</published>
    <title>A Joint Detection-Classification Model for Audio Tagging of Weakly
  Labelled Data</title>
    <summary>  Audio tagging aims to assign one or several tags to an audio clip. Most of
the datasets are weakly labelled, which means only the tags of the clip are
known, without knowing the occurrence time of the tags. The labeling of an
audio clip is often based on the audio events in the clip and no event level
label is provided to the user. Previous works have used the bag of frames model
assume the tags occur all the time, which is not the case in practice. We
propose a joint detection-classification (JDC) model to detect and classify the
audio clip simultaneously. The JDC model has the ability to attend to
informative and ignore uninformative sounds. Then only informative regions are
used for classification. Experimental results on the "CHiME Home" dataset show
that the JDC model reduces the equal error rate (EER) from 19.0% to 16.9%. More
interestingly, the audio event detector is trained successfully without needing
the event level label.
</summary>
    <author>
      <name>Qiuqiang Kong</name>
    </author>
    <author>
      <name>Yong Xu</name>
    </author>
    <author>
      <name>Wenwu Wang</name>
    </author>
    <author>
      <name>Mark Plumbley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ICASSP 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.01797v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.01797v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.02392v1</id>
    <updated>2016-10-07T19:57:41Z</updated>
    <published>2016-10-07T19:57:41Z</published>
    <title>An Automatic System for Acoustic Microphone Geometry Calibration based
  on Minimal Solvers</title>
    <summary>  In this paper, robust detection, tracking and geometry estimation methods are
developed and combined into a system for estimating time-difference estimates,
microphone localization and sound source movement. No assumptions on the 3D
locations of the microphones and sound sources are made. The system is capable
of tracking continuously moving sound sources in an reverberant environment.
The multi-path components are explicitly tracked and used in the geometry
estimation parts. The system is based on matching between pairs of channels
using GCC-PHAT. Instead of taking a single maximum at each time instant from
each such pair, we select the four strongest local maxima. This produce a set
of hypothesis to work with in the subsequent steps, where consistency
constraints between the channels and time-continuity constraints are exploited.
In the paper it demonstrated how such detections can be used to estimate
microphone positions, sound source movement and room geometry. The methods are
tested and verified using real data from several reverberant environments. The
evaluation demonstrated accuracy in the order of few millimeters.
</summary>
    <author>
      <name>Simayijiang Zhayida</name>
    </author>
    <author>
      <name>Simon Segerblom Rex</name>
    </author>
    <author>
      <name>Yubin Kuang</name>
    </author>
    <author>
      <name>Fredrik Andersson</name>
    </author>
    <author>
      <name>Kalle Åström</name>
    </author>
    <link href="http://arxiv.org/abs/1610.02392v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.02392v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.02831v2</id>
    <updated>2016-10-11T05:10:07Z</updated>
    <published>2016-10-10T10:09:49Z</published>
    <title>Domain adaptation based Speaker Recognition on Short Utterances</title>
    <summary>  This paper explores how the in- and out-domain probabilistic linear
discriminant analysis (PLDA) speaker verification behave when enrolment and
verification lengths are reduced. Experiment studies have found that when
full-length utterance is used for evaluation, in-domain PLDA approach shows
more than 28% improvement in EER and DCF values over out-domain PLDA approach
and when short utterances are used for evaluation, the performance gain of
in-domain speaker verification reduces at an increasing rate. Novel modified
inter dataset variability (IDV) compensation is used to compensate the mismatch
between in- and out-domain data and IDV-compensated out-domain PLDA shows
respectively 26% and 14% improvement over out-domain PLDA speaker verification
when SWB and NIST data are respectively used for S normalization. When the
evaluation utterance length is reduced, the performance gain by IDV also
reduces as short utterance evaluation data i-vectors have more variations due
to phonetic variations when compared to the dataset mismatch between in- and
out-domain data.
</summary>
    <author>
      <name>Ahilan Kanagasundaram</name>
    </author>
    <author>
      <name>David Dean</name>
    </author>
    <author>
      <name>Sridha Sridharan</name>
    </author>
    <author>
      <name>Clinton Fookes</name>
    </author>
    <link href="http://arxiv.org/abs/1610.02831v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.02831v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.03009v1</id>
    <updated>2016-10-10T18:03:29Z</updated>
    <published>2016-10-10T18:03:29Z</published>
    <title>Investigation of Synthetic Speech Detection Using Frame- and
  Segment-Specific Importance Weighting</title>
    <summary>  Speaker verification systems are vulnerable to spoofing attacks which
presents a major problem in their real-life deployment. To date, most of the
proposed synthetic speech detectors (SSDs) have weighted the importance of
different segments of speech equally. However, different attack methods have
different strengths and weaknesses and the traces that they leave may be short
or long term acoustic artifacts. Moreover, those may occur for only particular
phonemes or sounds. Here, we propose three algorithms that weigh
likelihood-ratio scores of individual frames, phonemes, and sound-classes
depending on their importance for the SSD. Significant improvement over the
baseline system has been obtained for known attack methods that were used in
training the SSDs. However, improvement with unknown attack types was not
substantial. Thus, the type of distortions that were caused by the unknown
systems were different and could not be captured better with the proposed SSD
compared to the baseline SSD.
</summary>
    <author>
      <name>Ali Khodabakhsh</name>
    </author>
    <author>
      <name>Cenk Demiroglu</name>
    </author>
    <link href="http://arxiv.org/abs/1610.03009v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.03009v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.03190v1</id>
    <updated>2016-10-11T05:04:25Z</updated>
    <published>2016-10-11T05:04:25Z</published>
    <title>DNN based Speaker Recognition on Short Utterances</title>
    <summary>  This paper investigates the effects of limited speech data in the context of
speaker verification using deep neural network (DNN) approach. Being able to
reduce the length of required speech data is important to the development of
speaker verification system in real world applications. The experimental
studies have found that DNN-senone-based Gaussian probabilistic linear
discriminant analysis (GPLDA) system respectively achieves above 50% and 18%
improvements in EER values over GMM-UBM GPLDA system on NIST 2010
coreext-coreext and truncated 15sec-15sec evaluation conditions. Further when
GPLDA model is trained on short-length utterances (30sec) rather than
full-length utterances (2min), DNN-senone GPLDA system achieves above 7%
improvement in EER values on truncated 15sec-15sec condition. This is because
short length development i-vectors have speaker, session and phonetic variation
and GPLDA is able to robustly model those variations. For several real world
applications, longer utterances (2min) can be used for enrollment and shorter
utterances (15sec) are required for verification, and in those conditions,
DNN-senone GPLDA system achieves above 26% improvement in EER values over
GMM-UBM GPLDA systems.
</summary>
    <author>
      <name>Ahilan Kanagasundaram</name>
    </author>
    <author>
      <name>David Dean</name>
    </author>
    <author>
      <name>Sridha Sridharan</name>
    </author>
    <author>
      <name>Clinton Fookes</name>
    </author>
    <link href="http://arxiv.org/abs/1610.03190v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.03190v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.03772v1</id>
    <updated>2016-10-12T16:24:54Z</updated>
    <published>2016-10-12T16:24:54Z</published>
    <title>RAVEN X High Performance Data Mining Toolbox for Bioacoustic Data
  Analysis</title>
    <summary>  Objective of this work is to integrate high performance computing (HPC)
technologies and bioacoustics data-mining capabilities by offering a
MATLAB-based toolbox called Raven-X. Raven-X will provide a
hardware-independent solution, for processing large acoustic datasets - the
toolkit will be available to the community at no cost. This goal will be
achieved by leveraging prior work done which successfully deployed MATLAB based
HPC tools within Cornell University's Bioacoustics Research Program (BRP).
These tools enabled commonly available multi-core computers to process data at
accelerated rates to detect and classify whale sounds in large multi-channel
sound archives. Through this collaboration, we will expand on this effort which
was featured through Mathworks research and industry forums incorporate new
cutting-edge detectors and classifiers, and disseminate Raven-X to the broader
bioacoustics community.
</summary>
    <author>
      <name>Peter J. Dugan</name>
    </author>
    <author>
      <name>Holger Klinck</name>
    </author>
    <author>
      <name>Marie A. Roch</name>
    </author>
    <author>
      <name>Tyler A. Helble</name>
    </author>
    <link href="http://arxiv.org/abs/1610.03772v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.03772v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.04695v1</id>
    <updated>2016-10-15T06:49:15Z</updated>
    <published>2016-10-15T06:49:15Z</published>
    <title>Non-negative matrix factorization-based subband decomposition for
  acoustic source localization</title>
    <summary>  A novel non-negative matrix factorization (NMF) based subband decomposition
in frequency spatial domain for acoustic source localization using a microphone
array is introduced. The proposed method decomposes source and noise subband
and emphasises source dominant frequency bins for more accurate source
representation. By employing NMF, delay basis vectors and their subband
information in frequency spatial domain for each frame is extracted. The
proposed algorithm is evaluated in both simulated noise and real noise with a
speech corpus database. Experimental results clearly indicate that the
algorithm performs more accurately than other conventional algorithms under
both reverberant and noisy acoustic environments.
</summary>
    <author>
      <name>Suwon Shon</name>
    </author>
    <author>
      <name>Seongkyu Mun</name>
    </author>
    <author>
      <name>David Han</name>
    </author>
    <author>
      <name>Hanseok Ko</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1049/el.2015.2665</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1049/el.2015.2665" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on IET Electronics Letters: 31 July 2015; Published
  E-first: 9 October 2015</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Electronics Letters 51 (2015) 1723-1724</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1610.04695v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.04695v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.04770v1</id>
    <updated>2016-10-15T18:14:57Z</updated>
    <published>2016-10-15T18:14:57Z</published>
    <title>Semi-Supervised Source Localization on Multiple-Manifolds with
  Distributed Microphones</title>
    <summary>  The problem of source localization with ad hoc microphone networks in noisy
and reverberant enclosures, given a training set of prerecorded measurements,
is addressed in this paper. The training set is assumed to consist of a limited
number of labelled measurements, attached with corresponding positions, and a
larger amount of unlabelled measurements from unknown locations. However,
microphone calibration is not required. We use a Bayesian inference approach
for estimating a function that maps measurement-based feature vectors to the
corresponding positions. The central issue is how to combine the information
provided by the different microphones in a unified statistical framework. To
address this challenge, we model this function using a Gaussian process with a
covariance function that encapsulates both the connections between pairs of
microphones and the relations among the samples in the training set. The
parameters of the process are estimated by optimizing a maximum likelihood (ML)
criterion. In addition, a recursive adaptation mechanism is derived where the
new streaming measurements are used to update the model. Performance is
demonstrated for 2-D localization of both simulated data and real-life
recordings in a variety of reverberation and noise levels.
</summary>
    <author>
      <name>Bracha Laufer-Goldshtein</name>
    </author>
    <author>
      <name>Ronen Talmon</name>
    </author>
    <author>
      <name>Sharon Gannot</name>
    </author>
    <link href="http://arxiv.org/abs/1610.04770v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.04770v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.04965v1</id>
    <updated>2016-10-17T03:36:42Z</updated>
    <published>2016-10-17T03:36:42Z</published>
    <title>Improving Short Utterance PLDA Speaker Verification using SUV Modelling
  and Utterance Partitioning Approach</title>
    <summary>  This paper analyses the short utterance probabilistic linear discriminant
analysis (PLDA) speaker verification with utterance partitioning and short
utterance variance (SUV) modelling approaches. Experimental studies have found
that instead of using single long-utterance as enrolment data, if long enrolled
utterance is partitioned into multiple short utterances and average of short
utterance i-vectors is used as enrolled data, that improves the Gaussian PLDA
(GPLDA) speaker verification. This is because short utterance i-vectors have
speaker, session and utterance variations, and utterance-partitioning approach
compensates the utterance variation. Subsequently, SUV-PLDA is also studied
with utterance partitioning approach, and utterance partitioning-based
SUV-GPLDA system shows relative improvement of 9% and 16% in EER for NIST 2008
and NIST 2010 truncated 10sec-10sec evaluation condition as utterance
partitioning approach compensates the utterance variation and SUV modelling
approach compensates the mismatch between full-length development data and
short-length evaluation data.
</summary>
    <author>
      <name>Ahilan Kanagasundaram</name>
    </author>
    <author>
      <name>David Dean</name>
    </author>
    <author>
      <name>Sridha Sridharan</name>
    </author>
    <author>
      <name>Clinton Fookes</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1610.02831</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.04965v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.04965v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.05948v1</id>
    <updated>2016-10-19T10:16:46Z</updated>
    <published>2016-10-19T10:16:46Z</published>
    <title>A Bayesian Approach to Estimation of Speaker Normalization Parameters</title>
    <summary>  In this work, a Bayesian approach to speaker normalization is proposed to
compensate for the degradation in performance of a speaker independent speech
recognition system. The speaker normalization method proposed herein uses the
technique of vocal tract length normalization (VTLN). The VTLN parameters are
estimated using a novel Bayesian approach which utilizes the Gibbs sampler, a
special type of Markov Chain Monte Carlo method. Additionally the
hyperparameters are estimated using maximum likelihood approach. This model is
used assuming that human vocal tract can be modeled as a tube of uniform cross
section. It captures the variation in length of the vocal tract of different
speakers more effectively, than the linear model used in literature. The work
has also investigated different methods like minimization of Mean Square Error
(MSE) and Mean Absolute Error (MAE) for the estimation of VTLN parameters. Both
single pass and two pass approaches are then used to build a VTLN based speech
recognizer. Experimental results on recognition of vowels and Hindi phrases
from a medium vocabulary indicate that the Bayesian method improves the
performance by a considerable margin.
</summary>
    <author>
      <name>Dhananjay Ram</name>
    </author>
    <author>
      <name>Debasis Kundu</name>
    </author>
    <author>
      <name>Rajesh M. Hegde</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 Pages, 9 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.05948v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.05948v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.00326v3</id>
    <updated>2017-04-20T18:43:29Z</updated>
    <published>2016-11-01T18:38:12Z</published>
    <title>Enhanced Factored Three-Way Restricted Boltzmann Machines for Speech
  Detection</title>
    <summary>  In this letter, we propose enhanced factored three way restricted Boltzmann
machines (EFTW-RBMs) for speech detection. The proposed model incorporates
conditional feature learning by multiplying the dynamical state of the third
unit, which allows a modulation over the visible-hidden node pairs. Instead of
stacking previous frames of speech as the third unit in a recursive manner, the
correlation related weighting coefficients are assigned to the contextual
neighboring frames. Specifically, a threshold function is designed to capture
the long-term features and blend the globally stored speech structure. A
factored low rank approximation is introduced to reduce the parameters of the
three-dimensional interaction tensor, on which non-negative constraint is
imposed to address the sparsity characteristic. The validations through the
area-under-ROC-curve (AUC) and signal distortion ratio (SDR) show that our
approach outperforms several existing 1D and 2D (i.e., time and time-frequency
domain) speech detection algorithms in various noisy environments.
</summary>
    <author>
      <name>Pengfei Sun</name>
    </author>
    <author>
      <name>Jun Qin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, Pattern Recognition Letter 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.00326v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.00326v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.04947v1</id>
    <updated>2016-11-15T17:23:17Z</updated>
    <published>2016-11-15T17:23:17Z</published>
    <title>Detection of north atlantic right whale upcalls using local binary
  patterns in a two-stage strategy</title>
    <summary>  In this paper, we investigate the effectiveness of two-stage classification
strategies in detecting north Atlantic right whale upcalls. Time-frequency
measurements of data from passive acoustic monitoring devices are evaluated as
images. Vocalization spectrograms are preprocessed for noise reduction and tone
removal. First stage of the algorithm eliminates non-upcalls by an energy
detection algorithm. In the second stage, two sets of features are extracted
from the remaining signals using contour-based and texture based methods. The
former is based on extraction of time-frequency features from upcall contours,
and the latter employs a Local Binary Pattern operator to extract
distinguishing texture features of the upcalls. Subsequently evaluation phase
is carried out by using several classifiers to assess the effectiveness of both
the contour-based and texture-based features for upcall detection. Experimental
results with the data set provided by the Cornell University Bioacoustics
Research Program reveal that classifiers show accuracy improvements of 3% to 4%
when using LBP features over time-frequency features. Classifiers such as the
Linear Discriminant Analysis, Support Vector Machine, and TreeBagger achieve
high upcall detection rates with LBP features.
</summary>
    <author>
      <name>Mahdi Esfahanian</name>
    </author>
    <author>
      <name>Hanqi Zhuang</name>
    </author>
    <author>
      <name>Nurgun Erdol</name>
    </author>
    <author>
      <name>Edmund Gerstein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages, 11 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.04947v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.04947v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.07351v1</id>
    <updated>2016-11-22T15:18:31Z</updated>
    <published>2016-11-22T15:18:31Z</published>
    <title>MOMOS-MT: Mobile Monophonic System for Music Transcription</title>
    <summary>  Music holds a significant cultural role in social identity and in the
encouragement of socialization. Technology, by the destruction of physical and
cultural distance, has lead to many changes in musical themes and the complete
loss of forms. Yet, it also allows for the preservation and distribution of
music from societies without a history of written sheet music. This paper
presents early work on a tool for musicians and ethnomusicologists to
transcribe sheet music from monophonic voiced pieces for preservation and
distribution. Using FFT, the system detects the pitch frequencies, also other
methods detect note durations, tempo, time signatures and generates sheet
music. The final system is able to be used in mobile platforms allowing the
user to take recordings and produce sheet music in situ to a performance.
</summary>
    <author>
      <name>Munir Makhmutov</name>
    </author>
    <author>
      <name>Joseph Alexander Brown</name>
    </author>
    <author>
      <name>Manuel Mazzara</name>
    </author>
    <author>
      <name>Leonard Johard</name>
    </author>
    <link href="http://arxiv.org/abs/1611.07351v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.07351v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.08749v2</id>
    <updated>2017-01-22T22:28:47Z</updated>
    <published>2016-11-26T22:16:35Z</published>
    <title>Fast Chirplet Transform to Enhance CNN Machine Listening - Validation on
  Animal calls and Speech</title>
    <summary>  The scattering framework offers an optimal hierarchical convolutional
decomposition according to its kernels. Convolutional Neural Net (CNN) can be
seen as an optimal kernel decomposition, nevertheless it requires large amount
of training data to learn its kernels. We propose a trade-off between these two
approaches: a Chirplet kernel as an efficient Q constant bioacoustic
representation to pretrain CNN. First we motivate Chirplet bioinspired auditory
representation. Second we give the first algorithm (and code) of a Fast
Chirplet Transform (FCT). Third, we demonstrate the computation efficiency of
FCT on large environmental data base: months of Orca recordings, and 1000 Birds
species from the LifeClef challenge. Fourth, we validate FCT on the vowels
subset of the Speech TIMIT dataset. The results show that FCT accelerates CNN
when it pretrains low level layers: it reduces training duration by -28\% for
birds classification, and by -26% for vowels classification. Scores are also
enhanced by FCT pretraining, with a relative gain of +7.8% of Mean Average
Precision on birds, and +2.3\% of vowel accuracy against raw audio CNN. We
conclude on perspectives on tonotopic FCT deep machine listening, and
inter-species bioacoustic transfer learning to generalise the representation of
animal communication systems.
</summary>
    <author>
      <name>Herve Glotin</name>
    </author>
    <author>
      <name>Julien Ricard</name>
    </author>
    <author>
      <name>Randall Balestriero</name>
    </author>
    <link href="http://arxiv.org/abs/1611.08749v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.08749v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.08905v1</id>
    <updated>2016-11-27T20:29:53Z</updated>
    <published>2016-11-27T20:29:53Z</published>
    <title>SISO and SIMO Accompaniment Cancellation for Live Solo Recordings Based
  on Short-Time ERB-Band Wiener Filtering and Spectral Subtraction</title>
    <summary>  Research in collaborative music learning is subject to unresolved problems
demanding new technological solutions. One such problem poses the suppression
of the accompaniment in a live recording of a performance during practice,
which can be for the purposes of self-assessment or further machine-aided
analysis. Being able to separate a solo from the accompaniment allows to create
learning agents that may act as personal tutors and help the apprentice improve
his or her technique. First, we start from the classical adaptive noise
cancelling approach, and adjust it to the problem at hand. In a second step, we
compare some adaptive and Wiener filtering approaches and assess their
performances on the task. Our findings underpin that adaptive filtering is
inapt of dealing with music signals and that Wiener filtering in the short-time
Fourier transform domain is a much more effective approach. In addition, it is
very cheap if carried out in the frequency bands of auditory filters. A
double-output extension based on maximal-ratio combining is also proposed.
</summary>
    <author>
      <name>Stanislaw Gorlow</name>
    </author>
    <author>
      <name>Mathieu Ramona</name>
    </author>
    <author>
      <name>François Pachet</name>
    </author>
    <link href="http://arxiv.org/abs/1611.08905v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.08905v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.09482v1</id>
    <updated>2016-11-29T04:16:44Z</updated>
    <published>2016-11-29T04:16:44Z</published>
    <title>Fast Wavenet Generation Algorithm</title>
    <summary>  This paper presents an efficient implementation of the Wavenet generation
process called Fast Wavenet. Compared to a naive implementation that has
complexity O(2^L) (L denotes the number of layers in the network), our proposed
approach removes redundant convolution operations by caching previous
calculations, thereby reducing the complexity to O(L) time. Timing experiments
show significant advantages of our fast implementation over a naive one. While
this method is presented for Wavenet, the same scheme can be applied anytime
one wants to perform autoregressive generation or online prediction using a
model with dilated convolution layers. The code for our method is publicly
available.
</summary>
    <author>
      <name>Tom Le Paine</name>
    </author>
    <author>
      <name>Pooya Khorrami</name>
    </author>
    <author>
      <name>Shiyu Chang</name>
    </author>
    <author>
      <name>Yang Zhang</name>
    </author>
    <author>
      <name>Prajit Ramachandran</name>
    </author>
    <author>
      <name>Mark A. Hasegawa-Johnson</name>
    </author>
    <author>
      <name>Thomas S. Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical Report</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.09482v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.09482v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.09526v1</id>
    <updated>2016-11-29T08:46:26Z</updated>
    <published>2016-11-29T08:46:26Z</published>
    <title>Learning Filter Banks Using Deep Learning For Acoustic Signals</title>
    <summary>  Designing appropriate features for acoustic event recognition tasks is an
active field of research. Expressive features should both improve the
performance of the tasks and also be interpret-able. Currently, heuristically
designed features based on the domain knowledge requires tremendous effort in
hand-crafting, while features extracted through deep network are difficult for
human to interpret. In this work, we explore the experience guided learning
method for designing acoustic features. This is a novel hybrid approach
combining both domain knowledge and purely data driven feature designing. Based
on the procedure of log Mel-filter banks, we design a filter bank learning
layer. We concatenate this layer with a convolutional neural network (CNN)
model. After training the network, the weight of the filter bank learning layer
is extracted to facilitate the design of acoustic features. We smooth the
trained weight of the learning layer and re-initialize it in filter bank
learning layer as audio feature extractor. For the environmental sound
recognition task based on the Urban- sound8K dataset, the experience guided
learning leads to a 2% accuracy improvement compared with the fixed feature
extractors (the log Mel-filter bank). The shape of the new filter banks are
visualized and explained to prove the effectiveness of the feature design
process.
</summary>
    <author>
      <name>Shuhui Qu</name>
    </author>
    <author>
      <name>Juncheng Li</name>
    </author>
    <author>
      <name>Wei Dai</name>
    </author>
    <author>
      <name>Samarjit Das</name>
    </author>
    <link href="http://arxiv.org/abs/1611.09526v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.09526v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.00171v1</id>
    <updated>2016-12-01T08:25:45Z</updated>
    <published>2016-12-01T08:25:45Z</published>
    <title>A Non Linear Multifractal Study to Illustrate the Evolution of Tagore
  Songs Over a Century</title>
    <summary>  The works of Rabindranath Tagore have been sung by various artistes over
generations spanning over almost 100 years. there are few songs which were
popular in the early years and have been able to retain their popularity over
the years while some others have faded away. In this study we look to find cues
for the singing style of these songs which have kept them alive for all these
years. For this we took 3 min clip of four Tagore songs which have been sung by
five generation of artistes over 100 years and analyze them with the help of
latest nonlinear techniques Multifractal Detrended Fluctuation Analysis
(MFDFA). The multifractal spectral width is a manifestation of the inherent
complexity of the signal and may prove to be an important parameter to identify
the singing style of particular generation of singers and how this style varies
over different generations. The results are discussed in detail.
</summary>
    <author>
      <name>Shankha Sanyal</name>
    </author>
    <author>
      <name>Archi Banerjee</name>
    </author>
    <author>
      <name>Tarit Guhathakurata</name>
    </author>
    <author>
      <name>Ranjan Sengupta</name>
    </author>
    <author>
      <name>Dipak Ghosh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 PAGES, 5 FIGURES, Presented in International Symposium on Frontiers
  of Research in Speech and Music (FRSM)2016 held in North Orissa University,
  11-12 November 2016. arXiv admin note: text overlap with arXiv:1601.07709</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.00171v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.00171v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.01840v3</id>
    <updated>2017-09-05T18:38:33Z</updated>
    <published>2016-12-06T14:58:59Z</published>
    <title>FMA: A Dataset For Music Analysis</title>
    <summary>  We introduce the Free Music Archive (FMA), an open and easily accessible
dataset suitable for evaluating several tasks in MIR, a field concerned with
browsing, searching, and organizing large music collections. The community's
growing interest in feature and end-to-end learning is however restrained by
the limited availability of large audio datasets. The FMA aims to overcome this
hurdle by providing 917 GiB and 343 days of Creative Commons-licensed audio
from 106,574 tracks from 16,341 artists and 14,854 albums, arranged in a
hierarchical taxonomy of 161 genres. It provides full-length and high-quality
audio, pre-computed features, together with track- and user-level metadata,
tags, and free-form text such as biographies. We here describe the dataset and
how it was created, propose a train/validation/test split and three subsets,
discuss some suitable MIR tasks, and evaluate some baselines for genre
recognition. Code, data, and usage examples are available at
https://github.com/mdeff/fma
</summary>
    <author>
      <name>Michaël Defferrard</name>
    </author>
    <author>
      <name>Kirell Benzi</name>
    </author>
    <author>
      <name>Pierre Vandergheynst</name>
    </author>
    <author>
      <name>Xavier Bresson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ISMIR 2017 camera-ready</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.01840v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.01840v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.01943v1</id>
    <updated>2016-12-06T18:37:30Z</updated>
    <published>2016-12-06T18:37:30Z</published>
    <title>Segmental Convolutional Neural Networks for Detection of Cardiac
  Abnormality With Noisy Heart Sound Recordings</title>
    <summary>  Heart diseases constitute a global health burden, and the problem is
exacerbated by the error-prone nature of listening to and interpreting heart
sounds. This motivates the development of automated classification to screen
for abnormal heart sounds. Existing machine learning-based systems achieve
accurate classification of heart sound recordings but rely on expert features
that have not been thoroughly evaluated on noisy recordings. Here we propose a
segmental convolutional neural network architecture that achieves automatic
feature learning from noisy heart sound recordings. Our experiments show that
our best model, trained on noisy recording segments acquired with an existing
hidden semi-markov model-based approach, attains a classification accuracy of
87.5% on the 2016 PhysioNet/CinC Challenge dataset, compared to the 84.6%
accuracy of the state-of-the-art statistical classifier trained and evaluated
on the same dataset. Our results indicate the potential of using neural
network-based methods to increase the accuracy of automated classification of
heart sound recordings for improved screening of heart diseases.
</summary>
    <author>
      <name>Yuhao Zhang</name>
    </author>
    <author>
      <name>Sandeep Ayyar</name>
    </author>
    <author>
      <name>Long-Huei Chen</name>
    </author>
    <author>
      <name>Ethan J. Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This work was finished in May 2016, and remains unpublished until
  December 2016 due to a request from the data provider</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.01943v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.01943v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.02198v2</id>
    <updated>2016-12-13T11:14:30Z</updated>
    <published>2016-12-07T11:18:21Z</published>
    <title>Towards computer-assisted understanding of dynamics in symphonic music</title>
    <summary>  Many people enjoy classical symphonic music. Its diverse instrumentation
makes for a rich listening experience. This diversity adds to the conductor's
expressive freedom to shape the sound according to their imagination. As a
result, the same piece may sound quite differently from one conductor to
another. Differences in interpretation may be noticeable subjectively to
listeners, but they are sometimes hard to pinpoint, presumably because of the
acoustic complexity of the sound. We describe a computational model that
interprets dynamics---expressive loudness variations in performances---in terms
of the musical score, highlighting differences between performances of the same
piece. We demonstrate experimentally that the model has predictive power, and
give examples of conductor ideosyncrasies found by using the model as an
explanatory tool. Although the present model is still in active development, it
may pave the road for a consumer-oriented companion to interactive classical
music understanding.
</summary>
    <author>
      <name>Maarten Grachten</name>
    </author>
    <author>
      <name>Carlos Eduardo Cancino-Chacón</name>
    </author>
    <author>
      <name>Thassilo Gadermaier</name>
    </author>
    <author>
      <name>Gerhard Widmer</name>
    </author>
    <link href="http://arxiv.org/abs/1612.02198v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.02198v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.03789v1</id>
    <updated>2016-12-12T17:06:19Z</updated>
    <published>2016-12-12T17:06:19Z</published>
    <title>A Unit Selection Methodology for Music Generation Using Deep Neural
  Networks</title>
    <summary>  Several methods exist for a computer to generate music based on data
including Markov chains, recurrent neural networks, recombinancy, and grammars.
We explore the use of unit selection and concatenation as a means of generating
music using a procedure based on ranking, where, we consider a unit to be a
variable length number of measures of music. We first examine whether a unit
selection method, that is restricted to a finite size unit library, can be
sufficient for encompassing a wide spectrum of music. We do this by developing
a deep autoencoder that encodes a musical input and reconstructs the input by
selecting from the library. We then describe a generative model that combines a
deep structured semantic model (DSSM) with an LSTM to predict the next unit,
where units consist of four, two, and one measures of music. We evaluate the
generative model using objective metrics including mean rank and accuracy and
with a subjective listening test in which expert musicians are asked to
complete a forced-choiced ranking task. We compare our model to a note-level
generative baseline that consists of a stacked LSTM trained to predict forward
by one note.
</summary>
    <author>
      <name>Mason Bretan</name>
    </author>
    <author>
      <name>Gil Weinberg</name>
    </author>
    <author>
      <name>Larry Heck</name>
    </author>
    <link href="http://arxiv.org/abs/1612.03789v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.03789v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.04028v3</id>
    <updated>2017-04-29T20:31:18Z</updated>
    <published>2016-12-13T04:56:47Z</published>
    <title>Adaptive DCTNet for Audio Signal Classification</title>
    <summary>  In this paper, we investigate DCTNet for audio signal classification. Its
output feature is related to Cohen's class of time-frequency distributions. We
introduce the use of adaptive DCTNet (A-DCTNet) for audio signals feature
extraction. The A-DCTNet applies the idea of constant-Q transform, with its
center frequencies of filterbanks geometrically spaced. The A-DCTNet is
adaptive to different acoustic scales, and it can better capture low frequency
acoustic information that is sensitive to human audio perception than features
such as Mel-frequency spectral coefficients (MFSC). We use features extracted
by the A-DCTNet as input for classifiers. Experimental results show that the
A-DCTNet and Recurrent Neural Networks (RNN) achieve state-of-the-art
performance in bird song classification rate, and improve artist identification
accuracy in music data. They demonstrate A-DCTNet's applicability to signal
processing problems.
</summary>
    <author>
      <name>Yin Xian</name>
    </author>
    <author>
      <name>Yunchen Pu</name>
    </author>
    <author>
      <name>Zhe Gan</name>
    </author>
    <author>
      <name>Liang Lu</name>
    </author>
    <author>
      <name>Andrew Thompson</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1121/1.4970932</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1121/1.4970932" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference of Acoustic and Speech Signal Processing
  (ICASSP). New Orleans, United States, March, 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.04028v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.04028v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.04056v2</id>
    <updated>2017-01-19T15:33:53Z</updated>
    <published>2016-12-13T08:13:03Z</published>
    <title>Joint Bayesian Gaussian discriminant analysis for speaker verification</title>
    <summary>  State-of-the-art i-vector based speaker verification relies on variants of
Probabilistic Linear Discriminant Analysis (PLDA) for discriminant analysis. We
are mainly motivated by the recent work of the joint Bayesian (JB) method,
which is originally proposed for discriminant analysis in face verification. We
apply JB to speaker verification and make three contributions beyond the
original JB. 1) In contrast to the EM iterations with approximated statistics
in the original JB, the EM iterations with exact statistics are employed and
give better performance. 2) We propose to do simultaneous diagonalization (SD)
of the within-class and between-class covariance matrices to achieve efficient
testing, which has broader application scope than the SVD-based efficient
testing method in the original JB. 3) We scrutinize similarities and
differences between various Gaussian PLDAs and JB, complementing the previous
analysis of comparing JB only with Prince-Elder PLDA. Extensive experiments are
conducted on NIST SRE10 core condition 5, empirically validating the
superiority of JB with faster convergence rate and 9-13% EER reduction compared
with state-of-the-art PLDA.
</summary>
    <author>
      <name>Yiyan Wang</name>
    </author>
    <author>
      <name>Haotian Xu</name>
    </author>
    <author>
      <name>Zhijian Ou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted by ICASSP2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.04056v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.04056v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.04742v3</id>
    <updated>2017-08-17T15:21:05Z</updated>
    <published>2016-12-14T17:33:38Z</published>
    <title>Imposing higher-level Structure in Polyphonic Music Generation using
  Convolutional Restricted Boltzmann Machines and Constraints</title>
    <summary>  We introduce a method for imposing higher-level structure on generated,
polyphonic music. A Convolutional Restricted Boltzmann Machine (C-RBM) as a
generative model is combined with gradient descent constraint optimization to
provide further control over the generation process. Among other things, this
allows for the use of a "template" piece, from which some structural properties
can be extracted, and transferred as constraints to newly generated material.
The sampling process is guided with Simulated Annealing in order to avoid local
optima, and find solutions that both satisfy the constraints, and are
relatively stable with respect to the C-RBM. Results show that with this
approach it is possible to control the higher level self-similarity structure,
the meter, as well as tonal properties of the resulting musical piece while
preserving its local musical coherence.
</summary>
    <author>
      <name>Stefan Lattner</name>
    </author>
    <author>
      <name>Maarten Grachten</name>
    </author>
    <author>
      <name>Gerhard Widmer</name>
    </author>
    <link href="http://arxiv.org/abs/1612.04742v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.04742v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.04928v1</id>
    <updated>2016-12-15T05:06:40Z</updated>
    <published>2016-12-15T05:06:40Z</published>
    <title>Music Generation with Deep Learning</title>
    <summary>  The use of deep learning to solve problems in literary arts has been a recent
trend that has gained a lot of attention and automated generation of music has
been an active area. This project deals with the generation of music using raw
audio files in the frequency domain relying on various LSTM architectures.
Fully connected and convolutional layers are used along with LSTM's to capture
rich features in the frequency domain and increase the quality of music
generated. The work is focused on unconstrained music generation and uses no
information about musical structure(notes or chords) to aid learning.The music
generated from various architectures are compared using blind fold tests. Using
the raw audio to train models is the direction to tapping the enormous amount
of mp3 files that exist over the internet without requiring the manual effort
to make structured MIDI files. Moreover, not all audio files can be represented
with MIDI files making the study of these models an interesting prospect to the
future of such models.
</summary>
    <author>
      <name>Vasanth Kalingeri</name>
    </author>
    <author>
      <name>Srikanth Grandhe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.04928v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.04928v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.05065v1</id>
    <updated>2016-12-15T14:01:50Z</updated>
    <published>2016-12-15T14:01:50Z</published>
    <title>Feature Learning for Chord Recognition: The Deep Chroma Extractor</title>
    <summary>  We explore frame-level audio feature learning for chord recognition using
artificial neural networks. We present the argument that chroma vectors
potentially hold enough information to model harmonic content of audio for
chord recognition, but that standard chroma extractors compute too noisy
features. This leads us to propose a learned chroma feature extractor based on
artificial neural networks. It is trained to compute chroma features that
encode harmonic information important for chord recognition, while being robust
to irrelevant interferences. We achieve this by feeding the network an audio
spectrum with context instead of a single frame as input. This way, the network
can learn to selectively compensate noise and resolve harmonic ambiguities.
  We compare the resulting features to hand-crafted ones by using a simple
linear frame-wise classifier for chord recognition on various data sets. The
results show that the learned feature extractor produces superior chroma
vectors for chord recognition.
</summary>
    <author>
      <name>Filip Korzeniowski</name>
    </author>
    <author>
      <name>Gerhard Widmer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the 17th International Society for Music
  Information Retrieval Conference (ISMIR), New York, USA, 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.05065v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.05065v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.05153v1</id>
    <updated>2016-12-15T17:32:11Z</updated>
    <published>2016-12-15T17:32:11Z</published>
    <title>On the Potential of Simple Framewise Approaches to Piano Transcription</title>
    <summary>  In an attempt at exploring the limitations of simple approaches to the task
of piano transcription (as usually defined in MIR), we conduct an in-depth
analysis of neural network-based framewise transcription. We systematically
compare different popular input representations for transcription systems to
determine the ones most suitable for use with neural networks. Exploiting
recent advances in training techniques and new regularizers, and taking into
account hyper-parameter tuning, we show that it is possible, by simple
bottom-up frame-wise processing, to obtain a piano transcriber that outperforms
the current published state of the art on the publicly available MAPS dataset
-- without any complex post-processing steps. Thus, we propose this simple
approach as a new baseline for this dataset, for future transcription research
to build on and improve.
</summary>
    <author>
      <name>Rainer Kelz</name>
    </author>
    <author>
      <name>Matthias Dorfer</name>
    </author>
    <author>
      <name>Filip Korzeniowski</name>
    </author>
    <author>
      <name>Sebastian Böck</name>
    </author>
    <author>
      <name>Andreas Arzt</name>
    </author>
    <author>
      <name>Gerhard Widmer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 17th International Society for Music Information
  Retrieval Conference (ISMIR 2016), New York, NY</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.05153v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.05153v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.05156v2</id>
    <updated>2017-09-06T11:35:59Z</updated>
    <published>2016-12-15T19:43:54Z</published>
    <title>A Phase Vocoder based on Nonstationary Gabor Frames</title>
    <summary>  We propose a new algorithm for time stretching music signals based on the
theory of nonstationary Gabor frames (NSGFs). The algorithm extends the
techniques of the classical phase vocoder (PV) by incorporating adaptive
time-frequency (TF) representations and adaptive phase locking. The adaptive TF
representations imply good time resolution for the onsets of attack transients
and good frequency resolution for the sinusoidal components. We estimate the
phase values only at peak channels and the remaining phases are then locked to
the values of the peaks in an adaptive manner. During attack transients we keep
the stretch factor equal to one and we propose a new strategy for determining
which channels are relevant for reinitializing the corresponding phase values.
In contrast to previously published algorithms we use a non-uniform NSGF to
obtain a low redundancy of the corresponding TF representation. We show that
with just three times as many TF coefficients as signal samples, artifacts such
as phasiness and transient smearing can be greatly reduced compared to the
classical PV. The proposed algorithm is tested on both synthetic and real world
signals and compared with state of the art algorithms in a reproducible manner.
</summary>
    <author>
      <name>Emil Solsbæk Ottosen</name>
    </author>
    <author>
      <name>Monika Dörfler</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TASLP.2017.2750767</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TASLP.2017.2750767" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.05156v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.05156v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.05369v2</id>
    <updated>2017-03-31T20:17:12Z</updated>
    <published>2016-12-16T05:09:14Z</published>
    <title>Neural networks based EEG-Speech Models</title>
    <summary>  In this paper, we propose an end-to-end neural network (NN) based EEG-speech
(NES) modeling framework, in which three network structures are developed to
map imagined EEG signals to phonemes. The proposed NES models incorporate a
language model based EEG feature extraction layer, an acoustic feature mapping
layer, and a restricted Boltzmann machine (RBM) based the feature learning
layer. The NES models can jointly realize the representation of multichannel
EEG signals and the projection of acoustic speech signals. Among three proposed
NES models, two augmented networks utilize spoken EEG signals as either bias or
gate information to strengthen the feature learning and translation of imagined
EEG signals. Experimental results show that all three proposed NES models
outperform the baseline support vector machine (SVM) method on EEG-speech
classification. With respect to binary classification, our approach achieves
comparable results relative to deep believe network approach.
</summary>
    <author>
      <name>Pengfei Sun</name>
    </author>
    <author>
      <name>Jun Qin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.05369v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.05369v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.05432v1</id>
    <updated>2016-12-16T11:05:52Z</updated>
    <published>2016-12-16T11:05:52Z</published>
    <title>Basis-Function Modeling of Loudness Variations in Ensemble Performance</title>
    <summary>  This paper describes a computational model of loudness variations in
expressive ensemble performance. The model predicts and explains the continuous
variation of loudness as a function of information extracted automatically from
the written score. Although such models have been proposed for expressive
performance in solo instruments, this is (to the best of our knowledge) the
first attempt to define a model for expressive performance in ensembles. To
that end, we extend an existing model that was designed to model expressive
piano performances, and describe the additional steps necessary for the model
to deal with scores of arbitrary instrumentation, including orchestral scores.
We test both linear and non-linear variants of the extended model n a data set
of audio recordings of symphonic music, in a leave-one-out setting. The
experiments reveal that the most successful model variant is a recurrent,
non-linear model. Even if the accuracy of the predicted loudness varies from
one recording to another, in several cases the model explains well over 50% of
the variance in loudness.
</summary>
    <author>
      <name>Thassilo Gadermaier</name>
    </author>
    <author>
      <name>Maarten Grachten</name>
    </author>
    <author>
      <name>Carlos Eduardo Cancino Chacón</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 3 figures, 2 tables. Originally in 2nd International
  Conference on New Music Concepts (ICNMC 2016), Treviso, Italy. This version
  may have a different layout</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.05432v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.05432v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.05489v1</id>
    <updated>2016-12-16T14:40:43Z</updated>
    <published>2016-12-16T14:40:43Z</published>
    <title>On-bird Sound Recordings: Automatic Acoustic Recognition of Activities
  and Contexts</title>
    <summary>  We introduce a novel approach to studying animal behaviour and the context in
which it occurs, through the use of microphone backpacks carried on the backs
of individual free-flying birds. These sensors are increasingly used by animal
behaviour researchers to study individual vocalisations of freely behaving
animals, even in the field. However such devices may record more than an
animals vocal behaviour, and have the potential to be used for investigating
specific activities (movement) and context (background) within which
vocalisations occur. To facilitate this approach, we investigate the automatic
annotation of such recordings through two different sound scene analysis
paradigms: a scene-classification method using feature learning, and an
event-detection method using probabilistic latent component analysis (PLCA). We
analyse recordings made with Eurasian jackdaws (Corvus monedula) in both
captive and field settings. Results are comparable with the state of the art in
sound scene analysis; we find that the current recognition quality level
enables scalable automatic annotation of audio logger data, given partial
annotation, but also find that individual differences between animals and/or
their backpacks limit the generalisation from one individual to another. we
consider the interrelation of 'scenes' and 'events' in this particular task,
and issues of temporal resolution.
</summary>
    <author>
      <name>Dan Stowell</name>
    </author>
    <author>
      <name>Emmanouil Benetos</name>
    </author>
    <author>
      <name>Lisa F. Gill</name>
    </author>
    <link href="http://arxiv.org/abs/1612.05489v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.05489v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.06151v3</id>
    <updated>2017-03-09T08:47:38Z</updated>
    <published>2016-12-19T12:19:25Z</published>
    <title>HRTF-based two-dimensional robust least-squares frequency-invariant
  beamformer design for robot audition</title>
    <summary>  In this work, we propose a two-dimensional Head-Related Transfer Function
(HRTF)-based robust beamformer design for robot audition, which allows for
explicit control of the beamformer response for the entire three-dimensional
sound field surrounding a humanoid robot. We evaluate the proposed method by
means of both signal-independent and signal-dependent measures in a robot
audition scenario. Our results confirm the effectiveness of the proposed
two-dimensional HRTF-based beamformer design, compared to our previously
published one-dimensional HRTF-based beamformer design, which was carried out
for a fixed elevation angle only.
</summary>
    <author>
      <name>Hendrik Barfuss</name>
    </author>
    <author>
      <name>Michael Buerger</name>
    </author>
    <author>
      <name>Jasper Podschus</name>
    </author>
    <author>
      <name>Walter Kellermann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Joint Workshop on Hands-free Speech Communication and Microphone
  Arrays (HSCMA), March 2017, San Francisco, CA, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.06151v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.06151v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.06287v1</id>
    <updated>2016-12-14T15:40:44Z</updated>
    <published>2016-12-14T15:40:44Z</published>
    <title>VAST : The Virtual Acoustic Space Traveler Dataset</title>
    <summary>  This paper introduces a new paradigm for sound source lo-calization referred
to as virtual acoustic space traveling (VAST) and presents a first dataset
designed for this purpose. Existing sound source localization methods are
either based on an approximate physical model (physics-driven) or on a
specific-purpose calibration set (data-driven). With VAST, the idea is to learn
a mapping from audio features to desired audio properties using a massive
dataset of simulated room impulse responses. This virtual dataset is designed
to be maximally representative of the potential audio scenes that the
considered system may be evolving in, while remaining reasonably compact. We
show that virtually-learned mappings on this dataset generalize to real data,
overcoming some intrinsic limitations of traditional binaural sound
localization methods based on time differences of arrival.
</summary>
    <author>
      <name>Clément Gaultier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PANAMA</arxiv:affiliation>
    </author>
    <author>
      <name>Saurabh Kataria</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PANAMA, IIT Kanpur</arxiv:affiliation>
    </author>
    <author>
      <name>Antoine Deleforge</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PANAMA</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Latent Variable Analysis and Signal
  Separation (LVA/ICA), Feb 2017, Grenoble, France. International Conference on
  Latent Variable Analysis and Signal Separation</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.06287v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.06287v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.06642v1</id>
    <updated>2016-12-20T13:04:33Z</updated>
    <published>2016-12-20T13:04:33Z</published>
    <title>Efficient Target Activity Detection based on Recurrent Neural Networks</title>
    <summary>  This paper addresses the problem of Target Activity Detection (TAD) for
binaural listening devices. TAD denotes the problem of robustly detecting the
activity of a target speaker in a harsh acoustic environment, which comprises
interfering speakers and noise (cocktail party scenario). In previous work, it
has been shown that employing a Feed-forward Neural Network (FNN) for detecting
the target speaker activity is a promising approach to combine the advantage of
different TAD features (used as network inputs). In this contribution, we
exploit a larger context window for TAD and compare the performance of FNNs and
Recurrent Neural Networks (RNNs) with an explicit focus on small network
topologies as desirable for embedded acoustic signal processing systems. More
specifically, the investigations include a comparison between three different
types of RNNs, namely plain RNNs, Long Short-Term Memories, and Gated Recurrent
Units. The results indicate that all versions of RNNs outperform FNNs for the
task of TAD.
</summary>
    <author>
      <name>Daniel Gerber</name>
    </author>
    <author>
      <name>Stefan Meier</name>
    </author>
    <author>
      <name>Walter Kellermann</name>
    </author>
    <link href="http://arxiv.org/abs/1612.06642v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.06642v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.07523v1</id>
    <updated>2016-12-22T10:14:59Z</updated>
    <published>2016-12-22T10:14:59Z</published>
    <title>Robustness of Voice Conversion Techniques Under Mismatched Conditions</title>
    <summary>  Most of the existing studies on voice conversion (VC) are conducted in
acoustically matched conditions between source and target signal. However, the
robustness of VC methods in presence of mismatch remains unknown. In this
paper, we report a comparative analysis of different VC techniques under
mismatched conditions. The extensive experiments with five different VC
techniques on CMU ARCTIC corpus suggest that performance of VC methods
substantially degrades in noisy conditions. We have found that bilinear
frequency warping with amplitude scaling (BLFWAS) outperforms other methods in
most of the noisy conditions. We further explore the suitability of different
speech enhancement techniques for robust conversion. The objective evaluation
results indicate that spectral subtraction and log minimum mean square error
(logMMSE) based speech enhancement techniques can be used to improve the
performance in specific noisy conditions.
</summary>
    <author>
      <name>Monisankha Pal</name>
    </author>
    <author>
      <name>Dipjyoti Paul</name>
    </author>
    <author>
      <name>Md Sahidullah</name>
    </author>
    <author>
      <name>Goutam Saha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.07523v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.07523v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.07837v2</id>
    <updated>2017-02-11T20:04:46Z</updated>
    <published>2016-12-22T23:28:47Z</published>
    <title>SampleRNN: An Unconditional End-to-End Neural Audio Generation Model</title>
    <summary>  In this paper we propose a novel model for unconditional audio generation
based on generating one audio sample at a time. We show that our model, which
profits from combining memory-less modules, namely autoregressive multilayer
perceptrons, and stateful recurrent neural networks in a hierarchical structure
is able to capture underlying sources of variations in the temporal sequences
over very long time spans, on three datasets of different nature. Human
evaluation on the generated samples indicate that our model is preferred over
competing models. We also show how each component of the model contributes to
the exhibited performance.
</summary>
    <author>
      <name>Soroush Mehri</name>
    </author>
    <author>
      <name>Kundan Kumar</name>
    </author>
    <author>
      <name>Ishaan Gulrajani</name>
    </author>
    <author>
      <name>Rithesh Kumar</name>
    </author>
    <author>
      <name>Shubham Jain</name>
    </author>
    <author>
      <name>Jose Sotelo</name>
    </author>
    <author>
      <name>Aaron Courville</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as a conference paper at ICLR 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.07837v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.07837v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.09089v3</id>
    <updated>2017-06-05T13:32:12Z</updated>
    <published>2016-12-29T10:24:57Z</published>
    <title>What Makes Audio Event Detection Harder than Classification?</title>
    <summary>  There is a common observation that audio event classification is easier to
deal with than detection. So far, this observation has been accepted as a fact
and we lack of a careful analysis. In this paper, we reason the rationale
behind this fact and, more importantly, leverage them to benefit the audio
event detection task. We present an improved detection pipeline in which a
verification step is appended to augment a detection system. This step employs
a high-quality event classifier to postprocess the benign event hypotheses
outputted by the detection system and reject false alarms. To demonstrate the
effectiveness of the proposed pipeline, we implement and pair up different
event detectors based on the most common detection schemes and various event
classifiers, ranging from the standard bag-of-words model to the
state-of-the-art bank-of-regressors one. Experimental results on the ITC-Irst
dataset show significant improvements to detection performance. More
importantly, these improvements are consistent for all detector-classifier
combinations.
</summary>
    <author>
      <name>Huy Phan</name>
    </author>
    <author>
      <name>Philipp Koch</name>
    </author>
    <author>
      <name>Fabrice Katzberg</name>
    </author>
    <author>
      <name>Marco Maass</name>
    </author>
    <author>
      <name>Radoslaw Mazur</name>
    </author>
    <author>
      <name>Ian McLoughlin</name>
    </author>
    <author>
      <name>Alfred Mertins</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for EUSIPCO 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.09089v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.09089v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.09150v2</id>
    <updated>2016-12-30T08:46:05Z</updated>
    <published>2016-12-29T14:05:56Z</published>
    <title>Phase-incorporating Speech Enhancement Based on Complex-valued Gaussian
  Process Latent Variable Model</title>
    <summary>  Traditional speech enhancement techniques modify the magnitude of a speech in
time-frequency domain, and use the phase of a noisy speech to resynthesize a
time domain speech. This work proposes a complex-valued Gaussian process latent
variable model (CGPLVM) to enhance directly the complex-valued noisy spectrum,
modifying not only the magnitude but also the phase. The main idea that
underlies the developed method is the modeling of short-time Fourier transform
(STFT) coefficients across the time frames of a speech as a proper complex
Gaussian process (GP) with noise added. The proposed method is based on
projecting the spectrum into a low-dimensional subspace. The likelihood
criterion is used to optimize the hyperparameters of the model. Experiments
were carried out on the CHTTL database, which contains the digits zero to nine
in Mandarin. Several standard measures are used to demonstrate that the
proposed method outperforms baseline methods.
</summary>
    <author>
      <name>Sih-Huei Chen</name>
    </author>
    <author>
      <name>Yuan-Shan Lee</name>
    </author>
    <author>
      <name>Jia-Ching Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">add author name</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.09150v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.09150v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.06078v2</id>
    <updated>2017-01-24T16:25:15Z</updated>
    <published>2017-01-21T20:15:08Z</published>
    <title>Lyrics-to-Audio Alignment by Unsupervised Discovery of Repetitive
  Patterns in Vowel Acoustics</title>
    <summary>  Most of the previous approaches to lyrics-to-audio alignment used a
pre-developed automatic speech recognition (ASR) system that innately suffered
from several difficulties to adapt the speech model to individual singers. A
significant aspect missing in previous works is the self-learnability of
repetitive vowel patterns in the singing voice, where the vowel part used is
more consistent than the consonant part. Based on this, our system first learns
a discriminative subspace of vowel sequences, based on weighted symmetric
non-negative matrix factorization (WS-NMF), by taking the self-similarity of a
standard acoustic feature as an input. Then, we make use of canonical time
warping (CTW), derived from a recent computer vision technique, to find an
optimal spatiotemporal transformation between the text and the acoustic
sequences. Experiments with Korean and English data sets showed that deploying
this method after a pre-developed, unsupervised, singing source separation
achieved more promising results than other state-of-the-art unsupervised
approaches and an existing ASR-based system.
</summary>
    <author>
      <name>Sungkyun Chang</name>
    </author>
    <author>
      <name>Kyogu Lee</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ACCESS.2017.2738558</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ACCESS.2017.2738558" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.06078v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.06078v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.00025v1</id>
    <updated>2017-01-31T19:21:41Z</updated>
    <published>2017-01-31T19:21:41Z</published>
    <title>An Experimental Analysis of the Entanglement Problem in
  Neural-Network-based Music Transcription Systems</title>
    <summary>  Several recent polyphonic music transcription systems have utilized deep
neural networks to achieve state of the art results on various benchmark
datasets, pushing the envelope on framewise and note-level performance
measures. Unfortunately we can observe a sort of glass ceiling effect. To
investigate this effect, we provide a detailed analysis of the particular kinds
of errors that state of the art deep neural transcription systems make, when
trained and tested on a piano transcription task. We are ultimately forced to
draw a rather disheartening conclusion: the networks seem to learn combinations
of notes, and have a hard time generalizing to unseen combinations of notes.
Furthermore, we speculate on various means to alleviate this situation.
</summary>
    <author>
      <name>Rainer Kelz</name>
    </author>
    <author>
      <name>Gerhard Widmer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to AES Conference on Semantic Audio, Erlangen, Germany,
  2017 June 22, 24</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.00025v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.00025v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.00178v2</id>
    <updated>2017-03-31T11:24:42Z</updated>
    <published>2017-02-01T09:44:44Z</published>
    <title>On the Futility of Learning Complex Frame-Level Language Models for
  Chord Recognition</title>
    <summary>  Chord recognition systems use temporal models to post-process frame-wise
chord preditions from acoustic models. Traditionally, first-order models such
as Hidden Markov Models were used for this task, with recent works suggesting
to apply Recurrent Neural Networks instead. Due to their ability to learn
longer-term dependencies, these models are supposed to learn and to apply
musical knowledge, instead of just smoothing the output of the acoustic model.
In this paper, we argue that learning complex temporal models at the level of
audio frames is futile on principle, and that non-Markovian models do not
perform better than their first-order counterparts. We support our argument
through three experiments on the McGill Billboard dataset. The first two show
1) that when learning complex temporal models at the frame level, improvements
in chord sequence modelling are marginal; and 2) that these improvements do not
translate when applied within a full chord recognition system. The third, still
rather preliminary experiment gives first indications that the use of complex
sequential models for chord prediction at higher temporal levels might be more
promising.
</summary>
    <author>
      <name>Filip Korzeniowski</name>
    </author>
    <author>
      <name>Gerhard Widmer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.17743/aesconf.2017.978-1-942220-15-2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.17743/aesconf.2017.978-1-942220-15-2" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at AES Conference on Semantic Audio 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.00178v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.00178v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.00956v2</id>
    <updated>2017-02-06T03:37:28Z</updated>
    <published>2017-02-03T10:15:29Z</published>
    <title>KU-ISPL Speaker Recognition Systems under Language mismatch condition
  for NIST 2016 Speaker Recognition Evaluation</title>
    <summary>  Korea University Intelligent Signal Processing Lab. (KU-ISPL) developed
speaker recognition system for SRE16 fixed training condition. Data for
evaluation trials are collected from outside North America, spoken in Tagalog
and Cantonese while training data only is spoken English. Thus, main issue for
SRE16 is compensating the discrepancy between different languages. As
development dataset which is spoken in Cebuano and Mandarin, we could prepare
the evaluation trials through preliminary experiments to compensate the
language mismatched condition. Our team developed 4 different approaches to
extract i-vectors and applied state-of-the-art techniques as backend. To
compensate language mismatch, we investigated and endeavored unique method such
as unsupervised language clustering, inter language variability compensation
and gender/language dependent score normalization.
</summary>
    <author>
      <name>Suwon Shon</name>
    </author>
    <author>
      <name>Hanseok Ko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SRE16, NIST SRE 2016 system description</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.00956v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.00956v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.01999v1</id>
    <updated>2017-02-07T13:19:08Z</updated>
    <published>2017-02-07T13:19:08Z</published>
    <title>Identification of Voice Utterance with Aging Factor Using the Method of
  MFCC Multichannel</title>
    <summary>  This research was conducted to develop a method to identify voice utterance.
For voice utterance that encounters change caused by aging factor, with the
interval of 10 to 25 years. The change of voice utterance influenced by aging
factor might be extracted by MFCC (Mel Frequency Cepstrum Coefficient).
However, the level of the compatibility of the feature may be dropped down to
55%. While the ones which do not encounter it may reach 95%. To improve the
compatibility of the changing voice feature influenced by aging factor, then
the method of the more specific feature extraction is developed: which is by
separating the voice into several channels, suggested as MFCC multichannel,
consisting of multichannel 5 filterbank (M5FB), multichannel 2 filterbank
(M2FB) and multichannel 1 filterbank (M1FB). The result of the test shows that
for model M5FB and M2FB have the highest score in the level of compatibility
with 85% and 82% with 25 years interval. While model M5FB gets the highest
score of 86% for 10 years time interval.
</summary>
    <author>
      <name>Roy Rudolf Huizen</name>
    </author>
    <author>
      <name>Jazi Eko Istiyanto</name>
    </author>
    <author>
      <name>Agfianto Eko Putra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 Pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Huizen, R.R., Istiyanto, J.E. and Putra, A.E., 2017, International
  Journal of Advanced Studies in Computer Science and Engineering (IJASCSE),
  Volume 6 Issue 01</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1702.01999v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.01999v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.02130v2</id>
    <updated>2017-04-11T12:23:37Z</updated>
    <published>2017-02-07T18:41:31Z</published>
    <title>On the Importance of Temporal Context in Proximity Kernels: A Vocal
  Separation Case Study</title>
    <summary>  Musical source separation methods exploit source-specific spectral
characteristics to facilitate the decomposition process. Kernel Additive
Modelling (KAM) models a source applying robust statistics to time-frequency
bins as specified by a source-specific kernel, a function defining similarity
between bins. Kernels in existing approaches are typically defined using
metrics between single time frames. In the presence of noise and other sound
sources information from a single-frame, however, turns out to be unreliable
and often incorrect frames are selected as similar. In this paper, we
incorporate a temporal context into the kernel to provide additional
information stabilizing the similarity search. Evaluated in the context of
vocal separation, our simple extension led to a considerable improvement in
separation quality compared to previous kernels.
</summary>
    <author>
      <name>Delia Fano Yela</name>
    </author>
    <author>
      <name>Sebastian Ewert</name>
    </author>
    <author>
      <name>Derry FitzGerald</name>
    </author>
    <author>
      <name>Mark Sandler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2017 AES International Conference on Semantic Audio</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.02130v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.02130v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.03791v1</id>
    <updated>2017-02-13T14:44:17Z</updated>
    <published>2017-02-13T14:44:17Z</published>
    <title>DNN Filter Bank Cepstral Coefficients for Spoofing Detection</title>
    <summary>  With the development of speech synthesis techniques, automatic speaker
verification systems face the serious challenge of spoofing attack. In order to
improve the reliability of speaker verification systems, we develop a new
filter bank based cepstral feature, deep neural network filter bank cepstral
coefficients (DNN-FBCC), to distinguish between natural and spoofed speech. The
deep neural network filter bank is automatically generated by training a filter
bank neural network (FBNN) using natural and synthetic speech. By adding
restrictions on the training rules, the learned weight matrix of FBNN is
band-limited and sorted by frequency, similar to the normal filter bank. Unlike
the manually designed filter bank, the learned filter bank has different filter
shapes in different channels, which can capture the differences between natural
and synthetic speech more effectively. The experimental results on the ASVspoof
{2015} database show that the Gaussian mixture model maximum-likelihood
(GMM-ML) classifier trained by the new feature performs better than the
state-of-the-art linear frequency cepstral coefficients (LFCC) based
classifier, especially on detecting unknown attacks.
</summary>
    <author>
      <name>Hong Yu</name>
    </author>
    <author>
      <name>Zheng-Hua Tan</name>
    </author>
    <author>
      <name>Zhanyu Ma</name>
    </author>
    <author>
      <name>Jun Guo</name>
    </author>
    <link href="http://arxiv.org/abs/1702.03791v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.03791v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.06724v4</id>
    <updated>2017-06-08T06:48:33Z</updated>
    <published>2017-02-22T09:36:48Z</published>
    <title>A new cosine series antialiasing function and its application to
  aliasing-free glottal source models for speech and singing synthesis</title>
    <summary>  We formulated and implemented a procedure to generate aliasing-free
excitation source signals. It uses a new antialiasing filter in the continuous
time domain followed by an IIR digital filter for response equalization. We
introduced a cosine-series-based general design procedure for the new
antialiasing function. We applied this new procedure to implement the
antialiased Fujisaki-Ljungqvist model. We also applied it to revise our
previous implementation of the antialiased Fant-Liljencrants model. A
combination of these signals and a lattice implementation of the time varying
vocal tract model provides a reliable and flexible basis to test fo extractors
and source aperiodicity analysis methods. MATLAB implementations of these
antialiased excitation source models are available as part of our open source
tools for speech science.
</summary>
    <author>
      <name>Hideki Kawahara</name>
    </author>
    <author>
      <name>Ken-Ichi Sakakibara</name>
    </author>
    <author>
      <name>Hideki Banno</name>
    </author>
    <author>
      <name>Masanori Morise</name>
    </author>
    <author>
      <name>Tomoki Toda</name>
    </author>
    <author>
      <name>Toshio Irino</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Interspeech 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.06724v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.06724v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.00384v1</id>
    <updated>2017-02-28T11:00:19Z</updated>
    <published>2017-02-28T11:00:19Z</published>
    <title>Nonlinear Volterra model of a loudspeaker behavior based on Laser
  Doppler Vibrometry</title>
    <summary>  We demonstrate the capabilities of nonlinear Volterra models to simulate the
behavior of an audio system and compare them to linear filters. In this paper a
nonlinear model of an audio system based on Volterra series is presented and
Normalized Least Mean Square algorithm is used to determine the Volterra series
to third order. Training data for the models were collected measuring a
physical speaker using a laser interferometer. We explore several training
signals and filter's parameters. Results indicate a decrease in Mean Squared
Error compared to the linear model with a dependency on the particular test
signal, the order and the parameters of the model.
</summary>
    <author>
      <name>Alessandro Loriga</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Intranet Standard GmbH, Munich, Germany</arxiv:affiliation>
    </author>
    <author>
      <name>Parvin Moyassari</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Intranet Standard GmbH, Munich, Germany</arxiv:affiliation>
    </author>
    <author>
      <name>Daniele Bernardini</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Intranet Standard GmbH, Munich, Germany</arxiv:affiliation>
    </author>
    <author>
      <name>Gregorio Landi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Dipartimento di Fisica, Universita di Firenze, Italy</arxiv:affiliation>
    </author>
    <author>
      <name>Francesca Venturini</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Institute of Applied Mathematics and Physics, Zurich University of Applied Sciences, Winterthur, Switzerland</arxiv:affiliation>
    </author>
    <author>
      <name>Elisabeth Dumont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Institute of Applied Mathematics and Physics, Zurich University of Applied Sciences, Winterthur, Switzerland</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1703.00384v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.00384v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5; I.2.6; I.6.3; J.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.01789v2</id>
    <updated>2017-05-22T04:46:36Z</updated>
    <published>2017-03-06T09:49:48Z</published>
    <title>Sample-level Deep Convolutional Neural Networks for Music Auto-tagging
  Using Raw Waveforms</title>
    <summary>  Recently, the end-to-end approach that learns hierarchical representations
from raw data using deep convolutional neural networks has been successfully
explored in the image, text and speech domains. This approach was applied to
musical signals as well but has been not fully explored yet. To this end, we
propose sample-level deep convolutional neural networks which learn
representations from very small grains of waveforms (e.g. 2 or 3 samples)
beyond typical frame-level input representations. Our experiments show how deep
architectures with sample-level filters improve the accuracy in music
auto-tagging and they provide results comparable to previous state-of-the-art
performances for the Magnatagatune dataset and Million Song Dataset. In
addition, we visualize filters learned in a sample-level DCNN in each layer to
identify hierarchically learned features and show that they are sensitive to
log-scaled frequency along layer, such as mel-frequency spectrogram that is
widely used in music classification systems.
</summary>
    <author>
      <name>Jongpil Lee</name>
    </author>
    <author>
      <name>Jiyoung Park</name>
    </author>
    <author>
      <name>Keunhyoung Luke Kim</name>
    </author>
    <author>
      <name>Juhan Nam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, Sound and Music Computing Conference (SMC), 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.01789v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.01789v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.02318v1</id>
    <updated>2017-03-07T10:36:50Z</updated>
    <published>2017-03-07T10:36:50Z</published>
    <title>Linear and Circular Microphone Array for Remote Surveillance: Simulated
  Performance Analysis</title>
    <summary>  Acoustic beamforming with a microphone array represents an adequate
technology for remote acoustic surveillance, as the system has no mechanical
parts and it has moderate size. However, in order to accomplish real
implementation, several challenges need to be addressed, such as array
geometry, microphone characteristics, and the digital beamforming algorithms.
This paper presents a simulated analysis on the effect of the array geometry in
the beamforming response. Two geometries are considered, namely, the linear and
the circular geometry. The analysis is performed with computer simulations to
mimic reality. The future steps comprise the construction of the physical
microphone array, and the software implementation on a multichannel digital
signal processing (DSP) system.
</summary>
    <author>
      <name>Abdulla AlShehhi</name>
    </author>
    <author>
      <name>M. Luai Hammadih</name>
    </author>
    <author>
      <name>M. Sami Zitouni</name>
    </author>
    <author>
      <name>Saif AlKindi</name>
    </author>
    <author>
      <name>Nazar Ali</name>
    </author>
    <author>
      <name>Luis Weruaga</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">BCS International IT Conference 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.02318v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.02318v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.04770v2</id>
    <updated>2017-06-05T12:41:28Z</updated>
    <published>2017-03-14T22:17:49Z</published>
    <title>Audio Scene Classification with Deep Recurrent Neural Networks</title>
    <summary>  We introduce in this work an efficient approach for audio scene
classification using deep recurrent neural networks. An audio scene is firstly
transformed into a sequence of high-level label tree embedding feature vectors.
The vector sequence is then divided into multiple subsequences on which a deep
GRU-based recurrent neural network is trained for sequence-to-label
classification. The global predicted label for the entire sequence is finally
obtained via aggregation of subsequence classification outputs. We will show
that our approach obtains an F1-score of 97.7% on the LITIS Rouen dataset,
which is the largest dataset publicly available for the task. Compared to the
best previously reported result on the dataset, our approach is able to reduce
the relative classification error by 35.3%.
</summary>
    <author>
      <name>Huy Phan</name>
    </author>
    <author>
      <name>Philipp Koch</name>
    </author>
    <author>
      <name>Fabrice Katzberg</name>
    </author>
    <author>
      <name>Marco Maass</name>
    </author>
    <author>
      <name>Radoslaw Mazur</name>
    </author>
    <author>
      <name>Alfred Mertins</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for Interspeech 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.04770v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.04770v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.04783v1</id>
    <updated>2017-03-14T22:28:51Z</updated>
    <published>2017-03-14T22:28:51Z</published>
    <title>Multichannel End-to-end Speech Recognition</title>
    <summary>  The field of speech recognition is in the midst of a paradigm shift:
end-to-end neural networks are challenging the dominance of hidden Markov
models as a core technology. Using an attention mechanism in a recurrent
encoder-decoder architecture solves the dynamic time alignment problem,
allowing joint end-to-end training of the acoustic and language modeling
components. In this paper we extend the end-to-end framework to encompass
microphone array signal processing for noise suppression and speech enhancement
within the acoustic encoding network. This allows the beamforming components to
be optimized jointly within the recognition architecture to improve the
end-to-end speech recognition objective. Experiments on the noisy speech
benchmarks (CHiME-4 and AMI) show that our multichannel end-to-end system
outperformed the attention-based baseline with input from a conventional
adaptive beamformer.
</summary>
    <author>
      <name>Tsubasa Ochiai</name>
    </author>
    <author>
      <name>Shinji Watanabe</name>
    </author>
    <author>
      <name>Takaaki Hori</name>
    </author>
    <author>
      <name>John R. Hershey</name>
    </author>
    <link href="http://arxiv.org/abs/1703.04783v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.04783v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.05344v1</id>
    <updated>2017-03-15T18:41:37Z</updated>
    <published>2017-03-15T18:41:37Z</published>
    <title>Deducing the severity of psychiatric symptoms from the human voice</title>
    <summary>  Psychiatric illnesses are often associated with multiple symptoms, whose
severity must be graded for accurate diagnosis and treatment. This grading is
usually done by trained clinicians based on human observations and judgments
made within doctor-patient sessions. Current research provides sufficient
reason to expect that the human voice may carry biomarkers or signatures of
many, if not all, these symptoms. Based on this conjecture, we explore the
possibility of objectively and automatically grading the symptoms of
psychiatric illnesses with reference to various standard psychiatric rating
scales. Using acoustic data from several clinician-patient interviews within
hospital settings, we use non-parametric models to learn and predict the
relations between symptom-ratings and voice. In the process, we show that
different articulatory-phonetic units of speech are able to capture the effects
of different symptoms differently, and use this to establish a plausible
methodology that could be employed for automatically grading psychiatric
symptoms for clinical purposes.
</summary>
    <author>
      <name>Rita Singh</name>
    </author>
    <author>
      <name>Justin Baker</name>
    </author>
    <author>
      <name>Luciana Pennant</name>
    </author>
    <author>
      <name>Louis-Philippe Morency</name>
    </author>
    <link href="http://arxiv.org/abs/1703.05344v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.05344v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.06052v1</id>
    <updated>2017-03-17T15:31:58Z</updated>
    <published>2017-03-17T15:31:58Z</published>
    <title>Attention and Localization based on a Deep Convolutional Recurrent Model
  for Weakly Supervised Audio Tagging</title>
    <summary>  Audio tagging aims to perform multi-label classification on audio chunks and
it is a newly proposed task in the Detection and Classification of Acoustic
Scenes and Events 2016 (DCASE 2016) challenge. This task encourages research
efforts to better analyze and understand the content of the huge amounts of
audio data on the web. The difficulty in audio tagging is that it only has a
chunk-level label without a frame-level label. This paper presents a weakly
supervised method to not only predict the tags but also indicate the temporal
locations of the occurred acoustic events. The attention scheme is found to be
effective in identifying the important frames while ignoring the unrelated
frames. The proposed framework is a deep convolutional recurrent model with two
auxiliary modules: an attention module and a localization module. The proposed
algorithm was evaluated on the Task 4 of DCASE 2016 challenge. State-of-the-art
performance was achieved on the evaluation set with equal error rate (EER)
reduced from 0.13 to 0.11, compared with the convolutional recurrent baseline
system.
</summary>
    <author>
      <name>Yong Xu</name>
    </author>
    <author>
      <name>Qiuqiang Kong</name>
    </author>
    <author>
      <name>Qiang Huang</name>
    </author>
    <author>
      <name>Wenwu Wang</name>
    </author>
    <author>
      <name>Mark D. Plumbley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, submitted to interspeech2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.06052v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.06052v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.06697v2</id>
    <updated>2017-06-02T11:10:47Z</updated>
    <published>2017-03-20T12:00:04Z</published>
    <title>Timbre Analysis of Music Audio Signals with Convolutional Neural
  Networks</title>
    <summary>  The focus of this work is to study how to efficiently tailor Convolutional
Neural Networks (CNNs) towards learning timbre representations from log-mel
magnitude spectrograms. We first review the trends when designing CNN
architectures. Through this literature overview we discuss which are the
crucial points to consider for efficiently learning timbre representations
using CNNs. From this discussion we propose a design strategy meant to capture
the relevant time-frequency contexts for learning timbre, which permits using
domain knowledge for designing architectures. In addition, one of our main
goals is to design efficient CNN architectures -- what reduces the risk of
these models to over-fit, since CNNs' number of parameters is minimized.
Several architectures based on the design principles we propose are
successfully assessed for different research tasks related to timbre: singing
voice phoneme classification, musical instrument recognition and music
auto-tagging.
</summary>
    <author>
      <name>Jordi Pons</name>
    </author>
    <author>
      <name>Olga Slizovskaia</name>
    </author>
    <author>
      <name>Rong Gong</name>
    </author>
    <author>
      <name>Emilia Gómez</name>
    </author>
    <author>
      <name>Xavier Serra</name>
    </author>
    <link href="http://arxiv.org/abs/1703.06697v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.06697v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.06812v1</id>
    <updated>2017-03-20T15:58:02Z</updated>
    <published>2017-03-20T15:58:02Z</published>
    <title>Simple empirical algorithm to obtain signal envelope in three steps</title>
    <summary>  Signal amplitude envelope allows to obtain information on the signal features
for different applications. It is commonly agreed that the envelope is a signal
that varies slowly and it should pass the prominent peaks of the data smoothly.
It has been widely used in sound analysis and also in different variables of
physiological data for animal and human studies. In order to get signal
envelope, a simple algorithm is proposed based on peak detection and it was
implemented with python libraries. This method can be used for different
applications in sound or in general in time series analysis for signals of
different origin or frequency content. As well, some aspects on the parameter
selection are discussed here to adapt the same method for different
applications and some traditional methods are also revisited.
</summary>
    <author>
      <name>Cecilia Jarne</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 5 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.06812v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.06812v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.07065v1</id>
    <updated>2017-03-21T06:09:32Z</updated>
    <published>2017-03-21T06:09:32Z</published>
    <title>Adaptive Multi-Class Audio Classification in Noisy In-Vehicle
  Environment</title>
    <summary>  With ever-increasing number of car-mounted electric devices and their
complexity, audio classification is increasingly important for the automotive
industry as a fundamental tool for human-device interactions. Existing
approaches for audio classification, however, fall short as the unique and
dynamic audio characteristics of in-vehicle environments are not appropriately
taken into account. In this paper, we develop an audio classification system
that classifies an audio stream into music, speech, speech+music, and noise,
adaptably depending on driving environments including highway, local road,
crowded city, and stopped vehicle. More than 420 minutes of audio data
including various genres of music, speech, speech+music, and noise are
collected from diverse driving environments. The results demonstrate that the
proposed approach improves the average classification accuracy up to 166%, and
64% for speech, and speech+music, respectively, compared with a non-adaptive
approach in our experimental settings.
</summary>
    <author>
      <name>Myounggyu Won</name>
    </author>
    <author>
      <name>Haitham Alsaadan</name>
    </author>
    <author>
      <name>Yongsoon Eun</name>
    </author>
    <link href="http://arxiv.org/abs/1703.07065v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.07065v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.07172v1</id>
    <updated>2017-03-21T12:35:21Z</updated>
    <published>2017-03-21T12:35:21Z</published>
    <title>Multi-Objective Learning and Mask-Based Post-Processing for Deep Neural
  Network Based Speech Enhancement</title>
    <summary>  We propose a multi-objective framework to learn both secondary targets not
directly related to the intended task of speech enhancement (SE) and the
primary target of the clean log-power spectra (LPS) features to be used
directly for constructing the enhanced speech signals. In deep neural network
(DNN) based SE we introduce an auxiliary structure to learn secondary
continuous features, such as mel-frequency cepstral coefficients (MFCCs), and
categorical information, such as the ideal binary mask (IBM), and integrate it
into the original DNN architecture for joint optimization of all the
parameters. This joint estimation scheme imposes additional constraints not
available in the direct prediction of LPS, and potentially improves the
learning of the primary target. Furthermore, the learned secondary information
as a byproduct can be used for other purposes, e.g., the IBM-based
post-processing in this work. A series of experiments show that joint LPS and
MFCC learning improves the SE performance, and IBM-based post-processing
further enhances listening quality of the reconstructed speech.
</summary>
    <author>
      <name>Yong Xu</name>
    </author>
    <author>
      <name>Jun Du</name>
    </author>
    <author>
      <name>Zhen Huang</name>
    </author>
    <author>
      <name>Li-Rong Dai</name>
    </author>
    <author>
      <name>Chin-Hui Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">interspeech2015 paper, Germany</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.07172v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.07172v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.09302v1</id>
    <updated>2017-03-27T20:37:33Z</updated>
    <published>2017-03-27T20:37:33Z</published>
    <title>Speech Enhancement using a Deep Mixture of Experts</title>
    <summary>  In this study we present a Deep Mixture of Experts (DMoE) neural-network
architecture for single microphone speech enhancement. By contrast to most
speech enhancement algorithms that overlook the speech variability mainly
caused by phoneme structure, our framework comprises a set of deep neural
networks (DNNs), each one of which is an 'expert' in enhancing a given speech
type corresponding to a phoneme. A gating DNN determines which expert is
assigned to a given speech segment. A speech presence probability (SPP) is then
obtained as a weighted average of the expert SPP decisions, with the weights
determined by the gating DNN. A soft spectral attenuation, based on the SPP, is
then applied to enhance the noisy speech signal. The experts and the gating
components of the DMoE network are trained jointly. As part of the training,
speech clustering into different subsets is performed in an unsupervised
manner. Therefore, unlike previous methods, a phoneme-labeled database is not
required for the training procedure. A series of experiments with different
noise types verified the applicability of the new algorithm to the task of
speech enhancement. The proposed scheme outperforms other schemes that either
do not consider phoneme structure or use a simpler training methodology.
</summary>
    <author>
      <name>Shlomo E. Chazan</name>
    </author>
    <author>
      <name>Jacob Goldberger</name>
    </author>
    <author>
      <name>Sharon Gannot</name>
    </author>
    <link href="http://arxiv.org/abs/1703.09302v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.09302v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.10893v2</id>
    <updated>2017-09-11T07:42:54Z</updated>
    <published>2017-03-30T08:59:24Z</published>
    <title>Audio-Visual Speech Enhancement based on Multimodal Deep Convolutional
  Neural Network</title>
    <summary>  Speech enhancement (SE) aims to reduce noise in speech signals. Most SE
techniques focus on addressing audio information only. In this work, inspired
by multimodal learning, which utilizes data from different modalities, and the
recent success of convolutional neural networks (CNNs) in SE, we propose an
audio-visual deep CNN (AVDCNN) SE model, which incorporates audio and visual
streams into a unified network model. In the proposed AVDCNN SE model, audio
and visual data are first processed using individual CNNs, and then, fused into
a joint network to generate enhanced speech at the output layer. The AVDCNN
model is trained in an end-to-end manner, and parameters are jointly learned
through back-propagation. We evaluate enhanced speech using five objective
criteria. Results show that the AVDCNN yields notably better performance,
compared with an audio-only CNN-based SE model and two conventional SE
approaches, confirming the effectiveness of integrating visual information into
the SE process.
</summary>
    <author>
      <name>Jen-Cheng Hou</name>
    </author>
    <author>
      <name>Syu-Siang Wang</name>
    </author>
    <author>
      <name>Ying-Hui Lai</name>
    </author>
    <author>
      <name>Yu Tsao</name>
    </author>
    <author>
      <name>Hsiu-Wen Chang</name>
    </author>
    <author>
      <name>Hsin-Min Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1703.10893v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.10893v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.01985v4</id>
    <updated>2017-06-19T10:57:38Z</updated>
    <published>2017-03-22T08:39:32Z</published>
    <title>Recognizing Multi-talker Speech with Permutation Invariant Training</title>
    <summary>  In this paper, we propose a novel technique for direct recognition of
multiple speech streams given the single channel of mixed speech, without first
separating them. Our technique is based on permutation invariant training (PIT)
for automatic speech recognition (ASR). In PIT-ASR, we compute the average
cross entropy (CE) over all frames in the whole utterance for each possible
output-target assignment, pick the one with the minimum CE, and optimize for
that assignment. PIT-ASR forces all the frames of the same speaker to be
aligned with the same output layer. This strategy elegantly solves the label
permutation problem and speaker tracing problem in one shot. Our experiments on
artificially mixed AMI data showed that the proposed approach is very
promising.
</summary>
    <author>
      <name>Dong Yu</name>
    </author>
    <author>
      <name>Xuankai Chang</name>
    </author>
    <author>
      <name>Yanmin Qian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 6 figures, InterSpeech2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.01985v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.01985v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.02373v1</id>
    <updated>2017-04-06T09:37:41Z</updated>
    <published>2017-04-06T09:37:41Z</published>
    <title>Time-Contrastive Learning Based Unsupervised DNN Feature Extraction for
  Speaker Verification</title>
    <summary>  In this paper, we present a time-contrastive learning (TCL)based unsupervised
bottleneck (BN) feature extraction method for speech signals with an
application to speaker verification. The method exploits the temporal structure
of a speech signal and more specifically, it trains deep neural networks (DNNs)
to discriminate temporal events obtained by uniformly segmenting the signal
without using any label information, in contrast to conventional DNN based BN
feature extraction methods that train DNNs using labeled data to discriminate
speakers or passphrases or phones or a combination of them. We consider
different strategies for TCL and its combination with transfer learning.
Experimental results on the RSR2015 database show that the TCL method is
superior to the conventional speaker and pass-phrase discriminant BN feature
and Mel-frequency cepstral coefficients (MFCCs) feature for text-dependent
speaker verification. The unsupervised TCL method further has the advantage of
being able to leverage the huge amount of unlabeled data that are often
available in real life.
</summary>
    <author>
      <name>Achintya Kr. Sarkar</name>
    </author>
    <author>
      <name>Zheng-Hua Tan</name>
    </author>
    <link href="http://arxiv.org/abs/1704.02373v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.02373v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.03626v1</id>
    <updated>2017-04-12T05:46:44Z</updated>
    <published>2017-04-12T05:46:44Z</published>
    <title>Sampling-based speech parameter generation using moment-matching
  networks</title>
    <summary>  This paper presents sampling-based speech parameter generation using
moment-matching networks for Deep Neural Network (DNN)-based speech synthesis.
Although people never produce exactly the same speech even if we try to express
the same linguistic and para-linguistic information, typical statistical speech
synthesis produces completely the same speech, i.e., there is no
inter-utterance variation in synthetic speech. To give synthetic speech natural
inter-utterance variation, this paper builds DNN acoustic models that make it
possible to randomly sample speech parameters. The DNNs are trained so that
they make the moments of generated speech parameters close to those of natural
speech parameters. Since the variation of speech parameters is compressed into
a low-dimensional simple prior noise vector, our algorithm has lower
computation cost than direct sampling of speech parameters. As the first step
towards generating synthetic speech that has natural inter-utterance variation,
this paper investigates whether or not the proposed sampling-based generation
deteriorates synthetic speech quality. In evaluation, we compare speech quality
of conventional maximum likelihood-based generation and proposed sampling-based
generation. The result demonstrates the proposed generation causes no
degradation in speech quality.
</summary>
    <author>
      <name>Shinnosuke Takamichi</name>
    </author>
    <author>
      <name>Tomoki Koriyama</name>
    </author>
    <author>
      <name>Hiroshi Saruwatari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to INTERSPEECH 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.03626v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.03626v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.03809v3</id>
    <updated>2017-08-17T12:20:01Z</updated>
    <published>2017-04-12T15:57:08Z</published>
    <title>A Neural Parametric Singing Synthesizer</title>
    <summary>  We present a new model for singing synthesis based on a modified version of
the WaveNet architecture. Instead of modeling raw waveform, we model features
produced by a parametric vocoder that separates the influence of pitch and
timbre. This allows conveniently modifying pitch to match any target melody,
facilitates training on more modest dataset sizes, and significantly reduces
training and generation times. Our model makes frame-wise predictions using
mixture density outputs rather than categorical outputs in order to reduce the
required parameter count. As we found overfitting to be an issue with the
relatively small datasets used in our experiments, we propose a method to
regularize the model and make the autoregressive generation process more robust
to prediction errors. Using a simple multi-stream architecture, harmonic,
aperiodic and voiced/unvoiced components can all be predicted in a coherent
manner. We compare our method to existing parametric statistical and
state-of-the-art concatenative methods using quantitative metrics and a
listening test. While naive implementations of the autoregressive generation
algorithm tend to be inefficient, using a smart algorithm we can greatly speed
up the process and obtain a system that's competitive in both speed and
quality.
</summary>
    <author>
      <name>Merlijn Blaauw</name>
    </author>
    <author>
      <name>Jordi Bonada</name>
    </author>
    <link href="http://arxiv.org/abs/1704.03809v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.03809v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.03934v1</id>
    <updated>2017-04-12T21:12:34Z</updated>
    <published>2017-04-12T21:12:34Z</published>
    <title>i Vector used in Speaker Identification by Dimension Compactness</title>
    <summary>  The automatic speaker identification procedure is used to extract features
that help to identify the components of the acoustic signal by discarding all
the other stuff like background noise, emotion, hesitation, etc. The acoustic
signal is generated by a human that is filtered by the shape of the vocal
tract, including tongue, teeth, etc. The shape of the vocal tract determines
and produced, what signal comes out in real time. The analytically develops
shape of the vocal tract, which exhibits envelop for the short time power
spectrum. The ASR needs efficient way of extracting features from the acoustic
signal that is used effectively to makes the shape of the individual vocal
tract. To identify any acoustic signal in the large collection of acoustic
signal i.e. corpora, it needs dimension compactness of total variability space
by using the GMM mean super vector. This work presents the efficient way to
implement dimension compactness in total variability space and using cosine
distance scoring to predict a fast output score for small size utterance.
</summary>
    <author>
      <name>Soumen Kanrar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages,7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.03934v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.03934v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.03939v1</id>
    <updated>2017-04-12T21:43:20Z</updated>
    <published>2017-04-12T21:43:20Z</published>
    <title>Speaker Identification by GMM based i Vector</title>
    <summary>  Speaker Identification process is to identify a particular vocal cord from a
set of existing speakers. In the speaker identification processes, unknown
speaker voice sample targets each of the existing speakers present in the
system and gives a predication. The predication may be more than one existing
known speaker voice and is very close to the unknown speaker voice. The model
is a Gaussian mixture model built by the extracted acoustic feature vectors
from voice. The i-vector based dimension compression mapping function of the
channel depended speaker, and super vector give better predicted scores
according to cosine distance scoring associated with the order pair of
speakers. In the order pair, the first coordinate is the unknown speaker i.e.
test speaker, and the second coordinates is the existing known speaker i.e.
target speaker. This paper presents the enhancement of the prediction based on
i- vector in compare to the normalized set of predicted score. In the
simulation, known speaker voices are collected through different channels and
in different languages. In the testing, the GMM voice models, and GMM based
i-Vector speaker voice models of the known speakers are used among the numbers
of clusters in the test data set.
</summary>
    <author>
      <name>Soumen Kanrar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 Pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.03939v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.03939v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.06008v1</id>
    <updated>2017-04-20T04:38:29Z</updated>
    <published>2017-04-20T04:38:29Z</published>
    <title>Effects of virtual acoustics on dynamic auditory distance perception</title>
    <summary>  Sound propagation encompasses various acoustic phenomena including
reverberation. Current virtual acoustic methods, ranging from parametric
filters to physically-accurate solvers, can simulate reverberation with varying
degrees of fidelity. We investigate the effects of reverberant sounds generated
using different propagation algorithms on acoustic distance perception, i.e.,
how faraway humans perceive a sound source. In particular, we evaluate two
classes of methods for real-time sound propagation in dynamic scenes based on
parametric filters and ray tracing. Our study shows that the more accurate
method shows less distance compression as compared to the approximate,
filter-based method. This suggests that accurate reverberation in VR results in
a better reproduction of acoustic distances. We also quantify the levels of
distance compression introduced by different propagation methods in a virtual
environment.
</summary>
    <author>
      <name>Atul Rungta</name>
    </author>
    <author>
      <name>Nicholas Rewkowski</name>
    </author>
    <author>
      <name>Roberta Klatzky</name>
    </author>
    <author>
      <name>Ming Lin</name>
    </author>
    <author>
      <name>Dinesh Manocha</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1121/1.4981234</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1121/1.4981234" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 Pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.06008v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.06008v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.08953v3</id>
    <updated>2017-08-01T13:27:34Z</updated>
    <published>2017-04-28T14:30:20Z</published>
    <title>Design of robust two-dimensional polynomial beamformers as a convex
  optimization problem with application to robot audition</title>
    <summary>  We propose a robust two-dimensional polynomial beamformer design method,
formulated as a convex optimization problem, which allows for flexible steering
of a previously proposed data-independent robust beamformer in both azimuth and
elevation direction.~As an exemplary application, the proposed two-dimensional
polynomial beamformer design is applied to a twelve-element microphone array,
integrated into the head of a humanoid robot. To account for the effects of the
robot's head on the sound field, measured head-related transfer functions are
integrated into the optimization problem as steering vectors. The
two-dimensional polynomial beamformer design is evaluated using
signal-independent and signal-dependent measures. The results confirm that the
proposed polynomial beamformer design approximates the original fixed
beamformer design very accurately, which makes it an attractive approach for
robust real-time data-independent beamforming.
</summary>
    <author>
      <name>Hendrik Barfuss</name>
    </author>
    <author>
      <name>Markus Bachmann</name>
    </author>
    <author>
      <name>Michael Buerger</name>
    </author>
    <author>
      <name>Martin Schneider</name>
    </author>
    <author>
      <name>Walter Kellerman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to IEEE Workshop on Applications of Signal Processing to
  Audio and Acoustics (WASPAA) 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.08953v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.08953v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.00919v1</id>
    <updated>2017-05-02T11:31:43Z</updated>
    <published>2017-05-02T11:31:43Z</published>
    <title>Broadband DOA estimation using Convolutional neural networks trained
  with noise signals</title>
    <summary>  A convolution neural network (CNN) based classification method for broadband
DOA estimation is proposed, where the phase component of the short-time Fourier
transform coefficients of the received microphone signals are directly fed into
the CNN and the features required for DOA estimation are learnt during
training. Since only the phase component of the input is used, the CNN can be
trained with synthesized noise signals, thereby making the preparation of the
training data set easier compared to using speech signals. Through experimental
evaluation, the ability of the proposed noise trained CNN framework to
generalize to speech sources is demonstrated. In addition, the robustness of
the system to noise, small perturbations in microphone positions, as well as
its ability to adapt to different acoustic conditions is investigated using
experiments with simulated and real data.
</summary>
    <author>
      <name>Soumitro Chakrabarty</name>
    </author>
    <author>
      <name>Emanuël. A. P. Habets</name>
    </author>
    <link href="http://arxiv.org/abs/1705.00919v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.00919v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.02514v1</id>
    <updated>2017-05-06T18:27:09Z</updated>
    <published>2017-05-06T18:27:09Z</published>
    <title>End-to-end Source Separation with Adaptive Front-Ends</title>
    <summary>  Source separation and other audio applications have traditionally relied on
the use of short-time Fourier transforms as a front-end frequency domain
representation step. The unavailability of a neural network equivalent to
forward and inverse transforms hinders the implementation of end-to-end
learning systems for these applications. We present an auto-encoder neural
network that can act as an equivalent to short-time front-end transforms. We
demonstrate the ability of the network to learn optimal, real-valued basis
functions directly from the raw waveform of a signal and further show how it
can be used as an adaptive front-end for supervised source separation. In terms
of separation performance, these transforms outperform their Fourier
counterparts with an optimal frequency domain representation that is defined on
a highly reduced set of basis functions and tailored to the input.
</summary>
    <author>
      <name>Shrikant Venkataramani</name>
    </author>
    <author>
      <name>Paris Smaragdis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.02514v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.02514v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.03670v1</id>
    <updated>2017-05-10T09:30:42Z</updated>
    <published>2017-05-10T09:30:42Z</published>
    <title>Deep Speaker Feature Learning for Text-independent Speaker Verification</title>
    <summary>  Recently deep neural networks (DNNs) have been used to learn speaker
features. However, the quality of the learned features is not sufficiently
good, so a complex back-end model, either neural or probabilistic, has to be
used to address the residual uncertainty when applied to speaker verification,
just as with raw features. This paper presents a convolutional time-delay deep
neural network structure (CT-DNN) for speaker feature learning. Our
experimental results on the Fisher database demonstrated that this CT-DNN can
produce high-quality speaker features: even with a single feature (0.3 seconds
including the context), the EER can be as low as 7.68%. This effectively
confirmed that the speaker trait is largely a deterministic short-time property
rather than a long-time distributional pattern, and therefore can be extracted
from just dozens of frames.
</summary>
    <author>
      <name>Lantian Li</name>
    </author>
    <author>
      <name>Yixiang Chen</name>
    </author>
    <author>
      <name>Ying Shi</name>
    </author>
    <author>
      <name>Zhiyuan Tang</name>
    </author>
    <author>
      <name>Dong Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">deep neural networks, speaker verification, speaker feature</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.03670v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.03670v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.03877v2</id>
    <updated>2017-05-15T20:49:42Z</updated>
    <published>2017-05-10T17:58:53Z</published>
    <title>Frequency Domain Singular Value Decomposition for Efficient Spatial
  Audio Coding</title>
    <summary>  Advances in virtual reality have generated substantial interest in accurately
reproducing and storing spatial audio in the higher order ambisonics (HOA)
representation, given its rendering flexibility. Recent standardization for HOA
compression adopted a framework wherein HOA data are decomposed into principal
components that are then encoded by standard audio coding, i.e., frequency
domain quantization and entropy coding to exploit psychoacoustic redundancy. A
noted shortcoming of this approach is the occasional mismatch in principal
components across blocks, and the resulting suboptimal transitions in the data
fed to the audio coder. Instead, we propose a framework where singular value
decomposition (SVD) is performed after transformation to the frequency domain
via the modified discrete cosine transform (MDCT). This framework not only
ensures smooth transition across blocks, but also enables frequency dependent
SVD for better energy compaction. Moreover, we introduce a novel noise
substitution technique to compensate for suppressed ambient energy in discarded
higher order ambisonics channels, which significantly enhances the perceptual
quality of the reconstructed HOA signal. Objective and subjective evaluation
results provide evidence for the effectiveness of the proposed framework in
terms of both higher compression gains and better perceptual quality, compared
to existing methods.
</summary>
    <author>
      <name>Sina Zamani</name>
    </author>
    <author>
      <name>Tejaswi Nanjundaswamy</name>
    </author>
    <author>
      <name>Kenneth Rose</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.03877v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.03877v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.04792v1</id>
    <updated>2017-05-13T06:43:46Z</updated>
    <published>2017-05-13T06:43:46Z</published>
    <title>Riddim: A Rhythm Analysis and Decomposition Tool Based On Independent
  Subspace Analysis</title>
    <summary>  The goal of this thesis was to implement a tool that, given a digital audio
input, can extract and represent rhythm and musical time. The purpose of the
tool is to help develop better models of rhythm for real-time computer based
performance and composition. This analysis tool, Riddim, uses Independent
Subspace Analysis (ISA) and a robust onset detection scheme to separate and
detect salient rhythmic and timing information from different sonic sources
within the input. This information is then represented in a format that can be
used by a variety of algorithms that interpret timing information to infer
rhythmic and musical structure. A secondary objective of this work is a "proof
of concept" as a non-real-time rhythm analysis system based on ISA. This is a
necessary step since ultimately it is desirable to incorporate this
functionality in a real-time plug-in for live performance and improvisation.
</summary>
    <author>
      <name>Iroro Orife</name>
    </author>
    <link href="http://arxiv.org/abs/1705.04792v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.04792v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.05472v1</id>
    <updated>2017-05-15T22:20:22Z</updated>
    <published>2017-05-15T22:20:22Z</published>
    <title>A Biomimetic Vocalisation System for MiRo</title>
    <summary>  There is increasing interest in the use of animal-like robots in applications
such as companionship and pet therapy. However, in the majority of cases it is
only the robot's physical appearance that mimics a given animal. In contrast,
MiRo is the first commercial biomimetic robot to be based on a hardware and
software architecture that is modelled on the biological brain. This paper
describes how MiRo's vocalisation system was designed, not using pre-recorded
animal sounds, but based on the implementation of a real-time parametric
general-purpose mammalian vocal synthesiser tailored to the specific physical
characteristics of the robot. The novel outcome has been the creation of an
'appropriate' voice for MiRo that is perfectly aligned to the physical and
behavioural affordances of the robot, thereby avoiding the 'uncanny valley'
effect and contributing strongly to the effectiveness of MiRo as an interactive
device.
</summary>
    <author>
      <name>Roger K. Moore</name>
    </author>
    <author>
      <name>Ben Mitchinson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages; accepted for publication at Living Machines 2017, Stanford,
  USA; 25-28 July 2017; http://livingmachinesconference.eu/2017/</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.05472v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.05472v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.05874v1</id>
    <updated>2017-05-16T18:45:41Z</updated>
    <published>2017-05-16T18:45:41Z</published>
    <title>Time-frequency or time-scale representation fission and fusion rules</title>
    <summary>  Time-frequency representations are important for the analysis of time series.
We have developed an online time-series analysis system and equipped it to
reliably handle re-alignment in the time-frequency plane. The system can deal
with issues like invalid regions in time-frequency representations and
discontinuities in data transmissions, making it suitable for on-line
processing in real-world situations. In retrospect the whole problem can be
considered to be a generalization of ideas present in overlap-and-add
filtering, but then for time-frequency representations and including the
calculation of non-causal features. Here we present our design for
time-frequency representation fission and fusion rules. We present these rules
in the context of two typical use cases, which facilitate understanding of the
underlying choices.
</summary>
    <author>
      <name>Coen Jonker</name>
    </author>
    <author>
      <name>Arryon D. Tijsma</name>
    </author>
    <author>
      <name>Ronald A. J. van Elburg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is accompanying the release of libSoundAnnotator. The
  whole implementation is open sourced through GitHub:
  https://github.com/soundappraisal/libsoundannotator under the Apache License,
  Version 2.0. Both UseCases described are available as documented code through
  GitHub https://github.com/soundappraisal/soundannotatordemo under the Apache
  License, Version 2.0. 11 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.05874v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.05874v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5; I.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.08660v1</id>
    <updated>2017-05-24T08:39:04Z</updated>
    <published>2017-05-24T08:39:04Z</published>
    <title>Matrix of Polynomials Model based Polynomial Dictionary Learning Method
  for Acoustic Impulse Response Modeling</title>
    <summary>  We study the problem of dictionary learning for signals that can be
represented as polynomials or polynomial matrices, such as convolutive signals
with time delays or acoustic impulse responses. Recently, we developed a method
for polynomial dictionary learning based on the fact that a polynomial matrix
can be expressed as a polynomial with matrix coefficients, where the
coefficient of the polynomial at each time lag is a scalar matrix. However, a
polynomial matrix can be also equally represented as a matrix with polynomial
elements. In this paper, we develop an alternative method for learning a
polynomial dictionary and a sparse representation method for polynomial signal
reconstruction based on this model. The proposed methods can be used directly
to operate on the polynomial matrix without having to access its coefficients
matrices. We demonstrate the performance of the proposed method for acoustic
impulse response modeling.
</summary>
    <author>
      <name>Jian Guan</name>
    </author>
    <author>
      <name>Xuan Wang</name>
    </author>
    <author>
      <name>Pengming Feng</name>
    </author>
    <author>
      <name>Jing Dong</name>
    </author>
    <author>
      <name>Wenwu Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.08660v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.08660v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.08858v1</id>
    <updated>2017-05-24T16:48:03Z</updated>
    <published>2017-05-24T16:48:03Z</published>
    <title>Audio-replay attack detection countermeasures</title>
    <summary>  This paper presents the Speech Technology Center (STC) replay attack
detection systems proposed for Automatic Speaker Verification Spoofing and
Countermeasures Challenge 2017. In this study we focused on comparison of
different spoofing detection approaches. These were GMM based methods, high
level features extraction with simple classifier and deep learning frameworks.
Experiments performed on the development and evaluation parts of the challenge
dataset demonstrated stable efficiency of deep learning approaches in case of
changing acoustic conditions. At the same time SVM classifier with high level
features provided a substantial input in the efficiency of the resulting STC
systems according to the fusion systems results.
</summary>
    <author>
      <name>Galina Lavrentyeva</name>
    </author>
    <author>
      <name>Sergey Novoselov</name>
    </author>
    <author>
      <name>Egor Malykh</name>
    </author>
    <author>
      <name>Alexander Kozlov</name>
    </author>
    <author>
      <name>Oleg Kudashev</name>
    </author>
    <author>
      <name>Vadim Shchemelinin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 3 figures, accepted for Specom 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.08858v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.08858v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.08865v1</id>
    <updated>2017-05-24T16:58:03Z</updated>
    <published>2017-05-24T16:58:03Z</published>
    <title>Anti-spoofing Methods for Automatic SpeakerVerification System</title>
    <summary>  Growing interest in automatic speaker verification (ASV)systems has lead to
significant quality improvement of spoofing attackson them. Many research works
confirm that despite the low equal er-ror rate (EER) ASV systems are still
vulnerable to spoofing attacks. Inthis work we overview different acoustic
feature spaces and classifiersto determine reliable and robust countermeasures
against spoofing at-tacks. We compared several spoofing detection systems,
presented so far,on the development and evaluation datasets of the Automatic
SpeakerVerification Spoofing and Countermeasures (ASVspoof) Challenge
2015.Experimental results presented in this paper demonstrate that the useof
magnitude and phase information combination provides a substantialinput into
the efficiency of the spoofing detection systems. Also wavelet-based features
show impressive results in terms of equal error rate. Inour overview we compare
spoofing performance for systems based on dif-ferent classifiers. Comparison
results demonstrate that the linear SVMclassifier outperforms the conventional
GMM approach. However, manyresearchers inspired by the great success of deep
neural networks (DNN)approaches in the automatic speech recognition, applied
DNN in thespoofing detection task and obtained quite low EER for known and
un-known type of spoofing attacks.
</summary>
    <author>
      <name>Galina Lavrentyeva</name>
    </author>
    <author>
      <name>Sergey Novoselov</name>
    </author>
    <author>
      <name>Konstantin Simonchik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 0 figures, published in Springer Communications in Computer
  and Information Science (CCIS) vol. 661</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.08865v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.08865v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.09185v1</id>
    <updated>2017-05-25T13:59:18Z</updated>
    <published>2017-05-25T13:59:18Z</published>
    <title>Investigation of Using VAE for i-Vector Speaker Verification</title>
    <summary>  New system for i-vector speaker recognition based on variational autoencoder
(VAE) is investigated. VAE is a promising approach for developing accurate deep
nonlinear generative models of complex data. Experiments show that VAE provides
speaker embedding and can be effectively trained in an unsupervised manner. LLR
estimate for VAE is developed. Experiments on NIST SRE 2010 data demonstrate
its correctness. Additionally, we show that the performance of VAE-based system
in the i-vectors space is close to that of the diagonal PLDA. Several
interesting results are also observed in the experiments with $\beta$-VAE. In
particular, we found that for $\beta\ll 1$, VAE can be trained to capture the
features of complex input data distributions in an effective way, which is hard
to obtain in the standard VAE ($\beta=1$).
</summary>
    <author>
      <name>Timur Pekhovsky</name>
    </author>
    <author>
      <name>Maxim Korenevsky</name>
    </author>
    <link href="http://arxiv.org/abs/1705.09185v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.09185v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.09289v1</id>
    <updated>2017-05-25T19:08:08Z</updated>
    <published>2017-05-25T19:08:08Z</published>
    <title>Improved I-vector-based Speaker Recognition for Utterances with Speaker
  Generated Non-speech sounds</title>
    <summary>  Conversational speech not only contains several variants of neutral speech
but is also prominently interlaced with several speaker generated non-speech
sounds such as laughter and breath. A robust speaker recognition system should
be capable of recognizing a speaker irrespective of these variations in his
speech. An understanding of whether the speaker-specific information
represented by these variations is similar or not helps build a good speaker
recognition system. In this paper, speaker variations captured by neutral
speech of a speaker is analyzed by considering speech-laugh (a variant of
neutral speech) and laughter (non-speech) sounds of the speaker. We study an
i-vector-based speaker recognition system trained only on neutral speech and
evaluate its performance on speech-laugh and laughter. Further, we analyze the
effect of including laughter sounds during training of an i-vector-basedspeaker
recognition system. Our experimental results show that the inclusion of
laughter sounds during training seem to provide complementary speaker-specific
information which results in an overall improved performance of the speaker
recognition system, especially on the utterances with speech-laugh segments.
</summary>
    <author>
      <name>Sri Harsha Dumpala</name>
    </author>
    <author>
      <name>Ashish Panda</name>
    </author>
    <author>
      <name>Sunil Kumar Kopparapu</name>
    </author>
    <link href="http://arxiv.org/abs/1705.09289v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.09289v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.10368v1</id>
    <updated>2017-05-29T19:24:14Z</updated>
    <published>2017-05-29T19:24:14Z</published>
    <title>DNN-based uncertainty estimation for weighted DNN-HMM ASR</title>
    <summary>  In this paper, the uncertainty is defined as the mean square error between a
given enhanced noisy observation vector and the corresponding clean one. Then,
a DNN is trained by using enhanced noisy observation vectors as input and the
uncertainty as output with a training database. In testing, the DNN receives an
enhanced noisy observation vector and delivers the estimated uncertainty. This
uncertainty in employed in combination with a weighted DNN-HMM based speech
recognition system and compared with an existing estimation of the noise
cancelling uncertainty variance based on an additive noise model. Experiments
were carried out with Aurora-4 task. Results with clean, multi-noise and
multi-condition training are presented.
</summary>
    <author>
      <name>José Novoa</name>
    </author>
    <author>
      <name>Josué Fredes</name>
    </author>
    <author>
      <name>Néstor Becerra Yoma</name>
    </author>
    <link href="http://arxiv.org/abs/1705.10368v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.10368v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.10874v2</id>
    <updated>2017-07-08T09:44:32Z</updated>
    <published>2017-05-30T21:31:25Z</published>
    <title>Deep Learning for Environmentally Robust Speech Recognition: An Overview
  of Recent Developments</title>
    <summary>  Eliminating the negative effect of non-stationary environmental noise is a
long-standing research topic for automatic speech recognition but still remains
an important challenge. Data-driven supervised approaches, especially the ones
based on deep neural networks, have recently emerged as potential alternatives
to traditional unsupervised approaches and with sufficient training, can
alleviate the shortcomings of the unsupervised methods in various real-life
acoustic environments. In this light, we review recently developed,
representative deep learning approaches for tackling non-stationary additive
and convolutional degradation of speech with the aim of providing guidelines
for those involved in the development of environmentally robust speech
recognition systems. We separately discuss single- and multi-channel techniques
developed for the front-end and back-end of speech recognition systems, as well
as joint front-end and back-end training frameworks.
</summary>
    <author>
      <name>Zixing Zhang</name>
    </author>
    <author>
      <name>Jürgen Geiger</name>
    </author>
    <author>
      <name>Jouni Pohjalainen</name>
    </author>
    <author>
      <name>Amr El-Desoky Mousa</name>
    </author>
    <author>
      <name>Wenyu Jin</name>
    </author>
    <author>
      <name>Björn Schuller</name>
    </author>
    <link href="http://arxiv.org/abs/1705.10874v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.10874v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.00114v1</id>
    <updated>2017-05-31T22:27:37Z</updated>
    <published>2017-05-31T22:27:37Z</published>
    <title>Mixed penalization in convolutive nonnegative matrix factorization for
  blind speech dereverberation</title>
    <summary>  When a signal is recorded in an enclosed room, it typically gets affected by
reverberation. This degradation represents a problem when dealing with audio
signals, particularly in the field of speech signal processing, such as
automatic speech recognition. Although there are some approaches to deal with
this issue that are quite satisfactory under certain conditions, constructing a
method that works well in a general context still poses a significant
challenge. In this article, we propose a method based on convolutive
nonnegative matrix factorization that mixes two penalizers in order to impose
certain characteristics over the time-frequency components of the restored
signal and the reverberant components. An algorithm for implementing the method
is described and tested. Comparisons of the results against those obtained with
state of the art methods are presented, showing significant improvement.
</summary>
    <author>
      <name>Francisco J. Ibarrola</name>
    </author>
    <author>
      <name>Leandro E. Di Persia</name>
    </author>
    <author>
      <name>Ruben D. Spies</name>
    </author>
    <link href="http://arxiv.org/abs/1706.00114v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.00114v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.01777v2</id>
    <updated>2017-06-25T10:10:35Z</updated>
    <published>2017-06-05T15:02:39Z</published>
    <title>Deep Factorization for Speech Signal</title>
    <summary>  Speech signals are complex intermingling of various informative factors, and
this information blending makes decoding any of the individual factors
extremely difficult. A natural idea is to factorize each speech frame into
independent factors, though it turns out to be even more difficult than
decoding each individual factor. A major encumbrance is that the speaker trait,
a major factor in speech signals, has been suspected to be a long-term
distributional pattern and so not identifiable at the frame level. In this
paper, we demonstrated that the speaker factor is also a short-time spectral
pattern and can be largely identified with just a few frames using a simple
deep neural network (DNN). This discovery motivated a cascade deep
factorization (CDF) framework that infers speech factors in a sequential way,
and factors previously inferred are used as conditional variables when
inferring other factors. Our experiment on an automatic emotion recognition
(AER) task demonstrated that this approach can effectively factorize speech
signals, and using these factors, the original speech spectrum can be recovered
with high accuracy. This factorization and reconstruction approach provides a
novel tool for many speech processing tasks.
</summary>
    <author>
      <name>Dong Wang</name>
    </author>
    <author>
      <name>Lantian Li</name>
    </author>
    <author>
      <name>Ying Shi</name>
    </author>
    <author>
      <name>Yixiang Chen</name>
    </author>
    <author>
      <name>Zhiyuan Tang</name>
    </author>
    <link href="http://arxiv.org/abs/1706.01777v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.01777v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.02101v1</id>
    <updated>2017-06-07T09:31:00Z</updated>
    <published>2017-06-07T09:31:00Z</published>
    <title>A Study on Replay Attack and Anti-Spoofing for Automatic Speaker
  Verification</title>
    <summary>  For practical automatic speaker verification (ASV) systems, replay attack
poses a true risk. By replaying a pre-recorded speech signal of the genuine
speaker, ASV systems tend to be easily fooled. An effective replay detection
method is therefore highly desirable. In this study, we investigate a major
difficulty in replay detection: the over-fitting problem caused by variability
factors in speech signal. An F-ratio probing tool is proposed and three
variability factors are investigated using this tool: speaker identity, speech
content and playback &amp; recording device. The analysis shows that device is the
most influential factor that contributes the highest over-fitting risk. A
frequency warping approach is studied to alleviate the over-fitting problem, as
verified on the ASV-spoof 2017 database.
</summary>
    <author>
      <name>Lantian Li</name>
    </author>
    <author>
      <name>Yixiang Chen</name>
    </author>
    <author>
      <name>Dong Wang</name>
    </author>
    <author>
      <name>Thomas Fang Zheng</name>
    </author>
    <link href="http://arxiv.org/abs/1706.02101v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.02101v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.02291v1</id>
    <updated>2017-06-07T06:01:48Z</updated>
    <published>2017-06-07T06:01:48Z</published>
    <title>Sound Event Detection Using Spatial Features and Convolutional Recurrent
  Neural Network</title>
    <summary>  This paper proposes to use low-level spatial features extracted from
multichannel audio for sound event detection. We extend the convolutional
recurrent neural network to handle more than one type of these multichannel
features by learning from each of them separately in the initial stages. We
show that instead of concatenating the features of each channel into a single
feature vector the network learns sound events in multichannel audio better
when they are presented as separate layers of a volume. Using the proposed
spatial features over monaural features on the same network gives an absolute
F-score improvement of 6.1% on the publicly available TUT-SED 2016 dataset and
2.7% on the TUT-SED 2009 dataset that is fifteen times larger.
</summary>
    <author>
      <name>Sharath Adavanne</name>
    </author>
    <author>
      <name>Pasi Pertilä</name>
    </author>
    <author>
      <name>Tuomas Virtanen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP 2017)</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.02291v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.02291v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.02293v1</id>
    <updated>2017-06-07T06:11:32Z</updated>
    <published>2017-06-07T06:11:32Z</published>
    <title>Sound Event Detection in Multichannel Audio Using Spatial and Harmonic
  Features</title>
    <summary>  In this paper, we propose the use of spatial and harmonic features in
combination with long short term memory (LSTM) recurrent neural network (RNN)
for automatic sound event detection (SED) task. Real life sound recordings
typically have many overlapping sound events, making it hard to recognize with
just mono channel audio. Human listeners have been successfully recognizing the
mixture of overlapping sound events using pitch cues and exploiting the stereo
(multichannel) audio signal available at their ears to spatially localize these
events. Traditionally SED systems have only been using mono channel audio,
motivated by the human listener we propose to extend them to use multichannel
audio. The proposed SED system is compared against the state of the art mono
channel method on the development subset of TUT sound events detection 2016
database. The usage of spatial and harmonic features are shown to improve the
performance of SED.
</summary>
    <author>
      <name>Sharath Adavanne</name>
    </author>
    <author>
      <name>Giambattista Parascandolo</name>
    </author>
    <author>
      <name>Pasi Pertilä</name>
    </author>
    <author>
      <name>Toni Heittola</name>
    </author>
    <author>
      <name>Tuomas Virtanen</name>
    </author>
    <link href="http://arxiv.org/abs/1706.02293v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.02293v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.02964v1</id>
    <updated>2017-06-09T14:11:22Z</updated>
    <published>2017-06-09T14:11:22Z</published>
    <title>A modulation property of time-frequency derivatives of filtered phase
  and its application to aperiodicity and fo estimation</title>
    <summary>  We introduce a simple and linear SNR (strictly speaking, periodic to random
power ratio) estimator (0dB to 80dB without additional
calibration/linearization) for providing reliable descriptions of aperiodicity
in speech corpus. The main idea of this method is to estimate the background
random noise level without directly extracting the background noise. The
proposed method is applicable to a wide variety of time windowing functions
with very low sidelobe levels. The estimate combines the frequency derivative
and the time-frequency derivative of the mapping from filter center frequency
to the output instantaneous frequency. This procedure can replace the
periodicity detection and aperiodicity estimation subsystems of recently
introduced open source vocoder, YANG vocoder. Source code of MATLAB
implementation of this method will also be open sourced.
</summary>
    <author>
      <name>Hideki Kawahara</name>
    </author>
    <author>
      <name>Ken-Ichi Sakakibara</name>
    </author>
    <author>
      <name>Masanori Morise</name>
    </author>
    <author>
      <name>Hideki Banno</name>
    </author>
    <author>
      <name>Tomoki Toda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages 9 figures, Submitted and accepted in Interspeech2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.02964v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.02964v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.04924v1</id>
    <updated>2017-06-15T15:24:44Z</updated>
    <published>2017-06-15T15:24:44Z</published>
    <title>Investigating the Potential of Pseudo Quadrature Mirror Filter-Banks in
  Music Source Separation Tasks</title>
    <summary>  Estimating audio and musical signals from single channel mixtures often, if
not always, involves a transformation of the mixture signal to the
time-frequency (T-F) domain in which a masking operation takes place. Masking
is realized as an element-wise multiplication of the mixture signal's T-F
representation with a ratio of computed sources' spectrogram. Studies have
shown that the performance of the overall source estimation scheme is subject
to the sparsity and disjointness properties of a given T-F representation. In
this work we investigate the potential of an optimized pseudo quadrature mirror
filter-bank (PQMF), as a T-F representation for music source separation tasks.
Experimental results, suggest that the PQMF maintains the aforementioned
desirable properties and can be regarded as an alternative for representing
mixtures of musical signals.
</summary>
    <author>
      <name>Stylianos Ioannis Mimilakis</name>
    </author>
    <author>
      <name>Gerald Schuller</name>
    </author>
    <link href="http://arxiv.org/abs/1706.04924v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.04924v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.07162v1</id>
    <updated>2017-06-22T04:13:43Z</updated>
    <published>2017-06-22T04:13:43Z</published>
    <title>A Wavenet for Speech Denoising</title>
    <summary>  Currently, most speech processing techniques use magnitude spectrograms as
front-end and are therefore by default discarding part of the signal: the
phase. In order to overcome this limitation, we propose an end-to-end learning
method for speech denoising based on Wavenet. The proposed model adaptation
retains Wavenet's powerful acoustic modeling capabilities, while significantly
reducing its time-complexity by eliminating its autoregressive nature.
Specifically, the model makes use of non-causal, dilated convolutions and
predicts target fields instead of a single target sample. These modifications
make the model highly parallelizable during both training and inference.
Furthermore, we propose a novel energy-conserving loss that directly operates
on the raw audio level. This loss also considers the quality of the estimated
background-noise signal (computed by applying a parameterless operation to the
input) during training. This direct link to the input enforces conserving the
energy of the signal throughout the pipeline. Both computational and perceptual
evaluations indicate that the proposed method is preferred to Wiener filtering,
a common method based on processing the magnitude spectrogram.
</summary>
    <author>
      <name>Dario Rethage</name>
    </author>
    <author>
      <name>Jordi Pons</name>
    </author>
    <author>
      <name>Xavier Serra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Code: https://github.com/drethage/speech-denoising-wavenet - Audio
  examples: http://jordipons.me/apps/speech-denoising-wavenet/</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.07162v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.07162v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.07326v1</id>
    <updated>2017-06-22T14:00:35Z</updated>
    <published>2017-06-22T14:00:35Z</published>
    <title>A universal negative group delay filter for the prediction of
  band-limited signals</title>
    <summary>  A filter for universal real-time prediction of band-limited signals is
presented. The filter consists of multiple time-delayed feedback terms in order
to accomplish anticipatory coupling, which again leads to a negative group
delay for frequencies in the baseband. The universality of the filter arises
from its property that it does not rely on a specific model of the signal.
Specifically, as long as the signal to be predicted is band-limited with a
known cutoff frequency, the filter order, the only parameter of the filter,
follows and the filter predicts the signal in real time up to a prediction
horizon that depends on the cutoff frequency, too. It is worked out in detail
how signal prediction arises from the negative group delay of the filter. Its
properties, including stability, are investigated theoretically, by numerical
simulations, and by application to a physiological signal. Possible control and
signal processing applications of this filter are discussed.
</summary>
    <author>
      <name>Henning U. Voss</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to IEEE-TSP</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.07326v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.07326v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.07859v1</id>
    <updated>2017-06-22T04:33:59Z</updated>
    <published>2017-06-22T04:33:59Z</published>
    <title>Deep Speaker Verification: Do We Need End to End?</title>
    <summary>  End-to-end learning treats the entire system as a whole adaptable black box,
which, if sufficient data are available, may learn a system that works very
well for the target task. This principle has recently been applied to several
prototype research on speaker verification (SV), where the feature learning and
classifier are learned together with an objective function that is consistent
with the evaluation metric. An opposite approach to end-to-end is feature
learning, which firstly trains a feature learning model, and then constructs a
back-end classifier separately to perform SV. Recently, both approaches
achieved significant performance gains on SV, mainly attributed to the smart
utilization of deep neural networks. However, the two approaches have not been
carefully compared, and their respective advantages have not been well
discussed. In this paper, we compare the end-to-end and feature learning
approaches on a text-independent SV task. Our experiments on a dataset sampled
from the Fisher database and involving 5,000 speakers demonstrated that the
feature learning approach outperformed the end-to-end approach. This is a
strong support for the feature learning approach, at least with data and
computation resources similar to ours.
</summary>
    <author>
      <name>Dong Wang</name>
    </author>
    <author>
      <name>Lantian Li</name>
    </author>
    <author>
      <name>Zhiyuan Tang</name>
    </author>
    <author>
      <name>Thomas Fang Zheng</name>
    </author>
    <link href="http://arxiv.org/abs/1706.07859v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.07859v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.07860v1</id>
    <updated>2017-06-22T04:26:39Z</updated>
    <published>2017-06-22T04:26:39Z</published>
    <title>Speaker Recognition with Cough, Laugh and "Wei"</title>
    <summary>  This paper proposes a speaker recognition (SRE) task with trivial speech
events, such as cough and laugh. These trivial events are ubiquitous in
conversations and less subjected to intentional change, therefore offering
valuable particularities to discover the genuine speaker from disguised speech.
However, trivial events are often short and idiocratic in spectral patterns,
making SRE extremely difficult. Fortunately, we found a very powerful deep
feature learning structure that can extract highly speaker-sensitive features.
By employing this tool, we studied the SRE performance on three types of
trivial events: cough, laugh and "Wei" (a short Chinese "Hello"). The results
show that there is rich speaker information within these trivial events, even
for cough that is intuitively less speaker distinguishable. With the deep
feature approach, the EER can reach 10%-14% with the three trivial events,
despite their extremely short durations (0.2-1.0 seconds).
</summary>
    <author>
      <name>Miao Zhang</name>
    </author>
    <author>
      <name>Yixiang Chen</name>
    </author>
    <author>
      <name>Lantian Li</name>
    </author>
    <author>
      <name>Dong Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1706.07860v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.07860v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.07861v1</id>
    <updated>2017-06-22T04:32:40Z</updated>
    <published>2017-06-22T04:32:40Z</published>
    <title>Cross-lingual Speaker Verification with Deep Feature Learning</title>
    <summary>  Existing speaker verification (SV) systems often suffer from performance
degradation if there is any language mismatch between model training, speaker
enrollment, and test. A major cause of this degradation is that most existing
SV methods rely on a probabilistic model to infer the speaker factor, so any
significant change on the distribution of the speech signal will impact the
inference. Recently, we proposed a deep learning model that can learn how to
extract the speaker factor by a deep neural network (DNN). By this feature
learning, an SV system can be constructed with a very simple back-end model. In
this paper, we investigate the robustness of the feature-based SV system in
situations with language mismatch. Our experiments were conducted on a complex
cross-lingual scenario, where the model training was in English, and the
enrollment and test were in Chinese or Uyghur. The experiments demonstrated
that the feature-based system outperformed the i-vector system with a large
margin, particularly with language mismatch between enrollment and test.
</summary>
    <author>
      <name>Lantian Li</name>
    </author>
    <author>
      <name>Dong Wang</name>
    </author>
    <author>
      <name>Askar Rozi</name>
    </author>
    <author>
      <name>Thomas Fang Zheng</name>
    </author>
    <link href="http://arxiv.org/abs/1706.07861v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.07861v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.07927v1</id>
    <updated>2017-06-24T08:50:20Z</updated>
    <published>2017-06-24T08:50:20Z</published>
    <title>A Variational EM Method for Pole-Zero Modeling of Speech with Mixed
  Block Sparse and Gaussian Excitation</title>
    <summary>  The modeling of speech can be used for speech synthesis and speech
recognition. We present a speech analysis method based on pole-zero modeling of
speech with mixed block sparse and Gaussian excitation. By using a pole-zero
model, instead of the all-pole model, a better spectral fitting can be
expected. Moreover, motivated by the block sparse glottal flow excitation
during voiced speech and the white noise excitation for unvoiced speech, we
model the excitation sequence as a combination of block sparse signals and
white noise. A variational EM (VEM) method is proposed for estimating the
posterior PDFs of the block sparse residuals and point estimates of mod- elling
parameters within a sparse Bayesian learning framework. Compared to
conventional pole-zero and all-pole based methods, experimental results show
that the proposed method has lower spectral distortion and good performance in
reconstructing of the block sparse excitation.
</summary>
    <author>
      <name>Liming Shi</name>
    </author>
    <author>
      <name>Jesper Kjær Nielsen</name>
    </author>
    <author>
      <name>Jesper Rindom Jensen</name>
    </author>
    <author>
      <name>Mads Græsbøll Christensen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in the 25th European Signal Processing Conference (EUSIPCO
  2017), published by EUROSIP, scheduled for Aug. 28 - Sep. 2 in Kos island,
  Greece</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.07927v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.07927v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.08231v1</id>
    <updated>2017-06-26T04:57:06Z</updated>
    <published>2017-06-26T04:57:06Z</published>
    <title>Between Homomorphic Signal Processing and Deep Neural Networks:
  Constructing Deep Algorithms for Polyphonic Music Transcription</title>
    <summary>  This paper presents a new approach in understanding how deep neural networks
(DNNs) work by applying homomorphic signal processing techniques. Focusing on
the task of multi-pitch estimation (MPE), this paper demonstrates the
equivalence relation between a generalized cepstrum and a DNN in terms of their
structures and functionality. Such an equivalence relation, together with pitch
perception theories and the recently established
rectified-correlations-on-a-sphere (RECOS) filter analysis, provide an
alternative way in explaining the role of the nonlinear activation function and
the multi-layer structure, both of which exist in a cepstrum and a DNN. To
validate the efficacy of this new approach, a new feature designed in the same
fashion is proposed for pitch salience function. The new feature outperforms
the one-layer spectrum in the MPE task and, as predicted, it addresses the
issue of the missing fundamental effect and also achieves better robustness to
noise.
</summary>
    <author>
      <name>Li Su</name>
    </author>
    <link href="http://arxiv.org/abs/1706.08231v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.08231v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.08612v1</id>
    <updated>2017-06-26T21:42:27Z</updated>
    <published>2017-06-26T21:42:27Z</published>
    <title>VoxCeleb: a large-scale speaker identification dataset</title>
    <summary>  Most existing datasets for speaker identification contain samples obtained
under quite constrained conditions, and are usually hand-annotated, hence
limited in size. The goal of this paper is to generate a large scale
text-independent speaker identification dataset collected 'in the wild'. We
make two contributions. First, we propose a fully automated pipeline based on
computer vision techniques to create the dataset from open-source media. Our
pipeline involves obtaining videos from YouTube; performing active speaker
verification using a two-stream synchronization Convolutional Neural Network
(CNN), and confirming the identity of the speaker using CNN based facial
recognition. We use this pipeline to curate VoxCeleb which contains hundreds of
thousands of 'real world' utterances for over 1,000 celebrities. Our second
contribution is to apply and compare various state of the art speaker
identification techniques on our dataset to establish baseline performance. We
show that a CNN based architecture obtains the best performance for both
identification and verification.
</summary>
    <author>
      <name>Arsha Nagrani</name>
    </author>
    <author>
      <name>Joon Son Chung</name>
    </author>
    <author>
      <name>Andrew Zisserman</name>
    </author>
    <link href="http://arxiv.org/abs/1706.08612v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.08612v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.08759v1</id>
    <updated>2017-06-27T10:03:52Z</updated>
    <published>2017-06-27T10:03:52Z</published>
    <title>Impulsive Sound Detection by a Novel Energy Formula and its Usage for
  Gunshot Recognition</title>
    <summary>  There are many methods proposed for the detection of impulsive sounds in
literature. Most of them are complex and require adaptation to ambient noise.
In this paper we propose a very simple and efficient method to detect impulsive
sounds. Although we use energy like most of the others to determine impulsive
sounds, the way we calculate the energy is quite different. Also our
calculation is immune to ambient noise and does not require any limit or
adaptation. We could detect impulsive sounds embedded in various kinds of
noises by using this formula.
  As our ultimate aim is to detect gunshots, next phase of impulsive sound
detection is gunshot recognition phase. Detected impulsive sounds are fed into
recognition phase in which we can decide on gunshots with high success rate.
</summary>
    <author>
      <name>Yüksel Arslan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.08759v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.08759v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09055v1</id>
    <updated>2017-06-27T21:28:31Z</updated>
    <published>2017-06-27T21:28:31Z</published>
    <title>Acoustic Modeling Using a Shallow CNN-HTSVM Architecture</title>
    <summary>  High-accuracy speech recognition is especially challenging when large
datasets are not available. It is possible to bridge this gap with careful and
knowledge-driven parsing combined with the biologically inspired CNN and the
learning guarantees of the Vapnik Chervonenkis (VC) theory. This work presents
a Shallow-CNN-HTSVM (Hierarchical Tree Support Vector Machine classifier)
architecture which uses a predefined knowledge-based set of rules with
statistical machine learning techniques. Here we show that gross errors present
even in state-of-the-art systems can be avoided and that an accurate acoustic
model can be built in a hierarchical fashion. The CNN-HTSVM acoustic model
outperforms traditional GMM-HMM models and the HTSVM structure outperforms a
MLP multi-class classifier. More importantly we isolate the performance of the
acoustic model and provide results on both the frame and phoneme level
considering the true robustness of the model. We show that even with a small
amount of data accurate and robust recognition rates can be obtained.
</summary>
    <author>
      <name>Christopher Dane Shulby</name>
    </author>
    <author>
      <name>Martha Dais Ferreira</name>
    </author>
    <author>
      <name>Rodrigo F. de Mello</name>
    </author>
    <author>
      <name>Sandra Maria Aluisio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Pre-review version of Bracis 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.09055v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09055v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09088v2</id>
    <updated>2017-06-29T02:33:06Z</updated>
    <published>2017-06-28T00:46:50Z</published>
    <title>Modeling Musical Context with Word2vec</title>
    <summary>  We present a semantic vector space model for capturing complex polyphonic
musical context. A word2vec model based on a skip-gram representation with
negative sampling was used to model slices of music from a dataset of
Beethoven's piano sonatas. A visualization of the reduced vector space using
t-distributed stochastic neighbor embedding shows that the resulting embedded
vector space captures tonal relationships, even without any explicit
information about the musical contents of the slices. Secondly, an excerpt of
the Moonlight Sonata from Beethoven was altered by replacing slices based on
context similarity. The resulting music shows that the selected slice based on
similar word2vec context also has a relatively short tonal distance from the
original slice.
</summary>
    <author>
      <name>Dorien Herremans</name>
    </author>
    <author>
      <name>Ching-Hua Chuan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the First International Conference on Deep Learning
  and Music, Anchorage, US, May, 2017 (arXiv:1706.08675v1 [cs.NE])</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the First International Workshop on Deep Learning
  and Music joint with IJCNN. Anchorage, US. 1(1). pp 11-18 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1706.09088v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09088v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3; H.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
