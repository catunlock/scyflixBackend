<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acs.IR%26id_list%3D%26start%3D0%26max_results%3D500" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:cs.IR&amp;id_list=&amp;start=0&amp;max_results=500</title>
  <id>http://arxiv.org/api/V16oEAkWrkG4vj1MWZPidXNE/LY</id>
  <updated>2017-10-08T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">3832</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">500</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/0710.1962v1</id>
    <updated>2007-10-10T10:03:03Z</updated>
    <published>2007-10-10T10:03:03Z</published>
    <title>Stanford Matrix Considered Harmful</title>
    <summary>  This note argues about the validity of web-graph data used in the literature.
</summary>
    <author>
      <name>Sebastiano Vigna</name>
    </author>
    <link href="http://arxiv.org/abs/0710.1962v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.1962v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0910.1869v2</id>
    <updated>2009-10-27T07:09:51Z</updated>
    <published>2009-10-09T21:42:58Z</published>
    <title>Management Of Volatile Information In Incremental Web Crawler</title>
    <summary>  Paper has been withdrawn.
</summary>
    <author>
      <name>Ravita Chahar</name>
    </author>
    <author>
      <name>Komal Hooda</name>
    </author>
    <author>
      <name>Annu Dhankhar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper has been withdrawn</arxiv:comment>
    <link href="http://arxiv.org/abs/0910.1869v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0910.1869v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0702067v1</id>
    <updated>2007-02-10T21:26:05Z</updated>
    <published>2007-02-10T21:26:05Z</published>
    <title>The Haar Wavelet Transform of a Dendrogram: Additional Notes</title>
    <summary>  We consider the wavelet transform of a finite, rooted, node-ranked, $p$-way
tree, focusing on the case of binary ($p = 2$) trees. We study a Haar wavelet
transform on this tree. Wavelet transforms allow for multiresolution analysis
through translation and dilation of a wavelet function. We explore how this
works in our tree context.
</summary>
    <author>
      <name>Fionn Murtagh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">37 pp, 1 fig. Supplementary material to "The Haar Wavelet Transform
  of a Dendrogram", http://arxiv.org/abs/cs.IR/0608107</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0702067v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0702067v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.3; H.3.1; I.1.m; I.7.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0402061v1</id>
    <updated>2004-02-27T14:13:01Z</updated>
    <published>2004-02-27T14:13:01Z</published>
    <title>A Correlation-Based Distance</title>
    <summary>  In this short technical report, we define on the sample space R^D a distance
between data points which depends on their correlation. We also derive an
expression for the center of mass of a set of points with respect to this
distance.
</summary>
    <author>
      <name>Jean-Luc Falcone</name>
    </author>
    <author>
      <name>Paul Albuquerque</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0402061v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0402061v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0410055v1</id>
    <updated>2004-10-21T15:51:20Z</updated>
    <published>2004-10-21T15:51:20Z</published>
    <title>Mathematical knowledge management is needed</title>
    <summary>  In this lecture I discuss some aspects of MKM, Mathematical Knowledge
Management, with particuar emphasis on information storage and information
retrieval.
</summary>
    <author>
      <name>Michiel Hazewinkel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Keynote speech at the November, 2003 MKM meeting ar Herriott-Watt,
  Edinburg, UK. Nine pages, one figure</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0410055v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0410055v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.0104v1</id>
    <updated>2014-08-30T11:39:32Z</updated>
    <published>2014-08-30T11:39:32Z</published>
    <title>Marginalizing over the PageRank Damping Factor</title>
    <summary>  In this note, we show how to marginalize over the damping parameter of the
PageRank equation so as to obtain a parameter-free version known as TotalRank.
Our discussion is meant as a reference and intended to provide a guided tour
towards an interesting result that has applications in information retrieval
and classification.
</summary>
    <author>
      <name>Christian Bauckhage</name>
    </author>
    <link href="http://arxiv.org/abs/1409.0104v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.0104v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.01649v1</id>
    <updated>2015-09-05T00:52:43Z</updated>
    <published>2015-09-05T00:52:43Z</published>
    <title>Using of Neuro-Indexes</title>
    <summary>  The article describes a new data structure called neuro-index. It is an
alternative to well-known file indexes. The neuro-index is fundamentally
different because it stores weight coefficients in neural network. It is not a
reference type like "keyword-position in a file".
</summary>
    <author>
      <name>Valerii Garnaga</name>
    </author>
    <link href="http://arxiv.org/abs/1509.01649v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.01649v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0003001v1</id>
    <updated>2000-03-01T18:11:08Z</updated>
    <published>2000-03-01T18:11:08Z</published>
    <title>Making news understandable to computers</title>
    <summary>  Computers and devices are largely unaware of events taking place in the
world. This could be changed if news were made available in a
computer-understandable form. In this paper we present XML documents called
NewsForms that represent the key points of 17 types of news events. We discuss
the benefits of computer-understandable news and present the NewsExtract
program for converting text news stories into NewsForms.
</summary>
    <author>
      <name>Erik T. Mueller</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0003001v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0003001v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.7.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0110026v1</id>
    <updated>2001-10-10T15:28:00Z</updated>
    <published>2001-10-10T15:28:00Z</published>
    <title>Information retrieval in Current Research Information Systems</title>
    <summary>  In this paper we describe the requirements for research information systems
and problems which arise in the development of such system. Here is shown which
problems could be solved by using of knowledge markup technologies. Ontology
for Research Information System offered. Architecture for collecting research
data and providing access to it is described.
</summary>
    <author>
      <name>Andrei Lopatenko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, ontology description included, position paper at the
  Workshop on Knowledge Markup and Semantic Annotation at K-CAP'2001</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0110026v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0110026v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; H.3.4; H.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0601103v1</id>
    <updated>2006-01-24T10:23:15Z</updated>
    <published>2006-01-24T10:23:15Z</published>
    <title>Google Web APIs - an Instrument for Webometric Analyses?</title>
    <summary>  This paper introduces Google Web APIs (Google APIs) as an instrument and
playground for webometric studies. Several examples of Google APIs
implementations are given. Our examples show that this Google Web Service can
be used successfully for informetric Internet based studies albeit with some
restrictions.
</summary>
    <author>
      <name>Philipp Mayr</name>
    </author>
    <author>
      <name>Fabio Tosques</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 2 figures, 10th International Conference of the
  International Society for Scientometrics and Informetrics</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0601103v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0601103v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0705.0751v1</id>
    <updated>2007-05-05T17:27:42Z</updated>
    <published>2007-05-05T17:27:42Z</published>
    <title>Approximate textual retrieval</title>
    <summary>  An approximate textual retrieval algorithm for searching sources with high
levels of defects is presented. It considers splitting the words in a query
into two overlapping segments and subsequently building composite regular
expressions from interlacing subsets of the segments. This procedure reduces
the probability of missed occurrences due to source defects, yet diminishes the
retrieval of irrelevant, non-contextual occurrences.
</summary>
    <author>
      <name>Pere Constans</name>
    </author>
    <link href="http://arxiv.org/abs/0705.0751v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0705.0751v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0812.4542v3</id>
    <updated>2009-09-15T14:52:52Z</updated>
    <published>2008-12-24T15:41:48Z</published>
    <title>Assessing scientific research performance and impact with single indices</title>
    <summary>  We provide a comprehensive and critical review of the h-index and its most
important modifications proposed in the literature, as well as of other similar
indicators measuring research output and impact. Extensions of some of these
indices are presented and illustrated.
</summary>
    <author>
      <name>John Panaretos</name>
    </author>
    <author>
      <name>Chrisovaladis Malesios</name>
    </author>
    <link href="http://arxiv.org/abs/0812.4542v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0812.4542v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0911.5378v1</id>
    <updated>2009-11-28T06:25:00Z</updated>
    <published>2009-11-28T06:25:00Z</published>
    <title>De la recherche sociale d'information à la recherche collaborative
  d'information</title>
    <summary>  In this paper, we explain social information retrieval (SIR) and
collaborative information retrieval (CIR). We see SIR as a way of knowing who
to collaborate with in resolving an information problem while CIR entails the
process of mutual understanding and solving of an information problem among
collaborators. We are interested in the transition from SIR to CIR hence we
developed a communication model to facilitate knowledge sharing during CIR.
</summary>
    <author>
      <name>Victor Odumuyiwa</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LORIA</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">7\`eme colloque du chapitre fran\c{c}ais de l'ISKO, Lyon : France
  (2009)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0911.5378v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.5378v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.4458v1</id>
    <updated>2010-06-23T09:50:06Z</updated>
    <published>2010-06-23T09:50:06Z</published>
    <title>Few Algorithms for ascertaining merit of a document and their
  applications</title>
    <summary>  Existing models for ranking documents(mostly in world wide web) are prestige
based. In this article, three algorithms to objectively judge the merit of a
document are proposed - 1) Citation graph maxflow 2) Recursive Gloss Overlap
based intrinsic merit scoring and 3) Interview algorithm. A short discussion on
generic judgement and its mathematical treatment is presented in introduction
to motivate these algorithms.
</summary>
    <author>
      <name>Ka. Shrinivaasan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1006.4458v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.4458v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1101.0766v1</id>
    <updated>2011-01-04T16:59:24Z</updated>
    <published>2011-01-04T16:59:24Z</published>
    <title>Information Retrieval of Jumbled Words</title>
    <summary>  It is known that humans can easily read words where the letters have been
jumbled in a certain way. This paper examines this problem by associating a
distance measure with the jumbling process. Modifications to text were
generated according to the Damerau-Levenshtein distance and it was checked if
the users are able to read it. Graphical representations of the results are
provided.
</summary>
    <author>
      <name>Venkata Ravinder Paruchuri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1101.0766v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1101.0766v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1101.1639v1</id>
    <updated>2011-01-09T14:42:04Z</updated>
    <published>2011-01-09T14:42:04Z</published>
    <title>Applying Science Models for Search</title>
    <summary>  The paper proposes three different kinds of science models as value-added
services that are integrated in the retrieval process to enhance retrieval
quality. The paper discusses the approaches Search Term Recommendation,
Bradfordizing and Author Centrality on a general level and addresses
implementation issues of the models within a real-life retrieval environment.
</summary>
    <author>
      <name>Philipp Mayr</name>
    </author>
    <author>
      <name>Peter Mutschke</name>
    </author>
    <author>
      <name>Vivien Petras</name>
    </author>
    <author>
      <name>Philipp Schaer</name>
    </author>
    <author>
      <name>York Sure</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 3 figures, ISI 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1101.1639v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1101.1639v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1101.3400v1</id>
    <updated>2011-01-18T08:39:12Z</updated>
    <published>2011-01-18T08:39:12Z</published>
    <title>Behavioral On-Line Advertising</title>
    <summary>  We present a new algorithm for behavioral targeting of banner advertisements.
We record different user's actions such as clicks, search queries and page
views. We use the collected information on the user to estimate in real time
the probability of a click on a banner. A banner is displayed if it either has
the highest probability of being clicked or if it is the one that generates the
highest average profit.
</summary>
    <author>
      <name>Fabrizio Caruso</name>
    </author>
    <author>
      <name>Giovanni Giuffrida</name>
    </author>
    <author>
      <name>Calogero Zarba</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1101.3400v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1101.3400v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1102.3306v1</id>
    <updated>2011-02-16T11:16:51Z</updated>
    <published>2011-02-16T11:16:51Z</published>
    <title>Efficient Error-Correcting Geocoding</title>
    <summary>  We study the problem of resolving a perhaps misspelled address of a location
into geographic coordinates of latitude and longitude. Our data structure
solves this problem within a few milliseconds even for misspelled and
fragmentary queries. Compared to major geographic search engines such as Google
or Bing we achieve results of significantly better quality.
</summary>
    <author>
      <name>Christian Jung</name>
    </author>
    <author>
      <name>Daniel Karch</name>
    </author>
    <author>
      <name>Sebastian Knopp</name>
    </author>
    <author>
      <name>Dennis Luxen</name>
    </author>
    <author>
      <name>Peter Sanders</name>
    </author>
    <link href="http://arxiv.org/abs/1102.3306v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1102.3306v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; E.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.3286v2</id>
    <updated>2012-09-17T18:53:14Z</updated>
    <published>2012-09-14T18:59:03Z</published>
    <title>Music Recommendation System for Million Song Dataset Challenge</title>
    <summary>  In this paper a system that took 8th place in Million Song Dataset challenge
is described. Given full listening history for 1 million of users and half of
listening history for 110000 users participatints should predict the missing
half. The system proposed here uses memory-based collaborative filtering
approach and user-based similarity. MAP@500 score of 0.15037 was achieved.
</summary>
    <author>
      <name>Nikolay Glazyrin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.3286v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.3286v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.0320v1</id>
    <updated>2012-11-01T22:16:57Z</updated>
    <published>2012-11-01T22:16:57Z</published>
    <title>TrackMeNot-so-good-after-all</title>
    <summary>  TrackMeNot is a Firefox plugin with laudable intentions - protecting your
privacy. By issuing a customizable stream of random search queries on its
users' behalf, TrackMeNot surmises that enough search noise will prevent its
users' true query profiles from being discerned. However, we find that
clustering queries by semantic relatedness allows us to disentangle a
nontrivial subset of true user queries from TrackMeNot issued noise.
</summary>
    <author>
      <name>Rami Al-Rfou'</name>
    </author>
    <author>
      <name>William Jannen</name>
    </author>
    <author>
      <name>Nikhil Patwardhan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1211.0320v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.0320v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.0074v1</id>
    <updated>2012-12-01T07:01:27Z</updated>
    <published>2012-12-01T07:01:27Z</published>
    <title>Challenges in Kurdish Text Processing</title>
    <summary>  Despite having a large number of speakers, the Kurdish language is among the
less-resourced languages. In this work we highlight the challenges and problems
in providing the required tools and techniques for processing texts written in
Kurdish. From a high-level perspective, the main challenges are: the inherent
diversity of the language, standardization and segmentation issues, and the
lack of language resources.
</summary>
    <author>
      <name>Kyumars Sheykh Esmaili</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.0074v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.0074v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.3906v1</id>
    <updated>2012-12-17T07:13:50Z</updated>
    <published>2012-12-17T07:13:50Z</published>
    <title>Simple Search Engine Model: Adaptive Properties</title>
    <summary>  In this paper we study the relationship between query and search engine by
exploring the adaptive properties based on a simple search engine. We used set
theory and utilized the words and terms for defining singleton space of event
in a search engine model, and then provided the inclusion between one singleton
to another.
</summary>
    <author>
      <name>Mahyuddin K. M. Nasution</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, noting, draf</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.3906v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.3906v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.4702v1</id>
    <updated>2012-12-19T15:18:09Z</updated>
    <published>2012-12-19T15:18:09Z</published>
    <title>Simple Search Engine Model: Adaptive Properties for Doubleton</title>
    <summary>  In this paper we study the relationship between query and search engine by
exploring the adaptive properties for doubleton as a space of event based on a
simple search engine. We employ set theory for defining doubleton and generate
some properties.
</summary>
    <author>
      <name>Mahyuddin K. M. Nasution</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, nothing, a draf</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.4702v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.4702v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.5633v1</id>
    <updated>2012-12-21T23:20:24Z</updated>
    <published>2012-12-21T23:20:24Z</published>
    <title>Design, implementation and experiment of a YeSQL Web Crawler</title>
    <summary>  We describe a novel, "focusable", scalable, distributed web crawler based on
GNU/Linux and PostgreSQL that we designed to be easily extendible and which we
have released under a GNU public licence. We also report a first use case
related to an analysis of Twitter's streams about the french 2012 presidential
elections and the URL's it contains.
</summary>
    <author>
      <name>Pierre Joulin</name>
    </author>
    <author>
      <name>Romain Deveaud</name>
    </author>
    <author>
      <name>Eric SanJuan-Ibekwe</name>
    </author>
    <author>
      <name>Jean-Marc Francony</name>
    </author>
    <author>
      <name>Françoise Para</name>
    </author>
    <link href="http://arxiv.org/abs/1212.5633v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.5633v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.6580v1</id>
    <updated>2013-02-26T20:55:42Z</updated>
    <published>2013-02-26T20:55:42Z</published>
    <title>Finding the Right Set of Users: Generalized Constraints for Group
  Recommendations</title>
    <summary>  Recently, group recommendations have attracted considerable attention. Rather
than recommending items to individual users, group recommenders recommend items
to groups of users. In this position paper, we introduce the problem of forming
an appropriate group of users to recommend an item when constraints apply to
the members of the group. We present a formal model of the problem and an
algorithm for its solution. Finally, we identify several directions for future
work.
</summary>
    <author>
      <name>Kostas Stefanidis</name>
    </author>
    <author>
      <name>Evaggelia Pitoura</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PersDB 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1302.6580v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.6580v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.3964v1</id>
    <updated>2013-03-16T10:21:33Z</updated>
    <published>2013-03-16T10:21:33Z</published>
    <title>Simple Search Engine Model: Selective Properties</title>
    <summary>  In this paper we study the relationship between query and search engine by
exploring the selective properties based on a simple search engine. We used the
set theory and utilized the words and terms for defining singleton and
doubleton in the event spaces and then provided their implementation for
proving the existence of the shadow of micro-cluster.
</summary>
    <author>
      <name>Mahyuddin K. M. Nasution</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1303.3964v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.3964v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.3563v1</id>
    <updated>2013-04-12T08:04:31Z</updated>
    <published>2013-04-12T08:04:31Z</published>
    <title>Data, text and web mining for business intelligence: a survey</title>
    <summary>  The Information and Communication Technologies revolution brought a digital
world with huge amounts of data available. Enterprises use mining technologies
to search vast amounts of data for vital insight and knowledge. Mining tools
such as data mining, text mining, and web mining are used to find hidden
knowledge in large databases or the Internet.
</summary>
    <author>
      <name>Abdul-Aziz Rashid Al-Azmi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijdkp.2013.3201</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijdkp.2013.3201" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 page, journal paper</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Data Mining &amp; Knowledge Management
  Process (IJDKP) Vol.3, No.2, March 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1304.3563v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.3563v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.1114v1</id>
    <updated>2013-05-06T08:42:19Z</updated>
    <published>2013-05-06T08:42:19Z</published>
    <title>Towards User Profile Modelling in Recommender System</title>
    <summary>  The notion of profile appeared in the 1970s decade, which was mainly due to
the need to create custom applications that could be adapted to the user. In
this paper, we treat the different aspects of the user's profile, defining it,
profile, its features and its indicators of interest, and then we describe the
different approaches of modelling and acquiring the user's interests.
</summary>
    <author>
      <name>Djallel Bouneffouf</name>
    </author>
    <link href="http://arxiv.org/abs/1305.1114v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.1114v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.1745v1</id>
    <updated>2013-05-08T08:38:04Z</updated>
    <published>2013-05-08T08:38:04Z</published>
    <title>Mobile Recommender Systems Methods: An Overview</title>
    <summary>  The information that mobiles can access becomes very wide nowadays, and the
user is faced with a dilemma: there is an unlimited pool of information
available to him but he is unable to find the exact information he is looking
for. This is why the current research aims to design Recommender Systems (RS)
able to continually send information that matches the user's interests in order
to reduce his navigation time. In this paper, we treat the different approaches
to recommend.
</summary>
    <author>
      <name>Djallel Bouneffouf</name>
    </author>
    <link href="http://arxiv.org/abs/1305.1745v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.1745v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.1787v1</id>
    <updated>2013-05-08T11:52:39Z</updated>
    <published>2013-05-08T11:52:39Z</published>
    <title>Evolution of the user's content: An Overview of the state of the art</title>
    <summary>  The evolution of the user's content still remains a problem for an accurate
recommendation.This is why the current research aims to design Recommender
Systems (RS) able to continually adapt information that matches the user's
interests. This paper aims to explain this problematic point in outlining the
proposals that have been made in research with their advantages and
disadvantages.
</summary>
    <author>
      <name>Djallel Bouneffouf</name>
    </author>
    <link href="http://arxiv.org/abs/1305.1787v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.1787v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.5330v1</id>
    <updated>2013-05-23T06:49:20Z</updated>
    <published>2013-05-23T06:49:20Z</published>
    <title>A toy model of information retrieval system based on quantum probability</title>
    <summary>  Recent numerical results show that non-Bayesian knowledge revision may be
helpful in search engine training and optimization. In order to demonstrate how
basic assumption about about the physical nature (and hence the observed
statistics) of retrieved documents can affect the performance of search engines
we suggest an idealized toy model with minimal number of parameters.
</summary>
    <author>
      <name>Roman Zapatrin</name>
    </author>
    <link href="http://arxiv.org/abs/1305.5330v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.5330v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.7014v1</id>
    <updated>2013-05-30T06:35:52Z</updated>
    <published>2013-05-30T06:35:52Z</published>
    <title>Tweets Miner for Stock Market Analysis</title>
    <summary>  In this paper, we present a software package for the data mining of Twitter
microblogs for the purpose of using them for the stock market analysis. The
package is written in R langauge using apropriate R packages. The model of
tweets has been considered. We have also compared stock market charts with
frequent sets of keywords in Twitter microblogs messages.
</summary>
    <author>
      <name>Bohdan Pavlyshenko</name>
    </author>
    <link href="http://arxiv.org/abs/1305.7014v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.7014v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.3872v1</id>
    <updated>2013-12-13T16:52:07Z</updated>
    <published>2013-12-13T16:52:07Z</published>
    <title>Eugene Garfield, Francis Narin, and PageRank: The Theoretical Bases of
  the Google Search Engine</title>
    <summary>  This paper presents a test of the validity of using Google Scholar to
evaluate the publications of researchers by comparing the premises on which its
search engine, PageRank, is based, to those of Garfield's theory of citation
indexing. It finds that the premises are identical and that PageRank and
Garfield's theory of citation indexing validate each other.
</summary>
    <author>
      <name>Stephen J. Bensman</name>
    </author>
    <link href="http://arxiv.org/abs/1312.3872v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.3872v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.1270v1</id>
    <updated>2014-02-06T08:29:19Z</updated>
    <published>2014-02-06T08:29:19Z</published>
    <title>Vers une interface pour l enrichissement des requetes en arabe dans un
  systeme de recherche d information</title>
    <summary>  This presentation focuses on the automatic expansion of Arabic request using
morphological analyzer and Arabic Wordnet. The expanded request is sent to
Google.
</summary>
    <author>
      <name>Abderrahim Mohammed El Amine</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, in French</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">CIIA'2009 : 2eme Conference Internationale sur Informatique et ses
  Applications, Saida - Algerie, 03 -04 Mai 2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1402.1270v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.1270v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.1842v1</id>
    <updated>2014-05-08T09:00:25Z</updated>
    <published>2014-05-08T09:00:25Z</published>
    <title>SocRecM: A Scalable Social Recommender Engine for Online Marketplaces</title>
    <summary>  In this paper, we present work-in-progress on SocRecM, a novel social
recommendation framework for online marketplaces. We demonstrate that SocRecM
is not only easy to integrate with existing Web technologies through a RESTful,
scalable and easy-to-extend service-based architecture but also reveal the
extent to which various social features and recommendation approaches are
useful in an online social marketplace environment.
</summary>
    <author>
      <name>Emanuel Lacic</name>
    </author>
    <author>
      <name>Dominik Kowald</name>
    </author>
    <author>
      <name>Christoph Trattner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.1842v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.1842v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.2584v1</id>
    <updated>2014-05-11T21:05:28Z</updated>
    <published>2014-05-11T21:05:28Z</published>
    <title>Sentiment Analysis: A Survey</title>
    <summary>  Sentiment analysis (also known as opinion mining) refers to the use of
natural language processing, text analysis and computational linguistics to
identify and extract subjective information in source materials. Mining
opinions expressed in the user generated content is a challenging yet
practically very useful problem. This survey would cover various approaches and
methodology used in Sentiment Analysis and Opinion Mining in general. The focus
would be on Internet text like, Product review, tweets and other social media.
</summary>
    <author>
      <name>Rahul Tejwani</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University at Buffalo</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1405.2584v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.2584v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.0296v1</id>
    <updated>2014-06-02T09:00:01Z</updated>
    <published>2014-06-02T09:00:01Z</published>
    <title>Using Mobile Agents for Information Retrival in B2B Systems</title>
    <summary>  This paper presents an architecture of an information retrieval system that
use the advantages offered by mobile agents to collect information from
different sources and bring the result to the calling user. Mobile agent
technology will be used for determine the traceability of a product and also
for searching information about a specific entity.
</summary>
    <author>
      <name>Felicia Florentina Giza</name>
    </author>
    <author>
      <name>Cristina Elena Turcu</name>
    </author>
    <author>
      <name>Ovidiu Andrei Schipor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures, in Romanian</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.0296v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.0296v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.2022v1</id>
    <updated>2014-06-08T20:05:36Z</updated>
    <published>2014-06-08T20:05:36Z</published>
    <title>Two-dimensional Sentiment Analysis of text</title>
    <summary>  Sentiment Analysis aims to get the underlying viewpoint of the text, which
could be anything that holds a subjective opinion, such as an online review,
Movie rating, Comments on Blog posts etc. This paper presents a novel approach
that classify text in two-dimensional Emotional space, based on the sentiments
of the author. The approach uses existing lexical resources to extract feature
set, which is trained using Supervised Learning techniques.
</summary>
    <author>
      <name>Rahul Tejwani</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University at Buffalo</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">sentiment analysis, two-dimensional</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.2022v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.2022v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.7357v1</id>
    <updated>2014-07-28T09:00:57Z</updated>
    <published>2014-07-28T09:00:57Z</published>
    <title>Text Classification Using Association Rules, Dependency Pruning and
  Hyperonymization</title>
    <summary>  We present new methods for pruning and enhancing item- sets for text
classification via association rule mining. Pruning methods are based on
dependency syntax and enhancing methods are based on replacing words by their
hyperonyms of various orders. We discuss the impact of these methods, compared
to pruning based on tfidf rank of words.
</summary>
    <author>
      <name>Yannis Haralambous</name>
    </author>
    <author>
      <name>Philippe Lenca</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 2 figures, presented at DMNLP 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.7357v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.7357v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.1260v1</id>
    <updated>2014-08-06T12:36:23Z</updated>
    <published>2014-08-06T12:36:23Z</published>
    <title>Unstable markup: A template-based information extraction from web sites
  with unstable markup</title>
    <summary>  This paper presents results of a work on crawling CEUR Workshop proceedings
web site to a Linked Open Data (LOD) dataset in the framework of ESWC 2014
Semantic Publishing Challenge 2014. Our approach is based on using an
extensible template-dependent crawler and DBpedia for linking extracted
entities, such as the names of universities and countries.
</summary>
    <author>
      <name>Maxim Kolchin</name>
    </author>
    <author>
      <name>Fedor Kozlov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ESWC 2014 Semantic Publishing Challenge, Task 1</arxiv:comment>
    <link href="http://arxiv.org/abs/1408.1260v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.1260v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.04920v1</id>
    <updated>2015-01-20T19:01:20Z</updated>
    <published>2015-01-20T19:01:20Z</published>
    <title>Regroupement sémantique de définitions en espagnol</title>
    <summary>  This article focuses on the description and evaluation of a new unsupervised
learning method of clustering of definitions in Spanish according to their
semantic. Textual Energy was used as a clustering measure, and we study an
adaptation of the Precision and Recall to evaluate our method.
</summary>
    <author>
      <name>Gerardo Sierra</name>
    </author>
    <author>
      <name>Juan-Manuel Torres-Moreno</name>
    </author>
    <author>
      <name>Alejandro Molina</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, in French, 5 figures. Workshop Evaluation des m\'ethodes
  d'Extraction de Connaissances dans les Donn\'ees EvalECD EGC'10, 2010 Tunis</arxiv:comment>
    <link href="http://arxiv.org/abs/1501.04920v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.04920v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.04032v1</id>
    <updated>2015-02-12T10:56:10Z</updated>
    <published>2015-02-12T10:56:10Z</published>
    <title>On Projection Based Operators in Lp space for Exact Similarity Search</title>
    <summary>  We investigate exact indexing for high dimensional Lp norms based on the
1-Lipschitz property and projection operators. The orthogonal projection that
satisfies the 1-Lipschitz property for the Lp norm is described. The adaptive
projection defined by the first principal component is introduced.
</summary>
    <author>
      <name>Andreas Wichert</name>
    </author>
    <author>
      <name>Catarina Moreira</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3233/FI-2015-1166</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3233/FI-2015-1166" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Fundamenta Informaticae: Annales Societatis Mathematicae Polonae,
  136: 1-14, 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1502.04032v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.04032v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.04163v1</id>
    <updated>2015-02-14T03:23:53Z</updated>
    <published>2015-02-14T03:23:53Z</published>
    <title>A Distributional Representation Model For Collaborative Filtering</title>
    <summary>  In this paper, we propose a very concise deep learning approach for
collaborative filtering that jointly models distributional representation for
users and items. The proposed framework obtains better performance when
compared against current state-of-art algorithms and that made the
distributional representation model a promising direction for further research
in the collaborative filtering.
</summary>
    <author>
      <name>Zhang Junlin</name>
    </author>
    <author>
      <name>Cai Heng</name>
    </author>
    <author>
      <name>Huang Tongwen</name>
    </author>
    <author>
      <name>Xue Huiping</name>
    </author>
    <link href="http://arxiv.org/abs/1502.04163v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.04163v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.05781v1</id>
    <updated>2015-03-19T14:31:28Z</updated>
    <published>2015-03-19T14:31:28Z</published>
    <title>Memantic: A Medical Knowledge Discovery Engine</title>
    <summary>  We present a system that constructs and maintains an up-to-date co-occurrence
network of medical concepts based on continuously mining the latest biomedical
literature. Users can explore this network visually via a concise online
interface to quickly discover important and novel relationships between medical
entities. This enables users to rapidly gain contextual understanding of their
medical topics of interest, and we believe this constitutes a significant user
experience improvement over contemporary search engines operating in the
biomedical literature domain.
</summary>
    <author>
      <name>Alexei Yavlinsky</name>
    </author>
    <link href="http://arxiv.org/abs/1503.05781v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.05781v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.06410v1</id>
    <updated>2015-03-22T11:32:34Z</updated>
    <published>2015-03-22T11:32:34Z</published>
    <title>What the F-measure doesn't measure: Features, Flaws, Fallacies and Fixes</title>
    <summary>  The F-measure or F-score is one of the most commonly used single number
measures in Information Retrieval, Natural Language Processing and Machine
Learning, but it is based on a mistake, and the flawed assumptions render it
unsuitable for use in most contexts! Fortunately, there are better
alternatives.
</summary>
    <author>
      <name>David M. W. Powers</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.06410v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.06410v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.06646v3</id>
    <updated>2015-06-28T14:13:03Z</updated>
    <published>2015-05-25T14:39:09Z</published>
    <title>A Survey on Retrieval of Mathematical Knowledge</title>
    <summary>  We present a short survey of the literature on indexing and retrieval of
mathematical knowledge, with pointers to 72 papers and tentative taxonomies of
both retrieval problems and recurring techniques.
</summary>
    <author>
      <name>F. Guidi</name>
    </author>
    <author>
      <name>C. Sacerdoti Coen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CICM 2015, 20 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.06646v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.06646v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="A.1; H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.03780v1</id>
    <updated>2015-11-12T05:24:35Z</updated>
    <published>2015-11-12T05:24:35Z</published>
    <title>A User's Guide to CARSKit</title>
    <summary>  Context-aware recommender systems extend traditional recommenders by adapting
their suggestions to users' contextual situations. CARSKit is a Java-based
open-source library specifically designed for the context-aware recommendation,
where the state-of-the-art context-aware recommendation algorithms have been
implemented. This report provides the basic user's guide to CARSKit, including
how to prepare the data set, how to configure the experimental settings, and
how to evaluate the algorithms, as well as interpreting the outputs.
</summary>
    <author>
      <name>Yong Zheng</name>
    </author>
    <link href="http://arxiv.org/abs/1511.03780v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.03780v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.07754v1</id>
    <updated>2016-01-28T13:43:30Z</updated>
    <published>2016-01-28T13:43:30Z</published>
    <title>Deep Learning Based Semantic Video Indexing and Retrieval</title>
    <summary>  We share the implementation details and testing results for video retrieval
system based exclusively on features extracted by convolutional neural
networks. We show that deep learned features might serve as universal signature
for semantic content of video useful in many search and retrieval tasks. We
further show that graph-based storage structure for video index allows to
efficiently retrieving the content with complicated spatial and temporal search
queries.
</summary>
    <author>
      <name>Anna Podlesnaya</name>
    </author>
    <author>
      <name>Sergey Podlesnyy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.07754v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.07754v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.06225v1</id>
    <updated>2016-04-21T09:25:11Z</updated>
    <published>2016-04-21T09:25:11Z</published>
    <title>OCR Error Correction Using Character Correction and Feature-Based Word
  Classification</title>
    <summary>  This paper explores the use of a learned classifier for post-OCR text
correction. Experiments with the Arabic language show that this approach, which
integrates a weighted confusion matrix and a shallow language model, improves
the vast majority of segmentation and recognition errors, the most frequent
types of error on our dataset.
</summary>
    <author>
      <name>Ido Kissos</name>
    </author>
    <author>
      <name>Nachum Dershowitz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 12th IAPR International Workshop on Document
  Analysis Systems (DAS2016), Santorini, Greece, April 11-14, 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.06225v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.06225v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.08420v1</id>
    <updated>2016-04-28T14:01:54Z</updated>
    <published>2016-04-28T14:01:54Z</published>
    <title>Matrix Factorization Method for Decentralized Recommender Systems</title>
    <summary>  Decentralized recommender system does not rely on the central service
provider, and the users can keep the ownership of their ratings. This article
brings the theoretically well-studied matrix factorization method into the
decentralized recommender system, where the formerly prevalent algorithms are
heuristic and hence lack of theoretical guarantee. Our preliminary simulation
results show that this method is promising.
</summary>
    <author>
      <name>Wenjie Zheng</name>
    </author>
    <link href="http://arxiv.org/abs/1604.08420v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.08420v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03066v1</id>
    <updated>2016-06-09T18:58:51Z</updated>
    <published>2016-06-09T18:58:51Z</published>
    <title>The Effects of Latency Penalties in Evaluating Push Notification Systems</title>
    <summary>  We examine the effects of different latency penalties in the evaluation of
push notification systems, as operationalized in the TREC 2015 Microblog track
evaluation. The purpose of this study is to inform the design of metrics for
the TREC 2016 Real-Time Summarization track, which is largely modeled after the
TREC 2015 evaluation design.
</summary>
    <author>
      <name>Luchen Tan</name>
    </author>
    <author>
      <name>Jimmy Lin</name>
    </author>
    <author>
      <name>Adam Roegiest</name>
    </author>
    <author>
      <name>Charles L. A. Clarke</name>
    </author>
    <link href="http://arxiv.org/abs/1606.03066v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03066v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.04223v1</id>
    <updated>2016-06-14T07:19:08Z</updated>
    <published>2016-06-14T07:19:08Z</published>
    <title>Learning Term Weights for Ad-hoc Retrieval</title>
    <summary>  Most Information Retrieval models compute the relevance score of a document
for a given query by summing term weights specific to a document or a query.
Heuristic approaches, like TF-IDF, or probabilistic models, like BM25, are used
to specify how a term weight is computed. In this paper, we propose to leverage
learning-to-rank principles to learn how to compute a term weight for a given
document based on the term occurrence pattern.
</summary>
    <author>
      <name>B. Piwowarski</name>
    </author>
    <link href="http://arxiv.org/abs/1606.04223v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04223v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.03176v1</id>
    <updated>2016-09-11T15:59:35Z</updated>
    <published>2016-09-11T15:59:35Z</published>
    <title>E3 : Keyphrase based News Event Exploration Engine</title>
    <summary>  This paper presents a novel system E3 for extracting keyphrases from news
content for the purpose of offering the news audience a broad overview of news
events, with especially high content volume. Given an input query, E3 extracts
keyphrases and enrich them by tagging, ranking and finding role for frequently
associated keyphrases. Also, E3 finds the novelty and activeness of keyphrases
using news publication date, to identify the most interesting and informative
keyphrases.
</summary>
    <author>
      <name>Nikita Jain</name>
    </author>
    <author>
      <name>Swati Gupta</name>
    </author>
    <author>
      <name>Dhaval Patel</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2914586.2914611</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2914586.2914611" rel="related"/>
    <link href="http://arxiv.org/abs/1609.03176v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.03176v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.01901v1</id>
    <updated>2016-10-06T15:04:10Z</updated>
    <published>2016-10-06T15:04:10Z</published>
    <title>Discriminative Information Retrieval for Knowledge Discovery</title>
    <summary>  We propose a framework for discriminative Information Retrieval (IR) atop
linguistic features, trained to improve the recall of tasks such as answer
candidate passage retrieval, the initial step in text-based Question Answering
(QA). We formalize this as an instance of linear feature-based IR (Metzler and
Croft, 2007), illustrating how a variety of knowledge discovery tasks are
captured under this approach, leading to a 44% improvement in recall for
candidate triage for QA.
</summary>
    <author>
      <name>Tongfei Chen</name>
    </author>
    <author>
      <name>Benjamin Van Durme</name>
    </author>
    <link href="http://arxiv.org/abs/1610.01901v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.01901v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.03316v1</id>
    <updated>2016-12-10T16:33:06Z</updated>
    <published>2016-12-10T16:33:06Z</published>
    <title>Label Visualization and Exploration in IR</title>
    <summary>  There is a renaissance in visual analytics systems for data analysis and
sharing, in particular, in the current wave of big data applications. We
introduce RAVE, a prototype that automates the generation of an interface that
uses facets and visualization techniques for exploring and analyzing relevance
assessments data sets collected via crowdsourcing. We present a technical
description of the main components and demonstrate its use.
</summary>
    <author>
      <name>Omar Alonso</name>
    </author>
    <link href="http://arxiv.org/abs/1612.03316v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.03316v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.04292v1</id>
    <updated>2017-01-16T14:02:19Z</updated>
    <published>2017-01-16T14:02:19Z</published>
    <title>Semantic classifier approach to document classification</title>
    <summary>  In this paper we propose a new document classification method, bridging
discrepancies (so-called semantic gap) between the training set and the
application sets of textual data. We demonstrate its superiority over classical
text classification approaches, including traditional classifier ensembles. The
method consists in combining a document categorization technique with a single
classifier or a classifier ensemble (SEMCOM algorithm - Committee with Semantic
Categorizer).
</summary>
    <author>
      <name>Piotr Borkowski</name>
    </author>
    <author>
      <name>Krzysztof Ciesielski</name>
    </author>
    <author>
      <name>Mieczysław A. Kłopotek</name>
    </author>
    <link href="http://arxiv.org/abs/1701.04292v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.04292v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.01845v1</id>
    <updated>2017-04-06T13:56:31Z</updated>
    <published>2017-04-06T13:56:31Z</published>
    <title>Report on TBAS 2012: Workshop on Task-Based and Aggregated Search</title>
    <summary>  The ECIR half-day workshop on Task-Based and Aggregated Search (TBAS) was
held in Barcelona, Spain on 1 April 2012. The program included a keynote talk
by Professor Jarvelin, six full paper presentations, two poster presentations,
and an interactive discussion among the approximately 25 participants. This
report overviews the aims and contents of the workshop and outlines the major
outcomes.
</summary>
    <author>
      <name>Birger Larsen</name>
    </author>
    <author>
      <name>Christina Lioma</name>
    </author>
    <author>
      <name>Arjen de Vries</name>
    </author>
    <link href="http://arxiv.org/abs/1704.01845v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.01845v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.07757v1</id>
    <updated>2017-04-25T16:01:50Z</updated>
    <published>2017-04-25T16:01:50Z</published>
    <title>User Profile Based Research Paper Recommendation</title>
    <summary>  We design a recommender system for research papers based on topic-modeling.
The users feedback to the results is used to make the results more relevant the
next time they fire a query. The user's needs are understood by observing the
change in the themes that the user shows a preference for over time.
</summary>
    <author>
      <name>Harshita Sahijwani</name>
    </author>
    <author>
      <name>Sourish Dasgupta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Work in progress. arXiv admin note: text overlap with
  arXiv:1611.04822</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.07757v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.07757v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.04009v1</id>
    <updated>2017-05-11T04:22:58Z</updated>
    <published>2017-05-11T04:22:58Z</published>
    <title>A survey of Community Question Answering</title>
    <summary>  With the advent of numerous community forums, tasks associated with the same
have gained importance in the recent past. With the influx of new questions
every day on these forums, the issues of identifying methods to find answers to
said questions, or even trying to detect duplicate questions, are of practical
importance and are challenging in their own right. This paper aims at surveying
some of the aforementioned issues, and methods proposed for tackling the same.
</summary>
    <author>
      <name>Barun Patra</name>
    </author>
    <link href="http://arxiv.org/abs/1705.04009v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.04009v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.04453v2</id>
    <updated>2017-08-16T06:58:38Z</updated>
    <published>2017-06-14T12:47:36Z</published>
    <title>Hybrid Collaborative Recommendation via Semi-AutoEncoder</title>
    <summary>  In this paper, we present a novel structure, Semi-AutoEncoder, based on
AutoEncoder. We generalize it into a hybrid collaborative filtering model for
rating prediction as well as personalized top-n recommendations. Experimental
results on two real-world datasets demonstrate its state-of-the-art
performances.
</summary>
    <author>
      <name>Shuai Zhang</name>
    </author>
    <author>
      <name>Lina Yao</name>
    </author>
    <author>
      <name>Xiwei Xu</name>
    </author>
    <author>
      <name>Sen Wang</name>
    </author>
    <author>
      <name>Liming Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, ICONIP 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.04453v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.04453v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.08928v1</id>
    <updated>2017-06-27T16:25:00Z</updated>
    <published>2017-06-27T16:25:00Z</published>
    <title>Classical Music Clustering Based on Acoustic Features</title>
    <summary>  In this paper we cluster 330 classical music pieces collected from MusicNet
database based on their musical note sequence. We use shingling and chord
trajectory matrices to create signature for each music piece and performed
spectral clustering to find the clusters. Based on different resolution, the
output clusters distinctively indicate composition from different classical
music era and different composing style of the musicians.
</summary>
    <author>
      <name>Xindi Wang</name>
    </author>
    <author>
      <name>Syed Arefinul Haque</name>
    </author>
    <link href="http://arxiv.org/abs/1706.08928v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.08928v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.03496v1</id>
    <updated>2017-09-10T23:19:41Z</updated>
    <published>2017-09-10T23:19:41Z</published>
    <title>SweetRS: Dataset for a recommender systems of sweets</title>
    <summary>  Benchmarking recommender system and matrix completion algorithms could be
greatly simplified if the entire matrix was known. We built a \url{sweetrs.org}
platform with $77$ candies and sweets to rank. Over $2000$ users submitted over
$44000$ grades resulting in a matrix with $28\%$ coverage. In this report, we
give the full description of the environment and we benchmark the
\textsc{Soft-Impute} algorithm on the dataset.
</summary>
    <author>
      <name>Łukasz Kidziński</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.03496v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.03496v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.05193v1</id>
    <updated>2017-09-14T07:24:08Z</updated>
    <published>2017-09-14T07:24:08Z</published>
    <title>Clustering of Musical Pieces through Complex Networks: an Assessment
  over Guitar Solos</title>
    <summary>  Musical pieces can be modeled as complex networks. This fosters innovative
ways to categorize music, paving the way towards novel applications in
multimedia domains, such as music didactics, multimedia entertainment and
digital music generation. Clustering these networks through their main metrics
allows grouping similar musical tracks. To show the viability of the approach,
we provide results on a dataset of guitar solos.
</summary>
    <author>
      <name>Stefano Ferretti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear in IEEE Multimedia magazine</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.05193v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.05193v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.5863v1</id>
    <updated>2011-10-17T21:19:58Z</updated>
    <published>2011-10-17T21:19:58Z</published>
    <title>A Wikipedia Literature Review</title>
    <summary>  This paper was originally designed as a literature review for a doctoral
dissertation focusing on Wikipedia. This exposition gives the structure of
Wikipedia and the latest trends in Wikipedia research.
</summary>
    <author>
      <name>Owen S. Martin</name>
    </author>
    <link href="http://arxiv.org/abs/1110.5863v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.5863v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.00251v2</id>
    <updated>2017-01-26T15:53:55Z</updated>
    <published>2016-01-31T14:22:47Z</published>
    <title>Do we have privacy in the digital world?</title>
    <summary>  Not really.
</summary>
    <author>
      <name>Kaveh Bakhtiyari</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.13140/RG.2.1.2492.5203/2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.13140/RG.2.1.2492.5203/2" rel="related"/>
    <link href="http://arxiv.org/abs/1602.00251v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.00251v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9907042v1</id>
    <updated>1999-07-27T16:42:18Z</updated>
    <published>1999-07-27T16:42:18Z</published>
    <title>Raising Reliability of Web Search Tool Research through Replication and
  Chaos Theory</title>
    <summary>  Because the World Wide Web is a dynamic collection of information, the Web
search tools (or "search engines") that index the Web are dynamic. Traditional
information retrieval evaluation techniques may not provide reliable results
when applied to the Web search tools. This study is the result of ten
replications of the classic 1996 Ding and Marchionini Web search tool research.
It explores the effects that replication can have on transforming unreliable
results from one iteration into replicable and therefore reliable results after
multiple iterations.
</summary>
    <author>
      <name>Scott Nicholson</name>
    </author>
    <link href="http://arxiv.org/abs/cs/9907042v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9907042v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.4; H.3.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0102002v1</id>
    <updated>2001-02-01T23:03:49Z</updated>
    <published>2001-02-01T23:03:49Z</published>
    <title>On the Automated Classification of Web Sites</title>
    <summary>  In this paper we discuss several issues related to automated text
classification of web sites. We analyze the nature of web content and metadata
in relation to requirements for text features. We find that HTML metatags are a
good source of text features, but are not in wide use despite their role in
search engine rankings. We present an approach for targeted spidering including
metadata extraction and opportunistic crawling of specific semantic hyperlinks.
We describe a system for automatically classifying web sites into industry
categories and present performance results based on different combinations of
text features and training data. This system can serve as the basis for a
generalized framework for automated metadata creation.
</summary>
    <author>
      <name>John M. Pierre</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, etendu.sty</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0102002v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0102002v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; I.5.2; H.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0204054v1</id>
    <updated>2002-04-26T22:45:30Z</updated>
    <published>2002-04-26T22:45:30Z</published>
    <title>Navigating the Small World Web by Textual Cues</title>
    <summary>  Can a Web crawler efficiently locate an unknown relevant page? While this
question is receiving much empirical attention due to its considerable
commercial value in the search engine community
[Cho98,Chakrabarti99,Menczer00,Menczer01], theoretical efforts to bound the
performance of focused navigation have only exploited the link structure of the
Web graph, neglecting other features [Kleinberg01,Adamic01,Kim02]. Here I
investigate the connection between linkage and a content-induced topology of
Web pages, suggesting that efficient paths can be discovered by decentralized
navigation algorithms based on textual cues.
</summary>
    <author>
      <name>Filippo Menczer</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0204054v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0204054v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.1; H.3.3; H.3.4; H.3.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0209021v1</id>
    <updated>2002-09-19T06:53:51Z</updated>
    <published>2002-09-19T06:53:51Z</published>
    <title>Activities, Context and Ubiquitous Computing</title>
    <summary>  Context and context-awareness provides computing environments with the
ability to usefully adapt the services or information they provide. It is the
ability to implicitly sense and automatically derive the user needs that
separates context-aware applications from traditionally designed applications,
and this makes them more attentive, responsive, and aware of their user's
identity, and their user's environment. This paper argues that context-aware
applications capable of supporting complex, cognitive activities can be built
from a model of context called Activity-Centric Context. A conceptual model of
Activity-Centric context is presented. The model is illustrated via a detailed
example.
</summary>
    <author>
      <name>Paul Prekop</name>
    </author>
    <author>
      <name>Mark Burnett</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computer Communications 26 (2003) 1168-1176</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0209021v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0209021v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.M; H1; H4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0306021v2</id>
    <updated>2003-08-30T06:35:35Z</updated>
    <published>2003-06-04T19:40:04Z</published>
    <title>Visualization for Periodic Population Movement between Distinct
  Localities</title>
    <summary>  We present a new visualization method to summarize and present periodic
population movement between distinct locations, such as floors, buildings,
cities, or the like. In the specific case of this paper, we have chosen to
focus on student movement between college dormitories on the Columbia
University campus. The visual information is presented to the information
analyst in the form of an interactive geographical map, in which specific
temporal periods as well as individual buildings can be singled out for
detailed data exploration. The navigational interface has been designed to
specifically meet a geographical setting.
</summary>
    <author>
      <name>Alexander Haubold</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Poster Summary: 2 pages, 4 figures, InfoVis 2003 Symposium</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0306021v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0306021v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0311015v2</id>
    <updated>2003-11-15T13:25:00Z</updated>
    <published>2003-11-14T09:53:51Z</published>
    <title>Make search become the internal function of Internet</title>
    <summary>  Domain Resource Integrated System (DRIS) is introduced in this paper. DRIS is
a distributed information retrieval system, which will solve problems like poor
coverage, long update interval in current web search system. The most distinct
character of DRIS is that it's a public opening system, and acts as an internal
component of Internet, but not the production of a company. The implementation
of DRIS is also represented.
</summary>
    <author>
      <name>Liang Wang</name>
    </author>
    <author>
      <name>Yiping Guo</name>
    </author>
    <author>
      <name>Ming Fang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0311015v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0311015v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3;H.3.5;H.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0312018v1</id>
    <updated>2003-12-11T20:07:39Z</updated>
    <published>2003-12-11T20:07:39Z</published>
    <title>Mapping Subsets of Scholarly Information</title>
    <summary>  We illustrate the use of machine learning techniques to analyze, structure,
maintain, and evolve a large online corpus of academic literature. An emerging
field of research can be identified as part of an existing corpus, permitting
the implementation of a more coherent community structure for its
practitioners.
</summary>
    <author>
      <name>Paul Ginsparg</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Cornell University</arxiv:affiliation>
    </author>
    <author>
      <name>Paul Houle</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Cornell University</arxiv:affiliation>
    </author>
    <author>
      <name>Thorsten Joachims</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Cornell University</arxiv:affiliation>
    </author>
    <author>
      <name>Jae-Hoon Sul</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Cornell University</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1073/pnas.0308253100</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1073/pnas.0308253100" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 figures, presented at Arthur M. Sackler Colloquium on
  "Mapping Knowledge Domains", 9--11 May 2003, Beckman Center, Irvine, CA,
  proceedings to appear in PNAS</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0312018v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0312018v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.1; H.3.6; I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0312033v1</id>
    <updated>2003-12-17T11:30:56Z</updated>
    <published>2003-12-17T11:30:56Z</published>
    <title>Using sensors in the web crawling process</title>
    <summary>  This paper offers a short description of an Internet information field
monitoring system, which places a special module-sensor on the side of the
Web-server to detect changes in information resources and subsequently
reindexes only the resources signalized by the corresponding sensor. Concise
results of simulation research and an implementation attempt of the given
"sensors" concept are provided.
</summary>
    <author>
      <name>Ilya Zemskov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 4 figures. The article was accepted for the IADIS
  International WWW/Internet 2003 Conference but not published in the
  proceedings due to the lack of financial support</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0312033v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0312033v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.4; I.6.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0405044v1</id>
    <updated>2004-05-12T20:18:51Z</updated>
    <published>2004-05-12T20:18:51Z</published>
    <title>Corpus structure, language models, and ad hoc information retrieval</title>
    <summary>  Most previous work on the recently developed language-modeling approach to
information retrieval focuses on document-specific characteristics, and
therefore does not take into account the structure of the surrounding corpus.
We propose a novel algorithmic framework in which information provided by
document-based language models is enhanced by the incorporation of information
drawn from clusters of similar documents. Using this framework, we develop a
suite of new algorithms. Even the simplest typically outperforms the standard
language-modeling approach in precision and recall, and our new interpolation
algorithm posts statistically significant improvements for both metrics over
all three corpora tested.
</summary>
    <author>
      <name>Oren Kurland</name>
    </author>
    <author>
      <name>Lillian Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear, SIGIR 2004</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0405044v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0405044v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0411026v1</id>
    <updated>2004-11-08T20:49:42Z</updated>
    <published>2004-11-08T20:49:42Z</published>
    <title>A Search Relevancy Tuning Method Using Expert Results Content Evaluation</title>
    <summary>  The article presents an online relevancy tuning method using explicit user
feedback. The author developed and tested a method of words' weights
modification based on search result evaluation by user. User decides whether
the result is useful or not after inspecting the full result content. The
experiment proved that the constantly accumulated words weights base leads to
better search quality in a specified data domain. The author also suggested
future improvements of the method.
</summary>
    <author>
      <name>Boris Mark Tylevich</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Moscow Institute of Physics and Technology, Moscow, Russia</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0411026v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0411026v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0503020v1</id>
    <updated>2005-03-08T22:26:07Z</updated>
    <published>2005-03-08T22:26:07Z</published>
    <title>Earlier Web Usage Statistics as Predictors of Later Citation Impact</title>
    <summary>  The use of citation counts to assess the impact of research articles is well
established. However, the citation impact of an article can only be measured
several years after it has been published. As research articles are
increasingly accessed through the Web, the number of times an article is
downloaded can be instantly recorded and counted. One would expect the number
of times an article is read to be related both to the number of times it is
cited and to how old the article is. This paper analyses how short-term Web
usage impact predicts medium-term citation impact. The physics e-print archive
(arXiv.org) is used to test this.
</summary>
    <author>
      <name>Tim Brody</name>
    </author>
    <author>
      <name>Stevan Harnad</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0503020v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0503020v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0505039v1</id>
    <updated>2005-05-14T17:48:07Z</updated>
    <published>2005-05-14T17:48:07Z</published>
    <title>Methods for comparing rankings of search engine results</title>
    <summary>  In this paper we present a number of measures that compare rankings of search
engine results. We apply these measures to five queries that were monitored
daily for two periods of about 21 days each. Rankings of the different search
engines (Google, Yahoo and Teoma for text searches and Google, Yahoo and
Picsearch for image searches) are compared on a daily basis, in addition to
longitudinal comparisons of the same engine for the same query over time. The
results and rankings of the two periods are compared as well.
</summary>
    <author>
      <name>Judit Bar-Ilan</name>
    </author>
    <author>
      <name>Mazlita Mat-Hassan</name>
    </author>
    <author>
      <name>Mark Levene</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 4 figures, 8 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0505039v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0505039v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0505051v1</id>
    <updated>2005-05-20T14:54:40Z</updated>
    <published>2005-05-20T14:54:40Z</published>
    <title>Sub-Optimum Signal Linear Detector Using Wavelets and Support Vector
  Machines</title>
    <summary>  The problem of known signal detection in Additive White Gaussian Noise is
considered. In previous work, a new detection scheme was introduced by the
authors, and it was demonstrated that optimum performance cannot be reached in
a real implementation. In this paper we analyse Support Vector Machines (SVM)
as an alternative, evaluating the results in terms of Probability of detection
curves for a fixed Probability of false alarm.
</summary>
    <author>
      <name>Jaime Gomez</name>
    </author>
    <author>
      <name>Ignacio Melgar</name>
    </author>
    <author>
      <name>Juan Seijas</name>
    </author>
    <author>
      <name>Diego Andina</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">WSEAS Transactions on Communications, ISSN 1109-2742, issue 4, vol
  2, p426-431, October-2003</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0505051v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0505051v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0505052v1</id>
    <updated>2005-05-20T15:01:20Z</updated>
    <published>2005-05-20T15:01:20Z</published>
    <title>Upgrading Pulse Detection with Time Shift Properties Using Wavelets and
  Support Vector Machines</title>
    <summary>  Current approaches in pulse detection use domain transformations so as to
concentrate frequency related information that can be distinguishable from
noise. In real cases we do not know when the pulse will begin, so we need a
time search process in which time windows are scheduled and analysed. Each
window can contain the pulsed signal (either complete or incomplete) and / or
noise. In this paper a simple search process will be introduced, allowing the
algorithm to process more information, upgrading the capabilities in terms of
probability of detection (Pd) and probability of false alarm (Pfa).
</summary>
    <author>
      <name>Jaime Gomez</name>
    </author>
    <author>
      <name>Ignacio Melgar</name>
    </author>
    <author>
      <name>Juan Seijas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the World Automation Congress (WAC-04), Sevilla,
  Spain, June-2004</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0505052v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0505052v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0505053v1</id>
    <updated>2005-05-20T15:06:40Z</updated>
    <published>2005-05-20T15:06:40Z</published>
    <title>Wavelet Time Shift Properties Integration with Support Vector Machines</title>
    <summary>  This paper presents a short evaluation about the integration of information
derived from wavelet non-linear-time-invariant (non-LTI) projection properties
using Support Vector Machines (SVM). These properties may give additional
information for a classifier trying to detect known patterns hidden by noise.
In the experiments we present a simple electromagnetic pulsed signal
recognition scheme, where some improvement is achieved with respect to previous
work. SVMs are used as a tool for information integration, exploiting some
unique properties not easily found in neural networks.
</summary>
    <author>
      <name>Jaime Gomez</name>
    </author>
    <author>
      <name>Ignacio Melgar</name>
    </author>
    <author>
      <name>Juan Seijas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">LNAI-3131 Modeling Decisions for Artificial Intelligence, ISSN
  0302-9743, p49-59, Barcelona, Spain, August-2004</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0505053v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0505053v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0505056v1</id>
    <updated>2005-05-23T07:04:49Z</updated>
    <published>2005-05-23T07:04:49Z</published>
    <title>Text Compression and Superfast Searching</title>
    <summary>  In this paper, a new compression scheme for text is presented. The same is
efficient in giving high compression ratios and enables super fast searching
within the compressed text. Typical compression ratios of 70-80% and reducing
the search time by 80-85% are the features of this paper. Till now, a trade-off
between high ratios and searchability within compressed text has been seen. In
this paper, we show that greater the compression, faster the search. This finds
applicability in so many places where data as natural language text is present.
</summary>
    <author>
      <name>Udayan Khurana</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Thapar Institute of Engineering and Technology</arxiv:affiliation>
    </author>
    <author>
      <name>Anirudh Koul</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Thapar Institute of Engineering and Technology</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0505056v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0505056v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0509020v1</id>
    <updated>2005-09-07T12:16:22Z</updated>
    <published>2005-09-07T12:16:22Z</published>
    <title>Transitive Text Mining for Information Extraction and Hypothesis
  Generation</title>
    <summary>  Transitive text mining - also named Swanson Linking (SL) after its primary
and principal researcher - tries to establish meaningful links between
literature sets which are virtually disjoint in the sense that each does not
mention the main concept of the other. If successful, SL may give rise to the
development of new hypotheses. In this communication we describe our approach
to transitive text mining which employs co-occurrence analysis of the medical
subject headings (MeSH), the descriptors assigned to papers indexed in PubMed.
In addition, we will outline the current state of our web-based information
system which will enable our users to perform literature-driven hypothesis
building on their own.
</summary>
    <author>
      <name>Johannes Stegmann</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Charite, Berlin</arxiv:affiliation>
    </author>
    <author>
      <name>Guenter Grohmann</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Charite, Berlin</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0509020v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0509020v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0509072v1</id>
    <updated>2005-09-23T13:27:18Z</updated>
    <published>2005-09-23T13:27:18Z</published>
    <title>Folksonomy as a Complex Network</title>
    <summary>  Folksonomy is an emerging technology that works to classify the information
over WWW through tagging the bookmarks, photos or other web-based contents. It
is understood to be organized by every user while not limited to the authors of
the contents and the professional editors. This study surveyed the folksonomy
as a complex network. The result indicates that the network, which is composed
of the tags from the folksonomy, displays both properties of small world and
scale-free. However, the statistics only shows a local and static slice of the
vast body of folksonomy which is still evolving.
</summary>
    <author>
      <name>Kaikai Shen</name>
    </author>
    <author>
      <name>Lide Wu</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0509072v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0509072v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0512032v1</id>
    <updated>2005-12-08T09:37:49Z</updated>
    <published>2005-12-08T09:37:49Z</published>
    <title>A Software Framework for Vehicle-Infrastructure Cooperative Applications</title>
    <summary>  A growing category of vehicle-infrastructure cooperative (VIC) applications
requires telematics software components distributed between an
infrastructure-based management center and a number of vehicles. This article
presents an approach based on a software framework, focusing on a Telematic
Management System (TMS), a component suite aimed to run inside an
infrastructure-based operations center, in some cases interacting with legacy
systems like Advanced Traffic Management Systems or Vehicle Relationship
Management. The TMS framework provides support for modular, flexible,
prototyping and implementation of VIC applications. This work has received the
support of the European Commission in the context of the projects REACT and
CyberCars.
</summary>
    <author>
      <name>Sebastián Bengochea</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt</arxiv:affiliation>
    </author>
    <author>
      <name>Angel Talamona</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt</arxiv:affiliation>
    </author>
    <author>
      <name>Michel Parent</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/cs/0512032v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0512032v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0604036v2</id>
    <updated>2006-04-27T00:41:01Z</updated>
    <published>2006-04-10T12:04:29Z</published>
    <title>Collaborative thesaurus tagging the Wikipedia way</title>
    <summary>  This paper explores the system of categories that is used to classify
articles in Wikipedia. It is compared to collaborative tagging systems like
del.icio.us and to hierarchical classification like the Dewey Decimal
Classification (DDC). Specifics and commonalitiess of these systems of subject
indexing are exposed. Analysis of structural and statistical properties
(descriptors per record, records per descriptor, descriptor levels) shows that
the category system of Wikimedia is a thesaurus that combines collaborative
tagging and hierarchical subject indexing in a special way.
</summary>
    <author>
      <name>Jakob Voss</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 7 figures, 7 tables; v2 with added appendix and fixed
  references</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0604036v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0604036v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0606097v2</id>
    <updated>2006-06-23T05:10:39Z</updated>
    <published>2006-06-22T14:17:26Z</published>
    <title>Synonym search in Wikipedia: Synarcher</title>
    <summary>  The program Synarcher for synonym (and related terms) search in the text
corpus of special structure (Wikipedia) was developed. The results of the
search are presented in the form of graph. It is possible to explore the graph
and search for graph elements interactively. Adapted HITS algorithm for synonym
search, program architecture, and program work evaluation with test examples
are presented in the paper. The proposed algorithm can be applied to a query
expansion by synonyms (in a search engine) and a synonym dictionary forming.
</summary>
    <author>
      <name>A. Krizhanovsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 2 figures, Synarcher program is available at
  http://synarcher.sourceforge.net</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0606097v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0606097v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.1; H.3.3; H.4.3; G.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0606105v1</id>
    <updated>2006-06-26T11:51:41Z</updated>
    <published>2006-06-26T11:51:41Z</published>
    <title>Iso9000 Based Advanced Quality Approach for Continuous Improvement of
  Manufacturing Processes</title>
    <summary>  The continuous improvement in TQM is considered as the core value by which
organisation could maintain a competitive edge. Several techniques and tools
are known to support this core value but most of the time these techniques are
informal and without modelling the interdependence between the core value and
tools. Thus, technique formalisation is one of TQM challenges for increasing
efficiency of quality process implementation. In that way, the paper proposes
and experiments an advanced quality modelling approach based on meta-modelling
the "process approach" as advocated by the standard ISO9000:2000. This
meta-model allows formalising the interdependence between technique, tools and
core value
</summary>
    <author>
      <name>Salah Deeb</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CRAN</arxiv:affiliation>
    </author>
    <author>
      <name>Benoît Iung</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CRAN</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">12th IFAC Symposium on Information Control Problems in
  Manufacturing, St-Etienne, France (17/05/2006) CDROM</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0606105v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0606105v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0606128v1</id>
    <updated>2006-06-30T15:17:36Z</updated>
    <published>2006-06-30T15:17:36Z</published>
    <title>Automatic forming lists of semantically related terms based on texts
  rating in the corpus with hyperlinks and categories (In Russian)</title>
    <summary>  HITS adapted algorithm for synonym search, the program architecture, and the
program work evaluation with test examples are presented in the paper.
Synarcher program for synonym (and related terms) search in the text corpus of
special structure (Wikipedia) was developed. The results of search are
presented in the form of a graph. It is possible to explore the graph and
search graph elements interactively. The proposed algorithm could be applied to
the search request extending and for synonym dictionary forming.
</summary>
    <author>
      <name>A. Krizhanovsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 figure, in Russian, PDF, for other formats see
  http://whinger.narod.ru/paper/index.html</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0606128v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0606128v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.1; H.3.3; H.4.3; G.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0608043v1</id>
    <updated>2006-08-08T13:19:08Z</updated>
    <published>2006-08-08T13:19:08Z</published>
    <title>Using Users' Expectations to Adapt Business Intelligence Systems</title>
    <summary>  This paper takes a look at the general characteristics of business or
economic intelligence system. The role of the user within this type of system
is emphasized. We propose two models which we consider important in order to
adapt this system to the user. The first model is based on the definition of
decisional problem and the second on the four cognitive phases of human
learning. We also describe the application domain we are using to test these
models in this type of system.
</summary>
    <author>
      <name>Babajide Afolabi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Odile Thiery</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LORIA</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Advances in Knowledge Organization 10 (2006) 247-254</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0608043v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0608043v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0608107v3</id>
    <updated>2007-02-19T20:33:37Z</updated>
    <published>2006-08-28T17:05:07Z</published>
    <title>The Haar Wavelet Transform of a Dendrogram</title>
    <summary>  We describe a new wavelet transform, for use on hierarchies or binary rooted
trees. The theoretical framework of this approach to data analysis is
described. Case studies are used to further exemplify this approach. A first
set of application studies deals with data array smoothing, or filtering. A
second set of application studies relates to hierarchical tree condensation.
Finally, a third study explores the wavelet decomposition, and the
reproducibility of data sets such as text, including a new perspective on the
generation or computability of such data objects.
</summary>
    <author>
      <name>Fionn Murtagh</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s00357-007-0007-9</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s00357-007-0007-9" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">38 pp, 8 figures. Forthcoming in Journal of Classification</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Classification, 24, 3-32, 2007</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0608107v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0608107v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.3; H.3.1; I.1.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0610091v4</id>
    <updated>2006-12-24T06:03:44Z</updated>
    <published>2006-10-14T17:03:46Z</published>
    <title>On the Behavior of Journal Impact Factor Rank-Order Distribution</title>
    <summary>  An empirical law for the rank-order behavior of journal impact factors is
found. Using an extensive data base on impact factors including journals on
Education, Agrosciences, Geosciences, Biosciences and Environ- mental,
Chemical, Computer, Engineering, Material, Mathematical, Medical and Physical
Sciences we have found extremely good fits out- performing other rank-order
models. Some extensions to other areas of knowledge are discussed.
</summary>
    <author>
      <name>R. Mansilla</name>
    </author>
    <author>
      <name>E. Köppen</name>
    </author>
    <author>
      <name>G. Cocho</name>
    </author>
    <author>
      <name>P. Miramontes</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to the Journal of Informetrics, redundat text cropped,
  bibliography corrected, new section added, typos corrected</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0610091v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0610091v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0703095v1</id>
    <updated>2007-03-20T14:52:52Z</updated>
    <published>2007-03-20T14:52:52Z</published>
    <title>Copula Component Analysis</title>
    <summary>  A framework named Copula Component Analysis (CCA) for blind source separation
is proposed as a generalization of Independent Component Analysis (ICA). It
differs from ICA which assumes independence of sources that the underlying
components may be dependent with certain structure which is represented by
Copula. By incorporating dependency structure, much accurate estimation can be
made in principle in the case that the assumption of independence is
invalidated. A two phrase inference method is introduced for CCA which is based
on the notion of multidimensional ICA.
</summary>
    <author>
      <name>Jian Ma</name>
    </author>
    <author>
      <name>Zengqi Sun</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0703095v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0703095v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0703101v1</id>
    <updated>2007-03-21T20:47:33Z</updated>
    <published>2007-03-21T20:47:33Z</published>
    <title>A Note on Approximate Nearest Neighbor Methods</title>
    <summary>  A number of authors have described randomized algorithms for solving the
epsilon-approximate nearest neighbor problem. In this note I point out that the
epsilon-approximate nearest neighbor property often fails to be a useful
approximation property, since epsilon-approximate solutions fail to satisfy the
necessary preconditions for using nearest neighbors for classification and
related tasks.
</summary>
    <author>
      <name>Thomas M. Breuel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The report was originally written in 2005 and does not reference
  information after that date</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0703101v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0703101v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0705.1161v1</id>
    <updated>2007-05-08T20:08:13Z</updated>
    <published>2007-05-08T20:08:13Z</published>
    <title>IDF revisited: A simple new derivation within the Robertson-Spärck
  Jones probabilistic model</title>
    <summary>  There have been a number of prior attempts to theoretically justify the
effectiveness of the inverse document frequency (IDF). Those that take as their
starting point Robertson and Sparck Jones's probabilistic model are based on
strong or complex assumptions. We show that a more intuitively plausible
assumption suffices. Moreover, the new assumption, while conceptually very
simple, provides a solution to an estimation problem that had been deemed
intractable by Robertson and Walker (1997).
</summary>
    <author>
      <name>Lillian Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear, Proceedings of SIGIR 2007, poster paper (2 pages)</arxiv:comment>
    <link href="http://arxiv.org/abs/0705.1161v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0705.1161v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0709.4669v1</id>
    <updated>2007-09-28T18:45:48Z</updated>
    <published>2007-09-28T18:45:48Z</published>
    <title>The Extended Edit Distance Metric</title>
    <summary>  Similarity search is an important problem in information retrieval. This
similarity is based on a distance. Symbolic representation of time series has
attracted many researchers recently, since it reduces the dimensionality of
these high dimensional data objects. We propose a new distance metric that is
applied to symbolic data objects and we test it on time series data bases in a
classification task. We compare it to other distances that are well known in
the literature for symbolic data objects. We also prove, mathematically, that
our distance is metric.
</summary>
    <author>
      <name>Muhammad Marwan Muhammad Fuad</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">VALORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Pierre-François Marteau</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">VALORIA</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CBMI.2008.4564953</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CBMI.2008.4564953" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical report</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Content-Based Multimedia Indexing, CBMI 2008, london : United
  Kingdom (2008)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0709.4669v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0709.4669v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0711.2832v1</id>
    <updated>2007-11-19T16:10:35Z</updated>
    <published>2007-11-19T16:10:35Z</published>
    <title>Première étape vers une navigation référentielle par l'image
  pour l'assistance à la conception des ambiances lumineuses</title>
    <summary>  In the first design stage, image reference plays a double role of means of
formulation and resolution of problems. In our approach, we consider image
reference as a support of creation activity to generate ideas and we propose a
tool for navigation in references by image in order to assist daylight ambience
design. Within this paper, we present, in a first part, the semantic indexation
method to be used for the indexation of our image database. In a second part we
propose a synthetic analysis of various modes of referential navigation in
order to propose a tool implementing all or a part of these modes.
</summary>
    <author>
      <name>Salma Chaabouni</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MAP / Crai</arxiv:affiliation>
    </author>
    <author>
      <name>Jc Bignon</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MAP / Crai</arxiv:affiliation>
    </author>
    <author>
      <name>Gilles Halin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MAP / Crai</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/0711.2832v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0711.2832v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0711.2867v1</id>
    <updated>2007-11-19T09:43:22Z</updated>
    <published>2007-11-19T09:43:22Z</published>
    <title>Maximizing PageRank via outlinks</title>
    <summary>  We analyze linkage strategies for a set I of webpages for which the webmaster
wants to maximize the sum of Google's PageRank scores. The webmaster can only
choose the hyperlinks starting from the webpages of I and has no control on the
hyperlinks from other webpages. We provide an optimal linkage strategy under
some reasonable assumptions.
</summary>
    <author>
      <name>Cristobald de Kerchove</name>
    </author>
    <author>
      <name>Laure Ninove</name>
    </author>
    <author>
      <name>Paul Van Dooren</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 14 figures, submitted to Linear Algebra Appl</arxiv:comment>
    <link href="http://arxiv.org/abs/0711.2867v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0711.2867v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.RA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0711.2917v1</id>
    <updated>2007-11-19T12:35:48Z</updated>
    <published>2007-11-19T12:35:48Z</published>
    <title>Use of Wikipedia Categories in Entity Ranking</title>
    <summary>  Wikipedia is a useful source of knowledge that has many applications in
language processing and knowledge representation. The Wikipedia category graph
can be compared with the class hierarchy in an ontology; it has some
characteristics in common as well as some differences. In this paper, we
present our approach for answering entity ranking queries from the Wikipedia.
In particular, we explore how to make use of Wikipedia categories to improve
entity ranking effectiveness. Our experiments show that using categories of
example entities works significantly better than using loosely defined target
categories.
</summary>
    <author>
      <name>James A. Thom</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">RMIT</arxiv:affiliation>
    </author>
    <author>
      <name>Jovan Pehcevski</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt / INRIA Sophia Antipolis</arxiv:affiliation>
    </author>
    <author>
      <name>Anne-Marie Vercoustre</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt / INRIA Sophia Antipolis</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans The 12th Australasian Document Computing Symposium (ADCS'07)
  (2007)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0711.2917v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0711.2917v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0801.3908v1</id>
    <updated>2008-01-25T10:40:27Z</updated>
    <published>2008-01-25T10:40:27Z</published>
    <title>Encoding changing country codes for the Semantic Web with ISO 3166 and
  SKOS</title>
    <summary>  This paper shows how authority files can be encoded for the Semantic Web with
the Simple Knowledge Organisation System (SKOS). In particular the application
of SKOS for encoding the structure, management, and utilization of country
codes as defined in ISO 3166 is demonstrated. The proposed encoding gives a use
case for SKOS that includes features that have only been discussed little so
far, such as multiple notations, nested concept schemes, changes by versioning.
</summary>
    <author>
      <name>Jakob Voss</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to appear in the proceedings of the 2nd International Con-
  ference on Metadata and Semantics Research (MTSR 2007)</arxiv:comment>
    <link href="http://arxiv.org/abs/0801.3908v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.3908v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0802.3522v5</id>
    <updated>2008-06-23T04:45:59Z</updated>
    <published>2008-02-24T17:18:50Z</published>
    <title>Time Warp Edit Distance</title>
    <summary>  This technical report details a family of time warp distances on the set of
discrete time series. This family is constructed as an editing distance whose
elementary operations apply on linear segments. A specific parameter allows
controlling the stiffness of the elastic matching. It is well suited for the
processing of event data for which each data sample is associated with a
timestamp, not necessarily obtained according to a constant sampling rate. Some
properties verified by these distances are proposed and proved in this report.
</summary>
    <author>
      <name>Pierre-François Marteau</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">VALORIA</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Pattern Recognition - Clustering - Algorithms - Similarity Measures</arxiv:comment>
    <link href="http://arxiv.org/abs/0802.3522v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0802.3522v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0807.1560v1</id>
    <updated>2008-07-10T00:01:20Z</updated>
    <published>2008-07-10T00:01:20Z</published>
    <title>Scientific Paper Summarization Using Citation Summary Networks</title>
    <summary>  Quickly moving to a new area of research is painful for researchers due to
the vast amount of scientific literature in each field of study. One possible
way to overcome this problem is to summarize a scientific topic. In this paper,
we propose a model of summarizing a single article, which can be further used
to summarize an entire topic. Our model is based on analyzing others' viewpoint
of the target article's contributions and the study of its citation summary
network using a clustering approach.
</summary>
    <author>
      <name>Vahed Qazvinian</name>
    </author>
    <author>
      <name>Dragomir R. Radev</name>
    </author>
    <link href="http://arxiv.org/abs/0807.1560v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0807.1560v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; H.3.1; I.2.7; G.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0811.4186v1</id>
    <updated>2008-11-25T23:11:55Z</updated>
    <published>2008-11-25T23:11:55Z</published>
    <title>Search Result Clustering via Randomized Partitioning of Query-Induced
  Subgraphs</title>
    <summary>  In this paper, we present an approach to search result clustering, using
partitioning of underlying link graph. We define the notion of "query-induced
subgraph" and formulate the problem of search result clustering as a problem of
efficient partitioning of given subgraph into topic-related clusters. Also, we
propose a novel algorithm for approximative partitioning of such graph, which
results in cluster quality comparable to the one obtained by deterministic
algorithms, while operating in more efficient computation time, suitable for
practical implementations. Finally, we present a practical clustering search
engine developed as a part of this research and use it to get results about
real-world performance of proposed concepts.
</summary>
    <author>
      <name>Aleksandar Bradic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16th Telecommunications Forum TELFOR 2008</arxiv:comment>
    <link href="http://arxiv.org/abs/0811.4186v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0811.4186v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; I.1.2; E.1; G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0902.1911v1</id>
    <updated>2009-02-11T15:14:11Z</updated>
    <published>2009-02-11T15:14:11Z</published>
    <title>Topological Centrality and Its Applications</title>
    <summary>  Recent development of network structure analysis shows that it plays an
important role in characterizing complex system of many branches of sciences.
Different from previous network centrality measures, this paper proposes the
notion of topological centrality (TC) reflecting the topological positions of
nodes and edges in general networks, and proposes an approach to calculating
the topological centrality. The proposed topological centrality is then used to
discover communities and build the backbone network. Experiments and
applications on research network show the significance of the proposed
approach.
</summary>
    <author>
      <name>Hai Zhuge</name>
    </author>
    <author>
      <name>Junsheng Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0902.1911v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0902.1911v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.5172v1</id>
    <updated>2009-03-30T11:06:03Z</updated>
    <published>2009-03-30T11:06:03Z</published>
    <title>Delocalization transition for the Google matrix</title>
    <summary>  We study the localization properties of eigenvectors of the Google matrix,
generated both from the World Wide Web and from the Albert-Barabasi model of
networks. We establish the emergence of a delocalization phase for the PageRank
vector when network parameters are changed. In the phase of localized PageRank,
a delocalization takes place in the complex plane of eigenvalues of the matrix,
leading to delocalized relaxation modes. We argue that the efficiency of
information retrieval by Google-type search is strongly affected in the phase
of delocalized PageRank.
</summary>
    <author>
      <name>Olivier Giraud</name>
    </author>
    <author>
      <name>Bertrand Georgeot</name>
    </author>
    <author>
      <name>Dima L. Shepelyansky</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.80.026107</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.80.026107" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 5 figures. Research done at
  http://www.quantware.ups-tlse.fr/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 80, 026107 (2009)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0903.5172v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.5172v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0905.1130v1</id>
    <updated>2009-05-07T20:21:45Z</updated>
    <published>2009-05-07T20:21:45Z</published>
    <title>Statistical Automatic Summarization in Organic Chemistry</title>
    <summary>  We present an oriented numerical summarizer algorithm, applied to producing
automatic summaries of scientific documents in Organic Chemistry. We present
its implementation named Yachs (Yet Another Chemistry Summarizer) that combines
a specific document pre-processing with a sentence scoring method relying on
the statistical properties of documents. We show that Yachs achieves the best
results among several other summarizers on a corpus of Organic Chemistry
articles.
</summary>
    <author>
      <name>Florian Boudin</name>
    </author>
    <author>
      <name>Patricia Velazquez-Morales</name>
    </author>
    <author>
      <name>Juan-Manuel Torres-Moreno</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0905.1130v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0905.1130v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.0080v2</id>
    <updated>2009-08-06T17:36:09Z</updated>
    <published>2009-05-30T14:22:04Z</published>
    <title>Reverse method for labeling the information from semi-structured web
  pages</title>
    <summary>  We propose a new technique to infer the structure and extract the tokens of
data from the semi-structured web sources which are generated using a
consistent template or layout with some implicit regularities. The attributes
are extracted and labeled reversely from the region of interest of targeted
contents. This is in contrast with the existing techniques which always
generate the trees from the root. We argue and show that our technique is
simpler, more accurate and effective especially to detect the changes of the
templates of targeted web pages.
</summary>
    <author>
      <name>Z. Akbar</name>
    </author>
    <author>
      <name>L. T. Handoko</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICSPS.2009.86</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICSPS.2009.86" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, Proceeding of the 2009 International Conference on Signal
  Processing Systems pp. 551-555</arxiv:comment>
    <link href="http://arxiv.org/abs/0906.0080v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.0080v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.5286v1</id>
    <updated>2009-06-29T15:00:43Z</updated>
    <published>2009-06-29T15:00:43Z</published>
    <title>Putting Recommendations on the Map -- Visualizing Clusters and Relations</title>
    <summary>  For users, recommendations can sometimes seem odd or counterintuitive.
Visualizing recommendations can remove some of this mystery, showing how a
recommendation is grouped with other choices. A drawing can also lead a user's
eye to other options. Traditional 2D-embeddings of points can be used to create
a basic layout, but these methods, by themselves, do not illustrate clusters
and neighborhoods very well. In this paper, we propose the use of geographic
maps to enhance the definition of clusters and neighborhoods, and consider the
effectiveness of this approach in visualizing similarities and recommendations
arising from TV shows and music selections. All the maps referenced in this
paper can be found in http://www.research.att.com/~volinsky/maps
</summary>
    <author>
      <name>Emden Gansner</name>
    </author>
    <author>
      <name>Yifan Hu</name>
    </author>
    <author>
      <name>Stephen Kobourov</name>
    </author>
    <author>
      <name>Chris Volinsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0906.5286v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.5286v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.5608v1</id>
    <updated>2009-06-30T18:42:59Z</updated>
    <published>2009-06-30T18:42:59Z</published>
    <title>Loading Arbitrary Knowledge Bases in Matrix Browser</title>
    <summary>  This paper describes the work done on Matrix Browser, which is a recently
developed graphical user interface to explore and navigate complex networked
information spaces. This approach presents a new way of navigating information
nets in windows explorer like widget. The problem on hand was how to export
arbitrary knowledge bases in Matrix Browser. This was achieved by identifying
the relationships present in knowledge bases and then by forming the
hierarchies from this data and these hierarchies are being exported to matrix
browser. This paper gives solution to this problem and informs about
implementation work.
</summary>
    <author>
      <name>Saqib Saeed</name>
    </author>
    <author>
      <name>Christoph Kunz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper was published in the proceedings of IEEE International
  Multi Topic Conference (INMIC 2004) Lahore, Pakistan 24th- 26th December 2004</arxiv:comment>
    <link href="http://arxiv.org/abs/0906.5608v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.5608v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.3315v1</id>
    <updated>2009-07-19T18:56:37Z</updated>
    <published>2009-07-19T18:56:37Z</published>
    <title>Effective Personalized Recommendation in Collaborative Tagging Systems</title>
    <summary>  Recently, collaborative tagging systems have attracted more and more
attention and have been widely applied in web systems. Tags provide highly
abstracted information about personal preferences and item content, and are
therefore potential to help in improving better personalized recommendations.
In this paper, we propose a tag-based recommendation algorithm considering the
personal vocabulary and evaluate it in a real-world dataset: Del.icio.us.
Experimental results demonstrate that the usage of tag information can
significantly improve the accuracy of personalized recommendations.
</summary>
    <author>
      <name>Zi-Ke Zhang</name>
    </author>
    <author>
      <name>Tao Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/0907.3315v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.3315v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0908.0595v1</id>
    <updated>2009-08-05T17:09:45Z</updated>
    <published>2009-08-05T17:09:45Z</published>
    <title>Towards a Model of Understanding Social Search</title>
    <summary>  Search engine researchers typically depict search as the solitary activity of
an individual searcher. In contrast, results from our critical-incident survey
of 150 users on Amazon's Mechanical Turk service suggest that social
interactions play an important role throughout the search process. Our main
contribution is that we have integrated models from previous work in
sensemaking and information seeking behavior to present a canonical social
model of user activities before, during, and after search, suggesting where in
the search process even implicitly shared information may be valuable to
individual searchers.
</summary>
    <author>
      <name>Brynn M. Evans</name>
    </author>
    <author>
      <name>Ed H. Chi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at 1st Intl Workshop on Collaborative Information Seeking,
  2008 (arXiv:0908.0583)</arxiv:comment>
    <link href="http://arxiv.org/abs/0908.0595v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0908.0595v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; H.5.2; H.5.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0908.0704v1</id>
    <updated>2009-08-05T16:48:18Z</updated>
    <published>2009-08-05T16:48:18Z</published>
    <title>A Taxonomy of Collaboration in Online Information Seeking</title>
    <summary>  People can help other people find information in networked information
seeking environments. Recently, many such systems and algorithms have
proliferated in industry and in academia. Unfortunately, it is difficult to
compare the systems in meaningful ways because they often define collaboration
in different ways. In this paper, we propose a model of possible kinds of
collaboration, and illustrate it with examples from literature. The model
contains four dimensions: intent, depth, concurrency and location. This model
can be used to classify existing systems and to suggest possible opportunities
for design in this space.
</summary>
    <author>
      <name>Gene Golovchinsky</name>
    </author>
    <author>
      <name>Jeremy Pickens</name>
    </author>
    <author>
      <name>Maribeth Back</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at 1st Intl Workshop on Collaborative Information Seeking,
  2008 (arXiv:0908.0583)</arxiv:comment>
    <link href="http://arxiv.org/abs/0908.0704v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0908.0704v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; H.5.2; H.5.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0908.0709v1</id>
    <updated>2009-08-05T17:33:03Z</updated>
    <published>2009-08-05T17:33:03Z</published>
    <title>Toward Collaborative Information Seeking (CIS)</title>
    <summary>  It is natural for humans to collaborate while dealing with complex problems.
In this article I consider this process of collaboration in the context of
information seeking. The study and discussion presented here are driven by two
dissatisfactions: (1) the majority of IR systems today do not facilitate
collaboration directly, and (2) the concept of collaboration itself is not
well-understood. I begin by probing the notion of collaboration and propose a
model that helps us understand the requirements for a successful collaboration.
A model of a Collaborative Information Seeking (CIS) environment is then
rendered based on an extended model of information seeking.
</summary>
    <author>
      <name>Chirag Shah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at 1st Intl Workshop on Collaborative Information Seeking,
  2008 (arXiv:0908.0583)</arxiv:comment>
    <link href="http://arxiv.org/abs/0908.0709v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0908.0709v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; H.5.2; H.5.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.3472v2</id>
    <updated>2010-03-09T12:43:28Z</updated>
    <published>2009-09-18T15:54:51Z</published>
    <title>The Universal Recommender</title>
    <summary>  We describe the Universal Recommender, a recommender system for semantic
datasets that generalizes domain-specific recommenders such as content-based,
collaborative, social, bibliographic, lexicographic, hybrid and other
recommenders. In contrast to existing recommender systems, the Universal
Recommender applies to any dataset that allows a semantic representation. We
describe the scalable three-stage architecture of the Universal Recommender and
its application to Internet Protocol Television (IPTV). To achieve good
recommendation accuracy, several novel machine learning and optimization
problems are identified. We finally give a brief argument supporting the need
for machine learning recommenders.
</summary>
    <author>
      <name>Jérôme Kunegis</name>
    </author>
    <author>
      <name>Alan Said</name>
    </author>
    <author>
      <name>Winfried Umbrath</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages; typo and references fixed</arxiv:comment>
    <link href="http://arxiv.org/abs/0909.3472v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.3472v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.4416v2</id>
    <updated>2009-09-25T14:17:26Z</updated>
    <published>2009-09-24T12:27:02Z</published>
    <title>A baseline for content-based blog classification</title>
    <summary>  A content-based network representation of web logs (blogs) using a basic
word-overlap similarity measure is presented. Due to a strong signal in blog
data the approach is sufficient for accurately classifying blogs. Using Swedish
blog data we demonstrate that blogs that treat similar subjects are organized
in clusters that, in turn, are hierarchically organized in higher-order
clusters. The simplicity of the representation renders it both computationally
tractable and transparent. We therefore argue that the approach is suitable as
a baseline when developing and analyzing more advanced content-based
representations of the blogosphere.
</summary>
    <author>
      <name>Olof Gornerup</name>
    </author>
    <author>
      <name>Magnus Boman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 4 figures New version has higher resolution for figures 2
  and 3</arxiv:comment>
    <link href="http://arxiv.org/abs/0909.4416v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.4416v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0910.1938v1</id>
    <updated>2009-10-10T19:26:13Z</updated>
    <published>2009-10-10T19:26:13Z</published>
    <title>Information Retrieval via Truncated Hilbert-Space Expansions</title>
    <summary>  In addition to the frequency of terms in a document collection, the
distribution of terms plays an important role in determining the relevance of
documents. In this paper, a new approach for representing term positions in
documents is presented. The approach allows an efficient evaluation of
term-positional information at query evaluation time. Three applications are
investigated: a function-based ranking optimization representing a user-defined
document region, a query expansion technique based on overlapping the term
distributions in the top-ranked documents, and cluster analysis of terms in
documents. Experimental results demonstrate the effectiveness of the proposed
approach.
</summary>
    <author>
      <name>Patricio Galeas</name>
    </author>
    <author>
      <name>Ralph Kretschmer</name>
    </author>
    <author>
      <name>Bernd Freisleben</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, submitted to proceedings of ECIR-2010</arxiv:comment>
    <link href="http://arxiv.org/abs/0910.1938v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0910.1938v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0911.0050v1</id>
    <updated>2009-10-31T01:03:27Z</updated>
    <published>2009-10-31T01:03:27Z</published>
    <title>How to Compare the Scientific Contributions between Research Groups</title>
    <summary>  We present a method to analyse the scientific contributions between research
groups. Given multiple research groups, we construct their journal/proceeding
graphs and then compute the similarity/gap between them using network analysis.
This analysis can be used for measuring similarity/gap of the topics/qualities
between research groups' scientific contributions. We demonstrate the
practicality of our method by comparing the scientific contributions by Korean
researchers with those by the global researchers for information security in
2006 - 2008. The empirical analysis shows that the current security research in
South Korea has been isolated from the global research trend.
</summary>
    <author>
      <name>Hyoungshick Kim</name>
    </author>
    <author>
      <name>Ji Won Yoon</name>
    </author>
    <link href="http://arxiv.org/abs/0911.0050v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.0050v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0911.4292v1</id>
    <updated>2009-11-22T21:35:51Z</updated>
    <published>2009-11-22T21:35:51Z</published>
    <title>Similarity Measures, Author Cocitation Analysis, and Information Theory</title>
    <summary>  The use of Pearson's correlation coefficient in Author Cocitation Analysis
was compared with Salton's cosine measure in a number of recent contributions.
Unlike the Pearson correlation, the cosine is insensitive to the number of
zeros. However, one has the option of applying a logarithmic transformation in
correlation analysis. Information calculus is based on both the logarithmic
transformation and provides a non-parametric statistics. Using this methodology
one can cluster a document set in a precise way and express the differences in
terms of bits of information. The algorithm is explained and used on the data
set which was made the subject of this discussion.
</summary>
    <author>
      <name>Loet Leydesdorff</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of the American Society for Information Science &amp;
  Technology, 56(7), 2005, 769-772</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0911.4292v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.4292v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0911.5046v2</id>
    <updated>2009-12-01T13:41:05Z</updated>
    <published>2009-11-26T10:27:14Z</published>
    <title>Integrating the Probabilistic Models BM25/BM25F into Lucene</title>
    <summary>  This document describes the BM25 and BM25F implementation using the Lucene
Java Framework. Both models have stood out at TREC by their performance and are
considered as state-of-the-art in the IR community. BM25 is applied to
retrieval on plain text documents, that is for documents that do not contain
fields, while BM25F is applied to documents with structure.
</summary>
    <author>
      <name>Joaquín Pérez-Iglesias</name>
    </author>
    <author>
      <name>José R. Pérez-Agüera</name>
    </author>
    <author>
      <name>Víctor Fresno</name>
    </author>
    <author>
      <name>Yuval Z. Feinstein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Software can be downloaded from:
  http://nlp.uned.es/~jperezi/Lucene-BM25/</arxiv:comment>
    <link href="http://arxiv.org/abs/0911.5046v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.5046v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; H.3.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.1294v1</id>
    <updated>2009-12-07T19:43:08Z</updated>
    <published>2009-12-07T19:43:08Z</published>
    <title>Conception d'un outil d'aide à l'indexation de ressources
  pédagogiques - Extraction automatique des thématiques et des mots-clefs
  de documents UNIT</title>
    <summary>  Indexing learning documents using the Learning Object Metadata (LOM) is often
carried out manually by archivists. Filling out the LOM fields is a long and
difficult task, requiring a complete reading and a full knowledge on the topic
dealt within the document. In this paper, we present an innovative model and
method to assist the archivists in finding the important concepts and keywords
of a learning document. The application is performed using wikipedia's category
links.
</summary>
    <author>
      <name>Carlo Abi Chahine</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LITIS</arxiv:affiliation>
    </author>
    <author>
      <name>Jean-Philippe Kotowicz</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LITIS</arxiv:affiliation>
    </author>
    <author>
      <name>Nathalie Chaignaud</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LITIS</arxiv:affiliation>
    </author>
    <author>
      <name>Jean-Pierre Pécuchet</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LITIS</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Environnements Informatiques pour l'Apprentissage Humain, Le Mans
  : France (2009)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0912.1294v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.1294v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.0440v1</id>
    <updated>2010-01-04T05:24:31Z</updated>
    <published>2010-01-04T05:24:31Z</published>
    <title>Tutoring System for Dance Learning</title>
    <summary>  Recent advances in hardware sophistication related to graphics display, audio
and video devices made available a large number of multimedia and hypermedia
applications. These multimedia applications need to store and retrieve the
different forms of media like text, hypertext, graphics, still images,
animations, audio and video. Dance is one of the important cultural forms of a
nation and dance video is one such multimedia types. Archiving and retrieving
the required semantics from these dance media collections is a crucial and
demanding multimedia application. This paper summarizes the difference dance
video archival techniques and systems. Keywords: Multimedia, Culture Media,
Metadata archival and retrieval systems, MPEG-7, XML.
</summary>
    <author>
      <name>Rajkumar Kannan</name>
    </author>
    <author>
      <name>Frederic Andres</name>
    </author>
    <author>
      <name>Balakrishnan Ramadoss</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Advance Computing Conference 2009, Patiala, India</arxiv:comment>
    <link href="http://arxiv.org/abs/1001.0440v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.0440v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.1; H.5.4; H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.0827v1</id>
    <updated>2010-01-06T07:51:23Z</updated>
    <published>2010-01-06T07:51:23Z</published>
    <title>Document Clustering with K-tree</title>
    <summary>  This paper describes the approach taken to the XML Mining track at INEX 2008
by a group at the Queensland University of Technology. We introduce the K-tree
clustering algorithm in an Information Retrieval context by adapting it for
document clustering. Many large scale problems exist in document clustering.
K-tree scales well with large inputs due to its low complexity. It offers
promising results both in terms of efficiency and quality. Document
classification was completed using Support Vector Machines.
</summary>
    <author>
      <name>Christopher M. De Vries</name>
    </author>
    <author>
      <name>Shlomo Geva</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-642-03761-0_43</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-642-03761-0_43" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, INEX 2008</arxiv:comment>
    <link href="http://arxiv.org/abs/1001.0827v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.0827v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.0830v1</id>
    <updated>2010-01-06T07:43:31Z</updated>
    <published>2010-01-06T07:43:31Z</published>
    <title>K-tree: Large Scale Document Clustering</title>
    <summary>  We introduce K-tree in an information retrieval context. It is an efficient
approximation of the k-means clustering algorithm. Unlike k-means it forms a
hierarchy of clusters. It has been extended to address issues with sparse
representations. We compare performance and quality to CLUTO using document
collections. The K-tree has a low time complexity that is suitable for large
document collections. This tree structure allows for efficient disk based
implementations where space requirements exceed that of main memory.
</summary>
    <author>
      <name>Christopher M. De Vries</name>
    </author>
    <author>
      <name>Shlomo Geva</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/1571941.1572094</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/1571941.1572094" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, SIGIR 2009</arxiv:comment>
    <link href="http://arxiv.org/abs/1001.0830v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.0830v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.0239v1</id>
    <updated>2010-02-01T13:09:52Z</updated>
    <published>2010-02-01T13:09:52Z</published>
    <title>Construction et enrichissement automatique d'ontologie à partir de
  ressources externes</title>
    <summary>  Automatic construction of ontologies from text is generally based on
retrieving text content. For a much more rich ontology we extend these
approaches by taking into account the document structure and some external
resources (like thesaurus of indexing terms of near domain). In this paper we
describe how these external resources are at first analyzed and then exploited.
This method has been applied on a geographical domain and the benefit has been
evaluated.
</summary>
    <author>
      <name>Eric Kergosien</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIUPPA</arxiv:affiliation>
    </author>
    <author>
      <name>Mouna Kamel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <author>
      <name>Christian Sallaberry</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIUPPA</arxiv:affiliation>
    </author>
    <author>
      <name>Marie-Noëlle Bessagnet</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIUPPA</arxiv:affiliation>
    </author>
    <author>
      <name>Nathalie Aussenac- Gilles</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <author>
      <name>Mauro Gaio</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIUPPA</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">JFO'09: 3es Journ\'ees Francophones sur les Ontologies, Poitiers :
  France (2009)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1002.0239v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.0239v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.0577v1</id>
    <updated>2010-02-02T20:07:22Z</updated>
    <published>2010-02-02T20:07:22Z</published>
    <title>Recherche de relations spatio-temporelles : une méthode basée sur
  l'analyse de corpus textuels</title>
    <summary>  This paper presents a work package realized for the G\'eOnto project. A new
method is proposed for an enrichment of a first geographical ontology developed
beforehand. This method relies on text analysis by lexico-syntactic patterns.
  From the retrieve of n-ary relations the method automatically detect those
involved in a spatial and/or temporal relation in a context of a description of
journeys.
</summary>
    <author>
      <name>Tien Nguyen Van</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIUPPA</arxiv:affiliation>
    </author>
    <author>
      <name>Mauro Gaio</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIUPPA</arxiv:affiliation>
    </author>
    <author>
      <name>Christian Sallaberry</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIUPPA</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">TIA'09WS: Acquisition et mod\'elisation de relations
  s\'emantiques, Toulouse : France (2009)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1002.0577v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.0577v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.2439v1</id>
    <updated>2010-02-11T21:45:48Z</updated>
    <published>2010-02-11T21:45:48Z</published>
    <title>Using Web Page Titles to Rediscover Lost Web Pages</title>
    <summary>  Titles are denoted by the TITLE element within a web page. We queried the
title against the the Yahoo search engine to determine the page's status
(found, not found). We conducted several tests based on elements of the title.
These tests were used to discern whether we could predict a pages status based
on the title. Our results increase our ability to determine bad titles but not
our ability to determine good titles.
</summary>
    <author>
      <name>Jeffery L. Shipman</name>
    </author>
    <author>
      <name>Martin Klein</name>
    </author>
    <author>
      <name>Michael L. Nelson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">49 pages, 18 figures, CS project report</arxiv:comment>
    <link href="http://arxiv.org/abs/1002.2439v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.2439v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.2858v3</id>
    <updated>2010-08-14T15:14:13Z</updated>
    <published>2010-02-15T12:47:16Z</published>
    <title>PageRank: Standing on the shoulders of giants</title>
    <summary>  PageRank is a Web page ranking technique that has been a fundamental
ingredient in the development and success of the Google search engine. The
method is still one of the many signals that Google uses to determine which
pages are most important. The main idea behind PageRank is to determine the
importance of a Web page in terms of the importance assigned to the pages
hyperlinking to it. In fact, this thesis is not new, and has been previously
successfully exploited in different contexts. We review the PageRank method and
link it to some renowned previous techniques that we have found in the fields
of Web information retrieval, bibliometrics, sociometry, and econometrics.
</summary>
    <author>
      <name>Massimo Franceschet</name>
    </author>
    <link href="http://arxiv.org/abs/1002.2858v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.2858v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.1048v1</id>
    <updated>2010-03-04T13:53:26Z</updated>
    <published>2010-03-04T13:53:26Z</published>
    <title>Tag Clusters as Information Retrieval Interfaces</title>
    <summary>  The paper presents our design of a next generation information retrieval
system based on tag co-occurrences and subsequent clustering. We help users
getting access to digital data through information visualization in the form of
tag clusters. Current problems like the absence of interactivity and semantics
between tags or the difficulty of adding additional search arguments are
solved. In the evaluation, based upon SERVQUAL and IT systems quality
indicators, we found out that tag clusters are perceived as more useful than
tag clouds, are much more trustworthy, and are more enjoyable to use.
</summary>
    <author>
      <name>Kathrin Knautz</name>
    </author>
    <author>
      <name>Simone Soubusta</name>
    </author>
    <author>
      <name>Wolfgang G. Stock</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 43th Annual Hawaii International Conference on
  System Sciences (HICSS-43), January 5-8, 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1003.1048v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.1048v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.3274v1</id>
    <updated>2010-04-19T18:24:02Z</updated>
    <published>2010-04-19T18:24:02Z</published>
    <title>A New Approach to Keyphrase Extraction Using Neural Networks</title>
    <summary>  Keyphrases provide a simple way of describing a document, giving the reader
some clues about its contents. Keyphrases can be useful in a various
applications such as retrieval engines, browsing interfaces, thesaurus
construction, text mining etc.. There are also other tasks for which keyphrases
are useful, as we discuss in this paper. This paper describes a neural network
based approach to keyphrase extraction from scientific articles. Our results
show that the proposed method performs better than some state-of-the art
keyphrase extraction approaches.
</summary>
    <author>
      <name>Kamal Sarkar</name>
    </author>
    <author>
      <name>Mita Nasipuri</name>
    </author>
    <author>
      <name>Suranjan Ghose</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science Issues online at
  http://ijcsi.org/articles/A-New-Approach-to-Keyphrase-Extraction-Using-Neural-Networks.php</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSI, Volume 7, Issue 2, March 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1004.3274v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.3274v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.3371v1</id>
    <updated>2010-04-20T07:49:07Z</updated>
    <published>2010-04-20T07:49:07Z</published>
    <title>Improving Update Summarization by Revisiting the MMR Criterion</title>
    <summary>  This paper describes a method for multi-document update summarization that
relies on a double maximization criterion. A Maximal Marginal Relevance like
criterion, modified and so called Smmr, is used to select sentences that are
close to the topic and at the same time, distant from sentences used in already
read documents. Summaries are then generated by assembling the high ranked
material and applying some ruled-based linguistic post-processing in order to
obtain length reduction and maintain coherency. Through a participation to the
Text Analysis Conference (TAC) 2008 evaluation campaign, we have shown that our
method achieves promising results.
</summary>
    <author>
      <name>Florian Boudin</name>
    </author>
    <author>
      <name>Juan-Manuel Torres-Moreno</name>
    </author>
    <author>
      <name>Marc El-Bèze</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 3 figures and 8 tables.</arxiv:comment>
    <link href="http://arxiv.org/abs/1004.3371v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.3371v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.4489v1</id>
    <updated>2010-04-26T11:36:38Z</updated>
    <published>2010-04-26T11:36:38Z</published>
    <title>MIREX: MapReduce Information Retrieval Experiments</title>
    <summary>  We propose to use MapReduce to quickly test new retrieval approaches on a
cluster of machines by sequentially scanning all documents. We present a small
case study in which we use a cluster of 15 low cost ma- chines to search a web
crawl of 0.5 billion pages showing that sequential scanning is a viable
approach to running large-scale information retrieval experiments with little
effort. The code is available to other researchers at:
http://mirex.sourceforge.net
</summary>
    <author>
      <name>Djoerd Hiemstra</name>
    </author>
    <author>
      <name>Claudia Hauff</name>
    </author>
    <link href="http://arxiv.org/abs/1004.4489v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.4489v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1007.4748v1</id>
    <updated>2010-07-27T15:16:36Z</updated>
    <published>2010-07-27T15:16:36Z</published>
    <title>Detecting influenza outbreaks by analyzing Twitter messages</title>
    <summary>  We analyze over 500 million Twitter messages from an eight month period and
find that tracking a small number of flu-related keywords allows us to forecast
future influenza rates with high accuracy, obtaining a 95% correlation with
national health statistics. We then analyze the robustness of this approach to
spurious keyword matches, and we propose a document classification component to
filter these misleading messages. We find that this document classifier can
reduce error rates by over half in simulated false alarm experiments, though
more research is needed to develop methods that are robust in cases of
extremely high noise.
</summary>
    <author>
      <name>Aron Culotta</name>
    </author>
    <link href="http://arxiv.org/abs/1007.4748v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1007.4748v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1008.3795v1</id>
    <updated>2010-08-23T11:30:15Z</updated>
    <published>2010-08-23T11:30:15Z</published>
    <title>Machine Science in Biomedicine: Practicalities, Pitfalls and Potential</title>
    <summary>  Machine Science, or Data-driven Research, is a new and interesting scientific
methodology that uses advanced computational techniques to identify, retrieve,
classify and analyse data in order to generate hypotheses and develop models.
In this paper we describe three recent biomedical Machine Science studies, and
use these to assess the current state of the art with specific emphasis on data
mining, data assessment, costs, limitations, skills and tool support.
</summary>
    <author>
      <name>T W Kelsey</name>
    </author>
    <author>
      <name>W H B Wallace</name>
    </author>
    <link href="http://arxiv.org/abs/1008.3795v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1008.3795v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.med-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.5003v1</id>
    <updated>2010-09-25T10:33:42Z</updated>
    <published>2010-09-25T10:33:42Z</published>
    <title>Demonstrating a Service-Enhanced Retrieval System</title>
    <summary>  This paper is a short description of an information retrieval system enhanced
by three model driven retrieval services: (1) co-word analysis based query
expansion, re-ranking via (2) Bradfordizing and (3) author centrality. The
different services each favor quite other - but still relevant - documents than
pure term-frequency based rankings. Each service can be interactively combined
with each other to allow an iterative retrieval refinement.
</summary>
    <author>
      <name>Philipp Schaer</name>
    </author>
    <author>
      <name>Philipp Mayr</name>
    </author>
    <author>
      <name>Peter Mutschke</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1002/meet.14504701395</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1002/meet.14504701395" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 1 figure, ASIST 2010 conference, Pittsburgh, PA, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/1009.5003v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.5003v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1010.6242v1</id>
    <updated>2010-10-29T15:11:05Z</updated>
    <published>2010-10-29T15:11:05Z</published>
    <title>GraphDuplex: visualisation simultanée de N réseaux couplés 2 par 2</title>
    <summary>  While social network analysis often focuses on graph structure of social
actors, an increasing number of communication networks now provide textual
content within social activity (email, instant messaging, blogging,
collaboration networks). We present an open source visualization software,
GraphDuplex, which brings together social structure and textual content, adding
a semantic dimension to social analysis. GraphDuplex eventually connects any
number of social or semantic graphs together, and through dynamic queries
enables user interaction and exploration across multiple graphs of different
nature.
</summary>
    <author>
      <name>Martine Hurault-Plantet</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIMSI</arxiv:affiliation>
    </author>
    <author>
      <name>Elie Naulleau</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CREM-EA3476</arxiv:affiliation>
    </author>
    <author>
      <name>Bernard Jacquemin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CREM-EA3476</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Conf\'erence en Recherche d'Information et Applications (CORIA
  2009), Prequ'\^ile de Giens : France (2009)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1010.6242v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1010.6242v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1011.5364v1</id>
    <updated>2010-11-24T13:06:18Z</updated>
    <published>2010-11-24T13:06:18Z</published>
    <title>Optimizing On-Line Advertising</title>
    <summary>  We want to find the optimal strategy for displaying advertisements e.g.
banners, videos, in given locations at given times under some realistic dynamic
constraints. Our primary goal is to maximize the expected revenue in a given
period of time, i.e. the total profit produced by the impressions, which
depends on profit-generating events such as the impressions themselves, the
ensuing clicks and registrations. Moreover we must take into consideration the
possibility that the constraints could change in time in a way that cannot
always be foreseen.
</summary>
    <author>
      <name>Fabrizio Caruso</name>
    </author>
    <author>
      <name>Giovanni Giuffrida</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1011.5364v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.5364v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.1609v1</id>
    <updated>2010-12-07T21:16:59Z</updated>
    <published>2010-12-07T21:16:59Z</published>
    <title>Building conceptual spaces for exploring and linking biomedical
  resources</title>
    <summary>  The establishment of links between data (e.g., patient records) and Web
resources (e.g., literature) and the proper visualization of such discovered
knowledge is still a challenge in most Life Science domains (e.g.,
biomedicine). In this paper we present our contribution to the community in the
form of an infrastructure to annotate information resources, to discover
relationships among them, and to represent and visualize the new discovered
knowledge. Furthermore, we have also implemented a Web-based prototype tool
which integrates the proposed infrastructure.
</summary>
    <author>
      <name>R. Berlanga</name>
    </author>
    <author>
      <name>E. Jimenez-Ruiz</name>
    </author>
    <author>
      <name>V. Nebot</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in Adrian Paschke, Albert Burger, Andrea Splendiani, M. Scott
  Marshall, Paolo Romano: Proceedings of the 3rd International Workshop on
  Semantic Web Applications and Tools for the Life Sciences, Berlin,Germany,
  December 8-10, 2010</arxiv:comment>
    <link href="http://arxiv.org/abs/1012.1609v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.1609v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.1666v1</id>
    <updated>2010-12-08T01:10:59Z</updated>
    <published>2010-12-08T01:10:59Z</published>
    <title>SPARQL Assist Language-Neutral Query Composer</title>
    <summary>  SPARQL query composition is difficult for the lay-person or even the
experienced bioinformatician in cases where the data model is unfamiliar.
Established best-practices and internationalization concerns dictate that
semantic web ontologies should use terms with opaque identifiers, further
complicating the task. We present SPARQL Assist: a web application that
addresses these issues by providing context-sensitive type-ahead completion to
existing web forms. Ontological terms are suggested using their labels and
descriptions, leveraging existing XML support for internationalization and
language-neutrality.
</summary>
    <author>
      <name>Luke McCarthy</name>
    </author>
    <author>
      <name>Ben Vandervalk</name>
    </author>
    <author>
      <name>Mark Wilkinson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in Adrian Paschke, Albert Burger, Andrea Splendiani, M. Scott
  Marshall, Paolo Romano: Proceedings of the 3rd International Workshop on
  Semantic Web Applications and Tools for the Life Sciences, Berlin,Germany,
  December 8-10, 2010</arxiv:comment>
    <link href="http://arxiv.org/abs/1012.1666v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.1666v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.3793v1</id>
    <updated>2010-12-17T00:58:02Z</updated>
    <published>2010-12-17T00:58:02Z</published>
    <title>A robust ranking algorithm to spamming</title>
    <summary>  Ranking problem of web-based rating system has attracted many attentions. A
good ranking algorithm should be robust against spammer attack. Here we
proposed a correlation based reputation algorithm to solve the ranking problem
of such rating systems where user votes some objects with ratings. In this
algorithm, reputation of user is iteratively determined by the correlation
coefficient between his/her rating vector and the corresponding objects'
weighted average rating vector. Comparing with iterative refinement (IR) and
mean score algorithm, results for both artificial and real data indicate that,
the present algorithm shows a higher robustness against spammer attack.
</summary>
    <author>
      <name>Yanbo Zhou</name>
    </author>
    <author>
      <name>Ting Lei</name>
    </author>
    <author>
      <name>Tao Zhou</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1209/0295-5075/94/48002</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1209/0295-5075/94/48002" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 4 figures, 3 Tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPL 94 (2011) 48002</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1012.3793v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.3793v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.2886v1</id>
    <updated>2011-03-15T11:48:30Z</updated>
    <published>2011-03-15T11:48:30Z</published>
    <title>Predicting User Preferences</title>
    <summary>  The many metrics employed for the evaluation of search engine results have
not themselves been conclusively evaluated. We propose a new measure for a
metric's ability to identify user preference of result lists. Using this
measure, we evaluate the metrics Discounted Cumulated Gain, Mean Average
Precision and classical precision, finding that the former performs best. We
also show that considering more results for a given query can impair rather
than improve a metric's ability to predict user preferences.
</summary>
    <author>
      <name>Pavel Sirotkin</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Information und Wissen: global, sozial und frei? Proceedings des
  12. Internationalen Symposiums f\"ur Informationswissenschaft. Joachim
  Griesbaum, Thomas Mandl, Christa Womser-Hacker (Editors). VWH, Boizenburg,
  2011. Pages 24-35</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1103.2886v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.2886v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1104.4063v1</id>
    <updated>2011-04-20T15:51:50Z</updated>
    <published>2011-04-20T15:51:50Z</published>
    <title>Fast redshift clustering with the Baire (ultra) metric</title>
    <summary>  The Baire metric induces an ultrametric on a dataset and is of linear
computational complexity, contrasted with the standard quadratic time
agglomerative hierarchical clustering algorithm. We apply the Baire distance to
spectrometric and photometric redshifts from the Sloan Digital Sky Survey
using, in this work, about half a million astronomical objects. We want to know
how well the (more cos\ tly to determine) spectrometric redshifts can predict
the (more easily obtained) photometric redshifts, i.e. we seek to regress the
spectrometric on the photometric redshifts, and we develop a clusterwise
nearest neighbor regression procedure for this.
</summary>
    <author>
      <name>Fionn Murtagh</name>
    </author>
    <author>
      <name>Pedro Contreras</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1142/9789814383295_0005</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1142/9789814383295_0005" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1104.4063v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.4063v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62H30, 85-08, 11S82" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.5; H.3; E.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.0121v1</id>
    <updated>2011-04-30T21:29:08Z</updated>
    <published>2011-04-30T21:29:08Z</published>
    <title>Methods of Hierarchical Clustering</title>
    <summary>  We survey agglomerative hierarchical clustering algorithms and discuss
efficient implementations that are available in R and other software
environments. We look at hierarchical self-organizing maps, and mixture models.
We review grid-based clustering, focusing on hierarchical density-based
approaches. Finally we describe a recently developed very efficient (linear
time) hierarchical clustering algorithm, which can also be viewed as a
hierarchical grid-based algorithm.
</summary>
    <author>
      <name>Fionn Murtagh</name>
    </author>
    <author>
      <name>Pedro Contreras</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 2 figures, 1 table, 69 references</arxiv:comment>
    <link href="http://arxiv.org/abs/1105.0121v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.0121v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62H30" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; H.2.8; G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.1406v1</id>
    <updated>2011-05-07T01:28:09Z</updated>
    <published>2011-05-07T01:28:09Z</published>
    <title>Comparison Latent Semantic and WordNet Approach for Semantic Similarity
  Calculation</title>
    <summary>  Information exchange among many sources in Internet is more autonomous,
dynamic and free. The situation drive difference view of concepts among
sources. For example, word 'bank' has meaning as economic institution for
economy domain, but for ecology domain it will be defined as slope of river or
lake. In this aper, we will evaluate latent semantic and WordNet approach to
calculate semantic similarity. The evaluation will be run for some concepts
from different domain with reference by expert or human. Result of the
evaluation can provide a contribution for mapping of concept, query rewriting,
interoperability, etc.
</summary>
    <author>
      <name>I Wayan Simri Wicaksana</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Gunadarma University</arxiv:affiliation>
    </author>
    <author>
      <name>Bambang Wahyudi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Gunadarma University</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Keywords: latent semantic, interoperability, WordNet</arxiv:comment>
    <link href="http://arxiv.org/abs/1105.1406v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.1406v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.4702v1</id>
    <updated>2011-05-24T08:01:15Z</updated>
    <published>2011-05-24T08:01:15Z</published>
    <title>Exploiting Conceptual Knowledge for Querying Information Systems</title>
    <summary>  Whereas today's information systems are well-equipped for efficient query
handling, their strict mathematical foundations hamper their use for everyday
tasks. In daily life, people expect information to be offered in a personalized
and focused way. But currently, personalization in digital systems still only
takes explicit knowledge into account and does not yet process conceptual
information often naturally implied by users. We discuss how to bridge the gap
between users and today's systems, building on results from cognitive
psychology.
</summary>
    <author>
      <name>Joachim Selke</name>
    </author>
    <author>
      <name>Wolf-Tilo Balke</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Philosophy's Relevance in Information
  Science (PRIS), Paderborn, Germany, 2008</arxiv:comment>
    <link href="http://arxiv.org/abs/1105.4702v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.4702v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.5789v1</id>
    <updated>2011-05-29T14:06:44Z</updated>
    <published>2011-05-29T14:06:44Z</published>
    <title>Clustering and Classification in Text Collections Using Graph Modularity</title>
    <summary>  A new fast algorithm for clustering and classification of large collections
of text documents is introduced. The new algorithm employs the bipartite graph
that realizes the word-document matrix of the collection. Namely, the
modularity of the bipartite graph is used as the optimization functional.
Experiments performed with the new algorithm on a number of text collections
had shown a competitive quality of the clustering (classification), and a
record-breaking speed.
</summary>
    <author>
      <name>Grigory Pivovarov</name>
    </author>
    <author>
      <name>Sergei Trunov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, submitted to JMLR</arxiv:comment>
    <link href="http://arxiv.org/abs/1105.5789v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.5789v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68U99" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.0217v1</id>
    <updated>2011-06-01T16:13:18Z</updated>
    <published>2011-06-01T16:13:18Z</published>
    <title>Using Lotkaian Informetrics for Ranking in Digital Libraries</title>
    <summary>  The purpose of this paper is to propose the use of models, theories and laws
in bibliometrics and scientometrics to enhance information retrieval processes,
especially ranking. A common pattern in many man-made data sets is Lotka's Law
which follows the well-known power-law distributions. These informetric
distributions can be used to give an alternative order to large and scattered
result sets and can be applied as a new ranking mechanism. The
polyrepresentation of information in Digital Library systems is used to enhance
the retrieval quality, to overcome the drawbacks of the typical term-based
ranking approaches and to enable users to explore retrieved document sets from
a different perspective.
</summary>
    <author>
      <name>Philipp Schaer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages; Proceedings of the ASIS&amp;T European Workshop 2011 (AEW 2011)</arxiv:comment>
    <link href="http://arxiv.org/abs/1106.0217v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.0217v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.5460v1</id>
    <updated>2011-08-27T15:49:28Z</updated>
    <published>2011-08-27T15:49:28Z</published>
    <title>Personalized Web Services for Web Information Extraction</title>
    <summary>  The field of information extraction from the Web emerged with the growth of
the Web and the multiplication of online data sources. This paper is an
analysis of information extraction methods. It presents a service oriented
approach for web information extraction considering both web data management
and extraction services. Then we propose an SOA based architecture to enhance
flexibility and on-the-fly modification of web extraction services. An
implementation of the proposed architecture is proposed on the middleware level
of Java Enterprise Edition (JEE) servers.
</summary>
    <author>
      <name>Zahi Jarir</name>
    </author>
    <author>
      <name>Mohamed Quafafou</name>
    </author>
    <author>
      <name>Mahammed Erradi</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Web Services Practices, Vol. 5, No.1
  (2010), pp. 22-31</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1108.5460v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.5460v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.5703v1</id>
    <updated>2011-08-26T07:02:35Z</updated>
    <published>2011-08-26T07:02:35Z</published>
    <title>Web Pages Clustering: A New Approach</title>
    <summary>  The rapid growth of web has resulted in vast volume of information.
Information availability at a rapid speed to the user is vital. English
language (or any for that matter) has lot of ambiguity in the usage of words.
So there is no guarantee that a keyword based search engine will provide the
required results. This paper introduces the use of dictionary (standardised) to
obtain the context with which a keyword is used and in turn cluster the results
based on this context. These ideas can be merged with a metasearch engine to
enhance the search efficiency.
</summary>
    <author>
      <name>Jeevan H E</name>
    </author>
    <author>
      <name>Prashanth P P</name>
    </author>
    <author>
      <name>Punith Kumar S N</name>
    </author>
    <author>
      <name>Vinay Hegde</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Clustering, concept mining, information retrieval, metasearch engine</arxiv:comment>
    <link href="http://arxiv.org/abs/1108.5703v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.5703v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.5784v1</id>
    <updated>2011-08-30T00:31:44Z</updated>
    <published>2011-08-30T00:31:44Z</published>
    <title>Probability Ranking in Vector Spaces</title>
    <summary>  The Probability Ranking Principle states that the document set with the
highest values of probability of relevance optimizes information retrieval
effectiveness given the probabilities are estimated as accurately as possible.
The key point of the principle is the separation of the document set into two
subsets with a given level of fallout and with the highest recall. The paper
introduces the separation between two vector subspaces and shows that the
separation yields a more effective performance than the optimal separation into
subsets with the same available evidence, the performance being measured with
recall and fallout. The result is proved mathematically and exemplified
experimentally.
</summary>
    <author>
      <name>Massimo Melucci</name>
    </author>
    <link href="http://arxiv.org/abs/1108.5784v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.5784v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.1088v1</id>
    <updated>2011-09-06T07:07:26Z</updated>
    <published>2011-09-06T07:07:26Z</published>
    <title>A Framework for Business Intelligence Application using Ontological
  Classification</title>
    <summary>  Every business needs knowledge about their competitors to survive better. One
of the information repositories is web. Retrieving Specific information from
the web is challenging. An Ontological model is developed to capture specific
information by using web semantics. From the Ontology model, the relations
between the data are mined using decision tree. From all these a new framework
is developed for Business Intelligence.
</summary>
    <author>
      <name>A. Martin</name>
    </author>
    <author>
      <name>D. Maladhy</name>
    </author>
    <author>
      <name>V. Prasanna Venkatesan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Classification, Ontology, Business Intelligence, Datamining, Inverted
  Index, Ontology Tree Index</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Engineering Science and Technology
  (IJEST) Vol. 3 No. 2, (2011) 1213-1221</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1109.1088v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.1088v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.1989v1</id>
    <updated>2011-09-09T13:00:18Z</updated>
    <published>2011-09-09T13:00:18Z</published>
    <title>Efficient Personalized Web Mining: Utilizing The Most Utilized Data</title>
    <summary>  Looking into the growth of information in the web it is a very tedious
process of getting the exact information the user is looking for. Many search
engines generate user profile related data listing. This paper involves one
such process where the rating is given to the link that the user is clicking
on. Rather than avoiding the uninterested links both interested links and the
uninterested links are listed. But sorted according to the weightings given to
each link by the number of visit made by the particular user and the amount of
time spent on the particular link.
</summary>
    <author>
      <name>L. K. Joshila Grace</name>
    </author>
    <author>
      <name>V. Maheswari</name>
    </author>
    <author>
      <name>Dhinaharan Nagamalai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">conference paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1109.1989v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.1989v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.1991v1</id>
    <updated>2011-09-09T13:01:49Z</updated>
    <published>2011-09-09T13:01:49Z</published>
    <title>Effective Personalized Web Mining by Utilizing The Most Utilized Data</title>
    <summary>  Looking into the growth of information in the web it is a very tedious
process of getting the exact information the user is looking for. Many search
engines generate user profile related data listing. This paper involves one
such process where the rating is given to the link that the user is clicking
on. Rather than avoiding the uninterested links both interested links and the
uninterested links are listed. But sorted according to the weightings given to
each link by the number of visit made by the particular user and the amount of
time spent on the particular link.
</summary>
    <author>
      <name>L. K. Joshila Grace</name>
    </author>
    <author>
      <name>V. Maheswari</name>
    </author>
    <author>
      <name>Dhinaharan Nagamalai</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijdms.2011.3309</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijdms.2011.3309" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, journal paper</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Database Management Systems ( IJDMS ),
  Vol.3, No.3, August 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1109.1991v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.1991v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.6862v1</id>
    <updated>2011-09-30T15:51:12Z</updated>
    <published>2011-09-30T15:51:12Z</published>
    <title>Video OCR for Video Indexing</title>
    <summary>  Video OCR is a technique that can greatly help to locate the topics of
interest in video via the automatic extraction and reading of captions and
annotations. Text in video can provide key indexing information. Recognizing
such text for search application is critical. Major difficult problem for
character recognition for videos is degraded and deformated characters, low
resolution characters or very complex background. To tackle the problem
preprocessing on text image plays vital role. Most of the OCR engines are
working on the binary image so to find a better binarization procedure for
image to get a desired result is important.Accurate binarization process
minimizes the error rate of video OCR.
</summary>
    <author>
      <name>Sankirti S.</name>
    </author>
    <author>
      <name>P. M. Kamade</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 Pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IACSIT International Journal of Engineering and Technology, Vol.3,
  No.3, June 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1109.6862v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.6862v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.6349v1</id>
    <updated>2011-11-28T05:45:43Z</updated>
    <published>2011-11-28T05:45:43Z</published>
    <title>XML Information Retrieval Systems: A Survey</title>
    <summary>  The continuous growth in the XML information repositories has been matched by
increasing efforts in development of XML retrieval systems, in large parts
aiming at supporting content-oriented XML retrieval. These systems exploit the
available structural information, as market up in XML documents, in order to
return documents components- the so called XML elements-instead of the
complement documents in repose to the user query. In this paper, we provide an
overview of the different XML information retrieval systems and classify them
according to their storage and query evaluation strategies.
</summary>
    <author>
      <name>Awny Sayed</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 25 references</arxiv:comment>
    <link href="http://arxiv.org/abs/1111.6349v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.6349v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1112.2031v1</id>
    <updated>2011-12-09T07:24:13Z</updated>
    <published>2011-12-09T07:24:13Z</published>
    <title>Learning Context for Text Categorization</title>
    <summary>  This paper describes our work which is based on discovering context for text
document categorization. The document categorization approach is derived from a
combination of a learning paradigm known as relation extraction and an
technique known as context discovery. We demonstrate the effectiveness of our
categorization approach using reuters 21578 dataset and synthetic real world
data from sports domain. Our experimental results indicate that the learned
context greatly improves the categorization performance as compared to
traditional categorization approaches.
</summary>
    <author>
      <name>Y. V. Haribhakta</name>
    </author>
    <author>
      <name>Dr. Parag Kulkarni</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, selected in IJDKP (International Journal of Data Mining and
  Knowledge Management Process)</arxiv:comment>
    <link href="http://arxiv.org/abs/1112.2031v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1112.2031v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1112.2807v2</id>
    <updated>2012-02-09T00:10:38Z</updated>
    <published>2011-12-13T06:46:26Z</published>
    <title>Design and Implementation of a Simple Web Search Engine</title>
    <summary>  We present a simple web search engine for indexing and searching html
documents using python programming language. Because python is well known for
its simple syntax and strong support for main operating systems, we hope it
will be beneficial for learning information retrieval techniques, especially
web search engine technology.
</summary>
    <author>
      <name>Andri Mirzal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Multimedia and Ubiquitous Engineering
  International Journal of Multimedia and Ubiquitous Engineering International
  Journal of Multimedia and Ubiquitous Engineering, Vol. 7, No. 1, January,
  2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1112.2807v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1112.2807v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.0040v1</id>
    <updated>2011-12-23T14:18:35Z</updated>
    <published>2011-12-23T14:18:35Z</published>
    <title>Spam filtering by quantitative profiles</title>
    <summary>  Instead of the 'bag-of-words' representation, in the quantitative profile
approach to spam filtering and email categorization, an email is represented by
an m-dimensional vector of numbers, with m fixed in advance. Inspired by Sroufe
et al. [Sroufe, P., Phithakkitnukoon, S., Dantu, R., and Cangussu, J. (2010).
Email shape analysis. In \emph{LNCS}, 5935, pp. 18-29] two instances of
quantitative profiles are considered: line profile and character profile.
Performance of these profiles is studied on the TREC 2007, CEAS 2008 and a
private corpuses. At low computational costs, the two quantitative profiles
achieve performance that is at least comparable to that of heuristic rules and
naive Bayes.
</summary>
    <author>
      <name>M. Grendár</name>
    </author>
    <author>
      <name>J. Škutová</name>
    </author>
    <author>
      <name>V. Špitalský</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">supplementary material including the commented R source code can be
  found at http://www.savbb.sk/~grendar/spam/Supplement.html</arxiv:comment>
    <link href="http://arxiv.org/abs/1201.0040v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.0040v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.2240v1</id>
    <updated>2012-01-11T04:56:59Z</updated>
    <published>2012-01-11T04:56:59Z</published>
    <title>Bengali text summarization by sentence extraction</title>
    <summary>  Text summarization is a process to produce an abstract or a summary by
selecting significant portion of the information from one or more texts. In an
automatic text summarization process, a text is given to the computer and the
computer returns a shorter less redundant extract or abstract of the original
text(s). Many techniques have been developed for summarizing English text(s).
But, a very few attempts have been made for Bengali text summarization. This
paper presents a method for Bengali text summarization which extracts important
sentences from a Bengali document to produce a summary.
</summary>
    <author>
      <name>Kamal Sarkar</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of International Conference on Business and
  Information Management(ICBIM-2012),NIT Durgapur, PP 233-245</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1201.2240v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.2240v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.2187v1</id>
    <updated>2012-02-10T04:49:58Z</updated>
    <published>2012-02-10T04:49:58Z</published>
    <title>Museum: Multidimensional web page segment evaluation model</title>
    <summary>  The evaluation of a web page with respect to a query is a vital task in the
web information retrieval domain. This paper proposes the evaluation of a web
page as a bottom-up process from the segment level to the page level. A model
for evaluating the relevancy is proposed incorporating six different
dimensions. An algorithm for evaluating the segments of a web page, using the
above mentioned six dimensions is proposed. The benefits of fine-granining the
evaluation process to the segment level instead of the page level are explored.
The proposed model can be incorporated for various tasks like web page
personalization, result re-ranking, mobile device page rendering etc.
</summary>
    <author>
      <name>K. S. Kuppusamy</name>
    </author>
    <author>
      <name>G. Aghila</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ISSN 2151-9617</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Computing Volume 3, Issue 3 (2011) 24-27</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1202.2187v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.2187v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P20" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.2393v1</id>
    <updated>2012-02-11T00:12:58Z</updated>
    <published>2012-02-11T00:12:58Z</published>
    <title>Statistical reliability and path diversity based PageRank algorithm
  improvements</title>
    <summary>  In this paper we present new improvement ideas of the original PageRank
algorithm. The first idea is to introduce an evaluation of the statistical
reliability of the ranking score of each node based on the local graph property
and the second one is to introduce the notion of the path diversity. The path
diversity can be exploited to dynamically modify the increment value of each
node in the random surfer model or to dynamically adapt the damping factor. We
illustrate the impact of such modifications through examples and simple
simulations.
</summary>
    <author>
      <name>Dohy Hong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1202.2393v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.2393v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.2.2; F.2.2; H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.2622v1</id>
    <updated>2012-02-13T04:21:15Z</updated>
    <published>2012-02-13T04:21:15Z</published>
    <title>A Model for Web Page Usage Mining Based on Segmentation</title>
    <summary>  The web page usage mining plays a vital role in enriching the page's content
and structure based on the feedbacks received from the user's interactions with
the page. This paper proposes a model for micro-managing the tracking
activities by fine-tuning the mining from the page level to the segment level.
The proposed model enables the web-master to identify the segments which
receives more focus from users comparing with others. The segment level
analytics of user actions provides an important metric to analyse the factors
which facilitate the increase in traffic for the page. The empirical validation
of the model is performed through prototype implementation.
</summary>
    <author>
      <name>K. S. Kuppusamy</name>
    </author>
    <author>
      <name>G. Aghila</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science and Information
  Technologies, Vol. 2, No 2 , 2011, 1144-1148</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1202.2622v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.2622v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P20" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.0747v2</id>
    <updated>2012-06-19T10:19:26Z</updated>
    <published>2012-03-04T16:33:41Z</published>
    <title>A review of EO image information mining</title>
    <summary>  We analyze the state of the art of content-based retrieval in Earth
observation image archives focusing on complete systems showing promise for
operational implementation. The different paradigms at the basis of the main
system families are introduced. The approaches taken are analyzed, focusing in
particular on the phases after primitive feature extraction. The solutions
envisaged for the issues related to feature simplification and synthesis,
indexing, semantic labeling are reviewed. The methodologies for query
specification and execution are analyzed.
</summary>
    <author>
      <name>Marco Quartulli</name>
    </author>
    <author>
      <name>Igor G. Olaizola</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Quartulli, Marco, and Igor G Olaizola. "A review of EO image
  information mining." ISPRS Journal of Photogrammetry and Remote Sensing 75:
  p11-28. 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1203.0747v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.0747v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.1793v1</id>
    <updated>2012-03-08T13:28:47Z</updated>
    <published>2012-03-08T13:28:47Z</published>
    <title>Using Hausdorff Distance for New Medical Image Annotation</title>
    <summary>  Medical images annotation is most of the time a repetitive hard task.
Collecting old similar annotations and assigning them to new medical images may
not only enhance the annotation process, but also reduce ambiguity caused by
repetitive annotations. The goal of this work is to propose an approach based
on Hausdorff distance able to compute similarity between a new medical image
and old stored images. User has to choose then one of the similar images and
annotations related to the selected one are assigned to the new one.
</summary>
    <author>
      <name>Riadh Bouslimi</name>
    </author>
    <author>
      <name>Jalel Akaichi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures, 2 tables; International Journal of Database
  Management Systems (IJDMS) Vol.4, No.1, February 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1203.1793v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.1793v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.2569v1</id>
    <updated>2012-03-12T17:57:40Z</updated>
    <published>2012-03-12T17:57:40Z</published>
    <title>When Index Term Probability Violates the Classical Probability Axioms
  Quantum Probability can be a Necessary Theory for Information Retrieval</title>
    <summary>  Probabilistic models require the notion of event space for defining a
probability measure. An event space has a probability measure which ensues the
Kolmogorov axioms. However, the probabilities observed from distinct sources,
such as that of relevance of documents, may not admit a single event space thus
causing some issues. In this article, some results are introduced for ensuring
whether the observed prob- abilities of relevance of documents admit a single
event space. More- over, an alternative framework of probability is introduced,
thus chal- lenging the use of classical probability for ranking documents. Some
reflections on the convenience of extending the classical probabilis- tic
retrieval toward a more general framework which encompasses the issues are
made.
</summary>
    <author>
      <name>Massimo Melucci</name>
    </author>
    <link href="http://arxiv.org/abs/1203.2569v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.2569v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.3764v1</id>
    <updated>2012-03-16T16:59:03Z</updated>
    <published>2012-03-16T16:59:03Z</published>
    <title>The Abzooba Smart Health Informatics Platform (SHIP) TM - From Patient
  Experiences to Big Data to Insights</title>
    <summary>  This paper describes a technology to connect patients to information in the
experiences of other patients by using the power of structured big data. The
approach, implemented in the Abzooba Smart Health Informatics Platform
(SHIP),is to distill concepts of facts and expressions from conversations and
discussions in health social media forums, and use those distilled concepts in
connecting patients to experiences and insights that are highly relevant to
them in particular. We envision our work, in progress, to provide new and
effective tools to exploit the richness of content in social media in health
for outcomes research.
</summary>
    <author>
      <name>Naveen Ashish</name>
    </author>
    <author>
      <name>Antarip Biswas</name>
    </author>
    <author>
      <name>Sumit Das</name>
    </author>
    <author>
      <name>Saurav Nag</name>
    </author>
    <author>
      <name>Rajiv Pratap</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1203.3764v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.3764v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.2032v3</id>
    <updated>2012-10-16T07:00:44Z</updated>
    <published>2012-04-10T02:53:03Z</published>
    <title>Multi-Output Recommender: Items, Groups and Friends, and Their Mutual
  Contributing Effects</title>
    <summary>  Due to the development of social media technology, it becomes easier for
users to gather together to form groups. Take the Last.fm for example, users
can join groups they may be interested where they can share their loved songs
and discuss topics about songs and singers. However, the number of groups grows
over time, users need effective groups recommendations in order to meet more
like-minded users.
</summary>
    <author>
      <name>Wei Zeng</name>
    </author>
    <author>
      <name>Li Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">withdraw the article</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.2032v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.2032v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.1602v1</id>
    <updated>2012-05-08T06:52:15Z</updated>
    <published>2012-05-08T06:52:15Z</published>
    <title>Indexing of Arabic documents automatically based on lexical analysis</title>
    <summary>  The continuous information explosion through the Internet and all information
sources makes it necessary to perform all information processing activities
automatically in quick and reliable manners. In this paper, we proposed and
implemented a method to automatically create and Index for books written in
Arabic language. The process depends largely on text summarization and
abstraction processes to collect main topics and statements in the book. The
process is developed in terms of accuracy and performance and results showed
that this process can effectively replace the effort of manually indexing books
and document, a process that can be very useful in all information processing
and retrieval applications.
</summary>
    <author>
      <name>Abdulrahman Al Molijy</name>
    </author>
    <author>
      <name>Ismail Hmeidi</name>
    </author>
    <author>
      <name>Izzat Alsmadi</name>
    </author>
    <link href="http://arxiv.org/abs/1205.1602v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.1602v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.1638v1</id>
    <updated>2012-05-08T09:19:10Z</updated>
    <published>2012-05-08T09:19:10Z</published>
    <title>Document summarization using positive pointwise mutual information</title>
    <summary>  The degree of success in document summarization processes depends on the
performance of the method used in identifying significant sentences in the
documents. The collection of unique words characterizes the major signature of
the document, and forms the basis for Term-Sentence-Matrix (TSM). The Positive
Pointwise Mutual Information, which works well for measuring semantic
similarity in the Term-Sentence-Matrix, is used in our method to assign weights
for each entry in the Term-Sentence-Matrix. The Sentence-Rank-Matrix generated
from this weighted TSM, is then used to extract a summary from the document.
Our experiments show that such a method would outperform most of the existing
methods in producing summaries from large documents.
</summary>
    <author>
      <name>Aji S</name>
    </author>
    <author>
      <name>Ramachandra Kaimal</name>
    </author>
    <link href="http://arxiv.org/abs/1205.1638v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.1638v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.3031v1</id>
    <updated>2012-05-14T14:01:56Z</updated>
    <published>2012-05-14T14:01:56Z</published>
    <title>The model of information retrieval based on the theory of hypercomplex
  numerical systems</title>
    <summary>  The paper provided a description of a new model of information retrieval,
which is an extension of vector-space model and is based on the principles of
the theory of hypercomplex numerical systems. The model allows to some extent
realize the idea of fuzzy search and allows you to apply in practice the model
of information retrieval practical developments in the field of hypercomplex
numerical systems.
</summary>
    <author>
      <name>D. V. Lande</name>
    </author>
    <author>
      <name>Ya. A. Kalinovskiy</name>
    </author>
    <author>
      <name>Yu. E. Boyarinova</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1205.3031v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.3031v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.3193v1</id>
    <updated>2012-05-14T21:08:05Z</updated>
    <published>2012-05-14T21:08:05Z</published>
    <title>A Comparative Study of Collaborative Filtering Algorithms</title>
    <summary>  Collaborative filtering is a rapidly advancing research area. Every year
several new techniques are proposed and yet it is not clear which of the
techniques work best and under what conditions. In this paper we conduct a
study comparing several collaborative filtering techniques -- both classic and
recent state-of-the-art -- in a variety of experimental contexts. Specifically,
we report conclusions controlling for number of items, number of users,
sparsity level, performance criteria, and computational complexity. Our
conclusions identify what algorithms work well and in what conditions, and
contribute to both industrial deployment collaborative filtering algorithms and
to the research community.
</summary>
    <author>
      <name>Joonseok Lee</name>
    </author>
    <author>
      <name>Mingxuan Sun</name>
    </author>
    <author>
      <name>Guy Lebanon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1205.3193v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.3193v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.5632v1</id>
    <updated>2012-05-25T08:43:06Z</updated>
    <published>2012-05-25T08:43:06Z</published>
    <title>Quantum contextuality in classical information retrieval</title>
    <summary>  Document ranking based on probabilistic evaluations of relevance is known to
exhibit non-classical correlations, which may be explained by admitting a
complex structure of the event space, namely, by assuming the events to emerge
from multiple sample spaces. The structure of event space formed by overlapping
sample spaces is known in quantum mechanics, they may exhibit some
counter-intuitive features, called quantum contextuality. In this Note I
observe that from the structural point of view quantum contextuality looks
similar to personalization of information retrieval scenarios. Along these
lines, Knowledge Revision is treated as operationalistic measurement and a way
to quantify the rate of personalization of Information Retrieval scenarios is
suggested.
</summary>
    <author>
      <name>Roman Zapatrin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1205.5632v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.5632v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="81P13" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.5923v1</id>
    <updated>2012-05-26T22:45:50Z</updated>
    <published>2012-05-26T22:45:50Z</published>
    <title>Conversion database of the shapes into XML data for shape matching</title>
    <summary>  We present a new approach to the matching of 2D shapes using XML language and
dynamic programming. Given a 2D shape, we extract its contour and which is
represented by set of points. The contour is divided into curves using corner
detection. After, each curve is described by local and global features; these
features are coded in a string of symbols and stored in a XML file. Finally,
using the dynamic programming, we find the optimal alignment between sequences
of symbols. Results are presented and compared with existing methods using
MATLAB for KIMIA-25 database and MPEG7 databases.
</summary>
    <author>
      <name>Noreddine Gherabi</name>
    </author>
    <author>
      <name>Mohamed Bahaj</name>
    </author>
    <link href="http://arxiv.org/abs/1205.5923v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.5923v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.1615v1</id>
    <updated>2012-06-07T21:04:15Z</updated>
    <published>2012-06-07T21:04:15Z</published>
    <title>Objects and Goals Extraction from Semantic Networks : Applications of
  Fuzzy SetS Theory</title>
    <summary>  In this paper we present a short survey of fuzzy and Semantic approaches to
Knowledge Extraction. The goal of such approaches is to define flexible
Knowledge Extraction Systems able to deal with the inherent vagueness and
uncertainty of the Extraction process. In this survey we address if and how
some approaches met their goal.
</summary>
    <author>
      <name>Mohamed Nazih Omri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1206.1042, arXiv:1206.0925</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Conf\'erence Internationale : Science \'Electroniques,
  Technologies de l'Information et des T\'el\'ecommunications(SETIT), p.
  275-282, Mahdia, Tunisie, 2003</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1206.1615v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.1615v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.1624v1</id>
    <updated>2012-06-07T21:36:08Z</updated>
    <published>2012-06-07T21:36:08Z</published>
    <title>Measure of Similarity between Fuzzy Concepts for Optimization of Fuzzy
  Semantic Nets</title>
    <summary>  This paper presents a method to measure the similarity between different
fuzzy concepts in order to optimize Semantic networks. The problem approached
is the minimization of the time of research and identification of user's
Objects and Goals. Indeed, it concerns to determine to each instant the
totality of Objects (respectively Goals) among which one can identify rapidly
the most satisfactory for the user's Object and Goal. Alone Objects and most
similar Goals to Objects and researched Goals of the viewpoint of attribute
values will be processed, what will avoid the analysis of all Objects and
system Goals far of needs of the user.
</summary>
    <author>
      <name>Mohamed nazih Omri</name>
    </author>
    <author>
      <name>Noureddine Chouigui</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14th International Conference on Systems Science. Wroclaw, Poland,
  2001</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.1624v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.1624v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.3278v1</id>
    <updated>2012-06-13T15:42:17Z</updated>
    <published>2012-06-13T15:42:17Z</published>
    <title>Topic Models Conditioned on Arbitrary Features with
  Dirichlet-multinomial Regression</title>
    <summary>  Although fully generative models have been successfully used to model the
contents of text documents, they are often awkward to apply to combinations of
text data and document metadata. In this paper we propose a
Dirichlet-multinomial regression (DMR) topic model that includes a log-linear
prior on document-topic distributions that is a function of observed features
of the document, such as author, publication venue, references, and dates. We
show that by selecting appropriate features, DMR topic models can meet or
exceed the performance of several previously published topic models designed
for specific data.
</summary>
    <author>
      <name>David Mimno</name>
    </author>
    <author>
      <name>Andrew McCallum</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.3278v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.3278v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.5584v1</id>
    <updated>2012-06-25T06:07:20Z</updated>
    <published>2012-06-25T06:07:20Z</published>
    <title>Web-page Prediction for Domain Specific Web-search using Boolean Bit
  Mask</title>
    <summary>  Search Engine is a Web-page retrieval tool. Nowadays Web searchers utilize
their time using an efficient search engine. To improve the performance of the
search engine, we are introducing a unique mechanism which will give Web
searchers more prominent search results. In this paper, we are going to discuss
a domain specific Web search prototype which will generate the predicted
Web-page list for user given search string using Boolean bit mask.
</summary>
    <author>
      <name>Sukanta Sinha</name>
    </author>
    <author>
      <name>Rana Duttagupta</name>
    </author>
    <author>
      <name>Debajyoti Mukhopadhyay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Advances in Computer Science, Eng. &amp; Appl., AISC 167, pp. 211-220,
  Springerlink, 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1206.5584v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.5584v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.6858v1</id>
    <updated>2012-06-27T16:26:46Z</updated>
    <published>2012-06-27T16:26:46Z</published>
    <title>Sequential Document Representations and Simplicial Curves</title>
    <summary>  The popular bag of words assumption represents a document as a histogram of
word occurrences. While computationally efficient, such a representation is
unable to maintain any sequential information. We present a continuous and
differentiable sequential document representation that goes beyond the bag of
words assumption, and yet is efficient and effective. This representation
employs smooth curves in the multinomial simplex to account for sequential
information. We discuss the representation and its geometric properties and
demonstrate its applicability for the task of text classification.
</summary>
    <author>
      <name>Guy Lebanon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Twenty-Second Conference on Uncertainty
  in Artificial Intelligence (UAI2006)</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.6858v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.6858v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.1414v1</id>
    <updated>2012-07-04T16:23:52Z</updated>
    <published>2012-07-04T16:23:52Z</published>
    <title>Two-Way Latent Grouping Model for User Preference Prediction</title>
    <summary>  We introduce a novel latent grouping model for predicting the relevance of a
new document to a user. The model assumes a latent group structure for both
users and documents. We compared the model against a state-of-the-art method,
the User Rating Profile model, where only users have a latent group structure.
We estimate both models by Gibbs sampling. The new method predicts relevance
more accurately for new documents that have few known ratings. The reason is
that generalization over documents then becomes necessary and hence the twoway
grouping is profitable.
</summary>
    <author>
      <name>Eerika Savia</name>
    </author>
    <author>
      <name>Kai Puolamaki</name>
    </author>
    <author>
      <name>Janne Sinkkonen</name>
    </author>
    <author>
      <name>Samuel Kaski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Twenty-First Conference on Uncertainty
  in Artificial Intelligence (UAI2005)</arxiv:comment>
    <link href="http://arxiv.org/abs/1207.1414v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.1414v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.3583v1</id>
    <updated>2012-07-16T06:07:47Z</updated>
    <published>2012-07-16T06:07:47Z</published>
    <title>Information Retrieval Model: A Social Network Extraction Perspective</title>
    <summary>  Future Information Retrieval, especially in connection with the internet,
will incorporate the content descriptions that are generated with social
network extraction technologies and preferably incorporate the probability
theory for assigning the semantic. Although there is an increasing interest
about social network extraction, but a little of them has a significant impact
to infomation retrieval. Therefore this paper proposes a model of information
retrieval from the social network extraction.
</summary>
    <author>
      <name>Mahyuddin K. M. Nasution</name>
    </author>
    <author>
      <name>Shahrul Azman Noah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1207.3583v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.3583v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.4152v1</id>
    <updated>2012-07-11T14:59:15Z</updated>
    <published>2012-07-11T14:59:15Z</published>
    <title>Maximum Entropy for Collaborative Filtering</title>
    <summary>  Within the task of collaborative filtering two challenges for computing
conditional probabilities exist. First, the amount of training data available
is typically sparse with respect to the size of the domain. Thus, support for
higher-order interactions is generally not present. Second, the variables that
we are conditioning upon vary for each query. That is, users label different
variables during each query. For this reason, there is no consistent input to
output mapping. To address these problems we purpose a maximum entropy approach
using a non-standard measure of entropy. This approach can be simplified to
solving a set of linear equations that can be efficiently solved.
</summary>
    <author>
      <name>Lawrence Zitnick</name>
    </author>
    <author>
      <name>Takeo Kanade</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Twentieth Conference on Uncertainty in
  Artificial Intelligence (UAI2004)</arxiv:comment>
    <link href="http://arxiv.org/abs/1207.4152v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.4152v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.4259v1</id>
    <updated>2012-07-18T04:11:55Z</updated>
    <published>2012-07-18T04:11:55Z</published>
    <title>Content Based Multimedia Information Retrieval to Support Digital
  Libraries</title>
    <summary>  Content-based multimedia information retrieval is an interesting research
area since it allows retrieval based on inherent characteristic of multimedia
objects. For example retrieval based on visual characteristics such as colour,
shapes or textures of objects in images or retrieval based on spatial
relationships among objects in the media (images or video clips). This paper
reviews some work done in image and video retrieval and then proposes an
integrated model that can handle images and video clips uniformly. Using this
model retrieval on images or video clips can be done based on the same
framework.
</summary>
    <author>
      <name>Mohammad Nabil Almunawar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, conference paper</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on New Information Technologies, 23-26
  July, 2001, Brunei Darussalam</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1207.4259v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.4259v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.6600v1</id>
    <updated>2012-07-27T17:16:59Z</updated>
    <published>2012-07-27T17:16:59Z</published>
    <title>Diversity in Ranking using Negative Reinforcement</title>
    <summary>  In this paper, we consider the problem of diversity in ranking of the nodes
in a graph. The task is to pick the top-k nodes in the graph which are both
'central' and 'diverse'. Many graph-based models of NLP like text
summarization, opinion summarization involve the concept of diversity in
generating the summaries. We develop a novel method which works in an iterative
fashion based on random walks to achieve diversity. Specifically, we use
negative reinforcement as a main tool to introduce diversity in the
Personalized PageRank framework. Experiments on two benchmark datasets show
that our algorithm is competitive to the existing methods.
</summary>
    <author>
      <name>Rama Badrinath</name>
    </author>
    <author>
      <name>C. E. Veni Madhavan</name>
    </author>
    <link href="http://arxiv.org/abs/1207.6600v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.6600v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.1011v1</id>
    <updated>2012-08-05T14:03:22Z</updated>
    <published>2012-08-05T14:03:22Z</published>
    <title>Credibility in Web Search Engines</title>
    <summary>  Web search engines apply a variety of ranking signals to achieve user
satisfaction, i.e., results pages that provide the best-possible results to the
user. While these ranking signals implicitly consider credibility (e.g., by
measuring popularity), explicit measures of credibility are not applied. In
this chapter, credibility in Web search engines is discussed in a broad
context: credibility as a measure for including documents in a search engine's
index, credibility as a ranking signal, credibility in the context of universal
search results, and the possibility of using credibility as an explicit measure
for ranking purposes. It is found that while search engines-at least to a
certain extent-show credible results to their users, there is no fully
integrated credibility framework for Web search engines.
</summary>
    <author>
      <name>Dirk Lewandowski</name>
    </author>
    <link href="http://arxiv.org/abs/1208.1011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.1011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.4147v3</id>
    <updated>2015-11-27T22:55:51Z</updated>
    <published>2012-08-21T00:28:32Z</published>
    <title>Generating ordered list of Recommended Items: a Hybrid Recommender
  System of Microblog</title>
    <summary>  Precise recommendation of followers helps in improving the user experience
and maintaining the prosperity of twitter and microblog platforms. In this
paper, we design a hybrid recommender system of microblog as a solution of KDD
Cup 2012, track 1 task, which requires predicting users a user might follow in
Tencent Microblog. We describe the background of the problem and present the
algorithm consisting of keyword analysis, user taxonomy, (potential)interests
extraction and item recommendation. Experimental result shows the high
performance of our algorithm. Some possible improvements are discussed, which
leads to further study.
</summary>
    <author>
      <name>Yingzhen Li</name>
    </author>
    <author>
      <name>Ye Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1208.4147v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.4147v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.1125v1</id>
    <updated>2012-09-05T21:28:32Z</updated>
    <published>2012-09-05T21:28:32Z</published>
    <title>Video Data Visualization System: Semantic Classification And
  Personalization</title>
    <summary>  We present in this paper an intelligent video data visualization tool, based
on semantic classification, for retrieving and exploring a large scale corpus
of videos. Our work is based on semantic classification resulting from semantic
analysis of video. The obtained classes will be projected in the visualization
space. The graph is represented by nodes and edges, the nodes are the keyframes
of video documents and the edges are the relation between documents and the
classes of documents. Finally, we construct the user's profile, based on the
interaction with the system, to render the system more adequate to its
references.
</summary>
    <author>
      <name>Jamel Slimi</name>
    </author>
    <author>
      <name>Anis Ben Ammar</name>
    </author>
    <author>
      <name>Adel M. Alimi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijcga.2012.2201</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijcga.2012.2201" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">graphics</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.1125v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.1125v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.1719v1</id>
    <updated>2012-09-08T14:02:12Z</updated>
    <published>2012-09-08T14:02:12Z</published>
    <title>Semi-metric networks for recommender systems</title>
    <summary>  Weighted graphs obtained from co-occurrence in user-item relations lead to
non-metric topologies. We use this semi-metric behavior to issue
recommendations, and discuss its relationship to transitive closure on fuzzy
graphs. Finally, we test the performance of this method against other item- and
user-based recommender systems on the Movielens benchmark. We show that
including highly semi-metric edges in our recommendation algorithms leads to
better recommendations.
</summary>
    <author>
      <name>Tiago Simas</name>
    </author>
    <author>
      <name>Luis M. Rocha</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2012 IEEE/WIC/ACM International Conference on Web Intelligence and
  Intelligent Agent Technology</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1209.1719v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.1719v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.4479v1</id>
    <updated>2012-09-20T09:59:53Z</updated>
    <published>2012-09-20T09:59:53Z</published>
    <title>Beyond Cumulated Gain and Average Precision: Including Willingness and
  Expectation in the User Model</title>
    <summary>  In this paper, we define a new metric family based on two concepts: The
definition of the stopping criterion and the notion of satisfaction, where the
former depends on the willingness and expectation of a user exploring search
results. Both concepts have been discussed so far in the IR literature, but we
argue in this paper that defining a proper single valued metric depends on
merging them into a single conceptual framework.
</summary>
    <author>
      <name>Benjamin Piwowarski</name>
    </author>
    <author>
      <name>Georges Dupret</name>
    </author>
    <author>
      <name>Mounia Lalmas</name>
    </author>
    <link href="http://arxiv.org/abs/1209.4479v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.4479v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; H.3.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.6492v1</id>
    <updated>2012-09-28T11:50:15Z</updated>
    <published>2012-09-28T11:50:15Z</published>
    <title>Information Retrieval on the web and its evaluation</title>
    <summary>  Internet is one of the main sources of information for millions of people.
One can find information related to practically all matters on internet.
Moreover if we want to retrieve information about some particular topic we may
find thousands of Web Pages related to that topic. But our main concern is to
find relevant Web Pages from among that collection. So in this paper I have
discussed that how information is retrieved from the web and the efforts
required for retrieving this information in terms of system and users efforts.
</summary>
    <author>
      <name>Deepika Sharma</name>
    </author>
    <author>
      <name>Deepak Garg</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Applications 40(3):26-31, 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1209.6492v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.6492v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.3312v1</id>
    <updated>2012-10-11T18:21:01Z</updated>
    <published>2012-10-11T18:21:01Z</published>
    <title>Artex is AnotheR TEXt summarizer</title>
    <summary>  This paper describes Artex, another algorithm for Automatic Text
Summarization. In order to rank sentences, a simple inner product is calculated
between each sentence, a document vector (text topic) and a lexical vector
(vocabulary used by a sentence). Summaries are then generated by assembling the
highest ranked sentences. No ruled-based linguistic post-processing is
necessary in order to obtain summaries. Tests over several datasets (coming
from Document Understanding Conferences (DUC), Text Analysis Conferences (TAC),
evaluation campaigns, etc.) in French, English and Spanish have shown that
summarizer achieves interesting results.
</summary>
    <author>
      <name>Juan-Manuel Torres-Moreno</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 5 figures. arXiv admin note: substantial text overlap with
  arXiv:1209.3126</arxiv:comment>
    <link href="http://arxiv.org/abs/1210.3312v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.3312v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.0689v1</id>
    <updated>2012-11-04T14:50:16Z</updated>
    <published>2012-11-04T14:50:16Z</published>
    <title>Enhancing Invenio Digital Library With An External Relevance Ranking
  Engine</title>
    <summary>  Invenio is a comprehensive web-based free digital library software suite
originally developed at CERN. In order to improve its information retrieval and
word similarity ranking capabilities, the goal of this thesis is to enhance
Invenio by bridging it with modern external information retrieval systems. In
the first part a comparison of various information retrieval systems such as
Solr and Xapian is made. In the second part a system-independent bridge for
word similarity ranking is designed and implemented. Subsequently, Solr and
Xapian are integrated in Invenio via adapters to the bridge. In the third part
scalability tests are performed. Finally, a future outlook is briefly
discussed.
</summary>
    <author>
      <name>Patrick O. Glauner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">70 pages, 34 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1211.0689v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.0689v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.3402v1</id>
    <updated>2012-11-14T20:04:51Z</updated>
    <published>2012-11-14T20:04:51Z</published>
    <title>Genetic Optimization of Keywords Subset in the Classification Analysis
  of Texts Authorship</title>
    <summary>  The genetic selection of keywords set, the text frequencies of which are
considered as attributes in text classification analysis, has been analyzed.
The genetic optimization was performed on a set of words, which is the fraction
of the frequency dictionary with given frequency limits. The frequency
dictionary was formed on the basis of analyzed text array of texts of English
fiction. As the fitness function which is minimized by the genetic algorithm,
the error of nearest k neighbors classifier was used. The obtained results show
high precision and recall of texts classification by authorship categories on
the basis of attributes of keywords set which were selected by the genetic
algorithm from the frequency dictionary.
</summary>
    <author>
      <name>Bohdan Pavlyshenko</name>
    </author>
    <link href="http://arxiv.org/abs/1211.3402v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.3402v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.1068v1</id>
    <updated>2012-12-05T15:58:19Z</updated>
    <published>2012-12-05T15:58:19Z</published>
    <title>Spectral properties of Google matrix of Wikipedia and other networks</title>
    <summary>  We study the properties of eigenvalues and eigenvectors of the Google matrix
of the Wikipedia articles hyperlink network and other real networks. With the
help of the Arnoldi method we analyze the distribution of eigenvalues in the
complex plane and show that eigenstates with significant eigenvalue modulus are
located on well defined network communities. We also show that the correlator
between PageRank and CheiRank vectors distinguishes different organizations of
information flow on BBC and Le Monde web sites.
</summary>
    <author>
      <name>Leonardo Ermann</name>
    </author>
    <author>
      <name>Klaus M. Frahm</name>
    </author>
    <author>
      <name>Dima L. Shepelyansky</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1140/epjb/e2013-31090-8</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1140/epjb/e2013-31090-8" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 9 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Eur. Phys. J. B 86, 193 (2013)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1212.1068v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.1068v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.1918v1</id>
    <updated>2012-12-09T20:55:52Z</updated>
    <published>2012-12-09T20:55:52Z</published>
    <title>Condensés de textes par des méthodes numériques</title>
    <summary>  Since information in electronic form is already a standard, and that the
variety and the quantity of information become increasingly large, the methods
of summarizing or automatic condensation of texts is a critical phase of the
analysis of texts. This article describes CORTEX a system based on numerical
methods, which allows obtaining a condensation of a text, which is independent
of the topic and of the length of the text. The structure of the system enables
it to find the abstracts in French or Spanish in very short times.
</summary>
    <author>
      <name>Juan-Manuel Torres-Moreno</name>
    </author>
    <author>
      <name>Patricia Velázquez-Morales</name>
    </author>
    <author>
      <name>Jean-Guy Meunier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Conf\'erence JADT 2002, Saint-Malo/France. 12 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.1918v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.1918v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.2065v1</id>
    <updated>2012-12-10T13:57:59Z</updated>
    <published>2012-12-10T13:57:59Z</published>
    <title>A Survey on Information Retrieval, Text Categorization, and Web Crawling</title>
    <summary>  This paper is a survey discussing Information Retrieval concepts, methods,
and applications. It goes deep into the document and query modelling involved
in IR systems, in addition to pre-processing operations such as removing stop
words and searching by synonym techniques. The paper also tackles text
categorization along with its application in neural networks and machine
learning. Finally, the architecture of web crawlers is to be discussed shedding
the light on how internet spiders index web documents and how they allow users
to search for items on the web.
</summary>
    <author>
      <name>Youssef Bassil</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">LACSC - Lebanese Association for Computational Sciences,
  http://www.lacsc.org</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Computer Science &amp; Research (JCSCR), Vol. 1, No. 6, pp.
  1-11, 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1212.2065v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.2065v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.2145v1</id>
    <updated>2012-12-10T17:39:44Z</updated>
    <published>2012-12-10T17:39:44Z</published>
    <title>A Scale-Space Theory for Text</title>
    <summary>  Scale-space theory has been established primarily by the computer vision and
signal processing communities as a well-founded and promising framework for
multi-scale processing of signals (e.g., images). By embedding an original
signal into a family of gradually coarsen signals parameterized with a
continuous scale parameter, it provides a formal framework to capture the
structure of a signal at different scales in a consistent way. In this paper,
we present a scale space theory for text by integrating semantic and spatial
filters, and demonstrate how natural language documents can be understood,
processed and analyzed at multiple resolutions, and how this scale-space
representation can be used to facilitate a variety of NLP and text analysis
tasks.
</summary>
    <author>
      <name>Shuang-Hong Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 6 figures; Nature language processing</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.2145v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.2145v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.3013v1</id>
    <updated>2012-12-12T23:25:46Z</updated>
    <published>2012-12-12T23:25:46Z</published>
    <title>Product/Brand extraction from WikiPedia</title>
    <summary>  In this paper we describe the task of extracting product and brand pages from
wikipedia. We present an experimental environment and setup built on top of a
dataset of wikipedia pages we collected. We introduce a method for recognition
of product pages modelled as a boolean probabilistic classification task. We
show that this approach can lead to promising results and we discuss
alternative approaches we considered.
</summary>
    <author>
      <name>K. Massoudi</name>
    </author>
    <author>
      <name>G. Modena</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages. Manuscript first creation date: November 27, 2009. At the
  time of first creation both authors were affiliated with the University of
  Amsterdam (The Netherlands)</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.3013v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.3013v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.3023v1</id>
    <updated>2012-12-13T00:34:23Z</updated>
    <published>2012-12-13T00:34:23Z</published>
    <title>Keyword Extraction for Identifying Social Actors</title>
    <summary>  Identifying the social actor has become one of tasks in Artificial
Intelligence, whereby extracting keyword from Web snippets depend on the use of
web is steadily gaining ground in this research. We develop therefore an
approach based on overlap principle for utilizing a collection of features in
web snippets, where use of keyword will eliminate the un-relevant web pages.
</summary>
    <author>
      <name>Mahyuddin K. M. Nasution</name>
    </author>
    <author>
      <name>Shahrul Azman Mohd Noah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, nothing, draft to ICOCSIM 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.3023v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.3023v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.5442v1</id>
    <updated>2012-12-21T14:00:23Z</updated>
    <published>2012-12-21T14:00:23Z</published>
    <title>Étude comparée de quatre logiciels de gestion de références
  bibliographiques libres ou gratuits</title>
    <summary>  This article is the result of the analysis of various bibliographic reference
management tools, especially those that are free. The use of editorial tools by
bibliographic editors has evolved rapidly since 2007. But, until recently, free
software has fallen short when it comes to ergonomics or use. The functional
and technical panorama offered by free software is the result of the comparison
of JabRef, Mendeley Desktop, BibDesk and Zotero software undertaken in January
2012 by two research professors affiliated with the Institut national
fran\c{c}ais des techniques de la documentation (INTD).
</summary>
    <author>
      <name>Gérald Kembellec</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DICEN CNAM</arxiv:affiliation>
    </author>
    <author>
      <name>Claire Scopsi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DICEN CNAM</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Documentation et Bibliotheques 58, 4 (2012) 187-197</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1212.5442v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.5442v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.2320v1</id>
    <updated>2013-01-10T16:27:15Z</updated>
    <published>2013-01-10T16:27:15Z</published>
    <title>Using Temporal Data for Making Recommendations</title>
    <summary>  We treat collaborative filtering as a univariate time series estimation
problem: given a user's previous votes, predict the next vote. We describe two
families of methods for transforming data to encode time order in ways amenable
to off-the-shelf classification and density estimation tools, and examine the
results of using these approaches on several real-world data sets. The
improvements in predictive accuracy we realize recommend the use of other
predictive algorithms that exploit the temporal order of data.
</summary>
    <author>
      <name>Andrew Zimdars</name>
    </author>
    <author>
      <name>David Maxwell Chickering</name>
    </author>
    <author>
      <name>Christopher Meek</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.2320v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.2320v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.4171v1</id>
    <updated>2013-01-17T17:46:27Z</updated>
    <published>2013-01-17T17:46:27Z</published>
    <title>Affinity Weighted Embedding</title>
    <summary>  Supervised (linear) embedding models like Wsabie and PSI have proven
successful at ranking, recommendation and annotation tasks. However, despite
being scalable to large datasets they do not take full advantage of the extra
data due to their linear nature, and typically underfit. We propose a new class
of models which aim to provide improved performance while retaining many of the
benefits of the existing class of embedding models. Our new approach works by
iteratively learning a linear embedding model where the next iteration's
features and labels are reweighted as a function of the previous iteration. We
describe several variants of the family, and give some initial results.
</summary>
    <author>
      <name>Jason Weston</name>
    </author>
    <author>
      <name>Ron Weiss</name>
    </author>
    <author>
      <name>Hector Yee</name>
    </author>
    <link href="http://arxiv.org/abs/1301.4171v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.4171v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.4916v1</id>
    <updated>2013-02-20T14:38:38Z</updated>
    <published>2013-02-20T14:38:38Z</published>
    <title>Stacking from Tags: Clustering Bookmarks around a Theme</title>
    <summary>  Since very recently, users on the social bookmarking service Delicious can
stack web pages in addition to tagging them. Stacking enables users to group
web pages around specific themes with the aim of recommending to others.
However, users still stack a small subset of what they tag, and thus many web
pages remain unstacked. This paper presents early research towards
automatically clustering web pages from tags to find stacks and extend
recommendations.
</summary>
    <author>
      <name>Arkaitz Zubiaga</name>
    </author>
    <author>
      <name>Alberto Pérez García-Plaza</name>
    </author>
    <author>
      <name>Víctor Fresno</name>
    </author>
    <author>
      <name>Raquel Martínez</name>
    </author>
    <link href="http://arxiv.org/abs/1302.4916v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.4916v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.7039v1</id>
    <updated>2013-02-28T00:21:38Z</updated>
    <published>2013-02-28T00:21:38Z</published>
    <title>Content Based Image Retrieval System Using NOHIS-tree</title>
    <summary>  Content-based image retrieval (CBIR) has been one of the most important
research areas in computer vision. It is a widely used method for searching
images in huge databases. In this paper we present a CBIR system called
NOHIS-Search. The system is based on the indexing technique NOHIS-tree. The two
phases of the system are described and the performance of the system is
illustrated with the image database ImagEval. NOHIS-Search system was compared
to other two CBIR systems; the first that using PDDP indexing algorithm and the
second system is that using the sequential search. Results show that
NOHIS-Search system outperforms the two other systems.
</summary>
    <author>
      <name>Mounira Taileb</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 10th International Conference on Advances in Mobile
  Computing &amp; Multimedia (MoMM2012)</arxiv:comment>
    <link href="http://arxiv.org/abs/1302.7039v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.7039v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.0481v2</id>
    <updated>2014-03-30T08:09:43Z</updated>
    <published>2013-03-03T09:32:44Z</published>
    <title>Situation-Aware Approach to Improve Context-based Recommender System</title>
    <summary>  In this paper, we introduce a novel situation aware approach to improve a
context based recommender system. To build situation aware user profiles, we
rely on evidence issued from retrieval situations. A retrieval situation refers
to the social spatio temporal context of the user when he interacts with the
recommender system. A situation is represented as a combination of social
spatio temporal concepts inferred from ontological knowledge given social
group, location and time information. User's interests are inferred from past
user's interaction with the recommender system related to the identified
situations. They are represented using concepts issued from a domain ontology.
We also propose a method to dynamically adapt the system to the user's
interest's evolution.
</summary>
    <author>
      <name>Djallel Bouneffouf</name>
    </author>
    <link href="http://arxiv.org/abs/1303.0481v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.0481v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.0485v2</id>
    <updated>2014-04-15T08:31:54Z</updated>
    <published>2013-03-03T10:23:38Z</published>
    <title>Optimizing an Utility Function for Exploration / Exploitation Trade-off
  in Context-Aware Recommender System</title>
    <summary>  In this paper, we develop a dynamic exploration/ exploitation (exr/exp)
strategy for contextual recommender systems (CRS). Specifically, our methods
can adaptively balance the two aspects of exr/exp by automatically learning the
optimal tradeoff. This consists of optimizing a utility function represented by
a linearized form of the probability distributions of the rewards of the
clicked and the non-clicked documents already recommended. Within an offline
simulation framework we apply our algorithms to a CRS and conduct an evaluation
with real event log data. The experimental results and detailed analysis
demonstrate that our algorithms outperform existing algorithms in terms of
click-through-rate (CTR).
</summary>
    <author>
      <name>Djallel Bouneffouf</name>
    </author>
    <link href="http://arxiv.org/abs/1303.0485v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.0485v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.2156v1</id>
    <updated>2013-03-09T02:01:04Z</updated>
    <published>2013-03-09T02:01:04Z</published>
    <title>The Powerful Model Adpredictor for Search Engine Switching Detection
  Challenge</title>
    <summary>  The purpose of the Switching Detection Challenge in the 2013 WSCD workshop
was to predict users' search engine swithcing actions given records about
search sessions and logs.Our solution adopted the powerful prediction model
Adpredictor and utilized the method of feature engineering. We successfully
applied the click through rate (CTR) prediction model Adpredicitor into our
solution framework, and then the discovery of effective features and the
multiple classification of different switching type make our model outperforms
many competitors. We achieved an AUC score of 0.84255 on the private
leaderboard and ranked the 5th among all the competitors in the competition.
</summary>
    <author>
      <name>Heng Gao</name>
    </author>
    <author>
      <name>Yongbao Li</name>
    </author>
    <author>
      <name>Qiudan Li</name>
    </author>
    <author>
      <name>Daniel Zeng</name>
    </author>
    <link href="http://arxiv.org/abs/1303.2156v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.2156v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.6906v1</id>
    <updated>2013-03-26T16:33:00Z</updated>
    <published>2013-03-26T16:33:00Z</published>
    <title>Large scale citation matching using Apache Hadoop</title>
    <summary>  During the process of citation matching links from bibliography entries to
referenced publications are created. Such links are indicators of topical
similarity between linked texts, are used in assessing the impact of the
referenced document and improve navigation in the user interfaces of digital
libraries. In this paper we present a citation matching method and show how to
scale it up to handle great amounts of data using appropriate indexing and a
MapReduce paradigm in the Hadoop environment.
</summary>
    <author>
      <name>Mateusz Fedoryszak</name>
    </author>
    <author>
      <name>Dominika Tkaczyk</name>
    </author>
    <author>
      <name>Łukasz Bolikowski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1303.6906v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.6906v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.3845v2</id>
    <updated>2014-03-30T08:19:52Z</updated>
    <published>2013-04-13T20:35:56Z</published>
    <title>The Impact of Situation Clustering in Contextual-Bandit Algorithm for
  Context-Aware Recommender Systems</title>
    <summary>  Most existing approaches in Context-Aware Recommender Systems (CRS) focus on
recommending relevant items to users taking into account contextual
information, such as time, location, or social aspects. However, few of them
have considered the problem of user's content dynamicity. We introduce in this
paper an algorithm that tackles the user's content dynamicity by modeling the
CRS as a contextual bandit algorithm and by including a situation clustering
algorithm to improve the precision of the CRS. Within a deliberately designed
offline simulation framework, we conduct evaluations with real online event log
data. The experimental results and detailed analysis reveal several important
discoveries in context aware recommender system.
</summary>
    <author>
      <name>Djallel Bouneffouf</name>
    </author>
    <link href="http://arxiv.org/abs/1304.3845v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.3845v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.6181v1</id>
    <updated>2013-04-23T06:42:55Z</updated>
    <published>2013-04-23T06:42:55Z</published>
    <title>Evaluating Web Content Quality via Multi-scale Features</title>
    <summary>  Web content quality measurement is crucial to various web content processing
applications. This paper will explore multi-scale features which may affect the
quality of a host, and develop automatic statistical methods to evaluate the
Web content quality. The extracted properties include statistical content
features, page and host level link features and TFIDF features. The experiments
on ECML/PKDD 2010 Discovery Challenge data set show that the algorithm is
effective and feasible for the quality tasks of multiple languages, and the
multi-scale features have different identification ability and provide good
complement to each other for most tasks.
</summary>
    <author>
      <name>Guang-Gang Geng</name>
    </author>
    <author>
      <name>Xiao-Bo Jin</name>
    </author>
    <author>
      <name>Xin-Chang Zhang</name>
    </author>
    <author>
      <name>De-Xian Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 1 figures, ecml/pkdd 2010 discovery challenge</arxiv:comment>
    <link href="http://arxiv.org/abs/1304.6181v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.6181v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.2831v1</id>
    <updated>2013-05-10T08:06:15Z</updated>
    <published>2013-05-10T08:06:15Z</published>
    <title>Test Model for Text Categorization and Text Summarization</title>
    <summary>  Text Categorization is the task of automatically sorting a set of documents
into categories from a predefined set and Text Summarization is a brief and
accurate representation of input text such that the output covers the most
important concepts of the source in a condensed manner. Document Summarization
is an emerging technique for understanding the main purpose of any kind of
documents. This paper presents a model that uses text categorization and text
summarization for searching a document based on user query.
</summary>
    <author>
      <name>Khushboo Thakkar</name>
    </author>
    <author>
      <name>Urmila Shrawankar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Pages: 07 Figures : 07</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal on Computer Science and Engineering (IJCSE),
  ISSN : 0975-3397, Vol. 3 No. 4 Apr 2011 pp 1539-1545</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1305.2831v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.2831v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.5827v1</id>
    <updated>2013-05-24T19:02:59Z</updated>
    <published>2013-05-24T19:02:59Z</published>
    <title>Semantic Web Search based on Ontology Modeling using Protege Reasoner</title>
    <summary>  The Semantic Web works on the existing Web which presents the meaning of
information as well-defined vocabularies understood by the people. Semantic
Search, at the same time, works on improving the accuracy if a search by
understanding the intent of the search and providing contextually relevant
results. This paper describes a semantic approach toward web search through a
PHP application. The goal was to parse through a user's browsing history and
return semantically relevant web pages for the search query provided.
</summary>
    <author>
      <name>Monica Shekhar</name>
    </author>
    <author>
      <name>Saravanaguru RA. K</name>
    </author>
    <link href="http://arxiv.org/abs/1305.5827v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.5827v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.4606v1</id>
    <updated>2013-06-19T16:37:23Z</updated>
    <published>2013-06-19T16:37:23Z</published>
    <title>Keyphrase Cloud Generation of Broadcast News</title>
    <summary>  This paper describes an enhanced automatic keyphrase extraction method
applied to Broadcast News. The keyphrase extraction process is used to create a
concept level for each news. On top of words resulting from a speech
recognition system output and news indexation and it contributes to the
generation of a tag/keyphrase cloud of the top news included in a Multimedia
Monitoring Solution system for TV and Radio news/programs, running daily, and
monitoring 12 TV channels and 4 Radios.
</summary>
    <author>
      <name>Luis Marujo</name>
    </author>
    <author>
      <name>Márcio Viveiros</name>
    </author>
    <author>
      <name>João Paulo da Silva Neto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceeding of Interspeech 2011: 12th Annual Conference of the
  International Speech Communication Association</arxiv:comment>
    <link href="http://arxiv.org/abs/1306.4606v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.4606v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.4758v1</id>
    <updated>2013-06-20T05:42:58Z</updated>
    <published>2013-06-20T05:42:58Z</published>
    <title>Analysing Word Importance for Image Annotation</title>
    <summary>  Image annotation provides several keywords automatically for a given image
based on various tags to describe its contents which is useful in Image
retrieval. Various researchers are working on text based and content based
image annotations [7,9]. It is seen, in traditional Image annotation
approaches, annotation words are treated equally without considering the
importance of each word in real world. In context of this, in this work, images
are annotated with keywords based on their frequency count and word
correlation. Moreover this work proposes an approach to compute importance
score of candidate keywords, having same frequency count.
</summary>
    <author>
      <name>Payal Gulati</name>
    </author>
    <author>
      <name>A. K. Sharma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures, Published in IJCSI (International Journal of
  Computer Science Issues) Journal, Volume 10, Issue 1, No 2, January 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1306.4758v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.4758v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.2015v1</id>
    <updated>2013-07-08T10:12:01Z</updated>
    <published>2013-07-08T10:12:01Z</published>
    <title>Full-text Support for Publish/Subscribe Ontology Systems</title>
    <summary>  We envision a publish/subscribe ontology system that is able to index
millions of user subscriptions and filter them against ontology data that
arrive in a streaming fashion. In this work, we propose a SPARQL extension
appropriate for a publish/subscribe setting; our extension builds on the
natural semantic graph matching of the language and supports the creation of
full-text subscriptions. Subsequently, we propose a main-memory subscription
indexing algorithm which performs both semantic and full-text matching at low
complexity and minimal filtering time. Thus, when ontology data are published
matching subscriptions are identified and notifications are forwarded to users.
</summary>
    <author>
      <name>Lefteris Zervakis</name>
    </author>
    <author>
      <name>Christos Tryfonopoulos</name>
    </author>
    <author>
      <name>Antonios Papadakis-Pesaresi</name>
    </author>
    <author>
      <name>Manolis Koubarakis</name>
    </author>
    <author>
      <name>Spiros Skiadopoulos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ESWC 2012 Demo</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.2015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.2015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.6422v1</id>
    <updated>2013-07-24T13:59:48Z</updated>
    <published>2013-07-24T13:59:48Z</published>
    <title>Mesure de la similarité entre termes et labels de concepts
  ontologiques</title>
    <summary>  We propose in this paper a method for measuring the similarity between
ontological concepts and terms. Our metric can take into account not only the
common words of two strings to compare but also other features such as the
position of the words in these strings, or the number of deletion, insertion or
replacement of words required for the construction of one of the two strings
from each other. The proposed method was then used to determine the ontological
concepts which are equivalent to the terms that qualify toponymes. It aims to
find the topographical type of the toponyme.
</summary>
    <author>
      <name>Van Tien Nguyen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIUPPA</arxiv:affiliation>
    </author>
    <author>
      <name>Christian Sallaberry</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIUPPA</arxiv:affiliation>
    </author>
    <author>
      <name>Mauro Gaio</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIUPPA</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">CORIA 2013, Neufch\^atel : Suisse (2013)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1307.6422v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.6422v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.8060v1</id>
    <updated>2013-07-30T17:36:53Z</updated>
    <published>2013-07-30T17:36:53Z</published>
    <title>Extracting Information-rich Part of Texts using Text Denoising</title>
    <summary>  The aim of this paper is to report on a novel text reduction technique,
called Text Denoising, that highlights information-rich content when processing
a large volume of text data, especially from the biomedical domain. The core
feature of the technique, the text readability index, embodies the hypothesis
that complex text is more information-rich than the rest. When applied on tasks
like biomedical relation bearing text extraction, keyphrase indexing and
extracting sentences describing protein interactions, it is evident that the
reduced set of text produced by text denoising is more information-rich than
the rest.
</summary>
    <author>
      <name>Rushdi Shams</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26th Canadian Conference on Artificial Intelligence (CAI-2013),
  Regina, Canada, May 29-31, 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.8060v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.8060v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.3177v1</id>
    <updated>2013-08-14T17:04:15Z</updated>
    <published>2013-08-14T17:04:15Z</published>
    <title>Normalized Google Distance of Multisets with Applications</title>
    <summary>  Normalized Google distance (NGD) is a relative semantic distance based on the
World Wide Web (or any other large electronic database, for instance Wikipedia)
and a search engine that returns aggregate page counts. The earlier NGD between
pairs of search terms (including phrases) is not sufficient for all
applications. We propose an NGD of finite multisets of search terms that is
better for many applications. This gives a relative semantics shared by a
multiset of search terms. We give applications and compare the results with
those obtained using the pairwise NGD. The derivation of NGD method is based on
Kolmogorov complexity.
</summary>
    <author>
      <name>Andrew R. Cohen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Dept Electrical and Comput. Engin., Drexel Univ.</arxiv:affiliation>
    </author>
    <author>
      <name>P. M. B. Vitanyi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CWI and Comput. Sci., Univ. Amsterdam</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, LaTeX, 3 figures/tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1308.3177v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.3177v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.3333v1</id>
    <updated>2013-10-12T03:48:38Z</updated>
    <published>2013-10-12T03:48:38Z</published>
    <title>Visualizing Bags of Vectors</title>
    <summary>  The motivation of this work is two-fold - a) to compare between two different
modes of visualizing data that exists in a bag of vectors format b) to propose
a theoretical model that supports a new mode of visualizing data. Visualizing
high dimensional data can be achieved using Minimum Volume Embedding, but the
data has to exist in a format suitable for computing similarities while
preserving local distances. This paper compares the visualization between two
methods of representing data and also proposes a new method providing sample
visualizations for that method.
</summary>
    <author>
      <name>Sriramkumar Balasubramanian</name>
    </author>
    <author>
      <name>Raghuram Reddy Nagireddy</name>
    </author>
    <link href="http://arxiv.org/abs/1310.3333v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.3333v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.6110v1</id>
    <updated>2013-10-23T04:41:18Z</updated>
    <published>2013-10-23T04:41:18Z</published>
    <title>A two-step model and the algorithm for recalling in recommender systems</title>
    <summary>  When a user finds an interesting recommendation in a recommender system, the
user may want to recall related items recommended in the past to reconsider or
to enjoy them again. If the system can pick up such "recalled" items at each
user's request, it must deepen the user experience.
  We propose a model and the algorithm for such personalized "recalling" in
conventional recommender systems, which is an application of neural networks
for associative memory. In our model, the "recalled" items can reflect each
user's personality beyond naive similarities between items.
</summary>
    <author>
      <name>Keisuke Hara</name>
    </author>
    <author>
      <name>Tomihisa Kamada</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, No figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1310.6110v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.6110v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.8226v1</id>
    <updated>2013-10-30T16:38:03Z</updated>
    <published>2013-10-30T16:38:03Z</published>
    <title>Bibliometric-enhanced Information Retrieval</title>
    <summary>  Bibliometric techniques are not yet widely used to enhance retrieval
processes in digital libraries, although they offer value-added effects for
users. In this workshop we will explore how statistical modelling of
scholarship, such as Bradfordizing or network analysis of coauthorship network,
can improve retrieval services for specific communities, as well as for large,
cross-domain collections. This workshop aims to raise awareness of the missing
link between information retrieval (IR) and bibliometrics/scientometrics and to
create a common ground for the incorporation of bibliometric-enhanced services
into retrieval at the digital library interface.
</summary>
    <author>
      <name>Philipp Mayr</name>
    </author>
    <author>
      <name>Andrea Scharnhorst</name>
    </author>
    <author>
      <name>Birger Larsen</name>
    </author>
    <author>
      <name>Philipp Schaer</name>
    </author>
    <author>
      <name>Peter Mutschke</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, accepted workshop proposal for ECIR 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1310.8226v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.8226v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.0667v1</id>
    <updated>2013-11-04T12:13:50Z</updated>
    <published>2013-11-04T12:13:50Z</published>
    <title>Developing a Visual Interactive Search History Exploration System</title>
    <summary>  As users advance in their search within a system, different queries are
conducted and various results are examined by them. These objects form an
implicit individual library representing the acquired knowledge. In our
research we aim to supply the user with visualizations of the search history
and interaction methods to organize the history. The fundamental question is
what role search history exploration can play in the users search process. In
this paper we want to introduce Ideas of a prototypical system for search
history exploration and discuss methods to address the questions mentioned
above.
</summary>
    <author>
      <name>Wilko van Hoek</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">KNOWeSCAPE 2013, 2 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.0667v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.0667v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P20" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.4151v1</id>
    <updated>2013-11-17T12:02:12Z</updated>
    <published>2013-11-17T12:02:12Z</published>
    <title>Lattice-cell : Hybrid approach for text categorization</title>
    <summary>  In this paper, we propose a new text categorization framework based on
Concepts Lattice and cellular automata. In this framework, concept structure
are modeled by a Cellular Automaton for Symbolic Induction (CASI). Our
objective is to reduce time categorization caused by the Concept Lattice. We
examine, by experiments the performance of the proposed approach and compare it
with other algorithms such as Naive Bayes and k nearest neighbors. The results
show performance improvement while reducing time categorization.
</summary>
    <author>
      <name>Hichem Benfriha</name>
    </author>
    <author>
      <name>Fatiha Barigou</name>
    </author>
    <author>
      <name>Baghdad Atmani</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/csit.2013.3817</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/csit.2013.3817" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Computer Science &amp; Information Technology (CS &amp; IT) 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.4151v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.4151v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.4900v1</id>
    <updated>2013-11-16T06:25:40Z</updated>
    <published>2013-11-16T06:25:40Z</published>
    <title>Query Interface Integrator For Domain Specific Hidden Web</title>
    <summary>  Web is title admittance today mainly relies on search engines. A large amount
of data is hidden in the databases behind the search interfaces referred to as
Hidden web, which needs to be indexed so in order to serve user query. In this
paper database and data mining techniques are used for query interface
integration. The query interface must resemble the look and feel of local
interface as much as possible despite being automatically generated without
human support.This technique keeps the related documents in the same domain so
that searching of documents becomes more efficient in terms of time complexity.
</summary>
    <author>
      <name>Sudhakar Ranjan</name>
    </author>
    <author>
      <name>Komal K. Bhatia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 Pages. International Journal of Computer Engineering and
  Applications, 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.4900v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.4900v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.6240v1</id>
    <updated>2013-11-25T09:12:15Z</updated>
    <published>2013-11-25T09:12:15Z</published>
    <title>A Decision Tree Approach to Classify Web Services using Quality
  Parameters</title>
    <summary>  With the increase in the number of web services, many web services are
available on internet providing the same functionality, making it difficult to
choose the best one, fulfilling users all requirements. This problem can be
solved by considering the quality of web services to distinguish functionally
similar web services. Nine different quality parameters are considered. Web
services can be classified and ranked using decision tree approach since they
do not require long training period and can be easily interpreted. Various
decision tree and rules approaches available are applied and tested to find the
optimal decision method to correctly classify functionally similar web services
considering their quality parameters.
</summary>
    <author>
      <name>Shilpa Sonawani</name>
    </author>
    <author>
      <name>Debajyoti Mukhopadhyay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 tables; ICWA 2013 International Conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.6240v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.6240v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.1448v1</id>
    <updated>2013-12-05T06:50:30Z</updated>
    <published>2013-12-05T06:50:30Z</published>
    <title>Food Recommendation using Ontology and Heuristics</title>
    <summary>  Recommender systems are needed to find food items of ones interest. We review
recommender systems and recommendation methods. We propose a food
personalization framework based on adaptive hypermedia. We extend Hermes
framework with food recommendation functionality. We combine TF-IDF term
extraction method with cosine similarity measure. Healthy heuristics and
standard food database are incorporated into the knowledgebase. Based on the
performed evaluation, we conclude that semantic recommender systems in general
outperform traditional recommenders systems with respect to accuracy,
precision, and recall, and that the proposed recommender has a better F-measure
than existing semantic recommenders.
</summary>
    <author>
      <name>M. A. El-Dosuky</name>
    </author>
    <author>
      <name>M. Z. Rashad</name>
    </author>
    <author>
      <name>T. T. Hamza</name>
    </author>
    <author>
      <name>A. H. EL-Bassiouny</name>
    </author>
    <link href="http://arxiv.org/abs/1312.1448v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.1448v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.1913v1</id>
    <updated>2013-12-06T16:34:14Z</updated>
    <published>2013-12-06T16:34:14Z</published>
    <title>Adapting Binary Information Retrieval Evaluation Metrics for
  Segment-based Retrieval Tasks</title>
    <summary>  This report describes metrics for the evaluation of the effectiveness of
segment-based retrieval based on existing binary information retrieval metrics.
This metrics are described in the context of a task for the hyperlinking of
video segments. This evaluation approach re-uses existing evaluation measures
from the standard Cranfield evaluation paradigm. Our adaptation approach can in
principle be used with any kind of effectiveness measure that uses binary
relevance, and for other segment-baed retrieval tasks. In our video
hyperlinking setting, we use precision at a cut-off rank n and mean average
precision.
</summary>
    <author>
      <name>Robin Aly</name>
    </author>
    <author>
      <name>Maria Eskevich</name>
    </author>
    <author>
      <name>Roeland Ordelman</name>
    </author>
    <author>
      <name>Gareth J. F. Jones</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Explanation of evaluation measures for the linking task of the
  MediaEval Workshop 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.1913v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.1913v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.4036v1</id>
    <updated>2013-12-14T12:13:58Z</updated>
    <published>2013-12-14T12:13:58Z</published>
    <title>Mind Your Language: Effects of Spoken Query Formulation on Retrieval
  Effectiveness</title>
    <summary>  Voice search is becoming a popular mode for interacting with search engines.
As a result, research has gone into building better voice transcription
engines, interfaces, and search engines that better handle inherent verbosity
of queries. However, when one considers its use by non- native speakers of
English, another aspect that becomes important is the formulation of the query
by users. In this paper, we present the results of a preliminary study that we
conducted with non-native English speakers who formulate queries for given
retrieval tasks. Our results show that the current search engines are sensitive
in their rankings to the query formulation, and thus highlights the need for
developing more robust ranking methods.
</summary>
    <author>
      <name>Apoorv Narang</name>
    </author>
    <author>
      <name>Srikanta Bedathur</name>
    </author>
    <link href="http://arxiv.org/abs/1312.4036v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.4036v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.4425v1</id>
    <updated>2013-12-16T16:49:32Z</updated>
    <published>2013-12-16T16:49:32Z</published>
    <title>An Ontology-based Model for Indexing and Retrieval</title>
    <summary>  Starting from an unsolved problem of information retrieval this paper
presents an ontology-based model for indexing and retrieval. The model combines
the methods and experiences of cognitive-to-interpret indexing languages with
the strengths and possibilities of formal knowledge representation. The core
component of the model uses inferences along the paths of typed relations
between the entities of a knowledge representation for enabling the
determination of hit quantities in the context of retrieval processes. The
entities are arranged in aspect-oriented facets to ensure a consistent
hierarchical structure. The possible consequences for indexing and retrieval
are discussed.
</summary>
    <author>
      <name>Winfried Gödert</name>
    </author>
    <link href="http://arxiv.org/abs/1312.4425v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.4425v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.4824v2</id>
    <updated>2014-01-16T04:05:36Z</updated>
    <published>2013-12-17T15:32:56Z</published>
    <title>Generation, Implementation and Appraisal of an N-gram based Stemming
  Algorithm</title>
    <summary>  A language independent stemmer has always been looked for. Single N-gram
tokenization technique works well, however, it often generates stems that start
with intermediate characters, rather than initial ones. We present a novel
technique that takes the concept of N gram stemming one step ahead and compare
our method with an established algorithm in the field, Porter's Stemmer.
Results indicate that our N gram stemmer is not inferior to Porter's linguistic
stemmer.
</summary>
    <author>
      <name>B. P. Pande</name>
    </author>
    <author>
      <name>Pawan Tamta</name>
    </author>
    <author>
      <name>H. S. Dhami</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.4824v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.4824v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.6802v1</id>
    <updated>2013-12-24T12:06:48Z</updated>
    <published>2013-12-24T12:06:48Z</published>
    <title>Suffix Stripping Problem as an Optimization Problem</title>
    <summary>  Stemming or suffix stripping, an important part of the modern Information
Retrieval systems, is to find the root word (stem) out of a given cluster of
words. Existing algorithms targeting this problem have been developed in a
haphazard manner. In this work, we model this problem as an optimization
problem. An Integer Program is being developed to overcome the shortcomings of
the existing approaches. The sample results of the proposed method are also
being compared with an established technique in the field for English language.
An AMPL code for the same IP has also been given.
</summary>
    <author>
      <name>B. P. Pande</name>
    </author>
    <author>
      <name>Pawan Tamta</name>
    </author>
    <author>
      <name>H. S. Dhami</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.6802v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.6802v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.1732v1</id>
    <updated>2014-01-08T15:46:35Z</updated>
    <published>2014-01-08T15:46:35Z</published>
    <title>Looking at Vector Space and Language Models for IR using Density
  Matrices</title>
    <summary>  In this work, we conduct a joint analysis of both Vector Space and Language
Models for IR using the mathematical framework of Quantum Theory. We shed light
on how both models allocate the space of density matrices. A density matrix is
shown to be a general representational tool capable of leveraging capabilities
of both VSM and LM representations thus paving the way for a new generation of
retrieval models. We analyze the possible implications suggested by our
findings.
</summary>
    <author>
      <name>Alessandro Sordoni</name>
    </author>
    <author>
      <name>Jian-Yun Nie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of Quantum Interaction 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.1732v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.1732v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.1947v1</id>
    <updated>2014-02-09T13:02:51Z</updated>
    <published>2014-02-09T13:02:51Z</published>
    <title>Classification Tree Diagrams in Health Informatics Applications</title>
    <summary>  Health informatics deal with the methods used to optimize the acquisition,
storage and retrieval of medical data, and classify information in healthcare
applications. Healthcare analysts are particularly interested in various
computer informatics areas such as; knowledge representation from data, anomaly
detection, outbreak detection methods and syndromic surveillance applications.
Although various parametric and non-parametric approaches are being proposed to
classify information from data, classification tree diagrams provide an
interactive visualization to analysts as compared to other methods. In this
work we discuss application of classification tree diagrams to classify
information from medical data in healthcare applications.
</summary>
    <author>
      <name>Farrukh Arslan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In the Proceedings of 7th International Conference on the Theory and
  Application of Diagrams 2012. 7th International Conference on the Theory and
  Application of Diagrams 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1402.1947v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.1947v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.3070v1</id>
    <updated>2014-02-13T09:54:01Z</updated>
    <published>2014-02-13T09:54:01Z</published>
    <title>Squeezing bottlenecks: exploring the limits of autoencoder semantic
  representation capabilities</title>
    <summary>  We present a comprehensive study on the use of autoencoders for modelling
text data, in which (differently from previous studies) we focus our attention
on the following issues: i) we explore the suitability of two different models
bDA and rsDA for constructing deep autoencoders for text data at the sentence
level; ii) we propose and evaluate two novel metrics for better assessing the
text-reconstruction capabilities of autoencoders; and iii) we propose an
automatic method to find the critical bottleneck dimensionality for text
language representations (below which structural information is lost).
</summary>
    <author>
      <name>Parth Gupta</name>
    </author>
    <author>
      <name>Rafael E. Banchs</name>
    </author>
    <author>
      <name>Paolo Rosso</name>
    </author>
    <link href="http://arxiv.org/abs/1402.3070v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.3070v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.3470v1</id>
    <updated>2014-02-14T14:12:17Z</updated>
    <published>2014-02-14T14:12:17Z</published>
    <title>Designing an Ontology for the Data Documentation Initiative</title>
    <summary>  An ontology of the DDI 3 data model will be designed by following the
ontology engineering methodology to be evolved based on state-of-the-art
methodologies. Hence DDI 3 data and metadata can be represented in form of a
standard web interchange format RDF and processed by highly available RDF
tools. As a consequence the DDI community has the possibility to publish and
link LOD data sets to become part of the LOD cloud.
</summary>
    <author>
      <name>Thomas Bosch</name>
    </author>
    <author>
      <name>Andias Wira-Alam</name>
    </author>
    <author>
      <name>Brigitte Mathiak</name>
    </author>
    <link href="http://arxiv.org/abs/1402.3470v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.3470v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.4362v1</id>
    <updated>2014-03-18T07:18:39Z</updated>
    <published>2014-03-18T07:18:39Z</published>
    <title>Concept Based vs. Pseudo Relevance Feedback Performance Evaluation for
  Information Retrieval System</title>
    <summary>  This article evaluates the performance of two techniques for query
reformulation in a system for information retrieval, namely, the concept based
and the pseudo relevance feedback reformulation. The experiments performed on a
corpus of Arabic text have allowed us to compare the contribution of these two
reformulation techniques in improving the performance of an information
retrieval system for Arabic texts.
</summary>
    <author>
      <name>Mohammed El Amine Abderrahim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1306.3955 by
  other authors</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computational Linguistics Research, ISSN:
  0976-416X, Volume 4, Issue 4, December, 2013, Pages 149-158</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1403.4362v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.4362v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.7162v1</id>
    <updated>2014-03-27T18:36:15Z</updated>
    <published>2014-03-27T18:36:15Z</published>
    <title>Information Retrieval (IR) through Semantic Web (SW): An Overview</title>
    <summary>  A large amount of data is present on the web. It contains huge number of web
pages and to find suitable information from them is very cumbersome task. There
is need to organize data in formal manner so that user can easily access and
use them. To retrieve information from documents, we have many Information
Retrieval (IR) techniques. Current IR techniques are not so advanced that they
can be able to exploit semantic knowledge within documents and give precise
results. IR technology is major factor responsible for handling annotations in
Semantic Web (SW) languages and in the present paper knowledgeable
representation languages used for retrieving information are discussed.
</summary>
    <author>
      <name>Gagandeep Singh</name>
    </author>
    <author>
      <name>Vishal Jain</name>
    </author>
    <link href="http://arxiv.org/abs/1403.7162v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.7162v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.0091v1</id>
    <updated>2014-04-01T00:53:37Z</updated>
    <published>2014-04-01T00:53:37Z</published>
    <title>Interestingness a Unifying Paradigm Bipolar Function Composition</title>
    <summary>  Interestingness is an important criterion by which we judge knowledge
discovery. But, interestingness has escaped all attempts to capture its
intuitive meaning into a concise and comprehensive form. A unifying paradigm is
formulated by function composition. We claim that composition is bipolar, i.e.
composition of exactly two functions, whose two semantic poles are relevance
and unexpectedness. The paradigm generality is demonstrated by case studies of
new interestingness functions, examples of known functions that fit the
framework, and counter-examples for which the paradigm points out to the
lacking pole.
</summary>
    <author>
      <name>Iaakov Exman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 2 figures, 1 table; in Proceedings of KDIR 2009, 1st
  International Conference on Knowledge Discovery and Information Retrieval,
  Madeira, Portugal, pp. 196-201</arxiv:comment>
    <link href="http://arxiv.org/abs/1404.0091v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.0091v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.1514v1</id>
    <updated>2014-04-05T19:58:38Z</updated>
    <published>2014-04-05T19:58:38Z</published>
    <title>Text Based Approach For Indexing And Retrieval Of Image And Video: A
  Review</title>
    <summary>  Text data present in multimedia contain useful information for automatic
annotation, indexing. Extracted information used for recognition of the overlay
or scene text from a given video or image. The Extracted text can be used for
retrieving the videos and images. In this paper, firstly, we are discussed the
different techniques for text extraction from images and videos. Secondly, we
are reviewed the techniques for indexing and retrieval of image and videos by
using extracted text.
</summary>
    <author>
      <name>Avinash N Bhute</name>
    </author>
    <author>
      <name>B. B. Meshram</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Advances in Vision: An International Journal, Vol 1, no. 1, March
  2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1404.1514v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.1514v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.3435v1</id>
    <updated>2014-04-13T22:01:48Z</updated>
    <published>2014-04-13T22:01:48Z</published>
    <title>Web Search of New Linearized Medical Drug Leads</title>
    <summary>  The Web is a potentially huge source of medical drug leads. But despite the
significant amount of multi- dimensional information about drugs, currently
commercial search engines accept only linear keyword strings as inputs. This
work uses linearized fragments of molecular structures as knowledge
representation units to serve as inputs to search engines. It is shown that
quite arbitrary fragments are surprisingly free of ambiguity, obtaining
relatively small result sets, which are both manageable and rich in novel
potential drug leads.
</summary>
    <author>
      <name>Iaakov Exman</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5220/0003705401080115</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5220/0003705401080115" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 9 figures, reprint of paper in SKY 2011 Workshop, Paris,
  France, October 2011, SciTePress Digital Library</arxiv:comment>
    <link href="http://arxiv.org/abs/1404.3435v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.3435v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.1740v1</id>
    <updated>2014-05-07T20:20:35Z</updated>
    <published>2014-05-07T20:20:35Z</published>
    <title>Turkish Text Retrieval Experiments Using Lemur Toolkit</title>
    <summary>  We used Lemur Toolkit, an open source toolkit designed for Information
Retrieval (IR) research, for our automated indexing and retrieval experiments
on a TREC-like test collection for Turkish. We study and compare three
retrieval models Lemur supports, especially Language modeling approach to IR,
combined with language specific preprocessing techniques. Our experiments show
that all retrieval models benefits from language specific preprocessing in
terms of retrieval quality. Also Language Modeling approach is the best
performing retrieval model when language specific preprocessing applied.
</summary>
    <author>
      <name>Kutlu Emre Yılmaz</name>
    </author>
    <author>
      <name>Ahmet Arslan</name>
    </author>
    <author>
      <name>Ozgur Yilmazel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IADIS AC 2009: Rome, Italy</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1405.1740v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.1740v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.2212v1</id>
    <updated>2014-05-09T12:10:11Z</updated>
    <published>2014-05-09T12:10:11Z</published>
    <title>Why we need an independent index of the Web</title>
    <summary>  The path to greater diversity, as we have seen, cannot be achieved by merely
hoping for a new search engine nor will government support for a single
alternative achieve this goal. What is instead required is to create the
conditions that will make establishing such a search engine possible in the
first place. I describe how building and maintaining a proprietary index is the
greatest deterrent to such an undertaking. We must first overcome this
obstacle. Doing so will still not solve the problem of the lack of diversity in
the search engine marketplace. But it may establish the conditions necessary to
achieve that desired end.
</summary>
    <author>
      <name>Dirk Lewandowski</name>
    </author>
    <link href="http://arxiv.org/abs/1405.2212v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.2212v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.2386v1</id>
    <updated>2014-05-10T03:43:14Z</updated>
    <published>2014-05-10T03:43:14Z</published>
    <title>Predicting Central Topics in a Blog Corpus from a Networks Perspective</title>
    <summary>  In today's content-centric Internet, blogs are becoming increasingly popular
and important from a data analysis perspective. According to Wikipedia, there
were over 156 million public blogs on the Internet as of February 2011. Blogs
are a reflection of our contemporary society. The contents of different blog
posts are important from social, psychological, economical and political
perspectives. Discovery of important topics in the blogosphere is an area which
still needs much exploring. We try to come up with a procedure using
probabilistic topic modeling and network centrality measures which identifies
the central topics in a blog corpus.
</summary>
    <author>
      <name>Srayan Datta</name>
    </author>
    <link href="http://arxiv.org/abs/1405.2386v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.2386v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.3353v1</id>
    <updated>2014-05-14T03:44:32Z</updated>
    <published>2014-05-14T03:44:32Z</published>
    <title>Which one is better: presentation-based or content-based math search?</title>
    <summary>  Mathematical content is a valuable information source and retrieving this
content has become an important issue. This paper compares two searching
strategies for math expressions: presentation-based and content-based
approaches. Presentation-based search uses state-of-the-art math search system
while content-based search uses semantic enrichment of math expressions to
convert math expressions into their content forms and searching is done using
these content-based expressions. By considering the meaning of math
expressions, the quality of search system is improved over presentation-based
systems.
</summary>
    <author>
      <name>Minh-Quoc Nghiem</name>
    </author>
    <author>
      <name>Giovanni Yoko Kristianto</name>
    </author>
    <author>
      <name>Goran Topic</name>
    </author>
    <author>
      <name>Akiko Aizawa</name>
    </author>
    <link href="http://arxiv.org/abs/1405.3353v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.3353v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.6287v1</id>
    <updated>2014-05-24T10:16:11Z</updated>
    <published>2014-05-24T10:16:11Z</published>
    <title>Étude des dimensions spécifiques du contexte dans un système de
  filtrage d'informations</title>
    <summary>  In the context of business information systems, e-commerce and access to
knowledge, the relevance of the information provided to use is a key fact to
the success of information systems. Therefore the quality of access is
determined by access to the right information at the right time, at the right
place. In this context, it is important to consider the users needs when access
to information and his contextual situation in order to provide relevant
information, tailored to their needs and context use. In what follows we
describe the prelude to a project that tries to combine all of these needs to
improve information systems.
</summary>
    <author>
      <name>Djallel Bouneffouf</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in French</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.6287v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.6287v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.6667v1</id>
    <updated>2014-05-26T18:25:35Z</updated>
    <published>2014-05-26T18:25:35Z</published>
    <title>Inferring gender of a Twitter user using celebrities it follows</title>
    <summary>  This paper addresses the task of user gender classification in social media,
with an application to Twitter. The approach automatically predicts gender by
leveraging observable information such as the tweet behavior, linguistic
content of the user's Twitter feed and the celebrities followed by the user.
This paper first evaluates linguistic content based features using LIWC
dictionary and popular neighborhood features using Wikipedia and Freebase. Then
augments both features which yielded a significant increase in the accuracy for
gender prediction. Results show that rich linguistic features combined with
popular neighborhood prove valuables and promising for additional user
classification needs.
</summary>
    <author>
      <name>Puneet Singh Ludu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted at CSE department, SUNY Buffalo, 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.6667v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.6667v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.6886v1</id>
    <updated>2014-05-27T12:34:24Z</updated>
    <published>2014-05-27T12:34:24Z</published>
    <title>A Topic Model Approach to Multi-Modal Similarity</title>
    <summary>  Calculating similarities between objects defined by many heterogeneous data
modalities is an important challenge in many multimedia applications. We use a
multi-modal topic model as a basis for defining such a similarity between
objects. We propose to compare the resulting similarities from different model
realizations using the non-parametric Mantel test. The approach is evaluated on
a music dataset.
</summary>
    <author>
      <name>Rasmus Troelsgård</name>
    </author>
    <author>
      <name>Bjørn Sand Jensen</name>
    </author>
    <author>
      <name>Lars Kai Hansen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">topic modelling workshop at NIPS 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.6886v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.6886v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.7869v1</id>
    <updated>2014-05-08T07:44:39Z</updated>
    <published>2014-05-08T07:44:39Z</published>
    <title>Integrating Vague Association Mining with Markov Model</title>
    <summary>  The increasing demand of world wide web raises the need of predicting the
user's web page request.The most widely used approach to predict the web pages
is the pattern discovery process of Web usage mining. This process involves
inevitability of many techniques like Markov model, association rules and
clustering. Fuzzy theory with different techniques has been introduced for the
better results. Our focus is on Markov models. This paper is introducing the
vague Rules with Markov models for more accuracy using the vague set theory.
</summary>
    <author>
      <name>Priya Bajaj</name>
    </author>
    <author>
      <name>Supriya Raheja</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.7869v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.7869v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.7975v1</id>
    <updated>2014-05-17T22:21:00Z</updated>
    <published>2014-05-17T22:21:00Z</published>
    <title>Multi-layered graph-based multi-document summarization model</title>
    <summary>  Multi-document summarization is a process of automatic generation of a
compressed version of the given collection of documents. Recently, the
graph-based models and ranking algorithms have been actively investigated by
the extractive document summarization community. While most work to date
focuses on homogeneous connecteness of sentences and heterogeneous connecteness
of documents and sentences (e.g. sentence similarity weighted by document
importance), in this paper we present a novel 3-layered graph model that
emphasizes not only sentence and document level relations but also the
influence of under sentence level relations (e.g. a part of sentence
similarity).
</summary>
    <author>
      <name>Ercan Canhasi</name>
    </author>
    <link href="http://arxiv.org/abs/1405.7975v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.7975v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.1143v1</id>
    <updated>2014-06-04T18:59:00Z</updated>
    <published>2014-06-04T18:59:00Z</published>
    <title>Identifying Duplicate and Contradictory Information in Wikipedia</title>
    <summary>  Our study identifies sentences in Wikipedia articles that are either
identical or highly similar by applying techniques for near-duplicate detection
of web pages. This is accomplished with a MapReduce implementation of minhash
to identify clusters of sentences with high Jaccard similarity. We show that
these clusters can be categorized into six different types, two of which are
particularly interesting: identical sentences quantify the extent to which
content in Wikipedia is copied and pasted, and near-duplicate sentences that
state contradictory facts point to quality issues in Wikipedia.
</summary>
    <author>
      <name>Sarah Weissman</name>
    </author>
    <author>
      <name>Samet Ayhan</name>
    </author>
    <author>
      <name>Joshua Bradley</name>
    </author>
    <author>
      <name>Jimmy Lin</name>
    </author>
    <link href="http://arxiv.org/abs/1406.1143v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.1143v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.1583v1</id>
    <updated>2014-06-06T05:12:38Z</updated>
    <published>2014-06-06T05:12:38Z</published>
    <title>Fuzzy clustering of web documents using equivalence relations and fuzzy
  hierarchical clustering</title>
    <summary>  The conventional clustering algorithms have difficulties in handling the
challenges posed by the collection of natural data which is often vague and
uncertain. Fuzzy clustering methods have the potential to manage such
situations efficiently. Fuzzy clustering method is offered to construct
clusters with uncertain boundaries and allows that one object belongs to one or
more clusters with some membership degree. In this paper, an algorithm and
experimental results are presented for fuzzy clustering of web documents using
equivalence relations and fuzzy hierarchical clustering.
</summary>
    <author>
      <name>Satendra kumar</name>
    </author>
    <author>
      <name>Mamta kathuria</name>
    </author>
    <author>
      <name>Alok Kumar Gupta</name>
    </author>
    <author>
      <name>Monika Rani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, Software Engineering (CONSEG), 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.1583v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.1583v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.6840v1</id>
    <updated>2014-06-26T11:01:11Z</updated>
    <published>2014-06-26T11:01:11Z</published>
    <title>From Citation count to Argumentation count: a new metric to indicate the
  usefulness of an article</title>
    <summary>  Citation count is a quantifiable measure to indicate the number of times an
article is cited by other articles. It is believed that if an article is cited
often then it must be an important or influential article; however, there is no
guarantee that the most cited articles are good in quality. In this paper, the
author suggests argumentation count, a new metric for citation analysis. The
proposed metric, argumentation count is a triplet of quantities for each
concept of an article that helps in providing a quantifiable measure about the
usefulness of an article.
</summary>
    <author>
      <name>Hardik Joshi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical Conference cum Workshop on Digital Library Using DSpace
  hosted by Gujarat National Law University on 21-23 March, 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.6840v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.6840v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.1539v1</id>
    <updated>2014-07-06T20:14:08Z</updated>
    <published>2014-07-06T20:14:08Z</published>
    <title>A Framework for Specific Term Recommendation Systems</title>
    <summary>  In this paper we present the IRSA framework that enables the automatic
creation of search term suggestion or recommendation systems (TS). Such TS are
used to operationalize interactive query expansion and help users in refining
their information need in the query formulation phase. Our recent research has
shown TS to be more effective when specific to a certain domain. The presented
technical framework allows owners of Digital Libraries to create their own
specific TS constructed via OAI-harvested metadata with very little effort.
</summary>
    <author>
      <name>Thomas Lüke</name>
    </author>
    <author>
      <name>Philipp Schaer</name>
    </author>
    <author>
      <name>Philipp Mayr</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2484028.2484207</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2484028.2484207" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 1 figure, SIGIR 13, July 28-August 1, 2013, Dublin, Ireland</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.1539v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.1539v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.1540v1</id>
    <updated>2014-07-06T20:20:13Z</updated>
    <published>2014-07-06T20:20:13Z</published>
    <title>Establishing an Online Access Panel for Interactive Information
  Retrieval Research</title>
    <summary>  We propose an online access panel to support the evaluation process of
Interactive Information Retrieval (IIR) systems - called IIRpanel. By
maintaining an online access panel with users of IIR systems we assume that the
recurring effort to recruit participants for web-based as well as for lab
studies can be minimized. We target on using the online access panel not only
for our own development processes but to open it for other interested
researchers in the field of IIR. In this paper we present the concept of
IIRpanel as well as first implementation details.
</summary>
    <author>
      <name>Dagmar Kern</name>
    </author>
    <author>
      <name>Peter Mutschke</name>
    </author>
    <author>
      <name>Philipp Mayr</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/JCDL.2014.6970231</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/JCDL.2014.6970231" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 1 figure, 2014 IEEE/ACM Joint Conference on Digital
  Libraries (JCDL), London, 8th-12th September 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.1540v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.1540v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.7072v1</id>
    <updated>2014-07-25T22:33:49Z</updated>
    <published>2014-07-25T22:33:49Z</published>
    <title>Fast Spammer Detection Using Structural Rank</title>
    <summary>  Comments for a product or a news article are rapidly growing and became a
medium of measuring quality products or services. Consequently, spammers have
been emerged in this area to bias them toward their favor. In this paper, we
propose an efficient spammer detection method using structural rank of author
specific term-document matrices. The use of structural rank was found effective
and far faster than similar methods.
</summary>
    <author>
      <name>Seungyeon Kim</name>
    </author>
    <author>
      <name>Haesun Park</name>
    </author>
    <author>
      <name>Guy Lebanon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.7072v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.7072v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.0096v1</id>
    <updated>2014-08-01T07:51:37Z</updated>
    <published>2014-08-01T07:51:37Z</published>
    <title>Conditional Restricted Boltzmann Machines for Cold Start Recommendations</title>
    <summary>  Restricted Boltzman Machines (RBMs) have been successfully used in
recommender systems. However, as with most of other collaborative filtering
techniques, it cannot solve cold start problems for there is no rating for a
new item. In this paper, we first apply conditional RBM (CRBM) which could take
extra information into account and show that CRBM could solve cold start
problem very well, especially for rating prediction task. CRBM naturally
combine the content and collaborative data under a single framework which could
be fitted effectively. Experiments show that CRBM can be compared favourably
with matrix factorization models, while hidden features learned from the former
models are more easy to be interpreted.
</summary>
    <author>
      <name>Jiankou Li</name>
    </author>
    <author>
      <name>Wei Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1408.0096v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.0096v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.2430v1</id>
    <updated>2014-08-11T14:58:30Z</updated>
    <published>2014-08-11T14:58:30Z</published>
    <title>Optimizing Component Combination in a Multi-Indexing Paragraph Retrieval
  System</title>
    <summary>  We demonstrate a method to optimize the combination of distinct components in
a paragraph retrieval system. Our system makes use of several indices, query
generators and filters, each of them potentially contributing to the quality of
the returned list of results. The components are combined with a weighed sum,
and we optimize the weights using a heuristic optimization algorithm. This
allows us to maximize the quality of our results, but also to determine which
components are most valuable in our system. We evaluate our approach on the
paragraph selection task of a Question Answering dataset.
</summary>
    <author>
      <name>Boris Iolis</name>
    </author>
    <author>
      <name>Gianluca Bontempi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure, unpublished</arxiv:comment>
    <link href="http://arxiv.org/abs/1408.2430v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.2430v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.0491v1</id>
    <updated>2014-09-01T17:20:13Z</updated>
    <published>2014-09-01T17:20:13Z</published>
    <title>Facets and Typed Relations as Tools for Reasoning Processes in
  Information Retrieval</title>
    <summary>  Faceted arrangement of entities and typed relations for representing
different associations between the entities are established tools in knowledge
representation. In this paper, a proposal is being discussed combining both
tools to draw inferences along relational paths. This approach may yield new
benefit for information retrieval processes, especially when modeled for
heterogeneous environments in the Semantic Web. Faceted arrangement can be used
as a se-lection tool for the semantic knowledge modeled within the knowledge
repre-sentation. Typed relations between the entities of different facets can
be used as restrictions for selecting them across the facets.
</summary>
    <author>
      <name>Winfried Gödert</name>
    </author>
    <link href="http://arxiv.org/abs/1409.0491v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.0491v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.0921v1</id>
    <updated>2014-09-02T23:41:58Z</updated>
    <published>2014-09-02T23:41:58Z</published>
    <title>A Generalized Framework for Ontology-Based Information Retrieval
  Application to a public-transportation system</title>
    <summary>  In this paper we present a generic framework for ontology-based information
retrieval. We focus on the recognition of semantic information extracted from
data sources and the mapping of this knowledge into ontology. In order to
achieve more scalability, we propose an approach for semantic indexing based on
entity retrieval model. In addition, we have used ontology of public
transportation domain in order to validate these proposals. Finally, we
evaluated our system using ontology mapping and real world data sources.
Experiments show that our framework can provide meaningful search results.
</summary>
    <author>
      <name>Amir Zidi</name>
    </author>
    <author>
      <name>Mourad Abed</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICAdLT.2013.6568453</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICAdLT.2013.6568453" rel="related"/>
    <link href="http://arxiv.org/abs/1409.0921v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.0921v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.2762v1</id>
    <updated>2014-09-09T14:54:49Z</updated>
    <published>2014-09-09T14:54:49Z</published>
    <title>Parallel and Distributed Collaborative Filtering: A Survey</title>
    <summary>  Collaborative filtering is amongst the most preferred techniques when
implementing recommender systems. Recently, great interest has turned towards
parallel and distributed implementations of collaborative filtering algorithms.
This work is a survey of the parallel and distributed collaborative filtering
implementations, aiming not only to provide a comprehensive presentation of the
field's development, but also to offer future research orientation by
highlighting the issues that need to be further developed.
</summary>
    <author>
      <name>Efthalia Karydi</name>
    </author>
    <author>
      <name>Konstantinos G. Margaritis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">46 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.2762v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.2762v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.4627v1</id>
    <updated>2014-09-16T13:24:44Z</updated>
    <published>2014-09-16T13:24:44Z</published>
    <title>DISA at ImageCLEF 2014 Revised: Search-based Image Annotation with DeCAF
  Features</title>
    <summary>  This paper constitutes an extension to the report on DISA-MU team
participation in the ImageCLEF 2014 Scalable Concept Image Annotation Task as
published in [3]. Specifically, we introduce a new similarity search component
that was implemented into the system, report on the results achieved by
utilizing this component, and analyze the influence of different similarity
search parameters on the annotation quality.
</summary>
    <author>
      <name>Petra Budikova</name>
    </author>
    <author>
      <name>Jan Botorek</name>
    </author>
    <author>
      <name>Michal Batko</name>
    </author>
    <author>
      <name>Pavel Zezula</name>
    </author>
    <link href="http://arxiv.org/abs/1409.4627v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.4627v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.5443v2</id>
    <updated>2014-09-27T08:23:24Z</updated>
    <published>2014-09-18T20:00:52Z</published>
    <title>Exploratory Analysis of a Terabyte Scale Web Corpus</title>
    <summary>  In this paper we present a preliminary analysis over the largest publicly
accessible web dataset: the Common Crawl Corpus. We measure nine web
characteristics from two levels of granularity using MapReduce and we comment
on the initial observations over a fraction of it. To the best of our knowledge
two of the characteristics, the language distribution and the HTML version of
pages have not been analyzed in previous work, while the specific dataset has
been only analyzed on page level.
</summary>
    <author>
      <name>Vasilis Kolias</name>
    </author>
    <author>
      <name>Ioannis Anagnostopoulos</name>
    </author>
    <author>
      <name>Eleftherios Kayafas</name>
    </author>
    <link href="http://arxiv.org/abs/1409.5443v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.5443v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.6805v1</id>
    <updated>2014-09-24T02:55:31Z</updated>
    <published>2014-09-24T02:55:31Z</published>
    <title>Improving Cross-domain Recommendation through Probabilistic
  Cluster-level Latent Factor Model--Extended Version</title>
    <summary>  Cross-domain recommendation has been proposed to transfer user behavior
pattern by pooling together the rating data from multiple domains to alleviate
the sparsity problem appearing in single rating domains. However, previous
models only assume that multiple domains share a latent common rating pattern
based on the user-item co-clustering. To capture diversities among different
domains, we propose a novel Probabilistic Cluster-level Latent Factor (PCLF)
model to improve the cross-domain recommendation performance. Experiments on
several real world datasets demonstrate that our proposed model outperforms the
state-of-the-art methods for the cross-domain recommendation task.
</summary>
    <author>
      <name>Siting Ren</name>
    </author>
    <author>
      <name>Sheng Gao</name>
    </author>
    <link href="http://arxiv.org/abs/1409.6805v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.6805v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.7729v1</id>
    <updated>2014-07-30T06:40:13Z</updated>
    <published>2014-07-30T06:40:13Z</published>
    <title>Context-Based Information Retrieval in Risky Environment</title>
    <summary>  Context-Based Information Retrieval is recently modelled as an exploration/
exploitation trade-off (exr/exp) problem, where the system has to choose
between maximizing its expected rewards dealing with its current knowledge
(exploitation) and learning more about the unknown user's preferences to
improve its knowledge (exploration). This problem has been addressed by the
reinforcement learning community but they do not consider the risk level of the
current user's situation, where it may be dangerous to explore the
non-top-ranked documents the user may not desire in his/her current situation
if the risk level is high. We introduce in this paper an algorithm named
CBIR-R-greedy that considers the risk level of the user's situation to
adaptively balance between exr and exp.
</summary>
    <author>
      <name>Djallel Bouneffouf</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1408.2195</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.7729v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.7729v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.8572v1</id>
    <updated>2014-09-29T17:17:52Z</updated>
    <published>2014-09-29T17:17:52Z</published>
    <title>Freshness-Aware Thompson Sampling</title>
    <summary>  To follow the dynamicity of the user's content, researchers have recently
started to model interactions between users and the Context-Aware Recommender
Systems (CARS) as a bandit problem where the system needs to deal with
exploration and exploitation dilemma. In this sense, we propose to study the
freshness of the user's content in CARS through the bandit problem. We
introduce in this paper an algorithm named Freshness-Aware Thompson Sampling
(FA-TS) that manages the recommendation of fresh document according to the
user's risk of the situation. The intensive evaluation and the detailed
analysis of the experimental results reveals several important discoveries in
the exploration/exploitation (exr/exp) behaviour.
</summary>
    <author>
      <name>Djallel Bouneffouf</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21st International Conference on Neural Information Processing. arXiv
  admin note: text overlap with arXiv:1409.7729</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.8572v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.8572v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.2085v1</id>
    <updated>2014-10-08T12:46:34Z</updated>
    <published>2014-10-08T12:46:34Z</published>
    <title>Low cost page quality factors to detect web spam</title>
    <summary>  Web spam is a big challenge for quality of search engine results. It is very
important for search engines to detect web spam accurately. In this paper we
present 32 low cost quality factors to classify spam and ham pages on real time
basis. These features can be divided in to three categories: (i) URL features,
(ii) Content features, and (iii) Link features. We developed a classifier using
Resilient Back-propagation learning algorithm of neural network and obtained
good accuracy. This classifier can be applied to search engine results on real
time because calculation of these features require very little CPU resources.
</summary>
    <author>
      <name>Ashish Chandra</name>
    </author>
    <author>
      <name>Mohammad Suaib</name>
    </author>
    <author>
      <name>Dr. Rizwan Beg</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Informatics Engineering, an International Journal (IEIJ) ,Vol.2,
  No.3, September 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1410.2085v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.2085v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.8068v1</id>
    <updated>2014-10-27T23:38:59Z</updated>
    <published>2014-10-27T23:38:59Z</published>
    <title>Health Information Search Behavior on the Web: A Pilot Study</title>
    <summary>  Searching health information on web has become an integral part of today's
world, and many people turn to the Web for healthcare information and
healthcare assessment. Our pilot study investigates users' preferences for the
type of search results (image, news, video, etc.), and investigates users'
ability to accurately interpret online health information for the purpose of
self diagnosis. The preliminary results reveal that blog and news articles are
most sought by users when searching online information and there exist
challenges in the use of online health information for self-diagnosis.
</summary>
    <author>
      <name>Shanu Sushmita</name>
    </author>
    <author>
      <name>Si-Chi Chin</name>
    </author>
    <link href="http://arxiv.org/abs/1410.8068v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.8068v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.1220v1</id>
    <updated>2014-11-05T10:42:29Z</updated>
    <published>2014-11-05T10:42:29Z</published>
    <title>Faster Exact Search using Document Clustering</title>
    <summary>  We show how full-text search based on inverted indices can be accelerated by
clustering the documents without losing results (SeCluD -- SEarch with
CLUstered Documents). We develop a fast multilevel clustering algorithm that
explicitly uses query cost for conjunctive queries as an objective function.
Depending on the inputs we get up to four times faster than non-clustered
search. The resulting clusters are also useful for data compression and for
distributing the work over many machines.
</summary>
    <author>
      <name>Jonathan Dimond</name>
    </author>
    <author>
      <name>Peter Sanders</name>
    </author>
    <link href="http://arxiv.org/abs/1411.1220v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.1220v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.1635v1</id>
    <updated>2014-11-06T15:26:37Z</updated>
    <published>2014-11-06T15:26:37Z</published>
    <title>Scientometrics and Information Retrieval - weak-links revitalized</title>
    <summary>  This special issue brings together eight papers from experts of communities
which often have been perceived as different once: bibliometrics,
scientometrics and informetrics on the one side and information retrieval on
the other. The idea of this special issue started at the workshop "Combining
Bibliometrics and Information Retrieval" held at the 14th International
Conference of Scientometrics and Informetrics, Vienna, July 14-19, 2013. Our
motivation as guest editors started from the observation that main discourses
in both fields are different, that communities are only partly overlapping and
from the belief that a knowledge transfer would be profitable for both sides.
</summary>
    <author>
      <name>Philipp Mayr</name>
    </author>
    <author>
      <name>Andrea Scharnhorst</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 1 figure, editorial for a special issue to appear in
  Scientometrics</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.1635v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.1635v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.3650v1</id>
    <updated>2014-11-13T18:27:10Z</updated>
    <published>2014-11-13T18:27:10Z</published>
    <title>DUM: Diversity-Weighted Utility Maximization for Recommendations</title>
    <summary>  The need for diversification of recommendation lists manifests in a number of
recommender systems use cases. However, an increase in diversity may undermine
the utility of the recommendations, as relevant items in the list may be
replaced by more diverse ones. In this work we propose a novel method for
maximizing the utility of the recommended items subject to the diversity of
user's tastes, and show that an optimal solution to this problem can be found
greedily. We evaluate the proposed method in two online user studies as well as
in an offline analysis incorporating a number of evaluation metrics. The
results of evaluations show the superiority of our method over a number of
baselines.
</summary>
    <author>
      <name>Azin Ashkan</name>
    </author>
    <author>
      <name>Branislav Kveton</name>
    </author>
    <author>
      <name>Shlomo Berkovsky</name>
    </author>
    <author>
      <name>Zheng Wen</name>
    </author>
    <link href="http://arxiv.org/abs/1411.3650v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.3650v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.3843v1</id>
    <updated>2014-11-14T09:43:24Z</updated>
    <published>2014-11-14T09:43:24Z</published>
    <title>Quantum emulation of query extension in information retrieval</title>
    <summary>  An operationalistic scheme, called Melucci metaphor, is suggested
representing Information Retrieval as physical measurements with beam of
particles playing the role of the flow of retrieved documents. The
possibilities of query expansion by extra term are studied from this
perspective, when the particles-`docuscles' are assumed to be of classical or
quantum nature. It is shown that in both cases the choice of an extra term
based on Bayesian belief revision is still valid on the qualitative level for
boosting the relevance of the retrieved documents.
</summary>
    <author>
      <name>Romàn Zapatrin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Latex, 8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.3843v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.3843v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.8281v1</id>
    <updated>2014-12-29T08:45:59Z</updated>
    <published>2014-12-29T08:45:59Z</published>
    <title>Interactive Retrieval Based on Wikipedia Concepts</title>
    <summary>  This paper presents a new user feedback mechanism based on Wikipedia concepts
for interactive retrieval. In this mechanism, the system presents to the user a
group of Wikipedia concepts, and the user can choose those relevant to refine
his/her query. To realize this mechanism, we propose methods to address two
problems: 1) how to select a small number of possibly relevant Wikipedia
concepts to show the user, and 2) how to re-rank retrieved documents given the
user-identified Wikipedia concepts. Our methods are evaluated on three TREC
data sets. The experiment results show that our methods can dramatically
improve retrieval performances.
</summary>
    <author>
      <name>Lanbo Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1412.8281v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.8281v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.00311v1</id>
    <updated>2015-01-01T20:51:25Z</updated>
    <published>2015-01-01T20:51:25Z</published>
    <title>QANUS: An Open-source Question-Answering Platform</title>
    <summary>  In this paper, we motivate the need for a publicly available, generic
software framework for question-answering (QA) systems. We present an
open-source QA framework QANUS which researchers can leverage on to build new
QA systems easily and rapidly. The framework implements much of the code that
will otherwise have been repeated across different QA systems. To demonstrate
the utility and practicality of the framework, we further present a fully
functioning factoid QA system QA-SYS built on top of QANUS.
</summary>
    <author>
      <name>Jun-Ping Ng</name>
    </author>
    <author>
      <name>Min-Yen Kan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures, demo paper describing QANUS</arxiv:comment>
    <link href="http://arxiv.org/abs/1501.00311v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.00311v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.00744v1</id>
    <updated>2015-01-05T01:49:11Z</updated>
    <published>2015-01-05T01:49:11Z</published>
    <title>Identifying Relevant Document Facets for Keyword-Based Search Queries</title>
    <summary>  As structured documents with rich metadata (such as products, movies, etc.)
become increasingly prevalent, searching those documents has become an
important IR problem. Although advanced search interfaces are widely available,
most users still prefer to use keyword-based queries to search those documents.
Query keywords often imply some hidden restrictions on the desired documents,
which can be represented as document facet-value pairs. To achieve high
retrieval performance, it's important to be able to identify the relevant
facet-value pairs hidden in a query. In this paper, we study the problem of
identifying document facet-value pairs that are relevant to a keyword-based
search query. We propose a machine learning approach and a set of useful
features, and evaluate our approach using a movie data set from INEX.
</summary>
    <author>
      <name>Lanbo Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1501.00744v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.00744v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.06380v1</id>
    <updated>2015-01-26T13:21:09Z</updated>
    <published>2015-01-26T13:21:09Z</published>
    <title>Document Distance for the Automated Expansion of Relevance Judgements
  for Information Retrieval Evaluation</title>
    <summary>  This paper reports the use of a document distance-based approach to
automatically expand the number of available relevance judgements when these
are limited and reduced to only positive judgements. This may happen, for
example, when the only available judgements are extracted from a list of
references in a published review paper. We compare the results on two document
sets: OHSUMED, based on medical research publications, and TREC-8, based on
news feeds. We show that evaluations based on these expanded relevance
judgements are more reliable than those using only the initially available
judgements, especially when the number of available judgements is very limited.
</summary>
    <author>
      <name>Diego Mollá</name>
    </author>
    <author>
      <name>Iman Amini</name>
    </author>
    <author>
      <name>David Martinez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SIGIR 2014 Workshop on Gathering Efficient Assessments of Relevance</arxiv:comment>
    <link href="http://arxiv.org/abs/1501.06380v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.06380v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4; H.3.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.07716v1</id>
    <updated>2015-01-30T09:55:24Z</updated>
    <published>2015-01-30T09:55:24Z</published>
    <title>Attention Please! A Hybrid Resource Recommender Mimicking
  Attention-Interpretation Dynamics</title>
    <summary>  Classic resource recommenders like Collaborative Filtering (CF) treat users
as being just another entity, neglecting non-linear user-resource dynamics
shaping attention and interpretation. In this paper, we propose a novel hybrid
recommendation strategy that refines CF by capturing these dynamics. The
evaluation results reveal that our approach substantially improves CF and,
depending on the dataset, successfully competes with a computationally much
more expensive Matrix Factorization variant.
</summary>
    <author>
      <name>Paul Seitlinger</name>
    </author>
    <author>
      <name>Dominik Kowald</name>
    </author>
    <author>
      <name>Simone Kopeinik</name>
    </author>
    <author>
      <name>Ilire Hasani-Mavriqi</name>
    </author>
    <author>
      <name>Tobias Ley</name>
    </author>
    <author>
      <name>Elisabeth Lex</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to WWW'15 WebScience Track</arxiv:comment>
    <link href="http://arxiv.org/abs/1501.07716v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.07716v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8; H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.00094v1</id>
    <updated>2015-01-31T12:15:53Z</updated>
    <published>2015-01-31T12:15:53Z</published>
    <title>Twitter Hash Tag Recommendation</title>
    <summary>  The rise in popularity of microblogging services like Twitter has led to
increased use of content annotation strategies like the hashtag. Hashtags
provide users with a tagging mechanism to help organize, group, and create
visibility for their posts. This is a simple idea but can be challenging for
the user in practice which leads to infrequent usage. In this paper, we will
investigate various methods of recommending hashtags as new posts are created
to encourage more widespread adoption and usage. Hashtag recommendation comes
with numerous challenges including processing huge volumes of streaming data
and content which is small and noisy. We will investigate preprocessing methods
to reduce noise in the data and determine an effective method of hashtag
recommendation based on the popular classification algorithms.
</summary>
    <author>
      <name>Roman Dovgopol</name>
    </author>
    <author>
      <name>Matt Nohelty</name>
    </author>
    <link href="http://arxiv.org/abs/1502.00094v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.00094v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.00527v1</id>
    <updated>2015-02-02T15:50:39Z</updated>
    <published>2015-02-02T15:50:39Z</published>
    <title>Context Models For Web Search Personalization</title>
    <summary>  We present our solution to the Yandex Personalized Web Search Challenge. The
aim of this challenge was to use the historical search logs to personalize
top-N document rankings for a set of test users. We used over 100 features
extracted from user- and query-depended contexts to train neural net and
tree-based learning-to-rank and regression models. Our final submission, which
was a blend of several different models, achieved an NDCG@10 of 0.80476 and
placed 4'th amongst the 194 teams winning 3'rd prize.
</summary>
    <author>
      <name>Maksims Volkovs</name>
    </author>
    <link href="http://arxiv.org/abs/1502.00527v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.00527v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.01057v1</id>
    <updated>2015-02-03T22:37:37Z</updated>
    <published>2015-02-03T22:37:37Z</published>
    <title>Personalized Web Search</title>
    <summary>  Personalization is important for search engines to improve user experience.
Most of the existing work do pure feature engineering and extract a lot of
session-style features and then train a ranking model. Here we proposed a novel
way to model both long term and short term user behavior using Multi-armed
bandit algorithm. Our algorithm can generalize session information across users
well, and as an Explore-Exploit style algorithm, it can generalize to new urls
and new users well. Experiments show that our algorithm can improve performance
over the default ranking and outperforms several popular Multi-armed bandit
algorithms.
</summary>
    <author>
      <name>Li Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/1502.01057v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.01057v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.01963v1</id>
    <updated>2015-02-06T17:35:34Z</updated>
    <published>2015-02-06T17:35:34Z</published>
    <title>Editorial for the Proceedings of the Workshop Knowledge Maps and
  Information Retrieval (KMIR2014) at Digital Libraries 2014</title>
    <summary>  Knowledge maps are promising tools for visualizing the structure of
large-scale information spaces, but still far away from being applicable for
searching. The first international workshop on "Knowledge Maps and Information
Retrieval (KMIR)", held as part of the International Conference on Digital
Libraries 2014 in London, aimed at bringing together experts in Information
Retrieval (IR) and knowledge mapping in order to discuss the potential of
interactive knowledge maps for information seeking purposes.
</summary>
    <author>
      <name>Peter Mutschke</name>
    </author>
    <author>
      <name>Philipp Mayr</name>
    </author>
    <author>
      <name>Andrea Scharnhorst</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">URL workshop proceedings: http://ceur-ws.org/Vol-1311/</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.01963v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.01963v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.01965v1</id>
    <updated>2015-02-06T17:38:48Z</updated>
    <published>2015-02-06T17:38:48Z</published>
    <title>How can heat maps of indexing vocabularies be utilized for information
  seeking purposes?</title>
    <summary>  The ability to browse an information space in a structured way by exploiting
similarities and dissimilarities between information objects is crucial for
knowledge discovery. Knowledge maps use visualizations to gain insights into
the structure of large-scale information spaces, but are still far away from
being applicable for searching. The paper proposes a use case for enhancing
search term recommendations by heat map visualizations of co-word
relation-ships taken from indexing vocabulary. By contrasting areas of
different "heat" the user is enabled to indicate mainstream areas of the field
in question more easily.
</summary>
    <author>
      <name>Peter Mutschke</name>
    </author>
    <author>
      <name>Karima Haddou ou Moussa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">URL workshop proceedings: http://ceur-ws.org/Vol-1311/</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.01965v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.01965v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.05957v1</id>
    <updated>2015-02-20T17:55:58Z</updated>
    <published>2015-02-20T17:55:58Z</published>
    <title>Web Similarity</title>
    <summary>  Normalized web distance (NWD) is a similarity or normalized semantic distance
based on the World Wide Web or any other large electronic database, for
instance Wikipedia, and a search engine that returns reliable aggregate page
counts. For sets of search terms the NWD gives a similarity on a scale from 0
(identical) to 1 (completely different). The NWD approximates the similarity
according to all (upper semi)computable properties. We develop the theory and
give applications. The derivation of the NWD method is based on Kolmogorov
complexity.
</summary>
    <author>
      <name>Andrew R. Cohen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Dept Electri. Comput. Eng., Drexel Univ.</arxiv:affiliation>
    </author>
    <author>
      <name>Paul M. B. Vitanyi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CWI and University of Amsterdam &amp; Drexel University</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">LaTeX 25 pages, 3 tables. A precursor is arXiv:1308.3177</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.05957v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.05957v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.01647v1</id>
    <updated>2015-03-05T14:34:02Z</updated>
    <published>2015-03-05T14:34:02Z</published>
    <title>Decentralized Recommender Systems</title>
    <summary>  This paper proposes a decentralized recommender system by formulating the
popular collaborative filleting (CF) model into a decentralized matrix
completion form over a set of users. In such a way, data storages and
computations are fully distributed. Each user could exchange limited
information with its local neighborhood, and thus it avoids the centralized
fusion. Advantages of the proposed system include a protection on user privacy,
as well as better scalability and robustness. We compare our proposed algorithm
with several state-of-the-art algorithms on the FlickerUserFavor dataset, and
demonstrate that the decentralized algorithm can gain a competitive performance
to others.
</summary>
    <author>
      <name>Zhangyang Wang</name>
    </author>
    <author>
      <name>Xianming Liu</name>
    </author>
    <author>
      <name>Shiyu Chang</name>
    </author>
    <author>
      <name>Jiayu Zhou</name>
    </author>
    <author>
      <name>Guo-Jun Qi</name>
    </author>
    <author>
      <name>Thomas S. Huang</name>
    </author>
    <link href="http://arxiv.org/abs/1503.01647v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.01647v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.03168v1</id>
    <updated>2015-03-10T10:34:06Z</updated>
    <published>2015-03-10T10:34:06Z</published>
    <title>Experimental Estimation of Number of Clusters Based on Cluster Quality</title>
    <summary>  Text Clustering is a text mining technique which divides the given set of
text documents into significant clusters. It is used for organizing a huge
number of text documents into a well-organized form. In the majority of the
clustering algorithms, the number of clusters must be specified apriori, which
is a drawback of these algorithms. The aim of this paper is to show
experimentally how to determine the number of clusters based on cluster
quality. Since partitional clustering algorithms are well-suited for clustering
large document datasets, we have confined our analysis to a partitional
clustering algorithm.
</summary>
    <author>
      <name>G. Hannah Grace</name>
    </author>
    <author>
      <name>Kalyani Desikan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 9 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of mathematics and computer science, Vol12 (2014), 304-315</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1503.03168v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.03168v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.03607v1</id>
    <updated>2015-03-12T06:51:06Z</updated>
    <published>2015-03-12T06:51:06Z</published>
    <title>A divisive hierarchical clustering-based method for indexing image
  information</title>
    <summary>  In most practical applications of image retrieval, high-dimensional feature
vectors are required, but current multi-dimensional indexing structures lose
their efficiency with growth of dimensions. Our goal is to propose a divisive
hierarchical clustering-based multi-dimensional indexing structure which is
efficient in high-dimensional feature spaces. A projection pursuit method has
been used for finding a component of the data, which data's projections onto it
maximizes the approximation of negentropy for preparing essential information
in order to partitioning of the data space. Various tests and experimental
results on high-dimensional datasets indicate the performance of proposed
method in comparison with others.
</summary>
    <author>
      <name>Najva Izadpanah</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Signal &amp; Image Processing : An International Journal (SIPIJ)
  Vol.6, No.1, February 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1503.03607v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.03607v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.07284v1</id>
    <updated>2015-03-25T05:35:05Z</updated>
    <published>2015-03-25T05:35:05Z</published>
    <title>A Rule-Based Short Query Intent Identification System</title>
    <summary>  Using SMS (Short Message System), cell phones can be used to query for
information about various topics. In an SMS based search system, one of the key
problems is to identify a domain (broad topic) associated with the user query;
so that a more comprehensive search can be carried out by the domain specific
search engine. In this paper we use a rule based approach, to identify the
domain, called Short Query Intent Identification System (SQIIS). We construct
two different rule-bases using different strategies to suit query intent
identification. We evaluate the two rule-bases experimentally.
</summary>
    <author>
      <name>Arijit De</name>
    </author>
    <author>
      <name>Sunil Kumar Kopparapu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICSIP.2010.5697471</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICSIP.2010.5697471" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2010 International Conference on Signal and Image Processing
  (ICSIP)</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.07284v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.07284v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.07474v1</id>
    <updated>2015-03-25T17:52:21Z</updated>
    <published>2015-03-25T17:52:21Z</published>
    <title>User Profiling Trends, Techniques and Applications</title>
    <summary>  The Personalization of information has taken recommender systems at a very
high level. With personalization these systems can generate user specific
recommendations accurately and efficiently. User profiling helps
personalization, where information retrieval is done to personalize a scenario
which maintains a separate user profile for individual user. The main objective
of this paper is to explore this field of personalization in context of user
profiling, to help researchers make aware of the user profiling. Various
trends, techniques and Applications have been discussed in paper which will
fulfill this motto.
</summary>
    <author>
      <name>Sumitkumar Kanoje</name>
    </author>
    <author>
      <name>Sheetal Girase</name>
    </author>
    <author>
      <name>Debajyoti Mukhopadhyay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 figure in IJAFRC, Vol.1, Issue 11, November 2014, ISSN:
  2348-4853. arXiv admin note: text overlap with arXiv:1503.06555</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.07474v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.07474v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.00305v1</id>
    <updated>2015-04-01T17:24:52Z</updated>
    <published>2015-04-01T17:24:52Z</published>
    <title>Study the effectiveness of genetic algorithm for documentary subject
  search</title>
    <summary>  This article presents results of experimental studies the effectiveness of
the genetic algorithm that was applied to effective queries creation and
relevant document selection. Studies were carried out to the comparative
analysis of the semantic relevance and quality ranking of the documents found
on the Internet in various ways. Analysis of the results shows that the
greatest effect of presented technology is achieved by finding new documents
for skilled users in the initial stages of the study of the topic.
Additionally, the number of unique and relevant results is significantly
increased.
</summary>
    <author>
      <name>V. K. Ivanov</name>
    </author>
    <author>
      <name>B. V. Palyukh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, in Russian</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">OSTIS-2015 1 (2015) 471-476</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1504.00305v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.00305v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.01183v1</id>
    <updated>2015-04-06T01:19:25Z</updated>
    <published>2015-04-06T01:19:25Z</published>
    <title>Document Clustering using K-Medoids</title>
    <summary>  People are always in search of matters for which they are prone to use
internet, but again it has huge assemblage of data due to which it becomes
difficult for the reader to get the most accurate data. To make it easier for
people to gather accurate data, similar information has to be clustered at one
place. There are many algorithms used for clustering of relevant information in
one platform. In this paper, K-Medoids clustering algorithm has been employed
for formation of clusters which is further used for document summarization.
</summary>
    <author>
      <name>Monica Jha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal on Advanced Computer Theory and Engineering
  (IJACTE), ISSN (Print): 2319-2526, Volume-4, Issue-1, 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1504.01183v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.01183v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.01433v1</id>
    <updated>2015-04-06T22:55:38Z</updated>
    <published>2015-04-06T22:55:38Z</published>
    <title>Automated System for Improving RSS Feeds Data Quality</title>
    <summary>  Nowadays, the majority of RSS feeds provide incomplete information about
their news items. The lack of information leads to engagement loss in users. We
present a new automated system for improving the RSS feeds' data quality. RSS
feeds provide a list of the latest news items ordered by date. Therefore, it
makes it easy for a web crawler to precisely locate the item and extract its
raw content. Then it identifies where the main content is located and extracts:
main text corpus, relevant keywords, bigrams, best image and predicts the
category of the item. The output of the system is an enhanced RSS feed. The
proposed system showed an average item data quality improvement from 39.98% to
95.62%.
</summary>
    <author>
      <name>Joan Hurtado</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.01433v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.01433v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.02362v1</id>
    <updated>2015-04-09T16:22:32Z</updated>
    <published>2015-04-09T16:22:32Z</published>
    <title>Approaches to the Intelligent Subject Search</title>
    <summary>  This article presents main results of the pilot study of approaches to the
subject information search based on automated semantic processing of mass
scientific and technical data. The authors focus on technology of building and
qualification of search queries with the following filtering and ranking of
search data. Software architecture, specific features of subject search and
research results application are considered.
</summary>
    <author>
      <name>V. K. Ivanov</name>
    </author>
    <author>
      <name>B. V. Palyukh</name>
    </author>
    <author>
      <name>A. N. Sotnikov</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.15439/2014F70 10.15439/978-83-60810-57-6</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.15439/2014F70" rel="related"/>
    <link title="doi" href="http://dx.doi.org/10.15439/978-83-60810-57-6" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">FedCSIS'2014 3 (2014) 13-20</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1504.02362v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.02362v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.04216v1</id>
    <updated>2015-04-16T13:05:19Z</updated>
    <published>2015-04-16T13:05:19Z</published>
    <title>Genetic algorithm implementation for effective document subject search</title>
    <summary>  This paper describes the software implementation of genetic algorithm for
identifying and selecting most relevant results received during sequentially
executed subject search operations. Simulated evolutionary process generates
sustainable and effective population of search queries, forms search pattern of
documents or semantic core, creates relevant sets of required documents, allows
automatic classification of search results. The paper discusses the features of
subject search, justifies the use of a genetic algorithm, describes arguments
of the fitness function and describes basic steps and parameters of the
algorithm.
</summary>
    <author>
      <name>V. K. Ivanov</name>
    </author>
    <author>
      <name>P. I. Meskin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.15827/0236-235X.108.118-126</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.15827/0236-235X.108.118-126" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in Russian</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Programmnye produkty i sistemy 4 (2014) 118-126</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1504.04216v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.04216v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.05473v1</id>
    <updated>2015-04-21T15:38:23Z</updated>
    <published>2015-04-21T15:38:23Z</published>
    <title>Can FCA-based Recommender System Suggest a Proper Classifier?</title>
    <summary>  The paper briefly introduces multiple classifier systems and describes a new
algorithm, which improves classification accuracy by means of recommendation of
a proper algorithm to an object classification. This recommendation is done
assuming that a classifier is likely to predict the label of the object
correctly if it has correctly classified its neighbors. The process of
assigning a classifier to each object is based on Formal Concept Analysis. We
explain the idea of the algorithm with a toy example and describe our first
experiments with real-world datasets.
</summary>
    <author>
      <name>Yury Kashnitsky</name>
    </author>
    <author>
      <name>Dmitry I. Ignatov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 1 figure, 4 tables, ECAI 2014, workshop "What FCA can do
  for "Artifficial Intelligence"</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">CEUR Workshop Proceedings, 1257, pp. 17-26 (2014)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1504.05473v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.05473v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62-07" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.06077v1</id>
    <updated>2015-04-23T08:27:29Z</updated>
    <published>2015-04-23T08:27:29Z</published>
    <title>Open Data Platform for Knowledge Access in Plant Health Domain : VESPA
  Mining</title>
    <summary>  Important data are locked in ancient literature. It would be uneconomic to
produce these data again and today or to extract them without the help of text
mining technologies. Vespa is a text mining project whose aim is to extract
data on pest and crops interactions, to model and predict attacks on crops, and
to reduce the use of pesticides. A few attempts proposed an agricultural
information access. Another originality of our work is to parse documents with
a dependency of the document architecture.
</summary>
    <author>
      <name>Nicolas Turenne</name>
    </author>
    <author>
      <name>Mathieu Andro</name>
    </author>
    <author>
      <name>Roselyne Corbière</name>
    </author>
    <author>
      <name>Tien T. Phan</name>
    </author>
    <link href="http://arxiv.org/abs/1504.06077v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.06077v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.00168v1</id>
    <updated>2015-05-01T12:21:24Z</updated>
    <published>2015-05-01T12:21:24Z</published>
    <title>Comparison Clustering using Cosine and Fuzzy set based Similarity
  Measures of Text Documents</title>
    <summary>  Keeping in consideration the high demand for clustering, this paper focuses
on understanding and implementing K-means clustering using two different
similarity measures. We have tried to cluster the documents using two different
measures rather than clustering it with Euclidean distance. Also a comparison
is drawn based on accuracy of clustering between fuzzy and cosine similarity
measure. The start time and end time parameters for formation of clusters are
used in deciding optimum similarity measure.
</summary>
    <author>
      <name>Manan Mohan Goyal</name>
    </author>
    <author>
      <name>Neha Agrawal</name>
    </author>
    <author>
      <name>Manoj Kumar Sarma</name>
    </author>
    <author>
      <name>Nayan Jyoti Kalita</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, International Conference on Computing and Communication
  Systems 2015 (I3CS'15), ISBM: 978-1-4799-5857-01, 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.00168v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.00168v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.00519v1</id>
    <updated>2015-05-04T03:38:04Z</updated>
    <published>2015-05-04T03:38:04Z</published>
    <title>Large Scale Discovery of Seasonal Music From User Data</title>
    <summary>  The consumption history of online media content such as music and video
offers a rich source of data from which to mine information. Trends in this
data are of particular interest because they reflect user preferences as well
as associated cultural contexts that can be exploited in systems such as
recommendation or search. This paper classifies songs as seasonal using a
large, real-world dataset of user listening data. Results show strong
performance of classification of Christmas music with Gaussian Mixture Models.
</summary>
    <author>
      <name>Cameron Summers</name>
    </author>
    <author>
      <name>Phillip Popp</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.00519v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.00519v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.00755v1</id>
    <updated>2015-05-04T19:04:19Z</updated>
    <published>2015-05-04T19:04:19Z</published>
    <title>Towards the Ontology Web Search Engine</title>
    <summary>  The project of the Ontology Web Search Engine is presented in this paper. The
main purpose of this paper is to develop such a project that can be easily
implemented. Ontology Web Search Engine is software to look for and index
ontologies in the Web. OWL (Web Ontology Languages) ontologies are meant, and
they are necessary for the functioning of the SWES (Semantic Web Expert
System). SWES is an expert system that will use found ontologies from the Web,
generating rules from them, and will supplement its knowledge base with these
generated rules. It is expected that the SWES will serve as a universal expert
system for the average user.
</summary>
    <author>
      <name>Olegs Verhodubs</name>
    </author>
    <link href="http://arxiv.org/abs/1505.00755v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.00755v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.00863v1</id>
    <updated>2015-05-05T02:06:23Z</updated>
    <published>2015-05-05T02:06:23Z</published>
    <title>A Feature-based Classification Technique for Answering Multi-choice
  World History Questions</title>
    <summary>  Our FRDC_QA team participated in the QA-Lab English subtask of the NTCIR-11.
In this paper, we describe our system for solving real-world university
entrance exam questions, which are related to world history. Wikipedia is used
as the main external resource for our system. Since problems with choosing
right/wrong sentence from multiple sentence choices account for about
two-thirds of the total, we individually design a classification based model
for solving this type of questions. For other types of questions, we also
design some simple methods.
</summary>
    <author>
      <name>Shuangyong Song</name>
    </author>
    <author>
      <name>Yao Meng</name>
    </author>
    <author>
      <name>Zhongguang Zheng</name>
    </author>
    <author>
      <name>Jun Sun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, no figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.00863v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.00863v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.02798v1</id>
    <updated>2015-05-11T20:39:48Z</updated>
    <published>2015-05-11T20:39:48Z</published>
    <title>Math Search for the Masses: Multimodal Search Interfaces and
  Appearance-Based Retrieval</title>
    <summary>  We summarize math search engines and search interfaces produced by the
Document and Pattern Recognition Lab in recent years, and in particular the min
math search interface and the Tangent search engine. Source code for both
systems are publicly available. "The Masses" refers to our emphasis on creating
systems for mathematical non-experts, who may be looking to define unfamiliar
notation, or browse documents based on the visual appearance of formulae rather
than their mathematical semantics.
</summary>
    <author>
      <name>Richard Zanibbi</name>
    </author>
    <author>
      <name>Awelemdy Orakwue</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper for Invited Talk at 2015 Conference on Intelligent Computer
  Mathematics (July, Washington DC)</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.02798v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.02798v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.03090v1</id>
    <updated>2015-05-12T17:17:28Z</updated>
    <published>2015-05-12T17:17:28Z</published>
    <title>Efficient Similarity Indexing and Searching in High Dimensions</title>
    <summary>  Efficient indexing and searching of high dimensional data has been an area of
active research due to the growing exploitation of high dimensional data and
the vulnerability of traditional search methods to the curse of dimensionality.
This paper presents a new approach for fast and effective searching and
indexing of high dimensional features using random partitions of the feature
space. Experiments on both handwritten digits and 3-D shape descriptors have
shown the proposed algorithm to be highly effective and efficient in indexing
and searching real data sets of several hundred dimensions. We also compare its
performance to that of the state-of-the-art locality sensitive hashing
algorithm.
</summary>
    <author>
      <name>Yu Zhong</name>
    </author>
    <link href="http://arxiv.org/abs/1505.03090v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.03090v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.03934v1</id>
    <updated>2015-05-15T01:08:57Z</updated>
    <published>2015-05-15T01:08:57Z</published>
    <title>Textual Spatial Cosine Similarity</title>
    <summary>  When dealing with document similarity many methods exist today, like cosine
similarity. More complex methods are also available based on the semantic
analysis of textual information, which are computationally expensive and rarely
used in the real time feeding of content as in enterprise-wide search
environments. To address these real-time constraints, we developed a new
measure of document similarity called Textual Spatial Cosine Similarity, which
is able to detect similitude at the semantic level using word placement
information contained in the document. We will see in this paper that two
degenerate cases exist for this model, which coincide with Cosine Similarity on
one side and with a paraphrasing detection model to the other.
</summary>
    <author>
      <name>Giancarlo Crocetti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 4 tables. Proceedings of 12th Annual Research Day, 2014 -
  Pace University</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.03934v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.03934v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; H.3.3; I.5.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.08155v1</id>
    <updated>2015-05-29T19:17:03Z</updated>
    <published>2015-05-29T19:17:03Z</published>
    <title>Performance Evaluation and Optimization of Math-Similarity Search</title>
    <summary>  Similarity search in math is to find mathematical expressions that are
similar to a user's query. We conceptualized the similarity factors between
mathematical expressions, and proposed an approach to math similarity search
(MSS) by defining metrics based on those similarity factors [11]. Our
preliminary implementation indicated the advantage of MSS compared to
non-similarity based search. In order to more effectively and efficiently
search similar math expressions, MSS is further optimized. This paper focuses
on performance evaluation and optimization of MSS. Our results show that the
proposed optimization process significantly improved the performance of MSS
with respect to both relevance ranking and recall.
</summary>
    <author>
      <name>Qun Zhang</name>
    </author>
    <author>
      <name>Abdou Youssef</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.08155v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.08155v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.01443v1</id>
    <updated>2015-07-06T13:26:02Z</updated>
    <published>2015-07-06T13:26:02Z</published>
    <title>Nonparametric Bayesian Modeling for Automated Database Schema Matching</title>
    <summary>  The problem of merging databases arises in many government and commercial
applications. Schema matching, a common first step, identifies equivalent
fields between databases. We introduce a schema matching framework that builds
nonparametric Bayesian models for each field and compares them by computing the
probability that a single model could have generated both fields. Our
experiments show that our method is more accurate and faster than the existing
instance-based matching algorithms in part because of the use of nonparametric
Bayesian models.
</summary>
    <author>
      <name>Erik M. Ferragut</name>
    </author>
    <author>
      <name>Jason Laska</name>
    </author>
    <link href="http://arxiv.org/abs/1507.01443v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.01443v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.02907v1</id>
    <updated>2015-07-10T13:59:00Z</updated>
    <published>2015-07-10T13:59:00Z</published>
    <title>Extending a Single-Document Summarizer to Multi-Document: a Hierarchical
  Approach</title>
    <summary>  The increasing amount of online content motivated the development of
multi-document summarization methods. In this work, we explore straightforward
approaches to extend single-document summarization methods to multi-document
summarization. The proposed methods are based on the hierarchical combination
of single-document summaries, and achieves state of the art results.
</summary>
    <author>
      <name>Luís Marujo</name>
    </author>
    <author>
      <name>Ricardo Ribeiro</name>
    </author>
    <author>
      <name>David Martins de Matos</name>
    </author>
    <author>
      <name>João P. Neto</name>
    </author>
    <author>
      <name>Anatole Gershman</name>
    </author>
    <author>
      <name>Jaime Carbonell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, Please cite: Proceedings of *SEM: the 4th Joint Conference
  on Lexical and Computational Semantics (bibtex:
  http://aclweb.org/anthology/S/S15/S15-1020.bib)</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.02907v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.02907v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.03928v1</id>
    <updated>2015-07-14T17:06:51Z</updated>
    <published>2015-07-14T17:06:51Z</published>
    <title>Pseudo-Query Reformulation</title>
    <summary>  Automatic query reformulation refers to rewriting a user's original query in
order to improve the ranking of retrieval results compared to the original
query. We present a general framework for automatic query reformulation based
on discrete optimization. Our approach, referred to as pseudo-query
reformulation, treats automatic query reformulation as a search problem over
the graph of unweighted queries linked by minimal transformations (e.g. term
additions, deletions). This framework allows us to test existing performance
prediction methods as heuristics for the graph search process. We demonstrate
the effectiveness of the approach on several publicly available datasets.
</summary>
    <author>
      <name>Fernando Diaz</name>
    </author>
    <link href="http://arxiv.org/abs/1507.03928v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.03928v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.05497v1</id>
    <updated>2015-07-20T13:58:30Z</updated>
    <published>2015-07-20T13:58:30Z</published>
    <title>RAPS: A Recommender Algorithm Based on Pattern Structures</title>
    <summary>  We propose a new algorithm for recommender systems with numeric ratings which
is based on Pattern Structures (RAPS). As the input the algorithm takes rating
matrix, e.g., such that it contains movies rated by users. For a target user,
the algorithm returns a rated list of items (movies) based on its previous
ratings and ratings of other users. We compare the results of the proposed
algorithm in terms of precision and recall measures with Slope One, one of the
state-of-the-art item-based algorithms, on Movie Lens dataset and RAPS
demonstrates the best or comparable quality.
</summary>
    <author>
      <name>Dmitry I. Ignatov</name>
    </author>
    <author>
      <name>Denis Kornilov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The paper presented at FCA4AI 2015 in conjunction with IJCAI 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.05497v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.05497v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="06F99" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; H.2.8; I.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.07382v1</id>
    <updated>2015-07-27T12:05:58Z</updated>
    <published>2015-07-27T12:05:58Z</published>
    <title>Application of Kullback-Leibler divergence for short-term user interest
  detection</title>
    <summary>  Classical approaches in recommender systems such as collaborative filtering
are concentrated mainly on static user preference extraction. This approach
works well as an example for music recommendations when a user behavior tends
to be stable over long period of time, however the most common situation in
e-commerce is different which requires reactive algorithms based on a
short-term user activity analysis. This paper introduces a small mathematical
framework for short-term user interest detection formulated in terms of item
properties and its application for recommender systems enhancing. The framework
is based on the fundamental concept of information theory --- Kullback-Leibler
divergence.
</summary>
    <author>
      <name>Maxim Borisyak</name>
    </author>
    <author>
      <name>Roman Zykov</name>
    </author>
    <author>
      <name>Artem Noskov</name>
    </author>
    <link href="http://arxiv.org/abs/1507.07382v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.07382v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.08439v1</id>
    <updated>2015-07-30T10:08:14Z</updated>
    <published>2015-07-30T10:08:14Z</published>
    <title>Metadata Embeddings for User and Item Cold-start Recommendations</title>
    <summary>  I present a hybrid matrix factorisation model representing users and items as
linear combinations of their content features' latent factors. The model
outperforms both collaborative and content-based models in cold-start or sparse
interaction data scenarios (using both user and item metadata), and performs at
least as well as a pure collaborative matrix factorisation model where
interaction data is abundant. Additionally, feature embeddings produced by the
model encode semantic information in a way reminiscent of word embedding
approaches, making them useful for a range of related tasks such as tag
recommendations.
</summary>
    <author>
      <name>Maciej Kula</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Lyst.com</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1507.08439v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.08439v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.01929v1</id>
    <updated>2015-08-08T17:18:40Z</updated>
    <published>2015-08-08T17:18:40Z</published>
    <title>Combining Text and Formula Queries in Math Information Retrieval:
  Evaluation of Query Results Merging Strategies</title>
    <summary>  Specific to Math Information Retrieval is combining text with mathematical
formulae both in documents and in queries. Rigorous evaluation of query
expansion and merging strategies combining math and standard textual keyword
terms in a query are given. It is shown that techniques similar to those known
from textual query processing may be applied in math information retrieval as
well, and lead to a cutting edge performance. Striping and merging partial
results from subqueries is one technique that improves results measured by
information retrieval evaluation metrics like Bpref.
</summary>
    <author>
      <name>Martin Líška</name>
    </author>
    <author>
      <name>Petr Sojka</name>
    </author>
    <author>
      <name>Michal Růžička</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2810355.2810359</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2810355.2810359" rel="related"/>
    <link href="http://arxiv.org/abs/1508.01929v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.01929v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; I.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.02127v1</id>
    <updated>2015-08-10T04:56:08Z</updated>
    <published>2015-08-10T04:56:08Z</published>
    <title>A novel design of hidden web crawler using ontology</title>
    <summary>  Deep Web is content hidden behind HTML forms. Since it represents a large
portion of the structured, unstructured and dynamic data on the Web, accessing
Deep-Web content has been a long challenge for the database community. This
paper describes a crawler for accessing Deep-Web using Ontologies. Performance
evaluation of the proposed work showed that this new approach has promising
results.
</summary>
    <author>
      <name> Manvi</name>
    </author>
    <author>
      <name>Komal Kumar Bhatia</name>
    </author>
    <author>
      <name>Ashutosh Dixit</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.14445/22315381/IJETT-V26P204</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.14445/22315381/IJETT-V26P204" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages,8 figures,2 tables, International Journal of Engineering
  Trends &amp; Technology (IJETT),August 2015, ISSN: 2231-5381</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.02127v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.02127v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.03298v1</id>
    <updated>2015-08-13T18:35:06Z</updated>
    <published>2015-08-13T18:35:06Z</published>
    <title>Enabling Complex Wikipedia Queries - Technical Report</title>
    <summary>  In this technical report we present a database schema used to store Wikipedia
so it can be easily used in query-intensive applications. In addition to
storing the information in a way that makes it highly accessible, our schema
enables users to easily formulate complex queries using information such as the
anchor-text of links and their location in the page, the titles and number of
redirect pages for each page and the paragraph structure of entity pages. We
have successfully used the schema in domains such as recommender systems,
information retrieval and sentiment analysis. In order to assist other
researchers, we now make the schema and its content available online.
</summary>
    <author>
      <name>Gilad Katz</name>
    </author>
    <author>
      <name>Bracha Shapira</name>
    </author>
    <link href="http://arxiv.org/abs/1508.03298v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.03298v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.03856v1</id>
    <updated>2015-08-16T19:27:35Z</updated>
    <published>2015-08-16T19:27:35Z</published>
    <title>Two-stage Cascaded Classifier for Purchase Prediction</title>
    <summary>  In this paper we describe our machine learning solution for the RecSys
Challenge, 2015. We have proposed a time efficient two-stage cascaded
classifier for the prediction of buy sessions and purchased items within such
sessions. Based on the model, several interesting features found, and formation
of our own test bed, we have achieved a reasonable score. Usage of Random
Forests helps us to cope with the effect of the multiplicity of good models
depending on varying subsets of features in the purchased items prediction and,
in its turn, boosting is used as a suitable technique to overcome severe class
imbalance of the buy-session prediction.
</summary>
    <author>
      <name>Sheikh Muhammad Sarwar</name>
    </author>
    <author>
      <name>Mahamudul Hasan</name>
    </author>
    <author>
      <name>Dmitry I. Ignatov</name>
    </author>
    <link href="http://arxiv.org/abs/1508.03856v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.03856v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.02010v2</id>
    <updated>2015-09-26T15:16:54Z</updated>
    <published>2015-09-07T12:36:19Z</published>
    <title>LocLinkVis: A Geographic Information Retrieval-Based System for
  Large-Scale Exploratory Search</title>
    <summary>  In this paper we present LocLinkVis (Locate-Link-Visualize); a system which
supports exploratory information access to a document collection based on
geo-referencing and visualization. It uses a gazetteer which contains
representations of places ranging from countries to buildings, and that is used
to recognize toponyms, disambiguate them into places, and to visualize the
resulting spatial footprints.
</summary>
    <author>
      <name>Alex Olieman</name>
    </author>
    <author>
      <name>Jaap Kamps</name>
    </author>
    <author>
      <name>Rosa Merino Claros</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SEM'15</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. Posters and Demos Track of 11th Int. Conf. on Semantic
  Systems (2015) 30-33</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1509.02010v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.02010v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.01991v1</id>
    <updated>2015-10-07T15:39:39Z</updated>
    <published>2015-10-07T15:39:39Z</published>
    <title>HDIdx: High-Dimensional Indexing for Efficient Approximate Nearest
  Neighbor Search</title>
    <summary>  Fast Nearest Neighbor (NN) search is a fundamental challenge in large-scale
data processing and analytics, particularly for analyzing multimedia contents
which are often of high dimensionality. Instead of using exact NN search,
extensive research efforts have been focusing on approximate NN search
algorithms. In this work, we present "HDIdx", an efficient high-dimensional
indexing library for fast approximate NN search, which is open-source and
written in Python. It offers a family of state-of-the-art algorithms that
convert input high-dimensional vectors into compact binary codes, making them
very efficient and scalable for NN search with very low space complexity.
</summary>
    <author>
      <name>Ji Wan</name>
    </author>
    <author>
      <name>Sheng Tang</name>
    </author>
    <author>
      <name>Yongdong Zhang</name>
    </author>
    <author>
      <name>Jintao Li</name>
    </author>
    <author>
      <name>Pengcheng Wu</name>
    </author>
    <author>
      <name>Steven C. H. Hoi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.neucom.2015.11.104</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.neucom.2015.11.104" rel="related"/>
    <link href="http://arxiv.org/abs/1510.01991v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.01991v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.02755v2</id>
    <updated>2015-12-12T17:56:52Z</updated>
    <published>2015-10-04T09:24:27Z</published>
    <title>A Novel Approach to Document Classification using WordNet</title>
    <summary>  Content based Document Classification is one of the biggest challenges in the
context of free text mining. Current algorithms on document classifications
mostly rely on cluster analysis based on bag-of-words approach. However that
method is still being applied to many modern scientific dilemmas. It has
established a strong presence in fields like economics and social science to
merit serious attention from the researchers. In this paper we would like to
propose and explore an alternative grounded more securely on the dictionary
classification and correlatedness of words and phrases. It is expected that
application of our existing knowledge about the underlying classification
structure may lead to improvement of the classifier's performance.
</summary>
    <author>
      <name>Koushiki Sarkar</name>
    </author>
    <author>
      <name>Ritwika Law</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">(Working Paper)</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.02755v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.02755v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.00722v1</id>
    <updated>2015-11-02T21:49:25Z</updated>
    <published>2015-11-02T21:49:25Z</published>
    <title>Identifying Actionable Messages on Social Media</title>
    <summary>  Text actionability detection is the problem of classifying user authored
natural language text, according to whether it can be acted upon by a
responding agent. In this paper, we propose a supervised learning framework for
domain-aware, large-scale actionability classification of social media
messages. We derive lexicons, perform an in-depth analysis for over 25 text
based features, and explore strategies to handle domains that have limited
training data. We apply these methods to over 46 million messages spanning 75
companies and 35 languages, from both Facebook and Twitter. The models achieve
an aggregate population-weighted F measure of 0.78 and accuracy of 0.74, with
values of over 0.9 in some cases.
</summary>
    <author>
      <name>Nemanja Spasojevic</name>
    </author>
    <author>
      <name>Adithya Rao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 2015 IEEE International Big Data Conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.00722v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.00722v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.02433v1</id>
    <updated>2015-11-08T03:25:28Z</updated>
    <published>2015-11-08T03:25:28Z</published>
    <title>Accelerating Recommender Systems using GPUs</title>
    <summary>  We describe GPU implementations of the matrix recommender algorithms CCD++
and ALS. We compare the processing time and predictive ability of the GPU
implementations with existing multi-core versions of the same algorithms.
Results on the GPU are better than the results of the multi-core versions
(maximum speedup of 14.8).
</summary>
    <author>
      <name>André Valente Rodrigues</name>
    </author>
    <author>
      <name>Alípio Jorge</name>
    </author>
    <author>
      <name>Inês Dutra</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2695664.2695850</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2695664.2695850" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SAC '15 Proceedings of the 30th Annual ACM Symposium on Applied
  Computing Pages 879-884 ACM New York, NY, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1511.02433v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.02433v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.05810v1</id>
    <updated>2015-11-18T14:43:22Z</updated>
    <published>2015-11-18T14:43:22Z</published>
    <title>The Influence of Commercial Intent of Search Results on Their Perceived
  Relevance</title>
    <summary>  We carried out a retrieval effectiveness test on the three major web search
engines (i.e., Google, Microsoft and Yahoo). In addition to relevance
judgments, we classified the results according to their commercial intent and
whether or not they carried any advertising. We found that all search engines
provide a large number of results with a commercial intent. Google provides
significantly more commercial results than the other search engines do.
However, the commercial intent of a result did not influence jurors in their
relevance judgments.
</summary>
    <author>
      <name>Dirk Lewandowski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Measurement, Performance, Experimentation, Worldwide Web, search
  engines, commerciality, evaluation</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.05810v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.05810v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.05817v1</id>
    <updated>2015-11-18T14:54:10Z</updated>
    <published>2015-11-18T14:54:10Z</published>
    <title>A Framework for Evaluating the Retrieval Effectiveness of Search Engines</title>
    <summary>  This chapter presents a theoretical framework for evaluating next generation
search engines. We focus on search engines whose results presentation is
enriched with additional information and does not merely present the usual list
of 10 blue links, that is, of ten links to results, accompanied by a short
description. While Web search is used as an example here, the framework can
easily be applied to search engines in any other area. The framework not only
addresses the results presentation, but also takes into account an extension of
the general design of retrieval effectiveness tests. The chapter examines the
ways in which this design might influence the results of such studies and how a
reliable test is best designed.
</summary>
    <author>
      <name>Dirk Lewandowski</name>
    </author>
    <link href="http://arxiv.org/abs/1511.05817v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.05817v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.00198v2</id>
    <updated>2015-12-04T10:12:53Z</updated>
    <published>2015-12-01T09:57:03Z</published>
    <title>Efficient filtering of adult content using textual information</title>
    <summary>  Nowadays adult content represents a non negligible proportion of the Web
content. It is of the utmost importance to protect children from this content.
Search engines, as an entry point for Web navigation are ideally placed to deal
with this issue.
  In this paper, we propose a method that builds a safe index i.e.
adult-content free for search engines. This method is based on a filter that
uses only textual information from the web page and the associated URL.
</summary>
    <author>
      <name>Thomas Largillier</name>
    </author>
    <author>
      <name>Guillaume Peyronnet</name>
    </author>
    <author>
      <name>Sylvain Peyronnet</name>
    </author>
    <link href="http://arxiv.org/abs/1512.00198v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.00198v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.05437v1</id>
    <updated>2015-12-17T01:48:37Z</updated>
    <published>2015-12-17T01:48:37Z</published>
    <title>A Method of Passage-Based Document Retrieval in Question Answering
  System</title>
    <summary>  We propose a method for using the scoring values of passages to effectively
retrieve documents in a Question Answering system.
  For this, we suggest evaluation function that considers proximity between
each question terms in passage. And using this evaluation function , we extract
a documents which involves scoring values in the highest collection, as a
suitable document for question.
  The proposed method is very effective in document retrieval of Korean
question answering system.
</summary>
    <author>
      <name>Man-Hung Jong</name>
    </author>
    <author>
      <name>Chong-Han Ri</name>
    </author>
    <author>
      <name>Hyok-Chol Choe</name>
    </author>
    <author>
      <name>Chol-Jun Hwang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.05437v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.05437v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.00855v1</id>
    <updated>2016-01-05T15:18:10Z</updated>
    <published>2016-01-05T15:18:10Z</published>
    <title>TimeMachine: Entity-centric Search and Visualization of News Archives</title>
    <summary>  We present a dynamic web tool that allows interactive search and
visualization of large news archives using an entity-centric approach. Users
are able to search entities using keyword phrases expressing news stories or
events and the system retrieves the most relevant entities to the user query
based on automatically extracted and indexed entity profiles. From the
computational journalism perspective, TimeMachine allows users to explore media
content through time using automatic identification of entity names, jobs,
quotations and relations between entities from co-occurrences networks
extracted from the news articles. TimeMachine demo is available at
http://maquinadotempo.sapo.pt/
</summary>
    <author>
      <name>Pedro Saleiro</name>
    </author>
    <author>
      <name>Jorge Teixeira</name>
    </author>
    <author>
      <name>Carlos Soares</name>
    </author>
    <author>
      <name>Eugénio Oliveira</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Advances in Information Retrieval: 38th European Conference on IR
  Research, ECIR 2016, Padua, Italy, March 20-23, 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.00855v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.00855v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.02300v1</id>
    <updated>2016-01-11T02:06:36Z</updated>
    <published>2016-01-11T02:06:36Z</published>
    <title>Temporal Multinomial Mixture for Instance-Oriented Evolutionary
  Clustering</title>
    <summary>  Evolutionary clustering aims at capturing the temporal evolution of clusters.
This issue is particularly important in the context of social media data that
are naturally temporally driven. In this paper, we propose a new probabilistic
model-based evolutionary clustering technique. The Temporal Multinomial Mixture
(TMM) is an extension of classical mixture model that optimizes feature
co-occurrences in the trade-off with temporal smoothness. Our model is
evaluated for two recent case studies on opinion aggregation over time. We
compare four different probabilistic clustering models and we show the
superiority of our proposal in the task of instance-oriented clustering.
</summary>
    <author>
      <name>Young-Min Kim</name>
    </author>
    <author>
      <name>Julien Velcin</name>
    </author>
    <author>
      <name>Stéphane Bonnevay</name>
    </author>
    <author>
      <name>Marian-Andrei Rizoiu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-16354-3_66</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-16354-3_66" rel="related"/>
    <link href="http://arxiv.org/abs/1601.02300v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.02300v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.02904v1</id>
    <updated>2016-01-12T15:16:17Z</updated>
    <published>2016-01-12T15:16:17Z</published>
    <title>Social Network Extraction: Superficial Method and Information Retrieval</title>
    <summary>  Social network has become one of the themes of government issues, mainly
dealing with the chaos. The use of web is steadily gaining ground in these
issues. However, most of the web documents are unstructured and lack of
semantic. In this paper we proposed an Information Retrieval driven method for
dealing with heterogeneity of features in the web. The proposed solution is to
compare some approaches have shown the capacity to extract social relation:
strength relations and relations based on online academic database.
</summary>
    <author>
      <name>Mahyuddin K. M. Nasution</name>
    </author>
    <author>
      <name>Shahrul Azman Mohd. Noah</name>
    </author>
    <author>
      <name>Saidah Saad</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 1 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceeding of International Conference on Informatics for
  Development (ICID'11), c2-110 - c2-115 (2011)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1601.02904v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.02904v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.04800v1</id>
    <updated>2016-01-19T04:48:42Z</updated>
    <published>2016-01-19T04:48:42Z</published>
    <title>Top-N Recommender System via Matrix Completion</title>
    <summary>  Top-N recommender systems have been investigated widely both in industry and
academia. However, the recommendation quality is far from satisfactory. In this
paper, we propose a simple yet promising algorithm. We fill the user-item
matrix based on a low-rank assumption and simultaneously keep the original
information. To do that, a nonconvex rank relaxation rather than the nuclear
norm is adopted to provide a better rank approximation and an efficient
optimization strategy is designed. A comprehensive set of experiments on real
datasets demonstrates that our method pushes the accuracy of Top-N
recommendation to a new level.
</summary>
    <author>
      <name>Zhao Kang</name>
    </author>
    <author>
      <name>Chong Peng</name>
    </author>
    <author>
      <name>Qiang Cheng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AAAI 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.04800v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.04800v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.01665v1</id>
    <updated>2016-02-04T13:19:31Z</updated>
    <published>2016-02-04T13:19:31Z</published>
    <title>Improved Query Topic Models via Pseudo-Relevant Pólya Document Models</title>
    <summary>  Query-expansion via pseudo-relevance feedback is a popular method of
overcoming the problem of vocabulary mismatch and of increasing average
retrieval effectiveness. In this paper, we develop a new method that estimates
a query topic model from a set of pseudo-relevant documents using a new
language modelling framework.
  We assume that documents are generated via a mixture of multivariate Polya
distributions, and we show that by identifying the topical terms in each
document, we can appropriately select terms that are likely to belong to the
query topic model. The results of experiments on several TREC collections show
that the new approach compares favourably to current state-of-the-art expansion
methods.
</summary>
    <author>
      <name>Ronan Cummins</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.01665v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.01665v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.02506v1</id>
    <updated>2016-02-08T09:40:43Z</updated>
    <published>2016-02-08T09:40:43Z</published>
    <title>Wikipedia Tools for Google Spreadsheets</title>
    <summary>  In this paper, we introduce the Wikipedia Tools for Google Spreadsheets.
Google Spreadsheets is part of a free, Web-based software office suite offered
by Google within its Google Docs service. It allows users to create and edit
spreadsheets online, while collaborating with other users in realtime.
Wikipedia is a free-access, free-content Internet encyclopedia, whose content
and data is available, among other means, through an API. With the Wikipedia
Tools for Google Spreadsheets, we have created a toolkit that facilitates
working with Wikipedia data from within a spreadsheet context. We make these
tools available as open-source on GitHub
[https://github.com/tomayac/wikipedia-tools-for-google-spreadsheets], released
under the permissive Apache 2.0 license.
</summary>
    <author>
      <name>Thomas Steiner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 Listings, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.02506v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.02506v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.02911v1</id>
    <updated>2016-02-09T09:22:40Z</updated>
    <published>2016-02-09T09:22:40Z</published>
    <title>Searching PubMed for articles relevant to clinical interpretation of
  rare human genetic variants</title>
    <summary>  Numerous challenges persist that delay clinical interpretation of human
genetic variants, to name a few: (1) un- structured PubMed articles are the
most abundant source of evidence, yet their variant annotations are difficult
to query uniformly, (2) variants can be reported many different ways, for
example as DNA sequence change or protein modification, (3) historical drift in
annotations over time between various genome reference assemblies and
transcript alignments, (4) no single laboratory has sufficient numbers of human
samples, necessitating precompetitive efforts to share evidence for clinical
interpretation.
</summary>
    <author>
      <name>Andrew J. McMurry</name>
    </author>
    <link href="http://arxiv.org/abs/1602.02911v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.02911v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.07799v1</id>
    <updated>2016-02-25T05:12:55Z</updated>
    <published>2016-02-25T05:12:55Z</published>
    <title>A Study on the usage of Data Structures in Information Retrieval</title>
    <summary>  This paper tries to throw light in the usage of data structures in the field
of information retrieval. Information retrieval is an area of study which is
gaining momentum as the need and urge for sharing and exploring information is
growing day by day. Data structures have been the area of research for a long
period in the arena of computer science. The need to have efficient data
structures has become even more important as the data grows in an exponential
nature.
</summary>
    <author>
      <name>V. R. Kanagavalli</name>
    </author>
    <author>
      <name>G. Maheeja</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">National Conference on Innovations in Communication and Computing
  Technologies Feb 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.07799v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.07799v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.07849v1</id>
    <updated>2016-03-25T08:49:39Z</updated>
    <published>2016-03-25T08:49:39Z</published>
    <title>A multinomial probabilistic model for movie genre predictions</title>
    <summary>  This paper proposes a movie genre-prediction based on multinomial probability
model. To the best of our knowledge, this problem has not been addressed yet in
the field of recommender system. The prediction of a movie genre has many
practical applications including complementing the items categories given by
experts and providing a surprise effect in the recommendations given to a user.
We employ mulitnomial event model to estimate a likelihood of a movie given
genre and the Bayes rule to evaluate the posterior probability of a genre given
a movie. Experiments with the MovieLens dataset validate our approach. We
achieved 70% prediction rate using only 15% of the whole set for training.
</summary>
    <author>
      <name>Eric Makita</name>
    </author>
    <author>
      <name>Artem Lenskiy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures, 8th International Conference on Machine Learning
  and Computing, Hong Kong</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.07849v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.07849v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.09522v1</id>
    <updated>2016-03-31T10:50:50Z</updated>
    <published>2016-03-31T10:50:50Z</published>
    <title>Image Retrieval with a Bayesian Model of Relevance Feedback</title>
    <summary>  A content-based image retrieval system based on multinomial relevance
feedback is proposed. The system relies on an interactive search paradigm where
at each round a user is presented with k images and selects the one closest to
their ideal target. Two approaches, one based on the Dirichlet distribution and
one based the Beta distribution, are used to model the problem motivating an
algorithm that trades exploration and exploitation in presenting the images in
each round. Experimental results show that the new approach compares favourably
with previous work.
</summary>
    <author>
      <name>Dorota Glowacka</name>
    </author>
    <author>
      <name>Yee Whye Teh</name>
    </author>
    <author>
      <name>John Shawe-Taylor</name>
    </author>
    <link href="http://arxiv.org/abs/1603.09522v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.09522v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.00223v1</id>
    <updated>2016-04-01T12:55:35Z</updated>
    <published>2016-04-01T12:55:35Z</published>
    <title>Lower-Cost epsilon-Private Information Retrieval</title>
    <summary>  Private Information Retrieval (PIR), despite being well studied, is
computationally costly and arduous to scale. We explore lower-cost relaxations
of information-theoretic PIR, based on dummy queries, sparse vectors, and
compositions with an anonymity system. We prove the security of each scheme
using a flexible differentially private definition for private queries that can
capture notions of imperfect privacy. We show that basic schemes are weak, but
some of them can be made arbitrarily safe by composing them with large
anonymity systems.
</summary>
    <author>
      <name>Raphael R. Toledo</name>
    </author>
    <author>
      <name>George Danezis</name>
    </author>
    <author>
      <name>Ian Goldberg</name>
    </author>
    <link href="http://arxiv.org/abs/1604.00223v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.00223v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.05462v1</id>
    <updated>2016-04-19T08:00:08Z</updated>
    <published>2016-04-19T08:00:08Z</published>
    <title>Ensemble Enabled Weighted PageRank</title>
    <summary>  This paper describes our solution for WSDM Cup 2016. Ranking the query
independent importance of scholarly articles is a critical and challenging
task, due to the heterogeneity and dynamism of entities involved. Our approach
is called Ensemble enabled Weighted PageRank (EWPR). To do this, we first
propose Time-Weighted PageRank that extends PageRank by introducing a time
decaying factor. We then develop an ensemble method to assemble the authorities
of the heterogeneous entities involved in scholarly articles. We finally
propose to use external data sources to further improve the ranking accuracy.
Our experimental study shows that our EWPR is a good choice for ranking
scholarly articles.
</summary>
    <author>
      <name>Dongsheng Luo</name>
    </author>
    <author>
      <name>Chen Gong</name>
    </author>
    <author>
      <name>Renjun Hu</name>
    </author>
    <author>
      <name>Liang Duan</name>
    </author>
    <author>
      <name>Shuai Ma</name>
    </author>
    <link href="http://arxiv.org/abs/1604.05462v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.05462v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07521v1</id>
    <updated>2016-04-26T05:08:13Z</updated>
    <published>2016-04-26T05:08:13Z</published>
    <title>Feedback-based Approach to Introduce Freshness in Recommendations</title>
    <summary>  Recommender systems usually face the problem of serving the same
recommendations across multiple sessions regardless of whether the user is
interested in them or not, thereby reducing their effectiveness. To add
freshness to the recommended products, we introduce a feedback loop where the
set of recommended products in the current session depend on the user's
interaction with the previously recommended sets. We also describe ways of
addressing freshness when there is little or even no direct user interaction.
We define a metric to quantify freshness by reducing the problem to measuring
temporal diversity.
</summary>
    <author>
      <name>Hari Krishna Malladi</name>
    </author>
    <author>
      <name>Saikiran Thunuguntla</name>
    </author>
    <link href="http://arxiv.org/abs/1604.07521v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07521v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.04624v1</id>
    <updated>2016-05-16T00:59:07Z</updated>
    <published>2016-05-16T00:59:07Z</published>
    <title>Learning to Rank Personalized Search Results in Professional Networks</title>
    <summary>  LinkedIn search is deeply personalized - for the same queries, different
searchers expect completely different results. This paper presents our approach
to achieving this by mining various data sources available in LinkedIn to infer
searchers' intents (such as hiring, job seeking, etc.), as well as extending
the concept of homophily to capture the searcher-result similarities on many
aspects. Then, learning-to-rank (LTR) is applied to combine these signals with
standard search features.
</summary>
    <author>
      <name>Viet Ha-Thuc</name>
    </author>
    <author>
      <name>Shakti Sinha</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SIGIR 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1605.04624v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.04624v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.05369v1</id>
    <updated>2016-05-17T21:10:07Z</updated>
    <published>2016-05-17T21:10:07Z</published>
    <title>Audio Features Affected by Music Expressiveness</title>
    <summary>  Within a Music Information Retrieval perspective, the goal of the study
presented here is to investigate the impact on sound features of the musician's
affective intention, namely when trying to intentionally convey emotional
contents via expressiveness. A preliminary experiment has been performed
involving $10$ tuba players. The recordings have been analysed by extracting a
variety of features, which have been subsequently evaluated by combining both
classic and machine learning statistical techniques. Results are reported and
discussed.
</summary>
    <author>
      <name>Alberto Introini</name>
    </author>
    <author>
      <name>Giorgio Presti</name>
    </author>
    <author>
      <name>Giuseppe Boccignone</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ACM SIGIR Conference on Research and Development in
  Information Retrieval (SIGIR 2016), Pisa, Italy, July 17-21, 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.05369v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.05369v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07891v2</id>
    <updated>2016-06-23T00:46:06Z</updated>
    <published>2016-05-25T14:09:00Z</published>
    <title>Query Expansion with Locally-Trained Word Embeddings</title>
    <summary>  Continuous space word embeddings have received a great deal of attention in
the natural language processing and machine learning communities for their
ability to model term similarity and other relationships. We study the use of
term relatedness in the context of query expansion for ad hoc information
retrieval. We demonstrate that word embeddings such as word2vec and GloVe, when
trained globally, underperform corpus and query specific embeddings for
retrieval tasks. These results suggest that other tasks benefiting from global
embeddings may also benefit from local embeddings.
</summary>
    <author>
      <name>Fernando Diaz</name>
    </author>
    <author>
      <name>Bhaskar Mitra</name>
    </author>
    <author>
      <name>Nick Craswell</name>
    </author>
    <link href="http://arxiv.org/abs/1605.07891v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07891v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03048v1</id>
    <updated>2016-06-07T06:38:32Z</updated>
    <published>2016-06-07T06:38:32Z</published>
    <title>A Minimum Spanning Tree Representation of Anime Similarities</title>
    <summary>  In this work, a new way to represent Japanese animation (anime) is presented.
We applied a minimum spanning tree to show the relation between anime. The
distance between anime is calculated through three similarity measurements,
namely crew, score histogram, and topic similarities. Finally the centralities
are also computed to reveal the most significance anime. The result shows that
the minimum spanning tree can be used to determine the similarity anime.
Furthermore, by using centralities calculation, we found some anime that are
significance to others.
</summary>
    <author>
      <name>Canggih Puspo Wibowo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.03048v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03048v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03783v1</id>
    <updated>2016-06-12T23:50:19Z</updated>
    <published>2016-06-12T23:50:19Z</published>
    <title>Retrieving and Ranking Similar Questions from Question-Answer Archives
  Using Topic Modelling and Topic Distribution Regression</title>
    <summary>  Presented herein is a novel model for similar question ranking within
collaborative question answer platforms. The presented approach integrates a
regression stage to relate topics derived from questions to those derived from
question-answer pairs. This helps to avoid problems caused by the differences
in vocabulary used within questions and answers, and the tendency for questions
to be shorter than answers. The performance of the model is shown to outperform
translation methods and topic modelling (without regression) on several
real-world datasets.
</summary>
    <author>
      <name>Pedro Chahuara</name>
    </author>
    <author>
      <name>Thomas Lampert</name>
    </author>
    <author>
      <name>Pierre Gancarski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Theory and Practice of Digital Libraries
  2016 (accepted)</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.03783v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03783v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.06991v1</id>
    <updated>2016-06-22T15:53:29Z</updated>
    <published>2016-06-22T15:53:29Z</published>
    <title>Toward Word Embedding for Personalized Information Retrieval</title>
    <summary>  This paper presents preliminary works on using Word Embedding (word2vec) for
query expansion in the context of Personalized Information Retrieval.
Traditionally, word embeddings are learned on a general corpus, like Wikipedia.
In this work we try to personalize the word embeddings learning, by achieving
the learning on the user's profile. The word embeddings are then in the same
context than the user interests. Our proposal is evaluated on the CLEF Social
Book Search 2016 collection. The results obtained show that some efforts should
be made in the way to apply Word Embedding in the context of Personalized
Information Retrieval.
</summary>
    <author>
      <name>Nawal Ould-Amer</name>
    </author>
    <author>
      <name>Philippe Mulhem</name>
    </author>
    <author>
      <name>Mathias Gery</name>
    </author>
    <link href="http://arxiv.org/abs/1606.06991v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.06991v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.00223v1</id>
    <updated>2016-07-01T12:45:43Z</updated>
    <published>2016-07-01T12:45:43Z</published>
    <title>Memory Based Collaborative Filtering with Lucene</title>
    <summary>  Memory Based Collaborative Filtering is a widely used approach to provide
recommendations. It exploits similarities between ratings across a population
of users by forming a weighted vote to predict unobserved ratings. Bespoke
solutions are frequently adopted to deal with the problem of high quality
recommendations on large data sets. A disadvantage of this approach, however,
is the loss of generality and flexibility of the general collaborative
filtering systems. In this paper, we have developed a methodology that allows
one to build a scalable and effective collaborative filtering system on top of
a conventional full-text search engine such as Apache Lucene.
</summary>
    <author>
      <name>Claudio Gennaro</name>
    </author>
    <link href="http://arxiv.org/abs/1607.00223v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.00223v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; H.3.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.02641v1</id>
    <updated>2016-07-09T18:10:06Z</updated>
    <published>2016-07-09T18:10:06Z</published>
    <title>Randomised Relevance Model</title>
    <summary>  Relevance Models are well-known retrieval models and capable of producing
competitive results. However, because they use query expansion they can be very
slow. We address this slowness by incorporating two variants of locality
sensitive hashing (LSH) into the query expansion process. Results on two
document collections suggest that we can obtain large reductions in the amount
of work, with a small reduction in effectiveness. Our approach is shown to be
additive when pruning query terms.
</summary>
    <author>
      <name>Dominik Wurzer</name>
    </author>
    <author>
      <name>Miles Osborne</name>
    </author>
    <author>
      <name>Victor Lavrenko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Information Retrieval, Query Expansion, Locality Sensitive Hashing,
  Randomized Algorithm, Relevance Model</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.02641v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.02641v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.02754v1</id>
    <updated>2016-07-10T15:32:03Z</updated>
    <published>2016-07-10T15:32:03Z</published>
    <title>Hybrid Recommender System Based on Personal Behavior Mining</title>
    <summary>  Recommender systems are mostly well known for their applications in
e-commerce sites and are mostly static models. Classical personalized
recommender algorithm includes item-based collaborative filtering method
applied in Amazon, matrix factorization based collaborative filtering algorithm
from Netflix, etc. In this article, we hope to combine traditional model with
behavior pattern extraction method. We use desensitized mobile transaction
record provided by T-mall, Alibaba to build a hybrid dynamic recommender
system. The sequential pattern mining aims to find frequent sequential pattern
in sequence database and is applied in this hybrid model to predict customers'
payment behavior thus contributing to the accuracy of the model.
</summary>
    <author>
      <name>Zhiyuan Fang</name>
    </author>
    <author>
      <name>Lingqi Zhang</name>
    </author>
    <author>
      <name>Kun Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.02754v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.02754v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.03296v1</id>
    <updated>2016-07-12T10:15:15Z</updated>
    <published>2016-07-12T10:15:15Z</published>
    <title>Implicit Negative Feedback in Clinical Information Retrieval</title>
    <summary>  In this paper, we reflect on ways to improve the quality of bio-medical
information retrieval by drawing implicit negative feedback from negated
information in noisy natural language search queries. We begin by studying the
extent to which negations occur in clinical texts and quantify their
detrimental effect on retrieval performance. Subsequently, we present a number
of query reformulation and ranking approaches that remedy these shortcomings by
resolving natural language negations. Our experimental results are based on
data collected in the course of the TREC Clinical Decision Support Track and
show consistent improvements compared to state-of-the-art methods. Using our
novel algorithms, we are able to reduce the negative impact of negations on
early precision by up to 65%.
</summary>
    <author>
      <name>Lorenz Kuhn</name>
    </author>
    <author>
      <name>Carsten Eickhoff</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2911451.2917761</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2911451.2917761" rel="related"/>
    <link href="http://arxiv.org/abs/1607.03296v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.03296v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.07326v1</id>
    <updated>2016-07-25T15:54:07Z</updated>
    <published>2016-07-25T15:54:07Z</published>
    <title>Meta-Prod2Vec - Product Embeddings Using Side-Information for
  Recommendation</title>
    <summary>  We propose Meta-Prod2vec, a novel method to compute item similarities for
recommendation that leverages existing item metadata. Such scenarios are
frequently encountered in applications such as content recommendation, ad
targeting and web search. Our method leverages past user interactions with
items and their attributes to compute low-dimensional embeddings of items.
Specifically, the item metadata is in- jected into the model as side
information to regularize the item embeddings. We show that the new item
representa- tions lead to better performance on recommendation tasks on an open
music dataset.
</summary>
    <author>
      <name>Flavian Vasile</name>
    </author>
    <author>
      <name>Elena Smirnova</name>
    </author>
    <author>
      <name>Alexis Conneau</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2959100.2959160</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2959100.2959160" rel="related"/>
    <link href="http://arxiv.org/abs/1607.07326v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.07326v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.00134v2</id>
    <updated>2017-02-22T15:22:14Z</updated>
    <published>2016-07-30T15:42:34Z</published>
    <title>A Graph Framework for Multimodal Medical Information Processing</title>
    <summary>  Multimodal medical information processing is currently the epicenter of
intense interdisciplinary research, as proper data fusion may lead to more
accurate diagnoses. Moreover, multimodality may disambiguate cases of
co-morbidity. This paper presents a framework for retrieving, analyzing, and
storing medical information as a multilayer graph, an abstract format suitable
for data fusion and further processing. At the same time, this paper addresses
the need for reliable medical information through co-author graph ranking. A
use case pertaining to frailty based on Python and Neo4j serves as an
illustration of the proposed framework.
</summary>
    <author>
      <name>Georgios Drakopoulos</name>
    </author>
    <author>
      <name>Vasileios Megalooikonomou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">We need to correct certain errors both in the software description as
  well as in the algorithms</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.00134v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.00134v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.00147v1</id>
    <updated>2016-07-30T16:53:05Z</updated>
    <published>2016-07-30T16:53:05Z</published>
    <title>Attention Span For Personalisation</title>
    <summary>  A click on an item is arguably the most widely used feature in recommender
systems. However, a click is one out of 174 events a browser can trigger. This
paper presents a framework to effectively collect and store data from event
streams. A set of mining methods is provided to extract user engagement
features such as: attention span, scrolling depth and visible impressions. In
this work, we present an experiment where recommendations based on attention
span drove 340% higher click-through-rate than clicks.
</summary>
    <author>
      <name>Joan Figuerola Hurtado</name>
    </author>
    <link href="http://arxiv.org/abs/1608.00147v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.00147v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.01068v1</id>
    <updated>2016-08-03T03:49:54Z</updated>
    <published>2016-08-03T03:49:54Z</published>
    <title>Ranking Entity Based on Both of Word Frequency and Word Sematic Features</title>
    <summary>  Entity search is a new application meeting either precise or vague
requirements from the search engines users. Baidu Cup 2016 Challenge just
provided such a chance to tackle the problem of the entity search. We achieved
the first place with the average MAP scores on 4 tasks including movie, tvShow,
celebrity and restaurant. In this paper, we propose a series of similarity
features based on both of the word frequency features and the word semantic
features and describe our ranking architecture and experiment details.
</summary>
    <author>
      <name>Xiao-Bo Jin</name>
    </author>
    <author>
      <name>Guang-Gang Geng</name>
    </author>
    <author>
      <name>Kaizhu Huang</name>
    </author>
    <author>
      <name>Zhi-Wei Yan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The paper decribes the apporoaches that help us to achieve the first
  place in Baidu Cup 2016 NLP Challenge</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.01068v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.01068v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.01247v2</id>
    <updated>2016-11-05T04:59:48Z</updated>
    <published>2016-08-03T16:33:32Z</published>
    <title>Query Clustering using Segment Specific Context Embeddings</title>
    <summary>  This paper presents a novel query clustering approach to capture the broad
interest areas of users querying search engines. We make use of recent advances
in NLP - word2vec and extend it to get query2vec, vector representations of
queries, based on query contexts, obtained from the top search results for the
query and use a highly scalable Divide &amp; Merge clustering algorithm on top of
the query vectors, to get the clusters. We have tried this approach on a
variety of segments, including Retail, Travel, Health, Phones and found the
clusters to be effective in discovering user's interest areas which have high
monetization potential.
</summary>
    <author>
      <name>S. K Kolluru</name>
    </author>
    <author>
      <name>Prasenjit Mukherjee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.01247v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.01247v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.01573v1</id>
    <updated>2016-08-04T15:14:04Z</updated>
    <published>2016-08-04T15:14:04Z</published>
    <title>Local Term Weight Models from Power Transformations: Development of
  BM25IR: A Best Match Model based on Inverse Regression</title>
    <summary>  In this article we show how power transformations can be used as a common
framework for the derivation of local term weights. We found that under some
parametric conditions, BM25 and inverse regression produce equivalent results.
As a special case of inverse regression, we show that the largest increment in
term weight occurs when a term is mentioned for the second time. A model based
on inverse regression (BM25IR) is presented. Simulations suggest that BM25IR
works fairly well for different BM25 parametric conditions and document
lengths.
</summary>
    <author>
      <name>Edel Garcia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 2 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.01573v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.01573v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.02904v3</id>
    <updated>2016-10-01T19:24:23Z</updated>
    <published>2016-08-09T18:29:04Z</published>
    <title>TweeTime: A Minimally Supervised Method for Recognizing and Normalizing
  Time Expressions in Twitter</title>
    <summary>  We describe TweeTIME, a temporal tagger for recognizing and normalizing time
expressions in Twitter. Most previous work in social media analysis has to rely
on temporal resolvers that are designed for well-edited text, and therefore
suffer from the reduced performance due to domain mismatch. We present a
minimally supervised method that learns from large quantities of unlabeled data
and requires no hand-engineered rules or hand-annotated training corpora.
TweeTIME achieves 0.68 F1 score on the end-to-end task of resolving date
expressions, outperforming a broad range of state-of-the-art systems.
</summary>
    <author>
      <name>Jeniya Tabassum</name>
    </author>
    <author>
      <name>Alan Ritter</name>
    </author>
    <author>
      <name>Wei Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.02904v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.02904v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.03905v1</id>
    <updated>2016-08-12T20:33:54Z</updated>
    <published>2016-08-12T20:33:54Z</published>
    <title>Using Centroids of Word Embeddings and Word Mover's Distance for
  Biomedical Document Retrieval in Question Answering</title>
    <summary>  We propose a document retrieval method for question answering that represents
documents and questions as weighted centroids of word embeddings and reranks
the retrieved documents with a relaxation of Word Mover's Distance. Using
biomedical questions and documents from BIOASQ, we show that our method is
competitive with PUBMED. With a top-k approximation, our method is fast, and
easily portable to other domains and languages.
</summary>
    <author>
      <name>Georgios-Ioannis Brokos</name>
    </author>
    <author>
      <name>Prodromos Malakasiotis</name>
    </author>
    <author>
      <name>Ion Androutsopoulos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 images, presented at BioNLP 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.03905v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.03905v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.06656v1</id>
    <updated>2016-08-23T21:07:50Z</updated>
    <published>2016-08-23T21:07:50Z</published>
    <title>Lexical Query Modeling in Session Search</title>
    <summary>  Lexical query modeling has been the leading paradigm for session search. In
this paper, we analyze TREC session query logs and compare the performance of
different lexical matching approaches for session search. Naive methods based
on term frequency weighing perform on par with specialized session models. In
addition, we investigate the viability of lexical query models in the setting
of session search. We give important insights into the potential and
limitations of lexical query modeling for session search and propose future
directions for the field of session search.
</summary>
    <author>
      <name>Christophe Van Gysel</name>
    </author>
    <author>
      <name>Evangelos Kanoulas</name>
    </author>
    <author>
      <name>Maarten de Rijke</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2970398.2970422</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2970398.2970422" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICTIR2016, Proceedings of the 2nd ACM International Conference on the
  Theory of Information Retrieval. 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.06656v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.06656v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.07400v2</id>
    <updated>2017-01-03T07:41:44Z</updated>
    <published>2016-08-26T09:20:21Z</published>
    <title>Collaborative Filtering with Recurrent Neural Networks</title>
    <summary>  We show that collaborative filtering can be viewed as a sequence prediction
problem, and that given this interpretation, recurrent neural networks offer
very competitive approach. In particular we study how the long short-term
memory (LSTM) can be applied to collaborative filtering, and how it compares to
standard nearest neighbors and matrix factorization methods on movie
recommendation. We show that the LSTM is competitive in all aspects, and
largely outperforms other methods in terms of item coverage and short term
predictions.
</summary>
    <author>
      <name>Robin Devooght</name>
    </author>
    <author>
      <name>Hugues Bersini</name>
    </author>
    <link href="http://arxiv.org/abs/1608.07400v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.07400v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.00683v1</id>
    <updated>2016-09-02T17:50:53Z</updated>
    <published>2016-09-02T17:50:53Z</published>
    <title>Pairwise, Magnitude, or Stars: What's the Best Way for Crowds to Rate?</title>
    <summary>  We compare three popular techniques of rating content: the ubiquitous five
star rating, the less used pairwise comparison, and the recently introduced (in
crowdsourcing) magnitude estimation approach. Each system has specific
advantages and disadvantages, in terms of required user effort, achievable user
preference prediction accuracy and number of ratings required.
  We design an experiment where the three techniques are compared in an
unbiased way. We collected 39'000 ratings on a popular crowdsourcing platform,
allowing us to release a dataset that will be useful for many related studies
on user rating techniques.
</summary>
    <author>
      <name>Alessandro Checco</name>
    </author>
    <author>
      <name>Gianluca Demartini</name>
    </author>
    <link href="http://arxiv.org/abs/1609.00683v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.00683v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.00689v1</id>
    <updated>2016-09-02T18:23:22Z</updated>
    <published>2016-09-02T18:23:22Z</published>
    <title>Ensemble Learned Vaccination Uptake Prediction using Web Search Queries</title>
    <summary>  We present a method that uses ensemble learning to combine clinical and
web-mined time-series data in order to predict future vaccination uptake. The
clinical data is official vaccination registries, and the web data is query
frequencies collected from Google Trends. Experiments with official vaccine
records show that our method predicts vaccination uptake effectively (4.7 Root
Mean Squared Error). Whereas performance is best when combining clinical and
web data, using solely web data yields comparative performance. To our
knowledge, this is the first study to predict vaccination uptake using web data
(with and without clinical data).
</summary>
    <author>
      <name>Niels Dalum Hansen</name>
    </author>
    <author>
      <name>Christina Lioma</name>
    </author>
    <author>
      <name>Kåre Mølbak</name>
    </author>
    <link href="http://arxiv.org/abs/1609.00689v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.00689v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.02171v1</id>
    <updated>2016-09-04T12:07:57Z</updated>
    <published>2016-09-04T12:07:57Z</published>
    <title>The Effect of Class Imbalance and Order on Crowdsourced Relevance
  Judgments</title>
    <summary>  In this paper we study the effect on crowd worker efficiency and
effectiveness of the dominance of one class in the data they process. We aim at
understanding if there is any positive or negative bias in workers seeing many
negative examples in the identification of positive labels. To test our
hypothesis, we design an experiment where crowd workers are asked to judge the
relevance of documents presented in different orders. Our findings indicate
that there is a significant improvement in the quality of relevance judgements
when presenting relevant results before the non-relevant ones.
</summary>
    <author>
      <name>Rehab K. Qarout</name>
    </author>
    <author>
      <name>Alessandro Checco</name>
    </author>
    <author>
      <name>Gianluca Demartini</name>
    </author>
    <link href="http://arxiv.org/abs/1609.02171v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.02171v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.04533v1</id>
    <updated>2016-02-17T19:33:47Z</updated>
    <published>2016-02-17T19:33:47Z</published>
    <title>A Comprehensive Comparative Study of Word and Sentence Similarity
  Measures</title>
    <summary>  Sentence similarity is considered the basis of many natural language tasks
such as information retrieval, question answering and text summarization. The
semantic meaning between compared text fragments is based on the words semantic
features and their relationships. This article reviews a set of word and
sentence similarity measures and compares them on benchmark datasets. On the
studied datasets, results showed that hybrid semantic measures perform better
than both knowledge and corpus based measures.
</summary>
    <author>
      <name>Issa Atoum</name>
    </author>
    <author>
      <name>Ahmed Otoom</name>
    </author>
    <author>
      <name>Narayanan Kulathuramaiyer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5120/ijca2016908259</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5120/ijca2016908259" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages,4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Applications,2016,135(1),
  Foundation of Computer Science (FCS), NY, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1610.04533v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.04533v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.00558v1</id>
    <updated>2016-11-02T11:52:49Z</updated>
    <published>2016-11-02T11:52:49Z</published>
    <title>Online bagging for recommendation with incremental matrix factorization</title>
    <summary>  Online recommender systems often deal with continuous, potentially fast and
unbounded flows of data. Ensemble methods for recommender systems have been
used in the past in batch algorithms, however they have never been studied with
incremental algorithms, that are capable of processing those data streams on
the fly. We propose online bagging, using an incremental matrix factorization
algorithm for positive-only data streams. Using prequential evaluation, we show
that bagging is able to improve accuracy more than 35% over the baseline with
small computational overhead.
</summary>
    <author>
      <name>João Vinagre</name>
    </author>
    <author>
      <name>Alípio Mário Jorge</name>
    </author>
    <author>
      <name>João Gama</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at STREAMEVOLV 2016, held in conjunction with ECML/PKDD
  2016, Riva del Garda, Italy, September 23rd, 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.00558v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.00558v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.02815v1</id>
    <updated>2016-11-08T15:25:02Z</updated>
    <published>2016-11-08T15:25:02Z</published>
    <title>An Automated System for Essay Scoring of Online Exams in Arabic based on
  Stemming Techniques and Levenshtein Edit Operations</title>
    <summary>  In this article, an automated system is proposed for essay scoring in Arabic
language for online exams based on stemming techniques and Levenshtein edit
operations. An online exam has been developed on the proposed mechanisms,
exploiting the capabilities of light and heavy stemming. The implemented online
grading system has shown to be an efficient tool for automated scoring of essay
questions.
</summary>
    <author>
      <name>Emad Fawzi Al-Shalabi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSI International Journal of Computer Science Issues, Volume 13,
  Issue 5, September 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1611.02815v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.02815v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.09028v1</id>
    <updated>2016-11-28T08:56:04Z</updated>
    <published>2016-11-28T08:56:04Z</published>
    <title>Analyzing Features for the Detection of Happy Endings in German Novels</title>
    <summary>  With regard to a computational representation of literary plot, this paper
looks at the use of sentiment analysis for happy ending detection in German
novels. Its focus lies on the investigation of previously proposed sentiment
features in order to gain insight about the relevance of specific features on
the one hand and the implications of their performance on the other hand.
Therefore, we study various partitionings of novels, considering the highly
variable concept of "ending". We also show that our approach, even though still
rather simple, can potentially lead to substantial findings relevant to
literary studies.
</summary>
    <author>
      <name>Fotis Jannidis</name>
    </author>
    <author>
      <name>Isabella Reger</name>
    </author>
    <author>
      <name>Albin Zehe</name>
    </author>
    <author>
      <name>Martin Becker</name>
    </author>
    <author>
      <name>Lena Hettinger</name>
    </author>
    <author>
      <name>Andreas Hotho</name>
    </author>
    <link href="http://arxiv.org/abs/1611.09028v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.09028v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.03597v1</id>
    <updated>2016-12-12T10:27:31Z</updated>
    <published>2016-12-12T10:27:31Z</published>
    <title>Search Personalization with Embeddings</title>
    <summary>  Recent research has shown that the performance of search personalization
depends on the richness of user profiles which normally represent the user's
topical interests. In this paper, we propose a new embedding approach to
learning user profiles, where users are embedded on a topical interest space.
We then directly utilize the user profiles for search personalization.
Experiments on query logs from a major commercial web search engine demonstrate
that our embedding approach improves the performance of the search engine and
also achieves better search performance than other strong baselines.
</summary>
    <author>
      <name>Thanh Vu</name>
    </author>
    <author>
      <name>Dat Quoc Nguyen</name>
    </author>
    <author>
      <name>Mark Johnson</name>
    </author>
    <author>
      <name>Dawei Song</name>
    </author>
    <author>
      <name>Alistair Willis</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-56608-5_54</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-56608-5_54" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the 39th European Conference on Information
  Retrieval, ECIR 2017, to appear</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.03597v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.03597v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.07117v1</id>
    <updated>2016-12-20T15:02:41Z</updated>
    <published>2016-12-20T15:02:41Z</published>
    <title>Classification and Learning-to-rank Approaches for Cross-Device Matching
  at CIKM Cup 2016</title>
    <summary>  In this paper, we propose two methods for tackling the problem of
cross-device matching for online advertising at CIKM Cup 2016. The first method
considers the matching problem as a binary classification task and solve it by
utilizing ensemble learning techniques. The second method defines the matching
problem as a ranking task and effectively solve it with using learning-to-rank
algorithms. The results show that the proposed methods obtain promising
results, in which the ranking-based method outperforms the classification-based
method for the task.
</summary>
    <author>
      <name>Nam Khanh Tran</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CIKM Cup 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.07117v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.07117v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.08391v1</id>
    <updated>2016-12-26T14:35:51Z</updated>
    <published>2016-12-26T14:35:51Z</published>
    <title>Audio-based Distributional Semantic Models for Music Auto-tagging and
  Similarity Measurement</title>
    <summary>  The recent development of Audio-based Distributional Semantic Models (ADSMs)
enables the computation of audio and lexical vector representations in a joint
acoustic-semantic space. In this work, these joint representations are applied
to the problem of automatic tag generation. The predicted tags together with
their corresponding acoustic representation are exploited for the construction
of acoustic-semantic clip embeddings. The proposed algorithms are evaluated on
the task of similarity measurement between music clips. Acoustic-semantic
models are shown to outperform the state-of-the-art for this task and produce
high quality tags for audio/music clips.
</summary>
    <author>
      <name>Giannis Karamanolakis</name>
    </author>
    <author>
      <name>Elias Iosif</name>
    </author>
    <author>
      <name>Athanasia Zlatintsi</name>
    </author>
    <author>
      <name>Aggelos Pikrakis</name>
    </author>
    <author>
      <name>Alexandros Potamianos</name>
    </author>
    <link href="http://arxiv.org/abs/1612.08391v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.08391v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.09062v1</id>
    <updated>2016-12-29T07:56:09Z</updated>
    <published>2016-12-29T07:56:09Z</published>
    <title>Condensedly: comprehending article contents through condensed texts</title>
    <summary>  Summary: Abstracts in biomedical articles can provide a quick overview of the
articles but detailed information cannot be obtained without reading full-text
contents. Full-text articles certainly generate more information and contents;
however, accessing full-text documents is usually time consuming. Condensedly
is a web-based application, which provides readers an easy and efficient way to
access full-text paragraphs using sentences in abstracts as fishing bait to
retrieve the big fish reside in full-text. Condensedly is based on the
paragraph ranking algorithm, which evaluates and ranks full-text paragraphs
based on their association scores with sentences in abstracts.
  Availability: http://140.116.247.185/~research/Condensedly
</summary>
    <author>
      <name>Chao-Hsuan Ke</name>
    </author>
    <author>
      <name>Tsung-Lu Michael Lee</name>
    </author>
    <author>
      <name>Jung-Hsien Chiang</name>
    </author>
    <link href="http://arxiv.org/abs/1612.09062v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.09062v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.00749v1</id>
    <updated>2017-01-03T17:17:34Z</updated>
    <published>2017-01-03T17:17:34Z</published>
    <title>Pyndri: a Python Interface to the Indri Search Engine</title>
    <summary>  We introduce pyndri, a Python interface to the Indri search engine. Pyndri
allows to access Indri indexes from Python at two levels: (1) dictionary and
tokenized document collection, (2) evaluating queries on the index. We hope
that with the release of pyndri, we will stimulate reproducible, open and
fast-paced IR research.
</summary>
    <author>
      <name>Christophe Van Gysel</name>
    </author>
    <author>
      <name>Evangelos Kanoulas</name>
    </author>
    <author>
      <name>Maarten de Rijke</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ECIR2017. Proceedings of the 39th European Conference on Information
  Retrieval. 2017. The final publication will be available at Springer</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.00749v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.00749v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.01250v1</id>
    <updated>2017-01-05T08:53:02Z</updated>
    <published>2017-01-05T08:53:02Z</published>
    <title>A Probabilistic View of Neighborhood-based Recommendation Methods</title>
    <summary>  Probabilistic graphic model is an elegant framework to compactly present
complex real-world observations by modeling uncertainty and logical flow
(conditionally independent factors). In this paper, we present a probabilistic
framework of neighborhood-based recommendation methods (PNBM) in which
similarity is regarded as an unobserved factor. Thus, PNBM leads the estimation
of user preference to maximizing a posterior over similarity. We further
introduce a novel multi-layer similarity descriptor which models and learns the
joint influence of various features under PNBM, and name the new framework
MPNBM. Empirical results on real-world datasets show that MPNBM allows very
accurate estimation of user preferences.
</summary>
    <author>
      <name>Jun Wang</name>
    </author>
    <author>
      <name>Qiang Tang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted by: ICDM 2016 - IEEE International Conference on Data Mining
  series (ICDM) workshop CLOUDMINE, 7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.01250v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.01250v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.01417v1</id>
    <updated>2017-01-05T18:36:26Z</updated>
    <published>2017-01-05T18:36:26Z</published>
    <title>Exploration of Proximity Heuristics in Length Normalization</title>
    <summary>  Ranking functions used in information retrieval are primarily used in the
search engines and they are often adopted for various language processing
applications. However, features used in the construction of ranking functions
should be analyzed before applying it on a data set. This paper gives
guidelines on construction of generalized ranking functions with
application-dependent features. The paper prescribes a specific case of a
generalized function for recommendation system using feature engineering
guidelines on the given data set. The behavior of both generalized and specific
functions are studied and implemented on the unstructured textual data. The
proximity feature based ranking function has outperformed by 52% from regular
BM25.
</summary>
    <author>
      <name>Pranav Agrawal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.01417v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.01417v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.01737v1</id>
    <updated>2017-01-06T19:04:34Z</updated>
    <published>2017-01-06T19:04:34Z</published>
    <title>Spotting Information biases in Chinese and Western Media</title>
    <summary>  Newswire and Social Media are the major sources of information in our time.
While the topical demographic of Western Media was subjects of studies in the
past, less is known about Chinese Media. In this paper, we apply event
detection and tracking technology to examine the information overlap and
differences between Chinese and Western - Traditional Media and Social Media.
Our experiments reveal a biased interest of China towards the West, which
becomes particularly apparent when comparing the interest in celebrities.
</summary>
    <author>
      <name>Dominik Wurzer</name>
    </author>
    <author>
      <name>Yumeng Qin</name>
    </author>
    <link href="http://arxiv.org/abs/1701.01737v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.01737v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.02617v1</id>
    <updated>2017-01-10T14:46:17Z</updated>
    <published>2017-01-10T14:46:17Z</published>
    <title>On Low Overlap Among Search Results of Academic Search Engines</title>
    <summary>  Number of published scholarly articles is growing exponentially. To tackle
this information overload, researchers are increasingly depending on niche
academic search engines. Recent works have shown that two major general web
search engines: Google and Bing, have high level of agreement in their top
search results. In contrast, we show that various academic search engines have
low degree of agreement among themselves. We performed experiments using 2500
queries over four academic search engines. We observe that overlap in search
result sets of any pair of academic search engines is significantly low and in
most of the cases the search result sets are mutually exclusive. We also
discuss implications of this low overlap.
</summary>
    <author>
      <name>Anasua Mitra</name>
    </author>
    <author>
      <name>Amit Awekar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, submitted to ACM WWW Conference 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.02617v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.02617v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.01713v1</id>
    <updated>2017-02-06T17:19:07Z</updated>
    <published>2017-02-06T17:19:07Z</published>
    <title>A dynamic multi-level collaborative filtering method for improved
  recommendations</title>
    <summary>  One of the most used approaches for providing recommendations in various
online environments such as e-commerce is collaborative filtering. Although,
this is a simple method for recommending items or services, accuracy and
quality problems still exist. Thus, we propose a dynamic multi-level
collaborative filtering method that improves the quality of the
recommendations. The proposed method is based on positive and negative
adjustments and can be used in different domains that utilize collaborative
filtering to increase the quality of the user experience. Furthermore, the
effectiveness of the proposed method is shown by providing an extensive
experimental evaluation based on three real datasets and by comparisons to
alternative methods.
</summary>
    <author>
      <name>Nikolaos Polatidis</name>
    </author>
    <author>
      <name>Christos K. Georgiadis</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.csi.2016.10.014</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.csi.2016.10.014" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computer Standards &amp; Interfaces, 51, 14-21 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1702.01713v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.01713v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.02107v1</id>
    <updated>2017-01-18T15:23:41Z</updated>
    <published>2017-01-18T15:23:41Z</published>
    <title>First Study on Data Readiness Level</title>
    <summary>  We introduce the idea of Data Readiness Level (DRL) to measure the relative
richness of data to answer specific questions often encountered by data
scientists. We first approach the problem in its full generality explaining its
desired mathematical properties and applications and then we propose and study
two DRL metrics. Specifically, we define DRL as a function of at least four
properties of data: Noisiness, Believability, Relevance, and Coherence. The
information-theoretic based metrics, Cosine Similarity and Document Disparity,
are proposed as indicators of Relevance and Coherence for a piece of data. The
proposed metrics are validated through a text-based experiment using Twitter
data.
</summary>
    <author>
      <name>Hui Guan</name>
    </author>
    <author>
      <name>Thanos Gentimis</name>
    </author>
    <author>
      <name>Hamid Krim</name>
    </author>
    <author>
      <name>James Keiser</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.02107v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.02107v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.04815v1</id>
    <updated>2017-02-15T23:31:44Z</updated>
    <published>2017-02-15T23:31:44Z</published>
    <title>Multimodal Content Representation and Similarity Ranking of Movies</title>
    <summary>  In this paper we examine the existence of correlation between movie
similarity and low level features from respective movie content. In particular,
we demonstrate the extraction of multi-modal representation models of movies
based on subtitles, audio and metadata mining. We emphasize our research in
topic modeling of movies based on their subtitles. In order to demonstrate the
proposed content representation approach, we have built a small dataset of 160
widely known movies. We assert movie similarities, as propagated by the
singular modalities and fusion models, in the form of recommendation rankings.
We showcase a novel topic model browser for movies that allows for exploration
of the different aspects of similarities between movies and an information
retrieval system for movie similarity based on multi-modal content.
</summary>
    <author>
      <name>Konstantinos Bougiatiotis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preliminary work</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.04815v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.04815v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.06247v2</id>
    <updated>2017-06-13T02:20:47Z</updated>
    <published>2017-02-21T03:09:10Z</published>
    <title>SAR: Semantic Analysis for Recommendation</title>
    <summary>  Recommendation system is a common demand in daily life and matrix completion
is a widely adopted technique for this task. However, most matrix completion
methods lack semantic interpretation and usually result in weak-semantic
recommendations. To this end, this paper proposes a $S$emantic $A$nalysis
approach for $R$ecommendation systems $(SAR)$, which applies a two-level
hierarchical generative process that assigns semantic properties and categories
for user and item. $SAR$ learns semantic representations of users/items merely
from user ratings on items, which offers a new path to recommendation by
semantic matching with the learned representations. Extensive experiments
demonstrate $SAR$ outperforms other state-of-the-art baselines substantially.
</summary>
    <author>
      <name>Han Xiao</name>
    </author>
    <author>
      <name>Minlie Huang</name>
    </author>
    <author>
      <name>Xiaoyan Zhu</name>
    </author>
    <link href="http://arxiv.org/abs/1702.06247v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.06247v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.06510v1</id>
    <updated>2017-02-21T18:24:52Z</updated>
    <published>2017-02-21T18:24:52Z</published>
    <title>Algorithmes de classification et d'optimisation: participation du
  LIA/ADOC á DEFT'14</title>
    <summary>  This year, the DEFT campaign (D\'efi Fouilles de Textes) incorporates a task
which aims at identifying the session in which articles of previous TALN
conferences were presented. We describe the three statistical systems developed
at LIA/ADOC for this task. A fusion of these systems enables us to obtain
interesting results (micro-precision score of 0.76 measured on the test corpus)
</summary>
    <author>
      <name>Luis Adrián Cabrera-Diego</name>
    </author>
    <author>
      <name>Stéphane Huet</name>
    </author>
    <author>
      <name>Bassam Jabaian</name>
    </author>
    <author>
      <name>Alejandro Molina</name>
    </author>
    <author>
      <name>Juan-Manuel Torres-Moreno</name>
    </author>
    <author>
      <name>Marc El-Bèze</name>
    </author>
    <author>
      <name>Barthélémy Durette</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 tables, Conference paper (in French)</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.06510v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.06510v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.08070v1</id>
    <updated>2017-02-26T19:09:59Z</updated>
    <published>2017-02-26T19:09:59Z</published>
    <title>PubTree: A Hierarchical Search Tool for the MEDLINE Database</title>
    <summary>  Keeping track of the ever-increasing body of scientific literature is an
escalating challenge. We present PubTree a hierarchical search tool that
efficiently searches the PubMed/MEDLINE dataset based upon a decision tree
constructed using &gt;26 million abstracts. The tool is implemented as a webpage,
where users are asked a series of eighteen questions to locate pertinent
articles. The implementation of this hierarchical search tool highlights issues
endemic with document retrieval. However, the construction of this tree
indicates that with future developments hierarchical search could become an
effective tool (or adjunct) in the mining of biological literature.
</summary>
    <author>
      <name>William Rowe</name>
    </author>
    <author>
      <name>Paul D. Dobson</name>
    </author>
    <author>
      <name>Bede Constantinides</name>
    </author>
    <author>
      <name>Mark Platt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.08070v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.08070v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.02915v1</id>
    <updated>2017-03-06T21:05:42Z</updated>
    <published>2017-03-06T21:05:42Z</published>
    <title>Kaggle Competition: Expedia Hotel Recommendations</title>
    <summary>  With hundreds, even thousands, of hotels to choose from at every destination,
it's difficult to know which will suit your personal preferences. Expedia wants
to take the proverbial rabbit hole out of hotel search by providing
personalized hotel recommendations to their users. This is no small task for a
site with hundreds of millions of visitors every month! Currently, Expedia uses
search parameters to adjust their hotel recommendations, but there aren't
enough customer specific data to personalize them for each user. In this
project, we have taken up the challenge to contextualize customer data and
predict the likelihood a user will stay at 100 different hotel groups.
</summary>
    <author>
      <name>Gourav G. Shenoy</name>
    </author>
    <author>
      <name>Mangirish A. Wagle</name>
    </author>
    <author>
      <name>Anwar Shaikh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 8 tables, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.02915v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.02915v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.03923v1</id>
    <updated>2017-03-11T07:35:28Z</updated>
    <published>2017-03-11T07:35:28Z</published>
    <title>A German Corpus for Text Similarity Detection Tasks</title>
    <summary>  Text similarity detection aims at measuring the degree of similarity between
a pair of texts. Corpora available for text similarity detection are designed
to evaluate the algorithms to assess the paraphrase level among documents. In
this paper we present a textual German corpus for similarity detection. The
purpose of this corpus is to automatically assess the similarity between a pair
of texts and to evaluate different similarity measures, both for whole
documents or for individual sentences. Therefore we have calculated several
simple measures on our corpus based on a library of similarity functions.
</summary>
    <author>
      <name>Juan-Manuel Torres-Moreno</name>
    </author>
    <author>
      <name>Gerardo Sierra</name>
    </author>
    <author>
      <name>Peter Peinl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">1 figure; 13 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint of International Journal of Computational Linguistics and
  Applications, vol. 5, no. 2, 2014, pp. 9-24</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1703.03923v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.03923v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.04336v1</id>
    <updated>2017-03-13T11:19:56Z</updated>
    <published>2017-03-13T11:19:56Z</published>
    <title>A Visual Representation of Wittgenstein's Tractatus Logico-Philosophicus</title>
    <summary>  In this paper we present a data visualization method together with its
potential usefulness in digital humanities and philosophy of language. We
compile a multilingual parallel corpus from different versions of
Wittgenstein's Tractatus Logico-Philosophicus, including the original in German
and translations into English, Spanish, French, and Russian. Using this corpus,
we compute a similarity measure between propositions and render a visual
network of relations for different languages.
</summary>
    <author>
      <name>Anca Bucur</name>
    </author>
    <author>
      <name>Sergiu Nisioi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Workshop on Language Technology Resources and Tools for Digital
  Humanities (LT4DH)</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.04336v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.04336v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.05123v2</id>
    <updated>2017-03-16T08:57:29Z</updated>
    <published>2017-03-15T12:37:22Z</published>
    <title>Character-based Neural Embeddings for Tweet Clustering</title>
    <summary>  In this paper we show how the performance of tweet clustering can be improved
by leveraging character-based neural networks. The proposed approach overcomes
the limitations related to the vocabulary explosion in the word-based models
and allows for the seamless processing of the multilingual content. Our
evaluation results and code are available on-line at
https://github.com/vendi12/tweet2vec_clustering
</summary>
    <author>
      <name>Svitlana Vakulenko</name>
    </author>
    <author>
      <name>Lyndon Nixon</name>
    </author>
    <author>
      <name>Mihai Lupu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the SocialNLP 2017 workshop held in conjunction with EACL
  2017, April 3, 2017, Valencia, Spain</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.05123v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.05123v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.05851v1</id>
    <updated>2017-03-17T00:02:42Z</updated>
    <published>2017-03-17T00:02:42Z</published>
    <title>Temporal Information Extraction for Question Answering Using Syntactic
  Dependencies in an LSTM-based Architecture</title>
    <summary>  In this paper, we propose to use a set of simple, uniform in architecture
LSTM-based models to recover different kinds of temporal relations from text.
Using the shortest dependency path between entities as input, the same
architecture is used to extract intra-sentence, cross-sentence, and document
creation time relations. A "double-checking" technique reverses entity pairs in
classification, boosting the recall of positive cases and reducing
misclassifications between opposite classes. An efficient pruning algorithm
resolves conflicts globally. Evaluated on QA-TempEval (SemEval2015 Task 5), our
proposed technique outperforms state-of-the-art methods by a large margin.
</summary>
    <author>
      <name>Yuanliang Meng</name>
    </author>
    <author>
      <name>Anna Rumshisky</name>
    </author>
    <author>
      <name>Alexey Romanov</name>
    </author>
    <link href="http://arxiv.org/abs/1703.05851v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.05851v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.06108v1</id>
    <updated>2017-03-17T17:16:02Z</updated>
    <published>2017-03-17T17:16:02Z</published>
    <title>Global Entity Ranking Across Multiple Languages</title>
    <summary>  We present work on building a global long-tailed ranking of entities across
multiple languages using Wikipedia and Freebase knowledge bases. We identify
multiple features and build a model to rank entities using a ground-truth
dataset of more than 10 thousand labels. The final system ranks 27 million
entities with 75% precision and 48% F1 score. We provide performance evaluation
and empirical evidence of the quality of ranking across languages, and open the
final ranked lists for future research.
</summary>
    <author>
      <name>Prantik Bhattacharyya</name>
    </author>
    <author>
      <name>Nemanja Spasojevic</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3041021.3054213</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3041021.3054213" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 Pages, 1 Figure, 2 Tables, WWW2017 Companion, WWW 2017 Companion</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.06108v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.06108v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.06324v1</id>
    <updated>2017-03-18T17:49:42Z</updated>
    <published>2017-03-18T17:49:42Z</published>
    <title>Deep Tensor Encoding</title>
    <summary>  Learning an encoding of feature vectors in terms of an over-complete
dictionary or a probabilistic information geometric (Fisher vectors) construct
is wide-spread in statistical signal processing and computer vision. In content
based information retrieval using deep-learning classifiers, such encodings are
learnt on the flattened last layer, without adherence to the multi-linear
structure of the underlying feature tensor. We illustrate a variety of feature
encodings incl. sparse dictionary coding and Fisher vectors along with
proposing that a structured tensor factorization scheme enables us to perform
retrieval that is at par, in terms of average precision, with Fisher vector
encoded image signatures. In short, we illustrate how structural constraints
increase retrieval fidelity.
</summary>
    <author>
      <name>B Sengupta</name>
    </author>
    <author>
      <name>E Vasquez</name>
    </author>
    <author>
      <name>Y Qian</name>
    </author>
    <link href="http://arxiv.org/abs/1703.06324v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.06324v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.07381v1</id>
    <updated>2017-03-21T18:29:05Z</updated>
    <published>2017-03-21T18:29:05Z</published>
    <title>Improving Statistical Multimedia Information Retrieval Model by using
  Ontology</title>
    <summary>  A typical IR system that delivers and stores information is affected by
problem of matching between user query and available content on web. Use of
Ontology represents the extracted terms in form of network graph consisting of
nodes, edges, index terms etc. The above mentioned IR approaches provide
relevance thus satisfying users query. The paper also emphasis on analyzing
multimedia documents and performs calculation for extracted terms using
different statistical formulas. The proposed model developed reduces semantic
gap and satisfies user needs efficiently.
</summary>
    <author>
      <name>Gagandeep Singh Narula</name>
    </author>
    <author>
      <name>Vishal Jain</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Applications ISSN No 0975 8887
  Volume 94 No 2, May 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1703.07381v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.07381v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.07384v1</id>
    <updated>2017-03-21T18:34:34Z</updated>
    <published>2017-03-21T18:34:34Z</published>
    <title>Ontology Based Pivoted normalization using Vector Based Approach for
  information Retrieval</title>
    <summary>  The proposed methodology is procedural i.e. it follows finite number of steps
that extracts relevant documents according to users query. It is based on
principles of Data Mining for analyzing web data. Data Mining first adapts
integration of data to generate warehouse. Then, it extracts useful information
with the help of algorithm. The task of representing extracted documents is
done by using Vector Based Statistical Approach that represents each document
in set of Terms.
</summary>
    <author>
      <name>Vishal Jain</name>
    </author>
    <author>
      <name>Dr. Mayank Singh</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">7th International Conference on Advanced Computing and
  Communication Technologies, 16th November, 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1703.07384v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.07384v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.09108v2</id>
    <updated>2017-04-20T09:50:03Z</updated>
    <published>2017-03-27T14:35:37Z</published>
    <title>Mr. DLib: Recommendations-as-a-Service (RaaS) for Academia</title>
    <summary>  Only few digital libraries and reference managers offer recommender systems,
although such systems could assist users facing information overload. In this
paper, we introduce Mr. DLib's recommendations-as-a-service, which allows third
parties to easily integrate a recommender system into their products. We
explain the recommender approaches implemented in Mr. DLib (content-based
filtering among others), and present details on 57 million recommendations,
which Mr. DLib delivered to its partner GESIS Sowiport. Finally, we outline our
plans for future development, including integration into JabRef, establishing a
living lab, and providing personalized recommendations.
</summary>
    <author>
      <name>Joeran Beel</name>
    </author>
    <author>
      <name>Akiko Aizawa</name>
    </author>
    <author>
      <name>Corinna Breitinger</name>
    </author>
    <author>
      <name>Bela Gipp</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication at the JCDL conference 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.09108v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.09108v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.7; H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.02552v1</id>
    <updated>2017-04-09T01:24:32Z</updated>
    <published>2017-04-09T01:24:32Z</published>
    <title>Embedded Collaborative Filtering for "Cold Start" Prediction</title>
    <summary>  Using only implicit data, many recommender systems fail in general to provide
a precise set of recommendations to users with limited interaction history.
This issue is regarded as the "Cold Start" problem and is typically resolved by
switching to content-based approaches where extra costly information is
required. In this paper, we use a dimensionality reduction algorithm, Word2Vec
(W2V), originally applied in Natural Language Processing problems under the
framework of Collaborative Filtering (CF) to tackle the "Cold Start" problem
using only implicit data. This combined method is named Embedded Collaborative
Filtering (ECF). An experiment is conducted to determine the performance of ECF
on two different implicit data sets. We show that the ECF approach outperforms
other popular and state-of-the-art approaches in "Cold Start" scenarios.
</summary>
    <author>
      <name>Yubo Zhou</name>
    </author>
    <author>
      <name>Ali Nadaf</name>
    </author>
    <link href="http://arxiv.org/abs/1704.02552v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.02552v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.03624v1</id>
    <updated>2017-04-12T05:36:28Z</updated>
    <published>2017-04-12T05:36:28Z</published>
    <title>Loklak - A Distributed Crawler and Data Harvester for Overcoming Rate
  Limits</title>
    <summary>  Modern social networks have become sources for vast quantities of data.
Having access to such big data can be very useful for various researchers and
data scientists. In this paper we describe Loklak, an open source distributed
peer to peer crawler and scraper for supporting such research on platforms like
Twitter, Weibo and other social networks. Social networks such as Twitter and
Weibo pose various limitations to the user on the rate at which one could
freely collect such data for research. Our crawler enables researchers to
continuously collect data while overcoming the barriers of authentication and
rate limits imposed to provide a repository of open data as a service.
</summary>
    <author>
      <name>Sudheesh Singanamalla</name>
    </author>
    <author>
      <name>Michael Peter Christen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.03624v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.03624v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.00894v1</id>
    <updated>2017-05-02T10:35:12Z</updated>
    <published>2017-05-02T10:35:12Z</published>
    <title>Talking Open Data</title>
    <summary>  Enticing users into exploring Open Data remains an important challenge for
the whole Open Data paradigm. Standard stock interfaces often used by Open Data
portals are anything but inspiring even for tech-savvy users, let alone those
without an articulated interest in data science. To address a broader range of
citizens, we designed an open data search interface supporting natural language
interactions via popular platforms like Facebook and Skype. Our data-aware
chatbot answers search requests and suggests relevant open datasets, bringing
fun factor and a potential of viral dissemination into Open Data exploration.
The current system prototype is available for Facebook
(https://m.me/OpenDataAssistant) and Skype
(https://join.skype.com/bot/6db830ca-b365-44c4-9f4d-d423f728e741) users.
</summary>
    <author>
      <name>Sebastian Neumaier</name>
    </author>
    <author>
      <name>Vadim Savenkov</name>
    </author>
    <author>
      <name>Svitlana Vakulenko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ESWC2017 demo track</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.00894v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.00894v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.04803v1</id>
    <updated>2017-05-13T09:06:52Z</updated>
    <published>2017-05-13T09:06:52Z</published>
    <title>Benchmark for Complex Answer Retrieval</title>
    <summary>  Retrieving paragraphs to populate a Wikipedia article is a challenging task.
The new TREC Complex Answer Retrieval (TREC CAR) track introduces a
comprehensive dataset that targets this retrieval scenario. We present early
results from a variety of approaches -- from standard information retrieval
methods (e.g., tf-idf) to complex systems that using query expansion using
knowledge bases and deep neural networks. The goal is to offer future
participants of this track an overview of some promising approaches to tackle
this problem.
</summary>
    <author>
      <name>Federico Nanni</name>
    </author>
    <author>
      <name>Bhaskar Mitra</name>
    </author>
    <author>
      <name>Matt Magnusson</name>
    </author>
    <author>
      <name>Laura Dietz</name>
    </author>
    <link href="http://arxiv.org/abs/1705.04803v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.04803v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.06056v2</id>
    <updated>2017-07-27T09:52:55Z</updated>
    <published>2017-05-17T09:06:42Z</published>
    <title>Target Type Identification for Entity-Bearing Queries</title>
    <summary>  Identifying the target types of entity-bearing queries can help improve
retrieval performance as well as the overall search experience. In this work,
we address the problem of automatically detecting the target types of a query
with respect to a type taxonomy. We propose a supervised learning approach with
a rich variety of features. Using a purpose-built test collection, we show that
our approach outperforms existing methods by a remarkable margin. This is an
extended version of the article published with the same title in the
Proceedings of SIGIR'17.
</summary>
    <author>
      <name>Darío Garigliotti</name>
    </author>
    <author>
      <name>Faegheh Hasibi</name>
    </author>
    <author>
      <name>Krisztian Balog</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3077136.3080659</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3077136.3080659" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of SIGIR'17 short paper, 5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.06056v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.06056v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.06504v2</id>
    <updated>2017-08-30T11:47:45Z</updated>
    <published>2017-05-18T10:08:38Z</published>
    <title>TableQA: Question Answering on Tabular Data</title>
    <summary>  Tabular data is difficult to analyze and to search through, yielding for new
tools and interfaces that would allow even non tech-savvy users to gain
insights from open datasets without resorting to specialized data analysis
tools or even without having to fully understand the dataset structure. The
goal of our demonstration is to showcase answering natural language questions
from tabular data, and to discuss related system configuration and model
training aspects. Our prototype is publicly available and open-sourced (see
https://svakulenko.ai.wu.ac.at/tableqa).
</summary>
    <author>
      <name>Svitlana Vakulenko</name>
    </author>
    <author>
      <name>Vadim Savenkov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of the demo paper accepted at SEMANTiCS 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.06504v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.06504v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.07311v1</id>
    <updated>2017-05-20T14:21:02Z</updated>
    <published>2017-05-20T14:21:02Z</published>
    <title>Personalized Ranking for Context-Aware Venue Suggestion</title>
    <summary>  Making personalized and context-aware suggestions of venues to the users is
very crucial in venue recommendation. These suggestions are often based on
matching the venues' features with the users' preferences, which can be
collected from previously visited locations. In this paper we present a novel
user-modeling approach which relies on a set of scoring functions for making
personalized suggestions of venues based on venues content and reviews as well
as users context. Our experiments, conducted on the dataset of the TREC
Contextual Suggestion Track, prove that our methodology outperforms
state-of-the-art approaches by a significant margin.
</summary>
    <author>
      <name>Mohammad Aliannejadi</name>
    </author>
    <author>
      <name>Ida Mele</name>
    </author>
    <author>
      <name>Fabio Crestani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 32nd ACM SIGAPP Symposium On Applied Computing (SAC), Marrakech,
  Morocco, April 4-6, 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.07311v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.07311v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.08154v1</id>
    <updated>2017-05-23T09:36:41Z</updated>
    <published>2017-05-23T09:36:41Z</published>
    <title>Reference String Extraction Using Line-Based Conditional Random Fields</title>
    <summary>  The extraction of individual reference strings from the reference section of
scientific publications is an important step in the citation extraction
pipeline. Current approaches divide this task into two steps by first detecting
the reference section areas and then grouping the text lines in such areas into
reference strings. We propose a classification model that considers every line
in a publication as a potential part of a reference string. By applying
line-based conditional random fields rather than constructing the graphical
model based on the individual words, dependencies and patterns that are typical
in reference sections provide strong features while the overall complexity of
the model is reduced.
</summary>
    <author>
      <name>Martin Körner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, preprint</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.08154v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.08154v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.08321v1</id>
    <updated>2017-05-02T17:04:42Z</updated>
    <published>2017-05-02T17:04:42Z</published>
    <title>Increasing Papers' Discoverability with Precise Semantic Labeling: the
  sci.AI Platform</title>
    <summary>  The number of published findings in biomedicine increases continually. At the
same time, specifics of the domain's terminology complicates the task of
relevant publications retrieval. In the current research, we investigate
influence of terms' variability and ambiguity on a paper's likelihood of being
retrieved. We obtained statistics that demonstrate significance of the issue
and its challenges, followed by presenting the sci.AI platform, which allows
precise terms labeling as a resolution.
</summary>
    <author>
      <name>Roman Gurinovich</name>
    </author>
    <author>
      <name>Alexander Pashuk</name>
    </author>
    <author>
      <name>Yuriy Petrovskiy</name>
    </author>
    <author>
      <name>Alex Dmitrievskij</name>
    </author>
    <author>
      <name>Oleg Kuryan</name>
    </author>
    <author>
      <name>Alexei Scerbacov</name>
    </author>
    <author>
      <name>Antonia Tiggre</name>
    </author>
    <author>
      <name>Elena Moroz</name>
    </author>
    <author>
      <name>Yuri Nikolsky</name>
    </author>
    <link href="http://arxiv.org/abs/1705.08321v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.08321v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.08804v1</id>
    <updated>2017-05-24T14:52:06Z</updated>
    <published>2017-05-24T14:52:06Z</published>
    <title>Beyond Parity: Fairness Objectives for Collaborative Filtering</title>
    <summary>  We study fairness in collaborative-filtering recommender systems, which are
sensitive to discrimination that exists in historical data. Biased data can
lead collaborative-filtering methods to make unfair predictions for users from
minority groups. We identify the insufficiency of existing fairness metrics and
propose four new metrics that address different forms of unfairness. These
fairness metrics can be optimized by adding fairness terms to the learning
objective. Experiments on synthetic and real data show that our new metrics can
better measure fairness than the baseline, and that the fairness objectives
effectively help reduce unfairness.
</summary>
    <author>
      <name>Sirui Yao</name>
    </author>
    <author>
      <name>Bert Huang</name>
    </author>
    <link href="http://arxiv.org/abs/1705.08804v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.08804v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.00695v1</id>
    <updated>2017-06-02T14:20:34Z</updated>
    <published>2017-06-02T14:20:34Z</published>
    <title>Hashtag-centric Immersive Search on Social Media</title>
    <summary>  Social media information distributes in different Online Social Networks
(OSNs). This paper addresses the problem integrating the cross-OSN information
to facilitate an immersive social media search experience. We exploit hashtag,
which is widely used to annotate and organize multi-modal items in different
OSNs, as the bridge for information aggregation and organization. A three-stage
solution framework is proposed for hashtag representation, clustering and
demonstration. Given an event query, the related items from three OSNs,
Twitter, Flickr and YouTube, are organized in cluster-hashtag-item hierarchy
for display. The effectiveness of the proposed solution is validated by
qualitative and quantitative experiments on hundreds of trending event queries.
</summary>
    <author>
      <name>Yuqi Gao</name>
    </author>
    <author>
      <name>Jitao Sang</name>
    </author>
    <author>
      <name>Tongwei Ren</name>
    </author>
    <author>
      <name>Changsheng Xu</name>
    </author>
    <link href="http://arxiv.org/abs/1706.00695v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.00695v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.02061v1</id>
    <updated>2017-06-07T06:57:25Z</updated>
    <published>2017-06-07T06:57:25Z</published>
    <title>An Extended Relevance Model for Session Search</title>
    <summary>  The session search task aims at best serving the user's information need
given her previous search behavior during the session. We propose an extended
relevance model that captures the user's dynamic information need in the
session. Our relevance modelling approach is directly driven by the user's
query reformulation (change) decisions and the estimate of how much the user's
search behavior affects such decisions. Overall, we demonstrate that, the
proposed approach significantly boosts session search performance.
</summary>
    <author>
      <name>Nir Levine</name>
    </author>
    <author>
      <name>Haggai Roitman</name>
    </author>
    <author>
      <name>Doron Cohen</name>
    </author>
    <link href="http://arxiv.org/abs/1706.02061v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.02061v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.03266v1</id>
    <updated>2017-06-10T18:29:19Z</updated>
    <published>2017-06-10T18:29:19Z</published>
    <title>An Empirical Study of Some Selected IR Models for Bengali Monolingual
  Information Retrieval</title>
    <summary>  This paper presents an evaluation and an analysis of some selected
information retrieval models for Bengali monolingual information retrieval
task. Two models, TF-IDF model and the Okapi BM25 model have been considered
for our study. The developed IR models are tested on FIRE ad hoc retrieval data
sets released for different years from 2008 to 2012 and the obtained results
have been reported in this paper.
</summary>
    <author>
      <name>Kamal Sarkar</name>
    </author>
    <author>
      <name>Avisek Gupta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, In Proceedings of ICBIM 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.03266v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.03266v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.07479v2</id>
    <updated>2017-09-02T11:13:43Z</updated>
    <published>2017-06-22T20:20:34Z</published>
    <title>Binary Latent Representations for Efficient Ranking: Empirical
  Assessment</title>
    <summary>  Large-scale recommender systems often face severe latency and storage
constraints at prediction time. These are particularly acute when the number of
items that could be recommended is large, and calculating predictions for the
full set is computationally intensive. In an attempt to relax these
constraints, we train recommendation models that use binary rather than
real-valued user and item representations, and show that while they are
substantially faster to evaluate, the gains in speed come at a large cost in
accuracy. In our Movielens 1M experiments, we show that reducing the latent
dimensionality of traditional models offers a more attractive accuracy/speed
trade-off than using binary representations.
</summary>
    <author>
      <name>Maciej Kula</name>
    </author>
    <link href="http://arxiv.org/abs/1706.07479v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.07479v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.08746v2</id>
    <updated>2017-07-24T16:03:03Z</updated>
    <published>2017-06-27T09:23:37Z</published>
    <title>DE-PACRR: Exploring Layers Inside the PACRR Model</title>
    <summary>  Recent neural IR models have demonstrated deep learning's utility in ad-hoc
information retrieval. However, deep models have a reputation for being black
boxes, and the roles of a neural IR model's components may not be obvious at
first glance. In this work, we attempt to shed light on the inner workings of a
recently proposed neural IR model, namely the PACRR model, by visualizing the
output of intermediate layers and by investigating the relationship between
intermediate weights and the ultimate relevance score produced. We highlight
several insights, hoping that such insights will be generally applicable.
</summary>
    <author>
      <name>Andrew Yates</name>
    </author>
    <author>
      <name>Kai Hui</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Neu-IR 2017 SIGIR Workshop on Neural Information Retrieval</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.08746v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.08746v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09200v1</id>
    <updated>2017-06-28T10:12:01Z</updated>
    <published>2017-06-28T10:12:01Z</published>
    <title>Energy-Based Sequence GANs for Recommendation and Their Connection to
  Imitation Learning</title>
    <summary>  Recommender systems aim to find an accurate and efficient mapping from
historic data of user-preferred items to a new item that is to be liked by a
user. Towards this goal, energy-based sequence generative adversarial nets
(EB-SeqGANs) are adopted for recommendation by learning a generative model for
the time series of user-preferred items. By recasting the energy function as
the feature function, the proposed EB-SeqGANs is interpreted as an instance of
maximum-entropy imitation learning.
</summary>
    <author>
      <name>Jaeyoon Yoo</name>
    </author>
    <author>
      <name>Heonseok Ha</name>
    </author>
    <author>
      <name>Jihun Yi</name>
    </author>
    <author>
      <name>Jongha Ryu</name>
    </author>
    <author>
      <name>Chanju Kim</name>
    </author>
    <author>
      <name>Jung-Woo Ha</name>
    </author>
    <author>
      <name>Young-Han Kim</name>
    </author>
    <author>
      <name>Sungroh Yoon</name>
    </author>
    <link href="http://arxiv.org/abs/1706.09200v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09200v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.03367v1</id>
    <updated>2017-07-11T17:12:50Z</updated>
    <published>2017-07-11T17:12:50Z</published>
    <title>Wextractor: Follow-up of the evolution of prices in web pages</title>
    <summary>  In the e-commerce world, the follow-up of prices in detail web pages is of
great interest for things like buying a product when it falls below some
threshold. For doing this task, instead of bookmarking the pages and revisiting
them, in this paper we propose a novel web data extraction system, called
Wextractor. It consists of an extraction method and a web app for listing the
retrieved prices. As for the final user, the main feature of Wextractor is
usability because (s)he only has to signal the pages of interest and our system
automatically extracts the price from the page.
</summary>
    <author>
      <name>Jorge Lloret-Gazo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.03367v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.03367v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.03569v1</id>
    <updated>2017-07-12T07:17:50Z</updated>
    <published>2017-07-12T07:17:50Z</published>
    <title>Multitask Learning for Fine-Grained Twitter Sentiment Analysis</title>
    <summary>  Traditional sentiment analysis approaches tackle problems like ternary
(3-category) and fine-grained (5-category) classification by learning the tasks
separately. We argue that such classification tasks are correlated and we
propose a multitask approach based on a recurrent neural network that benefits
by jointly learning them. Our study demonstrates the potential of multitask
models on this type of problems and improves the state-of-the-art results in
the fine-grained sentiment classification problem.
</summary>
    <author>
      <name>Georgios Balikas</name>
    </author>
    <author>
      <name>Simon Moura</name>
    </author>
    <author>
      <name>Massih-Reza Amini</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3077136.3080702</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3077136.3080702" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International ACM SIGIR Conference on Research and Development in
  Information Retrieval 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.03569v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.03569v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.05254v1</id>
    <updated>2017-07-12T23:18:58Z</updated>
    <published>2017-07-12T23:18:58Z</published>
    <title>Explainable Entity-based Recommendations with Knowledge Graphs</title>
    <summary>  Explainable recommendation is an important task. Many methods have been
proposed which generate explanations from the content and reviews written for
items. When review text is unavailable, generating explanations is still a hard
problem. In this paper, we illustrate how explanations can be generated in such
a scenario by leveraging external knowledge in the form of knowledge graphs.
Our method jointly ranks items and knowledge graph entities using a
Personalized PageRank procedure to produce recommendations together with their
explanations.
</summary>
    <author>
      <name>Rose Catherine</name>
    </author>
    <author>
      <name>Kathryn Mazaitis</name>
    </author>
    <author>
      <name>Maxine Eskenazi</name>
    </author>
    <author>
      <name>William Cohen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in the 11th ACM Conference on Recommender
  Systems (RecSys 2017) - Posters</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.05254v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.05254v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.06562v1</id>
    <updated>2017-07-20T15:06:43Z</updated>
    <published>2017-07-20T15:06:43Z</published>
    <title>From Task Classification Towards Similarity Measures for Recommendation
  in Crowdsourcing Systems</title>
    <summary>  Task selection in micro-task markets can be supported by recommender systems
to help individuals to find appropriate tasks. Previous work showed that for
the selection process of a micro-task the semantic aspects, such as the
required action and the comprehensibility, are rated more important than
factual aspects, such as the payment or the required completion time. This work
gives a foundation to create such similarity measures. Therefore, we show that
an automatic classification based on task descriptions is possible.
Additionally, we propose similarity measures to cluster micro-tasks according
to semantic aspects.
</summary>
    <author>
      <name>Steffen Schnitzer</name>
    </author>
    <author>
      <name>Svenja Neitzel</name>
    </author>
    <author>
      <name>Christoph Rensing</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Work in Progress Paper at HCOMP 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.06562v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.06562v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.08913v1</id>
    <updated>2017-07-27T15:52:18Z</updated>
    <published>2017-07-27T15:52:18Z</published>
    <title>Multi-Stakeholder Recommendation: Applications and Challenges</title>
    <summary>  Recommender systems have been successfully applied to assist decision making
by producing a list of item recommendations tailored to user preferences.
Traditional recommender systems only focus on optimizing the utility of the end
users who are the receiver of the recommendations. By contrast,
multi-stakeholder recommendation attempts to generate recommendations that
satisfy the needs of both the end users and other parties or stakeholders. This
paper provides an overview and discussion about the multi-stakeholder
recommendations from the perspective of practical applications, available data
sets, corresponding research challenges and potential solutions.
</summary>
    <author>
      <name>Yong Zheng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the 2017 Workshop on Value-Aware and Multistakeholder
  Recommendation</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.08913v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.08913v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.09258v1</id>
    <updated>2017-07-28T14:42:34Z</updated>
    <published>2017-07-28T14:42:34Z</published>
    <title>Patterns of Multistakeholder Recommendation</title>
    <summary>  Recommender systems are personalized information systems. However, in many
settings, the end-user of the recommendations is not the only party whose needs
must be represented in recommendation generation. Incorporating this insight
gives rise to the notion of multistakeholder recommendation, in which the
interests of multiple parties are represented in recommendation algorithms and
evaluation. In this paper, we identify patterns of stakeholder utility that
characterize different multistakeholder recommendation applications, and
provide a taxonomy of the different possible systems, only some of which have
currently been implemented.
</summary>
    <author>
      <name>Robin Burke</name>
    </author>
    <author>
      <name>Himan Abdollahpouri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the 2017 Workshop on Value-Aware and Multistakeholder
  Recommendation</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.09258v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.09258v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.09823v1</id>
    <updated>2017-07-31T12:48:45Z</updated>
    <published>2017-07-31T12:48:45Z</published>
    <title>Familia: An Open-Source Toolkit for Industrial Topic Modeling</title>
    <summary>  Familia is an open-source toolkit for pragmatic topic modeling in industry.
Familia abstracts the utilities of topic modeling in industry as two paradigms:
semantic representation and semantic matching. Efficient implementations of the
two paradigms are made publicly available for the first time. Furthermore, we
provide off-the-shelf topic models trained on large-scale industrial corpora,
including Latent Dirichlet Allocation (LDA), SentenceLDA and Topical Word
Embedding (TWE). We further describe typical applications which are
successfully powered by topic modeling, in order to ease the confusions and
difficulties of software engineers during topic model selection and
utilization.
</summary>
    <author>
      <name>Di Jiang</name>
    </author>
    <author>
      <name>Zeyu Chen</name>
    </author>
    <author>
      <name>Rongzhong Lian</name>
    </author>
    <author>
      <name>Siqi Bao</name>
    </author>
    <author>
      <name>Chen Li</name>
    </author>
    <link href="http://arxiv.org/abs/1707.09823v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.09823v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.02765v1</id>
    <updated>2017-08-09T09:00:03Z</updated>
    <published>2017-08-09T09:00:03Z</published>
    <title>Ephemeral Context to Support Robust and Diverse Music Recommendations</title>
    <summary>  While prior work on context-based music recommendation focused on fixed set
of contexts (e.g. walking, driving, jogging), we propose to use multiple
sensors and external data sources to describe momentary (ephemeral) context in
a rich way with a very large number of possible states (e.g. jogging fast along
in downtown of Sydney under a heavy rain at night being tired and angry). With
our approach, we address the problems which current approaches face: 1) a
limited ability to infer context from missing or faulty sensor data; 2) an
inability to use contextual information to support novel content discovery.
</summary>
    <author>
      <name>Pavel Kucherbaev</name>
    </author>
    <author>
      <name>Nava Tintarev</name>
    </author>
    <author>
      <name>Carlos Rodriguez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 1 figure, Machine Learning for Music Discovery workshop at
  ICML2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.02765v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.02765v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.04326v1</id>
    <updated>2017-08-14T21:04:36Z</updated>
    <published>2017-08-14T21:04:36Z</published>
    <title>Improved Answer Selection with Pre-Trained Word Embeddings</title>
    <summary>  This paper evaluates existing and newly proposed answer selection methods
based on pre-trained word embeddings. Word embeddings are highly effective in
various natural language processing tasks and their integration into
traditional information retrieval (IR) systems allows for the capture of
semantic relatedness between questions and answers. Empirical results on three
publicly available data sets show significant gains over traditional term
frequency based approaches in both supervised and unsupervised settings. We
show that combining these word embedding features with traditional
learning-to-rank techniques can achieve similar performance to state-of-the-art
neural networks trained for the answer selection task.
</summary>
    <author>
      <name>Rishav Chakravarti</name>
    </author>
    <author>
      <name>Jiri Navratil</name>
    </author>
    <author>
      <name>Cicero Nogueira dos Santos</name>
    </author>
    <link href="http://arxiv.org/abs/1708.04326v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.04326v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.06011v1</id>
    <updated>2017-08-20T19:41:58Z</updated>
    <published>2017-08-20T19:41:58Z</published>
    <title>Modelling Word Burstiness in Natural Language: A Generalised Polya
  Process for Document Language Models in Information Retrieval</title>
    <summary>  We introduce a generalised multivariate Polya process for document language
modelling. The framework outlined here generalises a number of statistical
language models used in information retrieval for modelling document
generation. In particular, we show that the choice of replacement matrix M
ultimately defines the type of random process and therefore defines a
particular type of document language model. We show that a particular variant
of the general model is useful for modelling term-specific burstiness.
Furthermore, via experimentation we show that this variant significantly
improves retrieval effectiveness over a strong baseline on a number of small
test collections.
</summary>
    <author>
      <name>Ronan Cummins</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.06011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.06011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.08289v1</id>
    <updated>2017-08-28T12:44:14Z</updated>
    <published>2017-08-28T12:44:14Z</published>
    <title>Generating Query Suggestions to Support Task-Based Search</title>
    <summary>  We address the problem of generating query suggestions to support users in
completing their underlying tasks (which motivated them to search in the first
place). Given an initial query, these query suggestions should provide a
coverage of possible subtasks the user might be looking for. We propose a
probabilistic modeling framework that obtains keyphrases from multiple sources
and generates query suggestions from these keyphrases. Using the test suites of
the TREC Tasks track, we evaluate and analyze each component of our model.
</summary>
    <author>
      <name>Darío Garigliotti</name>
    </author>
    <author>
      <name>Krisztian Balog</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3077136.3080745</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3077136.3080745" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 40th International ACM SIGIR Conference on
  Research and Development in Information Retrieval (SIGIR '17), 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.08289v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.08289v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.01256v1</id>
    <updated>2017-09-05T06:47:03Z</updated>
    <published>2017-09-05T06:47:03Z</published>
    <title>Semantic Document Distance Measures and Unsupervised Document Revision
  Detection</title>
    <summary>  In this paper, we model the document revision detection problem as a minimum
cost branching problem that relies on computing document distances.
Furthermore, we propose two new document distance measures, word vector-based
Dynamic Time Warping (wDTW) and word vector-based Tree Edit Distance (wTED).
Our revision detection system is designed for a large scale corpus and
implemented in Apache Spark. We demonstrate that our system can more precisely
detect revisions than state-of-the-art methods by utilizing the Wikipedia
revision dumps https://snap.stanford.edu/data/wiki-meta.html and simulated data
sets.
</summary>
    <author>
      <name>Xiaofeng Zhu</name>
    </author>
    <author>
      <name>Diego Klabjan</name>
    </author>
    <author>
      <name>Patrick Bless</name>
    </author>
    <link href="http://arxiv.org/abs/1709.01256v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.01256v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.01782v1</id>
    <updated>2017-09-06T11:47:31Z</updated>
    <published>2017-09-06T11:47:31Z</published>
    <title>Automatic Document Image Binarization</title>
    <summary>  Document image binarization is often a challenging task due to various forms
of degradation. Although there exist several binarization techniques in
literature, the binarized image is typically sensitive to control parameter
settings of the employed technique. This paper presents an automatic document
image binarization algorithm to segment the text from heavily degraded document
images. The proposed technique uses a two band-pass filtering approach for
background noise removal, and Bayesian optimization for automatic
hyperparameter selection for optimal results. The effectiveness of the proposed
binarization technique is empirically demonstrated on the Document Image
Binarization Competition (DIBCO) and the Handwritten Document Image
Binarization Competition (H-DIBCO) datasets.
</summary>
    <author>
      <name>Ekta Vats</name>
    </author>
    <author>
      <name>Anders Hast</name>
    </author>
    <author>
      <name>Prashant Singh</name>
    </author>
    <link href="http://arxiv.org/abs/1709.01782v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.01782v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.03260v1</id>
    <updated>2017-09-11T06:25:46Z</updated>
    <published>2017-09-11T06:25:46Z</published>
    <title>A Short Note on Proximity-based Scoring of Documents with Multiple
  Fields</title>
    <summary>  The BM25 ranking function is one of the most well known query relevance
document scoring functions and many variations of it are proposed. The BM25F
function is one of its adaptations designed for modeling documents with
multiple fields. The Expanded Span method extends a BM25-like function by
taking into considerations of the proximity between term occurrences. In this
note, we combine these two variations into one scoring method in view of
proximity-based scoring of documents with multiple fields.
</summary>
    <author>
      <name>Tomohiro Manabe</name>
    </author>
    <author>
      <name>Sumio Fujita</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.03260v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.03260v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.07654v1</id>
    <updated>2017-09-22T09:39:08Z</updated>
    <published>2017-09-22T09:39:08Z</published>
    <title>Annotation based automatic action processing</title>
    <summary>  With a strong motivational background in search engine optimization the
amount of structured data on the web is growing rapidly. The main search engine
providers are promising great increase in visibility through annotation of the
web page's content with the vocabulary of schema.org and thus providing it as
structured data. But besides the usage by search engines the data can be used
in various other ways, for example for automatic processing of annotated web
services or actions. In this work we present an approach to consume and process
schema.org annotated data on the web and give an idea how a best practice can
look like.
</summary>
    <author>
      <name>Elias Kärle</name>
    </author>
    <author>
      <name>Dieter Fensel</name>
    </author>
    <link href="http://arxiv.org/abs/1709.07654v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.07654v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.09214v1</id>
    <updated>2017-08-05T15:46:53Z</updated>
    <published>2017-08-05T15:46:53Z</published>
    <title>A Hybrid Approach using Ontology Similarity and Fuzzy Logic for Semantic
  Question Answering</title>
    <summary>  One of the challenges in information retrieval is providing accurate answers
to a user's question often expressed as uncertainty words. Most answers are
based on a Syntactic approach rather than a Semantic analysis of the query. In
this paper, our objective is to present a hybrid approach for a Semantic
question answering retrieval system using Ontology Similarity and Fuzzy logic.
We use a Fuzzy Co-clustering algorithm to retrieve the collection of documents
based on Ontology Similarity. The Fuzzy Scale uses Fuzzy type-1 for documents
and Fuzzy type-2 for words to prioritize answers. The objective of this work is
to provide retrieval system with more accurate answers than non-fuzzy Semantic
Ontology approach.
</summary>
    <author>
      <name>Monika Rani</name>
    </author>
    <author>
      <name>Maybin K. Muyeba</name>
    </author>
    <author>
      <name>O. P. Vyas</name>
    </author>
    <link href="http://arxiv.org/abs/1709.09214v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.09214v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.09836v1</id>
    <updated>2017-09-28T07:56:26Z</updated>
    <published>2017-09-28T07:56:26Z</published>
    <title>Towards a Semantic Search Engine for Scientific Articles</title>
    <summary>  Because of the data deluge in scientific publication, finding relevant
information is getting harder and harder for researchers and readers. Building
an enhanced scientific search engine by taking semantic relations into account
poses a great challenge. As a starting point, semantic relations between
keywords from scientific articles could be extracted in order to classify
articles. This might help later in the process of browsing and searching for
content in a meaningful scientific way. Indeed, by connecting keywords, the
context of the article can be extracted. This paper aims to provide ideas to
build such a smart search engine and describes the initial contributions
towards achieving such an ambitious goal.
</summary>
    <author>
      <name>Bastien Latard</name>
    </author>
    <author>
      <name>Jonathan Weber</name>
    </author>
    <author>
      <name>Germain Forestier</name>
    </author>
    <author>
      <name>Michel Hassenforder</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-67008-9_54</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-67008-9_54" rel="related"/>
    <link href="http://arxiv.org/abs/1709.09836v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.09836v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.00399v1</id>
    <updated>2017-10-01T19:42:53Z</updated>
    <published>2017-10-01T19:42:53Z</published>
    <title>Identifying Clickbait Posts on Social Media with an Ensemble of Linear
  Models</title>
    <summary>  The purpose of a clickbait is to make a link so appealing that people click
on it. However, the content of such articles is often not related to the title,
shows poor quality, and at the end leaves the reader unsatisfied.
  To help the readers, the organizers of the clickbait challenge
(http://www.clickbait-challenge.org/) asked the participants to build a machine
learning model for scoring articles with respect to their "clickbaitness".
  In this paper we propose to solve the clickbait problem with an ensemble of
Linear SVM models, and our approach was tested successfully in the challenge:
it showed great performance of 0.036 MSE and ranked 3rd among all the solutions
to the contest.
</summary>
    <author>
      <name>Alexey Grigorev</name>
    </author>
    <link href="http://arxiv.org/abs/1710.00399v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.00399v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0510084v1</id>
    <updated>2005-10-26T14:49:47Z</updated>
    <published>2005-10-26T14:49:47Z</published>
    <title>Réflexions sur la question fréquentielle en traitement du signal</title>
    <summary>  New definitions are suggested for frequencies which may be instantaneous or
not. The Heisenberg-Gabor inequality and the Shannon sampling theorem are
briefly discussed.
</summary>
    <author>
      <name>Michel Fliess</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0510084v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0510084v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.MP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0807.0070v1</id>
    <updated>2008-07-01T05:29:07Z</updated>
    <published>2008-07-01T05:29:07Z</published>
    <title>Quantitative Paradigm of Software Reliability as Content Relevance</title>
    <summary>  This paper presents a quantitative approach to software reliability and
content relevance definitions validated by the systems' potential reliability
law.Thus it is argued for the unified math nature or quantitative paradigm of
software reliability and content relevance.
</summary>
    <author>
      <name>Yuri Arkhipkin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0807.0070v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0807.0070v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.5; D.2.8; D.2.9; H.3.1; D.1.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.3183v2</id>
    <updated>2011-07-01T17:08:12Z</updated>
    <published>2010-04-19T13:11:56Z</published>
    <title>Statistical Physics for Natural Language Processing</title>
    <summary>  This paper has been withdrawn by the author.
</summary>
    <author>
      <name>Juan-Manuel Torres Moreno</name>
    </author>
    <author>
      <name>Silvia Fernandez</name>
    </author>
    <author>
      <name>Eric SanJuan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn</arxiv:comment>
    <link href="http://arxiv.org/abs/1004.3183v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.3183v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.1615v1</id>
    <updated>2012-04-07T09:28:19Z</updated>
    <published>2012-04-07T09:28:19Z</published>
    <title>Discrimination between Arabic and Latin from bilingual documents</title>
    <summary>  2011 International Conference on Communications, Computing and Control
Applications (CCCA)
</summary>
    <author>
      <name>Sofiene Haboubi</name>
    </author>
    <author>
      <name>Samia Maddouri</name>
    </author>
    <author>
      <name>Hamid Amiri</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CCCA.2011.6031496</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CCCA.2011.6031496" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.1615v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.1615v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.7917v1</id>
    <updated>2012-10-30T07:25:46Z</updated>
    <published>2012-10-30T07:25:46Z</published>
    <title>The Model of Semantic Concepts Lattice For Data Mining Of Microblogs</title>
    <summary>  The model of semantic concept lattice for data mining of microblogs has been
proposed in this work. It is shown that the use of this model is effective for
the semantic relations analysis and for the detection of associative rules of
key words.
</summary>
    <author>
      <name>Bohdan Pavlyshenko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Ukrainian</arxiv:comment>
    <link href="http://arxiv.org/abs/1210.7917v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.7917v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.08402v1</id>
    <updated>2016-04-28T13:11:54Z</updated>
    <published>2016-04-28T13:11:54Z</published>
    <title>Two Differentially Private Rating Collection Mechanisms for Recommender
  Systems</title>
    <summary>  We design two mechanisms for the recommender system to collect user ratings.
One is modified Laplace mechanism, and the other is randomized response
mechanism. We prove that they are both differentially private and preserve the
data utility.
</summary>
    <author>
      <name>Wenjie Zheng</name>
    </author>
    <link href="http://arxiv.org/abs/1604.08402v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.08402v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.02696v1</id>
    <updated>2016-12-08T15:25:35Z</updated>
    <published>2016-12-08T15:25:35Z</published>
    <title>A note on the triangle inequality for the Jaccard distance</title>
    <summary>  Two simple proofs of the triangle inequality for the Jaccard distance in
terms of nonnegative, monotone, submodular functions are given and discussed.
</summary>
    <author>
      <name>Sven Kosub</name>
    </author>
    <link href="http://arxiv.org/abs/1612.02696v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.02696v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0205022v1</id>
    <updated>2002-05-14T19:25:07Z</updated>
    <published>2002-05-14T19:25:07Z</published>
    <title>The Traits of the Personable</title>
    <summary>  Information personalization is fertile ground for application of AI
techniques. In this article I relate personalization to the ability to capture
partial information in an information-seeking interaction. The specific focus
is on personalizing interactions at web sites. Using ideas from partial
evaluation and explanation-based generalization, I present a modeling
methodology for reasoning about personalization. This approach helps identify
seven tiers of `personable traits' in web sites.
</summary>
    <author>
      <name>Naren Ramakrishnan</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0205022v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0205022v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.5; H.4.2; H.5.4; I.2.6; K.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0504074v1</id>
    <updated>2005-04-15T20:10:53Z</updated>
    <published>2005-04-15T20:10:53Z</published>
    <title>Metalinguistic Information Extraction for Terminology</title>
    <summary>  This paper describes and evaluates the Metalinguistic Operation Processor
(MOP) system for automatic compilation of metalinguistic information from
technical and scientific documents. This system is designed to extract
non-standard terminological resources that we have called Metalinguistic
Information Databases (or MIDs), in order to help update changing glossaries,
knowledge bases and ontologies, as well as to reflect the metastable dynamics
of special-domain knowledge.
</summary>
    <author>
      <name>Carlos Rodriguez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at CompuTerm 2004, COLING. Geneve</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0504074v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0504074v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0512007v1</id>
    <updated>2005-12-01T15:40:19Z</updated>
    <published>2005-12-01T15:40:19Z</published>
    <title>Entangled messages</title>
    <summary>  It is sometimes necessary to send copies of the same email to different
parties, but it is impossible to ensure that if one party reads the message the
other parties will bound to read it. We propose an entanglement based scheme
where if one party reads the message the other party will bound to read it
simultaneously.
</summary>
    <author>
      <name>Arindam Mitra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PDF, 2 Pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0512007v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0512007v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0611150v3</id>
    <updated>2006-12-05T15:04:52Z</updated>
    <published>2006-11-29T13:59:31Z</published>
    <title>A Novel Bayesian Classifier using Copula Functions</title>
    <summary>  A useful method for representing Bayesian classifiers is through
\emph{discriminant functions}. Here, using copula functions, we propose a new
model for discriminants. This model provides a rich and generalized class of
decision boundaries. These decision boundaries significantly boost the
classification accuracy especially for high dimensional feature spaces. We
strengthen our analysis through simulation results.
</summary>
    <author>
      <name>Saket Sathe</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0611150v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0611150v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0801.0386v1</id>
    <updated>2008-01-02T13:06:37Z</updated>
    <published>2008-01-02T13:06:37Z</published>
    <title>Spam: It's Not Just for Inboxes and Search Engines! Making Hirsch
  h-index Robust to Scientospam</title>
    <summary>  What is the 'level of excellence' of a scientist and the real impact of
his/her work upon the scientific thinking and practising? How can we design a
fair, an unbiased metric -- and most importantly -- a metric robust to
manipulation?
</summary>
    <author>
      <name>Dimitrios Katsaros</name>
    </author>
    <author>
      <name>Leonidas Akritidis</name>
    </author>
    <author>
      <name>Panayiotis Bozanis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/0801.0386v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.0386v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0802.3746v1</id>
    <updated>2008-02-26T05:22:30Z</updated>
    <published>2008-02-26T05:22:30Z</published>
    <title>Information Hiding Techniques: A Tutorial Review</title>
    <summary>  The purpose of this tutorial is to present an overview of various information
hiding techniques. A brief history of steganography is provided along with
techniques that were used to hide information. Text, image and audio based
information hiding techniques are discussed. This paper also provides a basic
introduction to digital watermarking.
</summary>
    <author>
      <name>Sabu M. Thampi</name>
    </author>
    <link href="http://arxiv.org/abs/0802.3746v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0802.3746v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0807.3006v1</id>
    <updated>2008-07-18T16:42:57Z</updated>
    <published>2008-07-18T16:42:57Z</published>
    <title>The rank convergence of HITS can be slow</title>
    <summary>  We prove that HITS, to "get right" h of the top k ranked nodes of an N&gt;=2k
node graph, can require h^(Omega(N h/k)) iterations (i.e. a substantial Omega(N
h log(h)/k) matrix multiplications even with a "squaring trick"). Our proof
requires no algebraic tools and is entirely self-contained.
</summary>
    <author>
      <name>Enoch Peserico</name>
    </author>
    <author>
      <name>Luca Pretto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure. Keywords: algorithm analysis, information
  retrieval, rank convergence</arxiv:comment>
    <link href="http://arxiv.org/abs/0807.3006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0807.3006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0808.0056v1</id>
    <updated>2008-08-01T04:45:17Z</updated>
    <published>2008-08-01T04:45:17Z</published>
    <title>I'm sorry to say, but your understanding of image processing
  fundamentals is absolutely wrong</title>
    <summary>  The ongoing discussion whether modern vision systems have to be viewed as
visually-enabled cognitive systems or cognitively-enabled vision systems is
groundless, because perceptual and cognitive faculties of vision are separate
components of human (and consequently, artificial) information processing
system modeling.
</summary>
    <author>
      <name>Emanuel Diamant</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published as chapter 5 in "Frontiers in Brain, Vision and AI",
  I-TECH Publisher, Viena, 2008</arxiv:comment>
    <link href="http://arxiv.org/abs/0808.0056v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0808.0056v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0811.1250v1</id>
    <updated>2008-11-08T23:23:08Z</updated>
    <published>2008-11-08T23:23:08Z</published>
    <title>Adaptive Base Class Boost for Multi-class Classification</title>
    <summary>  We develop the concept of ABC-Boost (Adaptive Base Class Boost) for
multi-class classification and present ABC-MART, a concrete implementation of
ABC-Boost. The original MART (Multiple Additive Regression Trees) algorithm has
been very successful in large-scale applications. For binary classification,
ABC-MART recovers MART. For multi-class classification, ABC-MART considerably
improves MART, as evaluated on several public data sets.
</summary>
    <author>
      <name>Ping Li</name>
    </author>
    <link href="http://arxiv.org/abs/0811.1250v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0811.1250v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.4530v2</id>
    <updated>2009-04-12T05:12:19Z</updated>
    <published>2009-03-26T08:39:45Z</published>
    <title>Nonnegative approximations of nonnegative tensors</title>
    <summary>  We study the decomposition of a nonnegative tensor into a minimal sum of
outer product of nonnegative vectors and the associated parsimonious naive
Bayes probabilistic model. We show that the corresponding approximation
problem, which is central to nonnegative PARAFAC, will always have optimal
solutions. The result holds for any choice of norms and, under a mild
assumption, even Bregman divergences.
</summary>
    <author>
      <name>Lek-Heng Lim</name>
    </author>
    <author>
      <name>Pierre Comon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0903.4530v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.4530v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.2; G.1.6; G.3; I.2.6; I.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0908.0932v1</id>
    <updated>2009-08-06T18:52:41Z</updated>
    <published>2009-08-06T18:52:41Z</published>
    <title>The Medical Algorithms Project</title>
    <summary>  The Medical Algorithms Project, a web-based resource located at
www.medal.org, is the world's largest collection of medical-related
spreadsheets, consisting of over 13,500 Excel spreadsheets each encoding a
medical algorithm from 45 different areas of medical practice. This free
resource is in use worldwide with over 106,000 registered users as of March 1,
2009.
</summary>
    <author>
      <name>M. Sriram Iyengar</name>
    </author>
    <author>
      <name>John R. Svirbely</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 Pages, 2 Colour Figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2009 113-118
  ISBN 978-1-905617-89-0</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0908.0932v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0908.0932v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.0700v1</id>
    <updated>2010-01-05T13:06:21Z</updated>
    <published>2010-01-05T13:06:21Z</published>
    <title>Vandalism Detection in Wikipedia: a Bag-of-Words Classifier Approach</title>
    <summary>  A bag-of-words based probabilistic classifier is trained using regularized
logistic regression to detect vandalism in the English Wikipedia. Isotonic
regression is used to calibrate the class membership probabilities. Learning
curve, reliability, ROC, and cost analysis are performed.
</summary>
    <author>
      <name>Amit Belani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1001.0700v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.0700v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.2.7; G.3; K.4.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.0363v1</id>
    <updated>2011-07-28T17:46:20Z</updated>
    <published>2011-07-28T17:46:20Z</published>
    <title>Typesafe Modeling in Text Mining</title>
    <summary>  Based on the concept of annotation-based agents, this report introduces tools
and a formal notation for defining and running text mining experiments using a
statically typed domain-specific language embedded in Scala. Using machine
learning for classification as an example, the framework is used to develop and
document text mining experiments, and to show how the concept of generic,
typesafe annotation corresponds to a general information model that goes beyond
text processing.
</summary>
    <author>
      <name>Fabian Steeg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">63 pages, in German</arxiv:comment>
    <link href="http://arxiv.org/abs/1108.0363v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.0363v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.4; I.2.5; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.2261v1</id>
    <updated>2012-01-11T08:37:50Z</updated>
    <published>2012-01-11T08:37:50Z</published>
    <title>Relationships in Large-Scale Graph Computing</title>
    <summary>  In 2009 Grzegorz Czajkowski from Google's system infrastructure team has
published an article which didn't get much attention in the SEO community at
the time. It was titled "Large-scale graph computing at Google" and gave an
excellent insight into the future of Google's search. This article highlights
some of the little known facts which lead to transformation of Google's
algorithm in the last two years.
</summary>
    <author>
      <name>Dan Petrovic</name>
    </author>
    <link href="http://arxiv.org/abs/1201.2261v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.2261v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.6158v1</id>
    <updated>2012-02-28T09:56:43Z</updated>
    <published>2012-02-28T09:56:43Z</published>
    <title>Optimized on-line computation of PageRank algorithm</title>
    <summary>  In this paper we present new ideas to accelerate the computation of the
eigenvector of the transition matrix associated to the PageRank algorithm. New
ideas are based on the decomposition of the matrix-vector product that can be
seen as a fluid diffusion model, associated to new algebraic equations. We show
through experiments on synthetic data and on real data-sets how much this
approach can improve the computation efficiency.
</summary>
    <author>
      <name>Dohy Hong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1202.6158v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.6158v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.2.2; F.2.2; H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.0249v1</id>
    <updated>2012-09-03T06:00:04Z</updated>
    <published>2012-09-03T06:00:04Z</published>
    <title>Robopinion: Opinion Mining Framework Inspired by Autonomous Robot
  Navigation</title>
    <summary>  Data association methods are used by autonomous robots to find matches
between the current landmarks and the new set of observed features. We seek a
framework for opinion mining to benefit from advancements in autonomous robot
navigation in both research and development
</summary>
    <author>
      <name>M. A. El-Dosuky</name>
    </author>
    <author>
      <name>M. Z. Rashad</name>
    </author>
    <author>
      <name>T. T. Hamza</name>
    </author>
    <author>
      <name>A. H. EL-Bassiouny</name>
    </author>
    <link href="http://arxiv.org/abs/1209.0249v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.0249v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.1483v1</id>
    <updated>2012-09-07T10:29:35Z</updated>
    <published>2012-09-07T10:29:35Z</published>
    <title>Underspecified Scientific Claims in Nanopublications</title>
    <summary>  The application range of nanopublications --- small entities of scientific
results in RDF representation --- could be greatly extended if complete formal
representations are not mandatory. To that aim, we present an approach to
represent and interlink scientific claims in an underspecified way, based on
independent English sentences.
</summary>
    <author>
      <name>Tobias Kuhn</name>
    </author>
    <author>
      <name>Michael Krauthammer</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the Web of Linked Entities Workshop (WoLE 2012),
  CEUR Workshop Proceedings, Volume 906, 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1209.1483v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.1483v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.2097v1</id>
    <updated>2012-09-07T10:27:48Z</updated>
    <published>2012-09-07T10:27:48Z</published>
    <title>Semantic web applications with regard to math and environment</title>
    <summary>  The following is an outline of possible strategies in using semantic web
techniques and math with regard to environmental issues. The article uses
concrete examples and applications and provides partially a rather basic
treatment of semantic web techniques and math in order to adress a broader
audience.
</summary>
    <author>
      <name>Nadja Kutz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.2097v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.2097v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="00A-xx, 06-xx, 90C-xx, 65K05, 94A-xx, 94C-xx, 97Bxx" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.6; D.2.12; D.3; E.m; G.m; H.3; H.5; J.2; J.6; K.3.m; K.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.2433v2</id>
    <updated>2012-10-06T22:38:57Z</updated>
    <published>2012-09-11T20:26:48Z</published>
    <title>Correlations between Google search data and Mortality Rates</title>
    <summary>  Inspired by correlations recently discovered between Google search data and
financial markets, we show correlations between Google search data mortality
rates. Words with negative connotations may provide for increased mortality
rates, while words with positive connotations may provide for decreased
mortality rates, and so statistical methods were employed to determine to
investigate further.
</summary>
    <author>
      <name>James Risk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 2 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.2433v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.2433v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.4471v1</id>
    <updated>2012-09-20T09:21:29Z</updated>
    <published>2012-09-20T09:21:29Z</published>
    <title>Stemmer for Serbian language</title>
    <summary>  In linguistic morphology and information retrieval, stemming is the process
for reducing inflected (or sometimes derived) words to their stem, base or root
form; generally a written word form. In this work is presented suffix stripping
stemmer for Serbian language, one of the highly inflectional languages.
</summary>
    <author>
      <name>Nikola Milošević</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 8 figures, code included</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.4471v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.4471v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.1343v1</id>
    <updated>2013-06-06T08:56:32Z</updated>
    <published>2013-06-06T08:56:32Z</published>
    <title>The User Feedback on SentiWordNet</title>
    <summary>  With the release of SentiWordNet 3.0 the related Web interface has been
restyled and improved in order to allow users to submit feedback on the
SentiWordNet entries, in the form of the suggestion of alternative triplets of
values for an entry. This paper reports on the release of the user feedback
collected so far and on the plans for the future.
</summary>
    <author>
      <name>Andrea Esuli</name>
    </author>
    <link href="http://arxiv.org/abs/1306.1343v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.1343v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.6944v1</id>
    <updated>2013-06-07T15:48:06Z</updated>
    <published>2013-06-07T15:48:06Z</published>
    <title>The DeLiVerMATH project - Text analysis in mathematics</title>
    <summary>  A high-quality content analysis is essential for retrieval functionalities
but the manual extraction of key phrases and classification is expensive.
Natural language processing provides a framework to automatize the process.
Here, a machine-based approach for the content analysis of mathematical texts
is described. A prototype for key phrase extraction and classification of
mathematical texts is presented.
</summary>
    <author>
      <name>Ulf Schöneberg</name>
    </author>
    <author>
      <name>Wolfram Sperber</name>
    </author>
    <link href="http://arxiv.org/abs/1306.6944v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.6944v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.0317v1</id>
    <updated>2013-07-01T10:03:58Z</updated>
    <published>2013-07-01T10:03:58Z</published>
    <title>Algorithms of the LDA model [REPORT]</title>
    <summary>  We review three algorithms for Latent Dirichlet Allocation (LDA). Two of them
are variational inference algorithms: Variational Bayesian inference and Online
Variational Bayesian inference and one is Markov Chain Monte Carlo (MCMC)
algorithm -- Collapsed Gibbs sampling. We compare their time complexity and
performance. We find that online variational Bayesian inference is the fastest
algorithm and still returns reasonably good results.
</summary>
    <author>
      <name>Jaka Špeh</name>
    </author>
    <author>
      <name>Andrej Muhič</name>
    </author>
    <author>
      <name>Jan Rupnik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures, report</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.0317v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.0317v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.3808v1</id>
    <updated>2013-10-14T19:49:34Z</updated>
    <published>2013-10-14T19:49:34Z</published>
    <title>Pennants for Descriptors</title>
    <summary>  We present a new technique (called pennants) for displaying the descriptors
related to a descriptor across literatures, rather in a thesaurus. It has
definite implications for online searching and browsing. Pennants, named for
the flag they resemble, are a form of algorithmic prediction. Their cognitive
base is in relevance theory (RT) from linguistic pragmatics (Sperber &amp; Wilson
1995).
</summary>
    <author>
      <name>Howard D. White</name>
    </author>
    <author>
      <name>Philipp Mayr</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 1 figure, paper presented at the NKOS workshop at TPDL 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1310.3808v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.3808v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.0543v1</id>
    <updated>2014-02-03T23:09:28Z</updated>
    <published>2014-02-03T23:09:28Z</published>
    <title>How Does Latent Semantic Analysis Work? A Visualisation Approach</title>
    <summary>  By using a small example, an analogy to photographic compression, and a
simple visualization using heatmaps, we show that latent semantic analysis
(LSA) is able to extract what appears to be semantic meaning of words from a
set of documents by blurring the distinctions between the words.
</summary>
    <author>
      <name>Jan Koeman</name>
    </author>
    <author>
      <name>William Rea</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 6 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1402.0543v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.0543v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.3610v1</id>
    <updated>2014-04-11T02:33:17Z</updated>
    <published>2014-04-11T02:33:17Z</published>
    <title>Targeting HIV-related Medication Side Effects and Sentiment Using
  Twitter Data</title>
    <summary>  We present a descriptive analysis of Twitter data. Our study focuses on
extracting the main side effects associated with HIV treatments. The crux of
our work was the identification of personal tweets referring to HIV. We
summarize our results in an infographic aimed at the general public. In
addition, we present a measure of user sentiment based on hand-rated tweets.
</summary>
    <author>
      <name>Cosme Adrover</name>
    </author>
    <author>
      <name>Todd Bodnar</name>
    </author>
    <author>
      <name>Marcel Salathe</name>
    </author>
    <link href="http://arxiv.org/abs/1404.3610v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.3610v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.3287v3</id>
    <updated>2015-02-18T19:44:24Z</updated>
    <published>2014-06-12T17:01:10Z</published>
    <title>A Clustering Analysis of Tweet Length and its Relation to Sentiment</title>
    <summary>  Sentiment analysis of Twitter data is performed. The researcher has made the
following contributions via this paper: (1) an innovative method for deriving
sentiment score dictionaries using an existing sentiment dictionary as seed
words is explored, and (2) an analysis of clustered tweet sentiment scores
based on tweet length is performed.
</summary>
    <author>
      <name>Matthew Mayo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.3287v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.3287v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.0007v2</id>
    <updated>2014-12-07T23:36:54Z</updated>
    <published>2014-11-27T22:52:53Z</published>
    <title>Paradigm shifts. Part I. Collagen. Confirming and complementing the work
  of Henry Small</title>
    <summary>  The paradigm shift in collagen research during the early 1970s marked by the
discovery of the collagen precursor molecule procollagen was traced using
co-citation analysis and title word frequency determination, confirming
previous work performed by Henry Small.
</summary>
    <author>
      <name>Johannes Stegmann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 figure, 3 tables, corrections</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.0007v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.0007v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.2416v2</id>
    <updated>2014-12-09T17:22:04Z</updated>
    <published>2014-12-07T23:51:35Z</published>
    <title>Paradigm shifts. Part II. Reverse Transcriptase. Analysis of reference
  stability and word frequencies</title>
    <summary>  The reverse transcription paradigm shift in RNA tumor virus research marked
by the discovery of the reverse transcriptase in 1970 was traced using
co-citation and title word frequency analysis. It is shown that this event is
associated with a break in citation patterns and the occurrence of previously
unknown technical terms.
</summary>
    <author>
      <name>Johannes Stegmann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 7 tables, 1 figure, corrections</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.2416v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.2416v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.02781v1</id>
    <updated>2015-03-10T06:23:56Z</updated>
    <published>2015-03-10T06:23:56Z</published>
    <title>Unravelling Graph-Exchange File Formats</title>
    <summary>  A graph is used to represent data in which the relationships between the
objects in the data are at least as important as the objects themselves. Over
the last two decades nearly a hundred file formats have been proposed or used
to provide portable access to such data. This paper seeks to review these
formats, and provide some insight to both reduce the ongoing creation of
unnecessary formats, and guide the development of new formats where needed.
</summary>
    <author>
      <name>Matthew Roughan</name>
    </author>
    <author>
      <name>Jonathan Tuke</name>
    </author>
    <link href="http://arxiv.org/abs/1503.02781v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.02781v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.05240v1</id>
    <updated>2015-05-20T04:09:47Z</updated>
    <published>2015-05-20T04:09:47Z</published>
    <title>Benchmarking KAZE and MCM for Multiclass Classification</title>
    <summary>  In this paper, we propose a novel approach for feature generation by
appropriately fusing KAZE and SIFT features. We then use this feature set along
with Minimal Complexity Machine(MCM) for object classification. We show that
KAZE and SIFT features are complementary. Experimental results indicate that an
elementary integration of these techniques can outperform the state-of-the-art
approaches.
</summary>
    <author>
      <name>Siddharth Srivastava</name>
    </author>
    <author>
      <name>Prerana Mukherjee</name>
    </author>
    <author>
      <name>Brejesh Lall</name>
    </author>
    <link href="http://arxiv.org/abs/1505.05240v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.05240v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.7; I.5.4; I.4.8; I.4.9; I.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.04412v1</id>
    <updated>2015-07-15T23:13:48Z</updated>
    <published>2015-07-15T23:13:48Z</published>
    <title>Bridge the gap between network-based inference method and global ranking
  method in personal recommendation</title>
    <summary>  In this paper, we study the relationship between the network-based inference
method and global ranking method in personal recommendation. By some
theoretical analysis, we prove that the recommendation result under the global
ranking method is the limit of applying network-based inference method with
infinity times.
</summary>
    <author>
      <name>Xiwei Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.04412v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.04412v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.08800v1</id>
    <updated>2016-02-29T02:40:05Z</updated>
    <published>2016-02-29T02:40:05Z</published>
    <title>Iterative Aggregation Method for Solving Principal Component Analysis
  Problems</title>
    <summary>  Motivated by the previously developed multilevel aggregation method for
solving structural analysis problems a novel two-level aggregation approach for
efficient iterative solution of Principal Component Analysis (PCA) problems is
proposed. The course aggregation model of the original covariance matrix is
used in the iterative solution of the eigenvalue problem by a power iterations
method. The method is tested on several data sets consisting of large number of
text documents.
</summary>
    <author>
      <name>Vitaly Bulgakov</name>
    </author>
    <link href="http://arxiv.org/abs/1602.08800v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.08800v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.02945v2</id>
    <updated>2016-05-11T17:20:26Z</updated>
    <published>2016-05-10T11:29:28Z</published>
    <title>The Yahoo Query Treebank, V. 1.0</title>
    <summary>  A description and annotation guidelines for the Yahoo Webscope release of
Query Treebank, Version 1.0, May 2016.
</summary>
    <author>
      <name>Yuval Pinter</name>
    </author>
    <author>
      <name>Roi Reichart</name>
    </author>
    <author>
      <name>Idan Szpektor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Co-released with the Webscope Dataset (L-28) and with Pinter et al.,
  Syntactic Parsing of Web Queries with Question Intent, NAACL-HLT 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.02945v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.02945v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.00167v1</id>
    <updated>2016-07-01T09:15:13Z</updated>
    <published>2016-07-01T09:15:13Z</published>
    <title>SentiBubbles: Topic Modeling and Sentiment Visualization of
  Entity-centric Tweets</title>
    <summary>  Social Media users tend to mention entities when reacting to news events. The
main purpose of this work is to create entity-centric aggregations of tweets on
a daily basis. By applying topic modeling and sentiment analysis, we create
data visualization insights about current events and people reactions to those
events from an entity-centric perspective.
</summary>
    <author>
      <name>João Oliveira</name>
    </author>
    <author>
      <name>Mike Pinto</name>
    </author>
    <author>
      <name>Pedro Saleiro</name>
    </author>
    <author>
      <name>Jorge Teixeira</name>
    </author>
    <link href="http://arxiv.org/abs/1607.00167v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.00167v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.06664v1</id>
    <updated>2016-08-23T22:44:42Z</updated>
    <published>2016-08-23T22:44:42Z</published>
    <title>Topic Grids for Homogeneous Data Visualization</title>
    <summary>  We propose the topic grids to detect anomaly and analyze the behavior based
on the access log content. Content-based behavioral risk is quantified in the
high dimensional space where the topics are generated from the log. The topics
are being projected homogeneously into a space that is perception- and
interaction-friendly to the human experts.
</summary>
    <author>
      <name>Shih-Chieh Su</name>
    </author>
    <author>
      <name>Joseph Vaughn</name>
    </author>
    <author>
      <name>Jean-Laurent Huynh</name>
    </author>
    <link href="http://arxiv.org/abs/1608.06664v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.06664v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.00872v1</id>
    <updated>2016-11-03T03:53:25Z</updated>
    <published>2016-11-03T03:53:25Z</published>
    <title>A Decision Support System for Inbound Marketers: An Empirical Use of
  Latent Dirichlet Allocation Topic Model to Guide Infographic Designers</title>
    <summary>  Infographic is a type of information presentation that inbound marketers use.
I suggest a method that can allow the infographic designers to benchmark their
design against the previous viral infographics to measure whether a given
design decision can help or hurt the probability of the design becoming viral.
</summary>
    <author>
      <name>Meisam Hejazi Nia</name>
    </author>
    <link href="http://arxiv.org/abs/1611.00872v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.00872v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.08511v1</id>
    <updated>2017-01-30T08:37:25Z</updated>
    <published>2017-01-30T08:37:25Z</published>
    <title>Binary adaptive embeddings from order statistics of random projections</title>
    <summary>  We use some of the largest order statistics of the random projections of a
reference signal to construct a binary embedding that is adapted to signals
correlated with such signal. The embedding is characterized from the analytical
standpoint and shown to provide improved performance on tasks such as
classification in a reduced-dimensionality space.
</summary>
    <author>
      <name>Diego Valsesia</name>
    </author>
    <author>
      <name>Enrico Magli</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/LSP.2016.2639036</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/LSP.2016.2639036" rel="related"/>
    <link href="http://arxiv.org/abs/1701.08511v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.08511v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9809121v1</id>
    <updated>1998-09-29T21:55:20Z</updated>
    <published>1998-09-29T21:55:20Z</published>
    <title>Using Local Optimality Criteria for Efficient Information Retrieval with
  Redundant Information Filters</title>
    <summary>  We consider information retrieval when the data, for instance multimedia, is
coputationally expensive to fetch. Our approach uses "information filters" to
considerably narrow the universe of possiblities before retrieval. We are
especially interested in redundant information filters that save time over more
general but more costly filters. Efficient retrieval requires that decision
must be made about the necessity, order, and concurrent processing of proposed
filters (an "execution plan"). We develop simple polynomial-time local criteria
for optimal execution plans, and show that most forms of concurrency are
suboptimal with information filters. Although the general problem of finding an
optimal execution plan is likely exponential in the number of filters, we show
experimentally that our local optimality criteria, used in a polynomial-time
algorithm, nearly always find the global optimum with 15 filters or less, a
sufficient number of filters for most applications. Our methods do not require
special hardware and avoid the high processor idleness that is characteristic
of massive parallelism solutions to this problem. We apply our ideas to an
important application, information retrieval of cpationed data using
natural-language understanding, a problem for which the natural-language
processing can be the bottleneck if not implemented well.
</summary>
    <author>
      <name>Neil C. Rowe</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Transactions on Information Systems, 14, 2 (April 1996),
  138-174</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9809121v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9809121v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9901004v1</id>
    <updated>1999-01-12T21:56:39Z</updated>
    <published>1999-01-12T21:56:39Z</published>
    <title>On the geometry of similarity search: dimensionality curse and
  concentration of measure</title>
    <summary>  We suggest that the curse of dimensionality affecting the similarity-based
search in large datasets is a manifestation of the phenomenon of concentration
of measure on high-dimensional structures. We prove that, under certain
geometric assumptions on the query domain $\Omega$ and the dataset $X$, if
$\Omega$ satisfies the so-called concentration property, then for most query
points $x^\ast$ the ball of radius $(1+\e)d_X(x^\ast)$ centred at $x^\ast$
contains either all points of $X$ or else at least $C_1\exp(-C_2\e^2n)$ of
them. Here $d_X(x^\ast)$ is the distance from $x^\ast$ to the nearest neighbour
in $X$ and $n$ is the dimension of $\Omega$.
</summary>
    <author>
      <name>Vladimir Pestov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, LaTeX 2e</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Information Processing Letters 73 (2000), 47-51.</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9901004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9901004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3;H.2.4;F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9902028v2</id>
    <updated>1999-02-25T16:35:07Z</updated>
    <published>1999-02-24T17:10:46Z</published>
    <title>A Scrollbar-based Visualization for Document Navigation</title>
    <summary>  We are interested in questions of improving user control in best-match
text-retrieval systems, specifically questions as to whether simple
visualizations that nonetheless go beyond the minimal ones generally available
can significantly help users. Recently, we have been investigating ways to help
users decide-given a set of documents retrieved by a query-which documents and
passages are worth closer examination. We built a document viewer incorporating
a visualization centered around a novel content-displaying scrollbar and color
term highlighting, and studied whether the visualization is helpful to
non-expert searchers. Participants' reaction to the visualization was very
positive, while the objective results were inconclusive.
</summary>
    <author>
      <name>Donald Byrd</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 figures, about 12 pages; corrected references, added librarians'
  Comments on the visualization</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9902028v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9902028v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H5.2; H3.3; I7.2; H3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9904002v2</id>
    <updated>1999-06-21T03:45:13Z</updated>
    <published>1999-04-07T04:16:02Z</published>
    <title>A geometric framework for modelling similarity search</title>
    <summary>  The aim of this paper is to propose a geometric framework for modelling
similarity search in large and multidimensional data spaces of general nature,
which seems to be flexible enough to address such issues as analysis of
complexity, indexability, and the `curse of dimensionality.' Such a framework
is provided by the concept of the so-called similarity workload, which is a
probability metric space $\Omega$ (query domain) with a distinguished finite
subspace $X$ (dataset), together with an assembly of concepts, techniques, and
results from metric geometry. They include such notions as metric transform,
$\e$-entropy, and the phenomenon of concentration of measure on
high-dimensional structures. In particular, we discuss the relevance of the
latter to understanding the curse of dimensionality. As some of those concepts
and techniques are being currently reinvented by the database community, it
seems desirable to try and bridge the gap between database research and the
relevant work already done in geometry and analysis.
</summary>
    <author>
      <name>Vladimir Pestov</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/DEXA.1999.795158</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/DEXA.1999.795158" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, LaTeX 2.e</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. 10-th Int. Workshop on Database and Expert Systems
  Applications (DEXA'99), Sept. 1-3, 1999, Florence, Italy, IEEE Comp. Soc.,
  pp. 150-154.</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9904002v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9904002v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.1; H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9910015v3</id>
    <updated>2000-04-26T03:05:48Z</updated>
    <published>1999-10-18T15:47:29Z</published>
    <title>PIPE: Personalizing Recommendations via Partial Evaluation</title>
    <summary>  It is shown that personalization of web content can be advantageously viewed
as a form of partial evaluation --- a technique well known in the programming
languages community. The basic idea is to model a recommendation space as a
program, then partially evaluate this program with respect to user preferences
(and features) to obtain specialized content. This technique supports both
content-based and collaborative approaches, and is applicable to a range of
applications that require automatic information integration from multiple web
sources. The effectiveness of this methodology is illustrated by two example
applications --- (i) personalizing content for visitors to the Blacksburg
Electronic Village (http://www.bev.net), and (ii) locating and selecting
scientific software on the Internet. The scalability of this technique is
demonstrated by its ability to interface with online web ontologies that index
thousands of web pages.
</summary>
    <author>
      <name>Naren Ramakrishnan</name>
    </author>
    <link href="http://arxiv.org/abs/cs/9910015v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9910015v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.4.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9912002v1</id>
    <updated>1999-12-06T03:04:43Z</updated>
    <published>1999-12-06T03:04:43Z</published>
    <title>A Geometric Model for Information Retrieval Systems</title>
    <summary>  This decade has seen a great deal of progress in the development of
information retrieval systems. Unfortunately, we still lack a systematic
understanding of the behavior of the systems and their relationship with
documents. In this paper we present a completely new approach towards the
understanding of the information retrieval systems. Recently, it has been
observed that retrieval systems in TREC 6 show some remarkable patterns in
retrieving relevant documents. Based on the TREC 6 observations, we introduce a
geometric linear model of information retrieval systems. We then apply the
model to predict the number of relevant documents by the retrieval systems. The
model is also scalable to a much larger data set. Although the model is
developed based on the TREC 6 routing test data, I believe it can be readily
applicable to other information retrieval systems. In Appendix, we explained a
simple and efficient way of making a better system from the existing systems.
</summary>
    <author>
      <name>Myung Ho Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9912002v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9912002v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.1.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0002010v1</id>
    <updated>2000-02-16T18:22:32Z</updated>
    <published>2000-02-16T18:22:32Z</published>
    <title>Biologically Motivated Distributed Designs for Adaptive Knowledge
  Management</title>
    <summary>  We discuss how distributed designs that draw from biological network
metaphors can largely improve the current state of information retrieval and
knowledge management of distributed information systems. In particular, two
adaptive recommendation systems named TalkMine and @ApWeb are discussed in more
detail. TalkMine operates at the semantic level of keywords. It leads different
databases to learn new and adapt existing keywords to the categories recognized
by its communities of users using distributed algorithms. @ApWeb operates at
the structural level of information resources, namely citation or hyperlink
structure. It relies on collective behavior to adapt such structure to the
expectations of users. TalkMine and @ApWeb are currently being implemented for
the research library of the Los Alamos National Laboratory under the Active
Recommendation Project. Together they define a biologically motivated
information retrieval system, recommending simultaneously at the level of user
knowledge categories expressed in keywords, and at the level of individual
documents and their associations to other documents. Rather than passive
information retrieval, with this system, users obtain an active, evolving
interaction with information resources.
</summary>
    <author>
      <name>Luis M. Rocha</name>
    </author>
    <author>
      <name>Johan Bollen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Design Principles for the Immune System and Other
  Distributed Autonomous Systems. i. Cohen and L. Segel (Eds.). Oxford
  University Press</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0002010v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0002010v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3, H.1, H.2, I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0007017v1</id>
    <updated>2000-07-13T07:58:53Z</updated>
    <published>2000-07-13T07:58:53Z</published>
    <title>Fuzzy data: XML may handle it</title>
    <summary>  Data modeling is one of the most difficult tasks in application engineering.
The engineer must be aware of the use cases and the required application
services and at a certain point of time he has to fix the data model which
forms the base for the application services. However, once the data model has
been fixed it is difficult to consider changing needs. This might be a problem
in specific domains, which are as dynamic as the healthcare domain. With fuzzy
data we address all those data that are difficult to organize in a single
database. In this paper we discuss a gradual and pragmatic approach that uses
the XML technology to conquer more model flexibility. XML may provide the clue
between unstructured text data and structured database solutions and shift the
paradigm from "organizing the data along a given model" towards "organizing the
data along user requirements".
</summary>
    <author>
      <name>R. Schweiger</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University Giessen</arxiv:affiliation>
    </author>
    <author>
      <name>S. Hoelzer</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Giessen</arxiv:affiliation>
    </author>
    <author>
      <name>J. Dudeck</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Giessen</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0007017v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0007017v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0104009v1</id>
    <updated>2001-04-03T22:07:28Z</updated>
    <published>2001-04-03T22:07:28Z</published>
    <title>Evaluating Recommendation Algorithms by Graph Analysis</title>
    <summary>  We present a novel framework for evaluating recommendation algorithms in
terms of the `jumps' that they make to connect people to artifacts. This
approach emphasizes reachability via an algorithm within the implicit graph
structure underlying a recommender dataset, and serves as a complement to
evaluation in terms of predictive accuracy. The framework allows us to consider
questions relating algorithmic parameters to properties of the datasets. For
instance, given a particular algorithm `jump,' what is the average path length
from a person to an artifact? Or, what choices of minimum ratings and jumps
maintain a connected graph? We illustrate the approach with a common jump
called the `hammock' using movie recommender datasets.
</summary>
    <author>
      <name>Batul J. Mirza</name>
    </author>
    <author>
      <name>Benjamin J. Keller</name>
    </author>
    <author>
      <name>Naren Ramakrishnan</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0104009v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0104009v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.4.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0108003v1</id>
    <updated>2001-08-07T20:27:39Z</updated>
    <published>2001-08-07T20:27:39Z</published>
    <title>The Partial Evaluation Approach to Information Personalization</title>
    <summary>  Information personalization refers to the automatic adjustment of information
content, structure, and presentation tailored to an individual user. By
reducing information overload and customizing information access,
personalization systems have emerged as an important segment of the Internet
economy. This paper presents a systematic modeling methodology - PIPE
(`Personalization is Partial Evaluation') - for personalization.
Personalization systems are designed and implemented in PIPE by modeling an
information-seeking interaction in a programmatic representation. The
representation supports the description of information-seeking activities as
partial information and their subsequent realization by partial evaluation, a
technique for specializing programs. We describe the modeling methodology at a
conceptual level and outline representational choices. We present two
application case studies that use PIPE for personalizing web sites and describe
how PIPE suggests a novel evaluation criterion for information system designs.
Finally, we mention several fundamental implications of adopting the PIPE model
for personalization and when it is (and is not) applicable.
</summary>
    <author>
      <name>Naren Ramakrishnan</name>
    </author>
    <author>
      <name>Saverio Perugini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Comprehensive overview of the PIPE model for personalization</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0108003v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0108003v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.3.4; H.4.2; H5.2; H5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0108004v1</id>
    <updated>2001-08-08T02:16:15Z</updated>
    <published>2001-08-08T02:16:15Z</published>
    <title>Links tell us about lexical and semantic Web content</title>
    <summary>  The latest generation of Web search tools is beginning to exploit hypertext
link information to improve ranking\cite{Brin98,Kleinberg98} and
crawling\cite{Menczer00,Ben-Shaul99etal,Chakrabarti99} algorithms. The hidden
assumption behind such approaches, a correlation between the graph structure of
the Web and its content, has not been tested explicitly despite increasing
research on Web topology\cite{Lawrence98,Albert99,Adamic99,Butler00}. Here I
formalize and quantitatively validate two conjectures drawing connections from
link information to lexical and semantic Web content. The clink-content
conjecture states that a page is similar to the pages that link to it, i.e.,
one can infer the lexical content of a page by looking at the pages that link
to it. I also show that lexical inferences based on link cues are quite
heterogeneous across Web communities. The link-cluster conjecture states that
pages about the same topic are clustered together, i.e., one can infer the
meaning of a page by looking at its neighbours. These results explain the
success of the newest search technologies and open the way for more dynamic and
scalable methods to locate information in a topic or user driven way.
</summary>
    <author>
      <name>Filippo Menczer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0108004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0108004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.1; H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0108018v1</id>
    <updated>2001-08-27T13:07:44Z</updated>
    <published>2001-08-27T13:07:44Z</published>
    <title>Bipartite graph partitioning and data clustering</title>
    <summary>  Many data types arising from data mining applications can be modeled as
bipartite graphs, examples include terms and documents in a text corpus,
customers and purchasing items in market basket analysis and reviewers and
movies in a movie recommender system. In this paper, we propose a new data
clustering method based on partitioning the underlying bipartite graph. The
partition is constructed by minimizing a normalized sum of edge weights between
unmatched pairs of vertices of the bipartite graph. We show that an approximate
solution to the minimization problem can be obtained by computing a partial
singular value decomposition (SVD) of the associated edge weight matrix of the
bipartite graph. We point out the connection of our clustering algorithm to
correspondence analysis used in multivariate analysis. We also briefly discuss
the issue of assigning data objects to multiple clusters. In the experimental
results, we apply our clustering algorithm to the problem of document
clustering to illustrate its effectiveness and efficiency.
</summary>
    <author>
      <name>H. Zha</name>
    </author>
    <author>
      <name>X. He</name>
    </author>
    <author>
      <name>C. Ding</name>
    </author>
    <author>
      <name>M. Gu</name>
    </author>
    <author>
      <name>H. Simon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of ACM CIKM 2001, the Tenth International Conference on
  Information and Knowledge Management, 2001</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0108018v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0108018v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; G.1.3; G.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0110053v1</id>
    <updated>2001-10-26T09:27:48Z</updated>
    <published>2001-10-26T09:27:48Z</published>
    <title>Machine Learning in Automated Text Categorization</title>
    <summary>  The automated categorization (or classification) of texts into predefined
categories has witnessed a booming interest in the last ten years, due to the
increased availability of documents in digital form and the ensuing need to
organize them. In the research community the dominant approach to this problem
is based on machine learning techniques: a general inductive process
automatically builds a classifier by learning, from a set of preclassified
documents, the characteristics of the categories. The advantages of this
approach over the knowledge engineering approach (consisting in the manual
definition of a classifier by domain experts) are a very good effectiveness,
considerable savings in terms of expert manpower, and straightforward
portability to different domains. This survey discusses the main approaches to
text categorization that fall within the machine learning paradigm. We will
discuss in detail issues pertaining to three different problems, namely
document representation, classifier construction, and classifier evaluation.
</summary>
    <author>
      <name>Fabrizio Sebastiani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication on ACM Computing Surveys</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0110053v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0110053v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.1;H.3.3;I.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0206028v2</id>
    <updated>2002-08-01T00:09:30Z</updated>
    <published>2002-06-19T22:13:41Z</published>
    <title>Knowledge management for enterprises (Wissensmanagement fuer
  Unternehmen)</title>
    <summary>  Although knowledge is one of the most valuable resource of enterprises and an
important production and competition factor, this intellectual potential is
often used (or maintained) only inadequate by the enterprises. Therefore, in a
globalised and growing market the optimal usage of existing knowledge
represents a key factor for enterprises of the future. Here, knowledge
management systems should engage facilitating. Because geographically far
distributed establishments cause, however, a distributed system, this paper
should uncover the spectrum connected with it and present a possible basic
approach which is based on ontologies and modern, platform independent
technologies. Last but not least this attempt, as well as general questions of
the knowledge management, are discussed.
</summary>
    <author>
      <name>Wolfgang Eiden</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">published in January 2000, 22 pages, 13 figures, german</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0206028v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0206028v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.0; I.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0211041v1</id>
    <updated>2002-11-28T17:33:19Z</updated>
    <published>2002-11-28T17:33:19Z</published>
    <title>An Approach to Automatic Indexing of Scientific Publications in High
  Energy Physics for Database SPIRES HEP</title>
    <summary>  We introduce an approach to automatic indexing of e-prints based on a
pattern-matching technique making extensive use of an Associative Patterns
Dictionary (APD), developed by us. Entries in the APD consist of natural
language phrases with the same semantic interpretation as a set of keywords
from a controlled vocabulary. The method also allows to recognize within
e-prints formulae written in TeX notations that might also appear as keywords.
We present an automatic indexing system, AUTEX, which we have applied to
keyword index e-prints in selected areas in high energy physics (HEP) making
use of the DESY-HEPI thesaurus as a controlled vocabulary.
</summary>
    <author>
      <name>A. V. Averin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">NSI, Moscow</arxiv:affiliation>
    </author>
    <author>
      <name>L. A. Vassilevskaya</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DESY, Hamburg</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0211041v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0211041v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.1; H.3.2; H.3.6; H.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0302023v1</id>
    <updated>2003-02-16T22:08:01Z</updated>
    <published>2003-02-16T22:08:01Z</published>
    <title>Segmentation, Indexing, and Visualization of Extended Instructional
  Videos</title>
    <summary>  We present a new method for segmenting, and a new user interface for indexing
and visualizing, the semantic content of extended instructional videos. Given a
series of key frames from the video, we generate a condensed view of the data
by clustering frames according to media type and visual similarities. Using
various visual filters, key frames are first assigned a media type (board,
class, computer, illustration, podium, and sheet). Key frames of media type
board and sheet are then clustered based on contents via an algorithm with
near-linear cost. A novel user interface, the result of two user studies,
displays related topics using icons linked topologically, allowing users to
quickly locate semantically related portions of the video. We analyze the
accuracy of the segmentation tool on 17 instructional videos, each of which is
from 75 to 150 minutes in duration (a total of 40 hours); the classification
accuracy exceeds 96%.
</summary>
    <author>
      <name>Alexander Haubold</name>
    </author>
    <author>
      <name>John R. Kender</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0302023v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0302023v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.1;H.3.3;I.4.8;I.5.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0302024v2</id>
    <updated>2003-08-30T06:47:47Z</updated>
    <published>2003-02-16T22:13:42Z</published>
    <title>Analysis and Interface for Instructional Video</title>
    <summary>  We present a new method for segmenting, and a new user interface for indexing
and visualizing, the semantic content of extended instructional videos. Using
various visual filters, key frames are first assigned a media type (board,
class, computer, illustration, podium, and sheet). Key frames of media type
board and sheet are then clustered based on contents via an algorithm with
near-linear cost. A novel user interface, the result of two user studies,
displays related topics using icons linked topologically, allowing users to
quickly locate semantically related portions of the video. We analyze the
accuracy of the segmentation tool on 17 instructional videos, each of which is
from 75 to 150 minutes in duration (a total of 40 hours); it exceeds 96%.
</summary>
    <author>
      <name>Alexander Haubold</name>
    </author>
    <author>
      <name>John R. Kender</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 8 figures, ICME 2003</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of 2003 IEEE International Conference on Multimedia &amp;
  Expo, Volume II, pages 705-708, July 2003</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0302024v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0302024v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.1;H.3.3;I.4.8;I.5.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0304007v1</id>
    <updated>2003-04-03T10:17:02Z</updated>
    <published>2003-04-03T10:17:02Z</published>
    <title>A Method for Clustering Web Attacks Using Edit Distance</title>
    <summary>  Cluster analysis often serves as the initial step in the process of data
classification. In this paper, the problem of clustering different length input
data is considered. The edit distance as the minimum number of elementary edit
operations needed to transform one vector into another is used. A heuristic for
clustering unequal length vectors, analogue to the well known k-means algorithm
is described and analyzed. This heuristic determines cluster centroids
expanding shorter vectors to the lengths of the longest ones in each cluster in
a specific way. It is shown that the time and space complexities of the
heuristic are linear in the number of input vectors. Experimental results on
real data originating from a system for classification of Web attacks are
given.
</summary>
    <author>
      <name>Slobodan Petrovic</name>
    </author>
    <author>
      <name>Gonzalo Alvarez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 2 figures, latex format</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0304007v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0304007v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3;K.6.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0306026v1</id>
    <updated>2003-06-05T12:20:32Z</updated>
    <published>2003-06-05T12:20:32Z</published>
    <title>BdbServer++: A User Driven Data Location and Retrieval Tool</title>
    <summary>  The adoption of Grid technology has the potential to greatly aid the BaBar
experiment. BdbServer was originally designed to extract copies of data from
the Objectivity/DB database at SLAC and IN2P3. With data now stored in multiple
locations in a variety of data formats, we are enhancing this tool. This will
enable users to extract selected deep copies of event collections and ship them
to the requested site using the facilities offered by the existing Grid
infrastructure. By building on the work done by various groups in BaBar, and
the European DataGrid, we have successfully expanded the capabilities of the
BdbServer software. This should provide a framework for future work in data
distribution.
</summary>
    <author>
      <name>A. D. Earl</name>
    </author>
    <author>
      <name>A. Hasan</name>
    </author>
    <author>
      <name>D. Boutigany</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper based on the poster from the 2003 Computing in High Energy and
  Nuclear Physics (CHEP03), La Jolla, Ca, USA, March 2003, 4 pages, LaTeX, 0
  figures. PSN TUCP011</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0306026v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0306026v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0307011v1</id>
    <updated>2003-07-04T13:44:04Z</updated>
    <published>2003-07-04T13:44:04Z</published>
    <title>Supporting Out-of-turn Interactions in a Multimodal Web Interface</title>
    <summary>  Multimodal interfaces are becoming increasingly important with the advent of
mobile devices, accessibility considerations, and novel software technologies
that combine diverse interaction media. This article investigates systems
support for web browsing in a multimodal interface. Specifically, we outline
the design and implementation of a software framework that integrates hyperlink
and speech modes of interaction. Instead of viewing speech as merely an
alternative interaction medium, the framework uses it to support out-of-turn
interaction, providing a flexibility of information access not possible with
hyperlinks alone. This approach enables the creation of websites that adapt to
the needs of users, yet permits the designer fine-grained control over what
interactions to support. Design methodology, implementation details, and two
case studies are presented.
</summary>
    <author>
      <name>Atul Shenoy</name>
    </author>
    <author>
      <name>Naren Ramakrishnan</name>
    </author>
    <author>
      <name>Manuel A. Perez-Quinones</name>
    </author>
    <author>
      <name>Srinidhi Varadarajan</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0307011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0307011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0308042v1</id>
    <updated>2003-08-27T13:32:29Z</updated>
    <published>2003-08-27T13:32:29Z</published>
    <title>Centralized reward system gives rise to fast and efficient work sharing
  for intelligent Internet agents lacking direct communication</title>
    <summary>  WWW has a scale-free structure where novel information is often difficult to
locate. Moreover, Intelligent agents easily get trapped in this structure. Here
a novel method is put forth, which turns these traps into information
repositories, supplies: We populated an Internet environment with intelligent
news foragers. Foraging has its associated cost whereas foragers are rewarded
if they detect not yet discovered novel information. The intelligent news
foragers crawl by using the estimated long-term cumulated reward, and also have
a finite sized memory: the list of most promising supplies. Foragers form an
artificial life community: the most successful ones are allowed to multiply,
while unsuccessful ones die out. The specific property of this community is
that there is no direct communication amongst foragers but the centralized
rewarding system. Still, fast division of work is achieved.
</summary>
    <author>
      <name>Zsolt Palotai</name>
    </author>
    <author>
      <name>Sandor Mandusitz</name>
    </author>
    <author>
      <name>Andras Lorincz</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0308042v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0308042v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; H.3.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0311029v1</id>
    <updated>2003-11-20T17:27:45Z</updated>
    <published>2003-11-20T17:27:45Z</published>
    <title>Staging Transformations for Multimodal Web Interaction Management</title>
    <summary>  Multimodal interfaces are becoming increasingly ubiquitous with the advent of
mobile devices, accessibility considerations, and novel software technologies
that combine diverse interaction media. In addition to improving access and
delivery capabilities, such interfaces enable flexible and personalized dialogs
with websites, much like a conversation between humans. In this paper, we
present a software framework for multimodal web interaction management that
supports mixed-initiative dialogs between users and websites. A
mixed-initiative dialog is one where the user and the website take turns
changing the flow of interaction. The framework supports the functional
specification and realization of such dialogs using staging transformations --
a theory for representing and reasoning about dialogs based on partial input.
It supports multiple interaction interfaces, and offers sessioning, caching,
and co-ordination functions through the use of an interaction manager. Two case
studies are presented to illustrate the promise of this approach.
</summary>
    <author>
      <name>Michael Narayan</name>
    </author>
    <author>
      <name>Chris Williams</name>
    </author>
    <author>
      <name>Saverio Perugini</name>
    </author>
    <author>
      <name>Naren Ramakrishnan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Describes framework and software architecture for multimodal web
  interaction management</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0311029v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0311029v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.4; H.5.2; F.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0402029v1</id>
    <updated>2004-02-14T03:55:53Z</updated>
    <published>2004-02-14T03:55:53Z</published>
    <title>Mapping Topics and Topic Bursts in PNAS</title>
    <summary>  Scientific research is highly dynamic. New areas of science continually
evolve;others gain or lose importance, merge or split. Due to the steady
increase in the number of scientific publications it is hard to keep an
overview of the structure and dynamic development of one's own field of
science, much less all scientific domains. However, knowledge of hot topics,
emergent research frontiers, or change of focus in certain areas is a critical
component of resource allocation decisions in research labs, governmental
institutions, and corporations. This paper demonstrates the utilization of
Kleinberg's burst detection algorithm, co-word occurrence analysis, and graph
layout techniques to generate maps that support the identification of major
research topics and trends. The approach was applied to analyze and map the
complete set of papers published in the Proceedings of the National Academy of
Sciences (PNAS) in the years 1982-2001. Six domain experts examined and
commented on the resulting maps in an attempt to reconstruct the evolution of
major research areas covered by PNAS.
</summary>
    <author>
      <name>Ketan Mane</name>
    </author>
    <author>
      <name>Katy Börner</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1073/pnas.0307626100</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1073/pnas.0307626100" rel="related"/>
    <link href="http://arxiv.org/abs/cs/0402029v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0402029v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; H.1.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0406032v1</id>
    <updated>2004-06-17T13:38:17Z</updated>
    <published>2004-06-17T13:38:17Z</published>
    <title>A Dynamic Clustering-Based Markov Model for Web Usage Mining</title>
    <summary>  Markov models have been widely utilized for modelling user web navigation
behaviour. In this work we propose a dynamic clustering-based method to
increase a Markov model's accuracy in representing a collection of user web
navigation sessions. The method makes use of the state cloning concept to
duplicate states in a way that separates in-links whose corresponding
second-order probabilities diverge. In addition, the new method incorporates a
clustering technique which determines an effcient way to assign in-links with
similar second-order probabilities to the same clone. We report on experiments
conducted with both real and random data and we provide a comparison with the
N-gram Markov concept. The results show that the number of additional states
induced by the dynamic clustering method can be controlled through a threshold
parameter, and suggest that the method's performance is linear time in the size
of the model.
</summary>
    <author>
      <name>José Borges</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">School of Engineering, University of Porto, Portuga</arxiv:affiliation>
    </author>
    <author>
      <name>Mark Levene</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Birkbeck, University of London, U.K</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/cs/0406032v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0406032v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0407053v1</id>
    <updated>2004-07-21T07:21:50Z</updated>
    <published>2004-07-21T07:21:50Z</published>
    <title>Design of a Parallel and Distributed Web Search Engine</title>
    <summary>  This paper describes the architecture of MOSE (My Own Search Engine), a
scalable parallel and distributed engine for searching the web. MOSE was
specifically designed to efficiently exploit affordable parallel architectures,
such as clusters of workstations. Its modular and scalable architecture can
easily be tuned to fulfill the bandwidth requirements of the application at
hand. Both task-parallel and data-parallel approaches are exploited within MOSE
in order to increase the throughput and efficiently use communication, storing
and computational resources. We used a collection of html documents as a
benchmark, and conducted preliminary experiments on a cluster of three SMP
Linux PCs.
</summary>
    <author>
      <name>Salvatore Orlando</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Dipartimento di Informatica, Università di Venezia - Mestre, Italy</arxiv:affiliation>
    </author>
    <author>
      <name>Raffaele Perego</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Istituto di Scienza e Tecnologia per l'Informazione</arxiv:affiliation>
    </author>
    <author>
      <name>Fabrizio Silvestri</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Dipartimento di Informatica, Università di Venezia - Mestre, Italy</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Dipartimento di Informatica, Università di Pisa, Italy</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages. In Proceedings of the 2001 Parallel Computing Conference
  (ParCo 2001), 4-7 September 2001, Naples, Italy, Imperial College Press, pp.
  197-204</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0407053v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0407053v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0407061v1</id>
    <updated>2004-07-28T13:42:26Z</updated>
    <published>2004-07-28T13:42:26Z</published>
    <title>A measure of similarity between graph vertices</title>
    <summary>  We introduce a concept of similarity between vertices of directed graphs. Let
G_A and G_B be two directed graphs. We define a similarity matrix whose (i,
j)-th real entry expresses how similar vertex j (in G_A) is to vertex i (in
G_B. The similarity matrix can be obtained as the limit of the normalized even
iterates of a linear transformation. In the special case where G_A=G_B=G, the
matrix is square and the (i, j)-th entry is the similarity score between the
vertices i and j of G. We point out that Kleinberg's "hub and authority" method
to identify web-pages relevant to a given query can be viewed as a special case
of our definition in the case where one of the graphs has two vertices and a
unique directed edge between them. In analogy to Kleinberg, we show that our
similarity scores are given by the components of a dominant eigenvector of a
non-negative matrix. Potential applications of our similarity concept are
numerous. We illustrate an application for the automatic extraction of synonyms
in a monolingual dictionary.
</summary>
    <author>
      <name>Vincent Blondel</name>
    </author>
    <author>
      <name>Anahi Gajardo</name>
    </author>
    <author>
      <name>Maureen Heymans</name>
    </author>
    <author>
      <name>Pierre Senellart</name>
    </author>
    <author>
      <name>Paul Van Dooren</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0407061v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0407061v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0408001v1</id>
    <updated>2004-07-31T14:04:04Z</updated>
    <published>2004-07-31T14:04:04Z</published>
    <title>Semantic Linking - a Context-Based Approach to Interactivity in
  Hypermedia</title>
    <summary>  The semantic Web initiates new, high level access schemes to online content
and applications. One area of superior need for a redefined content exploration
is given by on-line educational applications and their concepts of
interactivity in the framework of open hypermedia systems. In the present paper
we discuss aspects and opportunities of gaining interactivity schemes from
semantic notions of components. A transition from standard educational
annotation to semantic statements of hyperlinks is discussed. Further on we
introduce the concept of semantic link contexts as an approach to manage a
coherent rhetoric of linking. A practical implementation is introduced, as
well. Our semantic hyperlink implementation is based on the more general
Multimedia Information Repository MIR, an open hypermedia system supporting the
standards XML, Corba and JNDI.
</summary>
    <author>
      <name>Michael Engelhardt</name>
    </author>
    <author>
      <name>Thomas C. Schmidt</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">R. Tolksdorf, R. Eckstein: Proc. of Berliner XML Tage.
  Humboldt-Universitaet zu Berlin; pp. 55-66; ISBN 3-88579-116-1; Berlin; 2003</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0408001v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0408001v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.4; H.2.4; H.3.4; H.5.1; C.2.4; K.3.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0501077v2</id>
    <updated>2005-05-27T13:59:10Z</updated>
    <published>2005-01-26T14:11:38Z</published>
    <title>Ontology-Based Users &amp; Requests Clustering in Customer Service
  Management System</title>
    <summary>  Customer Service Management is one of major business activities to better
serve company customers through the introduction of reliable processes and
procedures. Today this kind of activities is implemented through e-services to
directly involve customers into business processes. Traditionally Customer
Service Management involves application of data mining techniques to discover
usage patterns from the company knowledge memory. Hence grouping of
customers/requests to clusters is one of major technique to improve the level
of company customization. The goal of this paper is to present an efficient for
implementation approach for clustering users and their requests. The approach
uses ontology as knowledge representation model to improve the semantic
interoperability between units of the company and customers. Some fragments of
the approach tested in an industrial company are also presented in the paper.
</summary>
    <author>
      <name>Alexander Smirnov</name>
    </author>
    <author>
      <name>Mikhail Pashkin</name>
    </author>
    <author>
      <name>Nikolai Chilov</name>
    </author>
    <author>
      <name>Tatiana Levashova</name>
    </author>
    <author>
      <name>Andrew Krizhanovsky</name>
    </author>
    <author>
      <name>Alexey Kashevnik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 4 figures, published in Lecture Notes in Computer Science</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Smirnov A., Pashkin M., Chilov N., Levashova T., Krizhanovsky A.,
  Kashevnik A. 2005. Ontology-Based Users and Requests Clustering in Customer
  Service Management System. Springer-Verlag GmbH, Lecture Notes in Computer
  Science, 3505: 231-246</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0501077v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0501077v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0503021v1</id>
    <updated>2005-03-08T22:44:37Z</updated>
    <published>2005-03-08T22:44:37Z</published>
    <title>Fast-Forward on the Green Road to Open Access: The Case Against Mixing
  Up Green and Gold</title>
    <summary>  This article is a critique of: "The 'Green' and 'Gold' Roads to Open Access:
The Case for Mixing and Matching" by Jean-Claude Guedon (in Serials Review
30(4) 2004).
  Open Access (OA) means: free online access to all peer-reviewed journal
articles. Jean-Claude Guedon argues against the efficacy of author
self-archiving of peer-reviewed journal articles (the "Green" road to OA). He
suggests instead that we should convert to Open Access Publishing (the "Golden"
road to OA) by "mixing and matching" Green and Gold as follows: o First,
self-archive dissertations (not published, peer-reviewed journal articles). o
Second, identify and tag how those dissertations have been evaluated and
reviewed. o Third, self-archive unrefereed preprints (not published,
peer-reviewed journal articles). o Fourth, develop new mechanisms for
evaluating and reviewing those unrefereed preprints, at multiple levels. The
result will be OA Publishing (Gold). I argue that rather than yet another 10
years of speculation like this, what is actually needed (and imminent) is for
OA self-archiving to be mandated by research funders and institutions so that
the self-archiving of published, peer-reviewed journal articles (Green) can be
fast-forwarded to 100% OA.
</summary>
    <author>
      <name>Stevan Harnad</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Ariadne 42 January 2005; http://www.ariadne.ac.uk/issue42/harnad/</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0503021v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0503021v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0504036v1</id>
    <updated>2005-04-11T13:52:55Z</updated>
    <published>2005-04-11T13:52:55Z</published>
    <title>Scientific impact quantity and quality: Analysis of two sources of
  bibliographic data</title>
    <summary>  Attempts to understand the consequence of any individual scientist's activity
within the long-term trajectory of science is one of the most difficult
questions within the philosophy of science. Because scientific publications
play such as central role in the modern enterprise of science, bibliometric
techniques which measure the ``impact'' of an individual publication as a
function of the number of citations it receives from subsequent authors have
provided some of the most useful empirical data on this question. Until
recently, Thompson/ISI has provided the only source of large-scale ``inverted''
bibliographic data of the sort required for impact analysis. In the end of
2004, Google introduced a new service, GoogleScholar, making much of this same
data available. Here we analyze 203 publications, collectively cited by more
than 4000 other publications. We show surprisingly good agreement between data
citation counts provided by the two services. Data quality across the systems
is analyzed, and potentially useful complementarities between are considered.
The additional robustness offered by multiple sources of such data promises to
increase the utility of these measurements as open citation protocols and open
access increase their impact on electronic scientific publication practices.
</summary>
    <author>
      <name>Richard K. Belew</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 1 table, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0504036v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0504036v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; H.3.7; H.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0505008v1</id>
    <updated>2005-05-02T15:27:45Z</updated>
    <published>2005-05-02T15:27:45Z</published>
    <title>Data Mining on Crash Simulation Data</title>
    <summary>  The work presented in this paper is part of the cooperative research project
AUTO-OPT carried out by twelve partners from the automotive industries. One
major work package concerns the application of data mining methods in the area
of automotive design. Suitable methods for data preparation and data analysis
are developed. The objective of the work is the re-use of data stored in the
crash-simulation department at BMW in order to gain deeper insight into the
interrelations between the geometric variations of the car during its design
and its performance in crash testing. In this paper a method for data analysis
of finite element models and results from crash simulation is proposed and
application to recent data from the industrial partner BMW is demonstrated. All
necessary steps from data pre-processing to re-integration into the working
environment of the engineer are covered.
</summary>
    <author>
      <name>A. Kuhlmann</name>
    </author>
    <author>
      <name>R. -M. Vetter</name>
    </author>
    <author>
      <name>Ch. Luebbing</name>
    </author>
    <author>
      <name>C. -A. Thole</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 10 figures. Accepted for Lecture Notes in Computer Science
  (LNCS)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Lecture Notes in Computer Science, Lecture Notes in Artificial
  Intelligence, Proceedings Conference MLDM 2005, Leipzig/Germany, Springer
  Verlag, LNAI 3587, ISBN: 3-540-26923-1, 2005</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0505008v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0505008v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8; D.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0506047v1</id>
    <updated>2005-06-12T16:39:01Z</updated>
    <published>2005-06-12T16:39:01Z</published>
    <title>Analyse et expansion des textes en question-réponse</title>
    <summary>  This paper presents an original methodology to consider question answering.
We noticed that query expansion is often incorrect because of a bad
understanding of the question. But the automatic good understanding of an
utterance is linked to the context length, and the question are often short.
This methodology proposes to analyse the documents and to construct an
informative structure from the results of the analysis and from a semantic text
expansion. The linguistic analysis identifies words (tokenization and
morphological analysis), links between words (syntactic analysis) and word
sense (semantic disambiguation). The text expansion adds to each word the
synonyms matching its sense and replaces the words in the utterances by
derivatives, modifying the syntactic schema if necessary. In this way, whatever
enrichment may be, the text keeps the same meaning, but each piece of
information matches many realisations. The questioning method consists in
constructing a local informative structure without enrichment, and matches it
with the documentary structure. If a sentence in the informative structure
matches the question structure, this sentence is the answer to the question.
</summary>
    <author>
      <name>Bernard Jacquemin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ISC</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pp</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Le poids des mots. Actes des 7es journ\'{e}es internationales
  d'Analyse statistique des Donn\'{e}es Textuelles (2004) 1219</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0506047v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0506047v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3; H.4; H.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0506048v1</id>
    <updated>2005-06-12T16:44:05Z</updated>
    <published>2005-06-12T16:44:05Z</published>
    <title>Enriching a Text by Semantic Disambiguation for Information Extraction</title>
    <summary>  External linguistic resources have been used for a very long time in
information extraction. These methods enrich a document with data that are
semantically equivalent, in order to improve recall. For instance, some of
these methods use synonym dictionaries. These dictionaries enrich a sentence
with words that have a similar meaning. However, these methods present some
serious drawbacks, since words are usually synonyms only in restricted
contexts. The method we propose here consists of using word sense
disambiguation rules (WSD) to restrict the selection of synonyms to only these
that match a specific syntactico-semantic context. We show how WSD rules are
built and how information extraction techniques can benefit from the
application of these rules.
</summary>
    <author>
      <name>Bernard Jacquemin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ISC</arxiv:affiliation>
    </author>
    <author>
      <name>Caroline Brun</name>
    </author>
    <author>
      <name>Claude Roux</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pp</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">LREC 2002 Workshop Proceedings "Using semantics for informaiton
  retrival and filtering" (2002) 45-51</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0506048v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0506048v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3; H.4; H.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0506078v1</id>
    <updated>2005-06-20T14:54:41Z</updated>
    <published>2005-06-20T14:54:41Z</published>
    <title>Dynamical Neural Network: Information and Topology</title>
    <summary>  A neural network works as an associative memory device if it has large
storage capacity and the quality of the retrieval is good enough. The learning
and attractor abilities of the network both can be measured by the mutual
information (MI), between patterns and retrieval states. This paper deals with
a search for an optimal topology, of a Hebb network, in the sense of the
maximal MI. We use small-world topology. The connectivity $\gamma$ ranges from
an extremely diluted to the fully connected network; the randomness $\omega$
ranges from purely local to completely random neighbors. It is found that,
while stability implies an optimal $MI(\gamma,\omega)$ at
$\gamma_{opt}(\omega)\to 0$, for the dynamics, the optimal topology holds at
certain $\gamma_{opt}&gt;0$ whenever $0\leq\omega&lt;0.3$.
</summary>
    <author>
      <name>David Dominguez</name>
    </author>
    <author>
      <name>Kostadin Koroutchev</name>
    </author>
    <author>
      <name>Eduardo Serrano</name>
    </author>
    <author>
      <name>Francisco B. Rodriguez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10pg, 5fig</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0506078v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0506078v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0507024v2</id>
    <updated>2005-08-09T13:52:09Z</updated>
    <published>2005-07-08T13:42:42Z</published>
    <title>Experiments in Clustering Homogeneous XML Documents to Validate an
  Existing Typology</title>
    <summary>  This paper presents some experiments in clustering homogeneous XMLdocuments
to validate an existing classification or more generally anorganisational
structure. Our approach integrates techniques for extracting knowledge from
documents with unsupervised classification (clustering) of documents. We focus
on the feature selection used for representing documents and its impact on the
emerging classification. We mix the selection of structured features with fine
textual selection based on syntactic characteristics.We illustrate and evaluate
this approach with a collection of Inria activity reports for the year 2003.
The objective is to cluster projects into larger groups (Themes), based on the
keywords or different chapters of these activity reports. We then compare the
results of clustering using different feature selections, with the official
theme structure used by Inria.
</summary>
    <author>
      <name>Thierry Despeyroux</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt / INRIA Sophia Antipolis</arxiv:affiliation>
    </author>
    <author>
      <name>Yves Lechevallier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt / INRIA Sophia Antipolis</arxiv:affiliation>
    </author>
    <author>
      <name>Brigitte Trousse</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt / INRIA Sophia Antipolis</arxiv:affiliation>
    </author>
    <author>
      <name>Anne-Marie Vercoustre</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt / INRIA Sophia Antipolis</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">(postprint); This version corrects a couple of errors in authors'
  names in the bibliography</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0507024v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0507024v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0508017v1</id>
    <updated>2005-08-02T15:05:18Z</updated>
    <published>2005-08-02T15:05:18Z</published>
    <title>Enhancing Content-And-Structure Information Retrieval using a Native XML
  Database</title>
    <summary>  Three approaches to content-and-structure XML retrieval are analysed in this
paper: first by using Zettair, a full-text information retrieval system; second
by using eXist, a native XML database, and third by using a hybrid XML
retrieval system that uses eXist to produce the final answers from likely
relevant articles retrieved by Zettair. INEX 2003 content-and-structure topics
can be classified in two categories: the first retrieving full articles as
final answers, and the second retrieving more specific elements within articles
as final answers. We show that for both topic categories our initial hybrid
system improves the retrieval effectiveness of a native XML database. For
ranking the final answer elements, we propose and evaluate a novel retrieval
model that utilises the structural relationships between the answer elements of
a native XML database and retrieves Coherent Retrieval Elements. The final
results of our experiments show that when the XML retrieval task focusses on
highly relevant elements our hybrid XML retrieval system with the Coherent
Retrieval Elements module is 1.8 times more effective than Zettair and 3 times
more effective than eXist, and yields an effective content-and-structure XML
retrieval.
</summary>
    <author>
      <name>Jovan Pehcevski</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">RMIT</arxiv:affiliation>
    </author>
    <author>
      <name>James A. Thom</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">RMIT</arxiv:affiliation>
    </author>
    <author>
      <name>Anne-Marie Vercoustre</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0508017v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0508017v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0508036v2</id>
    <updated>2005-08-09T13:15:48Z</updated>
    <published>2005-08-04T14:14:59Z</published>
    <title>Expériences de classification d'une collection de documents XML de
  structure homogène</title>
    <summary>  This paper presents some experiments in clustering homogeneous XMLdocuments
to validate an existing classification or more generally anorganisational
structure. Our approach integrates techniques for extracting knowledge from
documents with unsupervised classification (clustering) of documents. We focus
on the feature selection used for representing documents and its impact on the
emerging classification. We mix the selection of structured features with fine
textual selection based on syntactic characteristics.We illustrate and evaluate
this approach with a collection of Inria activity reports for the year 2003.
The objective is to cluster projects into larger groups (Themes), based on the
keywords or different chapters of these activity reports. We then compare the
results of clustering using different feature selections, with the official
theme structure used by Inria.
</summary>
    <author>
      <name>Thierry Despeyroux</name>
    </author>
    <author>
      <name>Yves Lechevallier</name>
    </author>
    <author>
      <name>Brigitte Trousse</name>
    </author>
    <author>
      <name>Anne-Marie Vercoustre</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Cette version corrige des erreurs dans le nom de 2 auteurs cites dans
  la bibliographie</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans 5\`{e}me Journ\'{e}es d' Extraction et de Gestion des
  Connaissances (EGC 2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0508036v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0508036v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0509005v1</id>
    <updated>2005-09-02T08:24:07Z</updated>
    <published>2005-09-02T08:24:07Z</published>
    <title>Combining Structured Corporate Data and Document Content to Improve
  Expertise Finding</title>
    <summary>  In this paper, we present an algorithm for automatically building expertise
evidence for finding experts within an organization by combining structured
corporate information with different content. We also describe our test data
collection and our evaluation method. Evaluation of the algorithm shows that
using organizational structure leads to a significant improvement in the
precision of finding an expert. Furthermore we evaluate the impact of using
different data sources on the quality of the results and conclude that Expert
Finding is not a "one engine fits all" solution. It requires an analysis of the
information space into which a solution will be placed and the appropriate
selection and weighting scheme of the data sources.
</summary>
    <author>
      <name>Alistair McLean</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CSIRO Ict Center</arxiv:affiliation>
    </author>
    <author>
      <name>Mingfang Wu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CSIRO Ict Center</arxiv:affiliation>
    </author>
    <author>
      <name>Anne-Marie Vercoustre</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CSIRO Ict Center</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">long version of the ADCS'03 paper</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0509005v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0509005v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
