<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acs.DB%26id_list%3D%26start%3D0%26max_results%3D500" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:cs.DB&amp;id_list=&amp;start=0&amp;max_results=500</title>
  <id>http://arxiv.org/api/Iu0ODpGX6RqeWuorUsjlImK/0JE</id>
  <updated>2017-10-08T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">3353</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">500</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/0709.4655v1</id>
    <updated>2007-09-28T17:08:39Z</updated>
    <published>2007-09-28T17:08:39Z</published>
    <title>Mining for trees in a graph is NP-complete</title>
    <summary>  Mining for trees in a graph is shown to be NP-complete.
</summary>
    <author>
      <name>Jan Van den Bussche</name>
    </author>
    <link href="http://arxiv.org/abs/0709.4655v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0709.4655v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.2764v2</id>
    <updated>2014-08-11T19:55:33Z</updated>
    <published>2010-09-14T20:15:14Z</published>
    <title>A Blink Tree latch method and protocol to support synchronous node
  deletion</title>
    <summary>  A Blink Tree latch method and protocol supports synchronous node deletion in
a high concurrency environment. Full source code is available.
</summary>
    <author>
      <name>Karl Malbrain</name>
    </author>
    <link href="http://arxiv.org/abs/1009.2764v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.2764v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.1575v1</id>
    <updated>2013-01-06T04:03:29Z</updated>
    <published>2013-01-06T04:03:29Z</published>
    <title>BigDB: Automatic Machine Learning Optimizer</title>
    <summary>  In this short vision paper, we introduce a machine learning optimizer for
data management and describe its architecture and main functionality.
</summary>
    <author>
      <name>Anna Pyayt</name>
    </author>
    <author>
      <name>Michael Gubanov</name>
    </author>
    <link href="http://arxiv.org/abs/1301.1575v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.1575v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.5313v1</id>
    <updated>2013-03-21T15:56:54Z</updated>
    <published>2013-03-21T15:56:54Z</published>
    <title>Incremental Maintenance for Leapfrog Triejoin</title>
    <summary>  We present an incremental maintenance algorithm for leapfrog triejoin. The
algorithm maintains rules in time proportional (modulo log factors) to the edit
distance between leapfrog triejoin traces.
</summary>
    <author>
      <name>Todd L. Veldhuizen</name>
    </author>
    <link href="http://arxiv.org/abs/1303.5313v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.5313v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.6778v1</id>
    <updated>2014-06-26T06:09:26Z</updated>
    <published>2014-06-26T06:09:26Z</published>
    <title>Performance Comparison of Two Streaming Data Clustering Algorithms</title>
    <summary>  The weighted fuzzy c-mean clustering algorithm and weighted fuzzy
c-mean-adaptive cluster number are extension of traditional fuzzy c-mean
Algorithm to stream data clustering algorithm.
</summary>
    <author>
      <name>Chandrakant Mahobiya</name>
    </author>
    <author>
      <name>M. Kumar</name>
    </author>
    <link href="http://arxiv.org/abs/1406.6778v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.6778v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.4468v1</id>
    <updated>2014-08-19T20:14:34Z</updated>
    <published>2014-08-19T20:14:34Z</published>
    <title>Undecidability of Finite Model Reasoning in DLFD</title>
    <summary>  We resolve an open problem concerning finite logical implication for path
functional dependencies (PFDs).
</summary>
    <author>
      <name>David Toman</name>
    </author>
    <author>
      <name>Grant Weddell</name>
    </author>
    <link href="http://arxiv.org/abs/1408.4468v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.4468v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.01229v1</id>
    <updated>2016-05-04T11:27:45Z</updated>
    <published>2016-05-04T11:27:45Z</published>
    <title>The lifecycle of provenance metadata and its associated challenges and
  opportunities</title>
    <summary>  This chapter outlines some of the challenges and opportunities associated
with adopting provenance principles and standards in a variety of disciplines,
including data publication and reuse, and information sciences.
</summary>
    <author>
      <name>Paolo Missier</name>
    </author>
    <link href="http://arxiv.org/abs/1605.01229v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.01229v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.00295v1</id>
    <updated>2016-06-01T14:07:56Z</updated>
    <published>2016-06-01T14:07:56Z</published>
    <title>A Review of Star Schema Benchmark</title>
    <summary>  This paper examines the Star Schema Benchmark, an alternative to the flawed
TPC-H decision support system and presents reasons why this benchmark should be
adopted over the industry standard for decision support systems.
</summary>
    <author>
      <name>Jimi Sanchez</name>
    </author>
    <link href="http://arxiv.org/abs/1606.00295v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.00295v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0402051v2</id>
    <updated>2004-05-11T20:56:14Z</updated>
    <published>2004-02-20T19:45:56Z</published>
    <title>Nested Intervals Tree Encoding with Continued Fractions</title>
    <summary>  We introduce a new variation of Tree Encoding with Nested Intervals, find
connections with Materialized Path, and suggest a method for moving parts of
the hierarchy.
</summary>
    <author>
      <name>Vadim Tropashko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0402051v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0402051v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0501053v3</id>
    <updated>2005-02-04T01:12:52Z</updated>
    <published>2005-01-21T20:07:32Z</published>
    <title>Relational Algebra as non-Distributive Lattice</title>
    <summary>  We reduce the set of classic relational algebra operators to two binary
operations: natural join and generalized union. We further demonstrate that
this set of operators is relationally complete and honors lattice axioms.
</summary>
    <author>
      <name>Vadim Tropashko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0501053v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0501053v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0701165v1</id>
    <updated>2007-01-26T00:23:07Z</updated>
    <published>2007-01-26T00:23:07Z</published>
    <title>Petascale Computational Systems</title>
    <summary>  Computational science is changing to be data intensive. Super-Computers must
be balanced systems; not just CPU farms but also petascale IO and networking
arrays. Anyone building CyberInfrastructure should allocate resources to
support a balanced Tier-1 through Tier-3 design.
</summary>
    <author>
      <name>Gordon Bell</name>
    </author>
    <author>
      <name>Jim Gray</name>
    </author>
    <author>
      <name>Alex Szalay</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0701165v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0701165v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0703103v3</id>
    <updated>2009-06-30T00:07:21Z</updated>
    <published>2007-03-22T03:31:27Z</published>
    <title>Concept of a Value in Multilevel Security Databases</title>
    <summary>  This paper has been withdrawn.
</summary>
    <author>
      <name>Jia Tao</name>
    </author>
    <author>
      <name>Shashi Gadia</name>
    </author>
    <author>
      <name>Tsz Shing Cheng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0703103v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0703103v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.2101v1</id>
    <updated>2014-01-09T17:55:33Z</updated>
    <published>2014-01-09T17:55:33Z</published>
    <title>NoSQL Databases</title>
    <summary>  In this document, I present the main notions of NoSQL databases and compare
four selected products (Riak, MongoDB, Cassandra, Neo4J) according to their
capabilities with respect to consistency, availability, and partition
tolerance, as well as performance. I also propose a few criteria for selecting
the right tool for the right situation.
</summary>
    <author>
      <name>Massimo Carro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">57 pages, 18 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.2101v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.2101v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.01817v2</id>
    <updated>2015-01-12T10:34:27Z</updated>
    <published>2015-01-08T12:28:45Z</published>
    <title>The Hunt for a Red Spider: Conjunctive Query Determinacy Is Undecidable</title>
    <summary>  We solve a well known, long-standing open problem in relational databases
theory, showing that the conjunctive query determinacy problem (in its
"unrestricted" version) is undecidable.
</summary>
    <author>
      <name>Tomasz Gogacz</name>
    </author>
    <author>
      <name>Jerzy Marcinkowski</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/LICS.2015.35</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/LICS.2015.35" rel="related"/>
    <link href="http://arxiv.org/abs/1501.01817v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.01817v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.00707v1</id>
    <updated>2016-01-05T00:17:21Z</updated>
    <published>2016-01-05T00:17:21Z</published>
    <title>A Survey of RDF Data Management Systems</title>
    <summary>  RDF is increasingly being used to encode data for the semantic web and for
data exchange. There have been a large number of works that address RDF data
management. In this paper we provide an overview of these works.
</summary>
    <author>
      <name>M. Tamer Özsu</name>
    </author>
    <link href="http://arxiv.org/abs/1601.00707v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.00707v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.00740v1</id>
    <updated>2016-06-02T16:07:15Z</updated>
    <published>2016-06-02T16:07:15Z</published>
    <title>The Meaning of Null in Databases and Programming Languages</title>
    <summary>  The meaning of null in relational databases is a major source of confusion
not only among database users but also among database textbook writers. The
purpose of this article is to examine what database nulls could mean and to
make some modest suggestions about how to reduce the confusion.
</summary>
    <author>
      <name>Kenneth Baclawski</name>
    </author>
    <link href="http://arxiv.org/abs/1606.00740v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.00740v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.3; H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.05136v2</id>
    <updated>2017-06-26T17:05:39Z</updated>
    <published>2017-04-17T21:58:45Z</published>
    <title>The Causality/Repair Connection in Databases: Causality-Programs</title>
    <summary>  In this work, answer-set programs that specify repairs of databases are used
as a basis for solving computational and reasoning problems about causes for
query answers from databases.
</summary>
    <author>
      <name>Leopoldo Bertossi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Proc. SUM'17 as short paper, 7-pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.05136v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.05136v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.03317v1</id>
    <updated>2017-06-11T07:28:48Z</updated>
    <published>2017-06-11T07:28:48Z</published>
    <title>Fault Tolerant Consensus Agreement Algorithm</title>
    <summary>  Recently a new fault tolerant and simple mechanism was designed for solving
commit consensus problem. It is based on replicated validation of messages sent
between transaction participants and a special dispatcher validator manager
node. This paper presents a correctness, safety proofs and performance analysis
of this algorithm.
</summary>
    <author>
      <name>Marius Rafailescu</name>
    </author>
    <link href="http://arxiv.org/abs/1706.03317v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.03317v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.01223v1</id>
    <updated>2017-07-05T06:03:02Z</updated>
    <published>2017-07-05T06:03:02Z</published>
    <title>Eclipse: Practicability Beyond 1NN and Skyline</title>
    <summary>  In this paper, we propose a novel Eclipse query which is more practical than
the existing 1-NN query and skyline query. In addition, we present a few
properties of Eclipse query and the general idea for computing Eclipse.
</summary>
    <author>
      <name>Jinfei Liu</name>
    </author>
    <author>
      <name>Li Xiong</name>
    </author>
    <author>
      <name>Qiuchen Zhang</name>
    </author>
    <author>
      <name>Jun Luo</name>
    </author>
    <link href="http://arxiv.org/abs/1707.01223v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.01223v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0106046v1</id>
    <updated>2001-06-21T16:33:18Z</updated>
    <published>2001-06-21T16:33:18Z</published>
    <title>Expressing the cone radius in the relational calculus with real
  polynomial constraints</title>
    <summary>  We show that there is a query expressible in first-order logic over the reals
that returns, on any given semi-algebraic set A, for every point a radius
around which A is conical. We obtain this result by combining famous results
from calculus and real algebraic geometry, notably Sard's theorem and Thom's
first isotopy lemma, with recent algorithmic results by Rannou.
</summary>
    <author>
      <name>Floris Geerts</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0106046v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0106046v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.3; H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0306034v1</id>
    <updated>2003-06-07T15:09:16Z</updated>
    <published>2003-06-07T15:09:16Z</published>
    <title>A ROOT/IO Based Software Framework for CMS</title>
    <summary>  The implementation of persistency in the Compact Muon Solenoid (CMS) Software
Framework uses the core I/O functionality of ROOT. We will discuss the current
ROOT/IO implementation, its evolution from the prior Objectivity/DB
implementation, and the plans and ongoing work for the conversion to "POOL",
provided by the LHC Computing Grid (LCG) persistency project.
</summary>
    <author>
      <name>William Tanenbaum</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ECONFC0303241:TUKT010,2003</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0306034v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0306034v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0310012v1</id>
    <updated>2003-10-08T00:18:31Z</updated>
    <published>2003-10-08T00:18:31Z</published>
    <title>A Formal Comparison of Visual Web Wrapper Generators</title>
    <summary>  We study the core fragment of the Elog wrapping language used in the Lixto
system (a visual wrapper generator) and formally compare Elog to other wrapping
languages proposed in the literature.
</summary>
    <author>
      <name>Georg Gottlob</name>
    </author>
    <author>
      <name>Christoph Koch</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 0 figures, second part of long version of PODS 2002 paper
  "Monadic Datalog and the Expressive Power of Languages for Web Information
  Extraction". First part accepted for JACM</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0310012v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0310012v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.1, F.4.1, F.4.3, H.2.3, I.7.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0406029v1</id>
    <updated>2004-06-17T09:11:49Z</updated>
    <published>2004-06-17T09:11:49Z</published>
    <title>Subset Queries in Relational Databases</title>
    <summary>  In this paper, we motivated the need for relational database systems to
support subset query processing. We defined new operators in relational
algebra, and new constructs in SQL for expressing subset queries. We also
illustrated the applicability of subset queries through different examples
expressed using extended SQL statements and relational algebra expressions. Our
aim is to show the utility of subset queries for next generation applications.
</summary>
    <author>
      <name>Satyanarayana R Valluri</name>
    </author>
    <author>
      <name>Kamalakar Karlapalem</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0406029v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0406029v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H2.3, H2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0410038v1</id>
    <updated>2004-10-16T13:14:54Z</updated>
    <published>2004-10-16T13:14:54Z</published>
    <title>Frequent Knot Discovery</title>
    <summary>  We explore the possibility of applying the framework of frequent pattern
mining to a class of continuous objects appearing in nature, namely knots. We
introduce the frequent knot mining problem and present a solution. The key
observation is that a database consisting of knots can be transformed into a
transactional database. This observation is based on the Prime Decomposition
Theorem of knots.
</summary>
    <author>
      <name>Floris Geerts</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures, recreational data mining</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0410038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0410038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0410070v1</id>
    <updated>2004-10-26T17:00:40Z</updated>
    <published>2004-10-26T17:00:40Z</published>
    <title>Using image partitions in 4th Dimension</title>
    <summary>  I have plotted an image by using mathematical functions in the Database "4th
Dimension". I'm going to show an alternative method to: detect which sector has
been clicked; highlight it and combine it with other sectors already
highlighted; store the graph information in an efficient way; load and splat
image layers to reconstruct the stored graph.
</summary>
    <author>
      <name>Giovanni Gasparri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0410070v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0410070v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0505059v1</id>
    <updated>2005-05-23T14:19:24Z</updated>
    <published>2005-05-23T14:19:24Z</published>
    <title>Consistent query answers on numerical databases under aggregate
  constraints</title>
    <summary>  The problem of extracting consistent information from relational databases
violating integrity constraints on numerical data is addressed. In particular,
aggregate constraints defined as linear inequalities on aggregate-sum queries
on input data are considered. The notion of repair as consistent set of updates
at attribute-value level is exploited, and the characterization of several
complexity issues related to repairing data and computing consistent query
answers is provided.
</summary>
    <author>
      <name>Sergio Flesca</name>
    </author>
    <author>
      <name>Filippo Furfaro</name>
    </author>
    <author>
      <name>Francesco Parisi</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0505059v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0505059v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0602039v1</id>
    <updated>2006-02-10T12:21:42Z</updated>
    <published>2006-02-10T12:21:42Z</published>
    <title>Path Summaries and Path Partitioning in Modern XML Databases</title>
    <summary>  We study the applicability of XML path summaries in the context of
current-day XML databases. We find that summaries provide an excellent basis
for optimizing data access methods, which furthermore mixes very well with
path-partitioned stores. We provide practical algorithms for building and
exploiting summaries, and prove its benefits through extensive experiments.
</summary>
    <author>
      <name>Andrei Arion</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Futurs</arxiv:affiliation>
    </author>
    <author>
      <name>Angela Bonifati</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Futurs</arxiv:affiliation>
    </author>
    <author>
      <name>Ioana Manolescu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Futurs</arxiv:affiliation>
    </author>
    <author>
      <name>Andrea Pugliese</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0602039v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0602039v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0603044v2</id>
    <updated>2006-03-15T04:54:50Z</updated>
    <published>2006-03-10T18:11:49Z</published>
    <title>First Steps in Relational Lattice</title>
    <summary>  Relational lattice reduces the set of six classic relational algebra
operators to two binary lattice operations: natural join and inner union. We
give an introduction to this theory with emphasis on formal algebraic laws. New
results include Spight distributivity criteria and its applications to query
transformations.
</summary>
    <author>
      <name>Marshall Spight</name>
    </author>
    <author>
      <name>Vadim Tropashko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0603044v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0603044v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0612102v2</id>
    <updated>2007-01-13T06:51:37Z</updated>
    <published>2006-12-20T21:11:05Z</published>
    <title>The Dichotomy of Conjunctive Queries on Probabilistic Structures</title>
    <summary>  We show that for every conjunctive query, the complexity of evaluating it on
a probabilistic database is either \PTIME or #\P-complete, and we give an
algorithm for deciding whether a given conjunctive query is \PTIME or
#\P-complete. The dichotomy property is a fundamental result on query
evaluation on probabilistic databases and it gives a complete classification of
the complexity of conjunctive queries.
</summary>
    <author>
      <name>Nilesh Dalvi</name>
    </author>
    <author>
      <name>Dan Suciu</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0612102v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0612102v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0701161v1</id>
    <updated>2007-01-25T23:51:22Z</updated>
    <published>2007-01-25T23:51:22Z</published>
    <title>Thousands of DebitCredit Transactions-Per-Second: Easy and Inexpensive</title>
    <summary>  A $2k computer can execute about 8k transactions per second. This is 80x more
than one of the largest US bank's 1970's traffic - it approximates the total US
1970's financial transaction volume. Very modest modern computers can easily
solve yesterday's problems.
</summary>
    <author>
      <name>Jim Gray</name>
    </author>
    <author>
      <name>Charles Levine</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0701161v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0701161v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0701162v1</id>
    <updated>2007-01-25T23:57:15Z</updated>
    <published>2007-01-25T23:57:15Z</published>
    <title>A Measure of Transaction Processing 20 Years Later</title>
    <summary>  This provides a retrospective of the paper "A Measure of Transaction
Processing" published in 1985. It shows that transaction processing peak
performance and price-peformance have improved about 100,000x respectively and
that sort/sequential performance has approximately doubled each year (so a
million fold improvement) even though processor performance plateaued in 1995.
</summary>
    <author>
      <name>Jim Gray</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This article appeared in the IEEE Data Engineering, Fall 2005</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0701162v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0701162v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0803.3404v2</id>
    <updated>2009-06-09T17:03:07Z</updated>
    <published>2008-03-24T13:59:53Z</published>
    <title>Some results on $\mathbb{R}$-computable structures</title>
    <summary>  This survey paper examines the effective model theory obtained with the BSS
model of real number computation. It treats the following topics: computable
ordinals, satisfaction of computable infinitary formulas, forcing as a
construction technique, effective categoricity, effective topology, and
relations with other models for the effective theory of uncountable structures.
</summary>
    <author>
      <name>Wesley Calvert</name>
    </author>
    <author>
      <name>John E. Porter</name>
    </author>
    <link href="http://arxiv.org/abs/0803.3404v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0803.3404v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0805.1593v2</id>
    <updated>2008-06-19T05:30:49Z</updated>
    <published>2008-05-12T08:52:16Z</published>
    <title>On the Probability Distribution of Superimposed Random Codes</title>
    <summary>  A systematic study of the probability distribution of superimposed random
codes is presented through the use of generating functions. Special attention
is paid to the cases of either uniformly distributed but not necessarily
independent or non uniform but independent bit structures. Recommendations for
optimal coding strategies are derived.
</summary>
    <author>
      <name>Bernd Günther</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TIT.2008.924658</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TIT.2008.924658" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Trans. Inf. Theory, 54(7):3206--3210, 2008</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0805.1593v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0805.1593v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0807.3795v1</id>
    <updated>2008-07-24T05:24:34Z</updated>
    <published>2008-07-24T05:24:34Z</published>
    <title>Relational Lattice Axioms</title>
    <summary>  Relational lattice is a formal mathematical model for Relational algebra. It
reduces the set of six classic relational algebra operators to two: natural
join and inner union. We continue to investigate Relational lattice properties
with emphasis onto axiomatic definition. New results include additional axioms,
equational definition for set difference (more generally anti-join), and case
study demonstrating application of the relational lattice theory for query
transformations.
</summary>
    <author>
      <name>Marshall Spight</name>
    </author>
    <author>
      <name>Vadim Tropashko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0807.3795v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0807.3795v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0807.4620v1</id>
    <updated>2008-07-29T11:22:01Z</updated>
    <published>2008-07-29T11:22:01Z</published>
    <title>A Compositional Query Algebra for Second-Order Logic and Uncertain
  Databases</title>
    <summary>  World-set algebra is a variable-free query language for uncertain databases.
It constitutes the core of the query language implemented in MayBMS, an
uncertain database system. This paper shows that world-set algebra captures
exactly second-order logic over finite structures, or equivalently, the
polynomial hierarchy. The proofs also imply that world-set algebra is closed
under composition, a previously open problem.
</summary>
    <author>
      <name>Christoph Koch</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/0807.4620v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0807.4620v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0812.0438v1</id>
    <updated>2008-12-02T09:00:27Z</updated>
    <published>2008-12-02T09:00:27Z</published>
    <title>An Introduction to Knowledge Management</title>
    <summary>  Knowledge has been lately recognized as one of the most important assets of
organizations. Managing knowledge has grown to be imperative for the success of
a company. This paper presents an overview of Knowledge Management and various
aspects of secure knowledge management. A case study of knowledge management
activities at Tata Steel is also discussed
</summary>
    <author>
      <name>Sabu M. Thampi</name>
    </author>
    <link href="http://arxiv.org/abs/0812.0438v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0812.0438v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0812.4986v1</id>
    <updated>2008-12-29T23:14:00Z</updated>
    <published>2008-12-29T23:14:00Z</published>
    <title>An Array Algebra</title>
    <summary>  This is a proposal of an algebra which aims at distributed array processing.
The focus lies on re-arranging and distributing array data, which may be
multi-dimensional. The context of the work is scientific processing; thus, the
core science operations are assumed to be taken care of in external libraries
or languages. A main design driver is the desire to carry over some of the
strategies of the relational algebra into the array domain.
</summary>
    <author>
      <name>Albrecht Schmidt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Five pages, no figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0812.4986v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0812.4986v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.1059v1</id>
    <updated>2009-03-05T18:33:52Z</updated>
    <published>2009-03-05T18:33:52Z</published>
    <title>Home Heating Systems Design using PHP and MySQL Databases</title>
    <summary>  This paper presents the use of a computer application based on a MySQL
database, managed by PHP programs, allowing the selection of a heating device
using coefficient-based calculus.
</summary>
    <author>
      <name>Tiberiu Marius Karnyanszky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages (121-128), 5th "Actualities and Perspectives in Hard and
  Soft", 2007</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Ann. Univ. Tibiscus Comp. Sci. Series 5 (2007), 121-128</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0903.1059v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.1059v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.4880v1</id>
    <updated>2010-01-27T09:33:54Z</updated>
    <published>2010-01-27T09:33:54Z</published>
    <title>The WebContent XML Store</title>
    <summary>  In this article, we describe the XML storage system used in the WebContent
project. We begin by advocating the use of an XML database in order to store
WebContent documents, and we present two different ways of storing and querying
these documents : the use of a centralized XML database and the use of a P2P
XML database.
</summary>
    <author>
      <name>Benjamin Nguyen</name>
    </author>
    <author>
      <name>Spyros Zoupanos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Must be compiled with pdflatex</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">RFIA 2010 Workshop "Sources Ouvertes et Services"</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1001.4880v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.4880v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.2; H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.4718v1</id>
    <updated>2010-04-27T05:56:24Z</updated>
    <published>2010-04-27T05:56:24Z</published>
    <title>A Data Cleansing Method for Clustering Large-scale Transaction Databases</title>
    <summary>  In this paper, we emphasize the need for data cleansing when clustering
large-scale transaction databases and propose a new data cleansing method that
improves clustering quality and performance. We evaluate our data cleansing
method through a series of experiments. As a result, the clustering quality and
performance were significantly improved by up to 165% and 330%, respectively.
</summary>
    <author>
      <name>Woong-Kee Loh</name>
    </author>
    <author>
      <name>Yang-Sae Moon</name>
    </author>
    <author>
      <name>Jun-Gyu Kang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1587/transinf.E93.D.3120</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1587/transinf.E93.D.3120" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1004.4718v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.4718v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.0198v1</id>
    <updated>2010-05-03T06:36:10Z</updated>
    <published>2010-05-03T06:36:10Z</published>
    <title>Personnalisation de Systèmes OLAP Annotés</title>
    <summary>  This paper deals with personalization of annotated OLAP systems. Data
constellation is extended to support annotations and user preferences.
Annotations reflect the decision-maker experience whereas user preferences
enable users to focus on the most interesting data. User preferences allow
annotated contextual recommendations helping the decision-maker during his/her
multidimensional navigations.
</summary>
    <author>
      <name>Houssem Jerbi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <author>
      <name>Geneviève Pujolle</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <author>
      <name>Franck Ravat</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <author>
      <name>Olivier Teste</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">XXVIII\`eme Congr\`es Informatique des Organisations et Syst\`emes
  d'Information et de D\'ecision - INFORSID'10, Marseille : France (2010)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1005.0198v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.0198v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.0575v1</id>
    <updated>2010-06-03T07:59:15Z</updated>
    <published>2010-06-03T07:59:15Z</published>
    <title>XQ2P: Efficient XQuery P2P Time Series Processing</title>
    <summary>  In this demonstration, we propose a model for the management of XML time
series (TS), using the new XQuery 1.1 window operator. We argue that
centralized computation is slow, and demonstrate XQ2P, our prototype of
efficient XQuery P2P TS computation in the context of financial analysis of
large data sets (&gt;1M values).
</summary>
    <author>
      <name>Bogdan Butnaru</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PRISM</arxiv:affiliation>
    </author>
    <author>
      <name>Benjamin Nguyen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PRISM</arxiv:affiliation>
    </author>
    <author>
      <name>Georges Gardarin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PRISM</arxiv:affiliation>
    </author>
    <author>
      <name>Laurent Yeh</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PRISM</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Bases de Donn\'ees Avanc\'ees (D\'emonstration), Namur : Belgium
  (2009)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1006.0575v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.0575v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.2077v1</id>
    <updated>2010-06-10T16:16:37Z</updated>
    <published>2010-06-10T16:16:37Z</published>
    <title>Multidimensi Pada Data Warehouse Dengan Menggunakan Rumus Kombinasi</title>
    <summary>  Multidimensional in data warehouse is a compulsion and become the most
important for information delivery, without multidimensional data warehouse is
incomplete. Multidimensional give the able to analyze business measurement in
many different ways. Multidimensional is also synonymous with online analytical
processing (OLAP).
</summary>
    <author>
      <name>H. L. H Spits Warnars</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The 2nd National Seminar Information Technology Application
  (SNATI) 2006, University of Islam Indonesia, pp. J1-J6, Yogyakarta, 17 June
  2006</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1006.2077v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.2077v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1008.1339v1</id>
    <updated>2010-08-07T13:02:01Z</updated>
    <published>2010-08-07T13:02:01Z</published>
    <title>Removal of Communication Gap</title>
    <summary>  This research is about an online forum designed and developed to improve the
communication process between alumni, new, old and upcoming students. In this
research paper we present targeted problems, designed architecture, used
technologies in development and final end product in detail.
</summary>
    <author>
      <name>Zeeshan Ahmed</name>
    </author>
    <author>
      <name>Sudhir Ganti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In the 5th Virtual Conference of the EU-funded FP6 I*PROMS Network of
  Excellence on Innovative Production Machines and Systems, IPROMS 2009, 6-17
  July, 2009</arxiv:comment>
    <link href="http://arxiv.org/abs/1008.1339v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1008.1339v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.4916v2</id>
    <updated>2012-03-09T10:40:21Z</updated>
    <published>2011-03-25T07:10:50Z</published>
    <title>Detection of Spatial Changes using Spatial Data Mining</title>
    <summary>  The Change detection based on analysis and samples are analyzed. Land
use/cover change detection based on SDM is discussed.
</summary>
    <author>
      <name>B. G. Kodge</name>
    </author>
    <author>
      <name>P. S. Hiremath</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by the author</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Advances in Information Mining, ISSN: 0975-3265, Volume 2, Issue
  2, 2010, pp-14-18</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1103.4916v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.4916v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8; I.5.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.5619v1</id>
    <updated>2011-08-26T07:06:03Z</updated>
    <published>2011-08-26T07:06:03Z</published>
    <title>Modification of GTD from Flat File Format to OLAP for Data Mining</title>
    <summary>  This document is part of original research work by the authors in a bid to
explore new fields for applying Data Mining Techniques. The sample data is part
of a large data set from University of Maryland (UMD) and outlines how more
meaningful patterns can be discovered by preprocessing the data in the form of
OLAP cubes.
</summary>
    <author>
      <name>Karanjit Singh</name>
    </author>
    <author>
      <name>Shuchita Bhasin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">GTD, OLAP, Data Mining, Terror Databases</arxiv:comment>
    <link href="http://arxiv.org/abs/1108.5619v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.5619v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.0242v1</id>
    <updated>2012-02-01T18:46:41Z</updated>
    <published>2012-02-01T18:46:41Z</published>
    <title>Weak Forms of Monotonicity and Coordination-Freeness</title>
    <summary>  Our earlier work titled: "Win-move is Coordination-Free (Sometimes)" has
shown that the classes of queries that can be distributedly computed in a
coordination-free manner form a strict hierarchy depending on the assumptions
of the model for distributed computations. In this paper, we further
characterize these classes by revealing a tight relationship between them and
novel weakened forms of monotonicity.
</summary>
    <author>
      <name>Daniel Zinn</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Early Research Report</arxiv:comment>
    <link href="http://arxiv.org/abs/1202.0242v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.0242v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.6560v1</id>
    <updated>2012-07-27T14:49:18Z</updated>
    <published>2012-07-27T14:49:18Z</published>
    <title>Covering Rough Sets From a Topological Point of View</title>
    <summary>  Covering-based rough set theory is an extension to classical rough set. The
main purpose of this paper is to study covering rough sets from a topological
point of view. The relationship among upper approximations based on topological
spaces are explored.
</summary>
    <author>
      <name>Nguyen Duc Thuan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Theory and Engineering, Vol. 1,
  No. 5, December, 2009, 606-609</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1207.6560v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.6560v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.3756v3</id>
    <updated>2012-12-18T16:36:22Z</updated>
    <published>2012-09-17T19:10:38Z</published>
    <title>Incomplete Information in RDF</title>
    <summary>  We extend RDF with the ability to represent property values that exist, but
are unknown or partially known, using constraints. Following ideas from the
incomplete information literature, we develop a semantics for this extension of
RDF, called RDFi, and study SPARQL query evaluation in this framework.
</summary>
    <author>
      <name>Charalampos Nikolaou</name>
    </author>
    <author>
      <name>Manolis Koubarakis</name>
    </author>
    <link href="http://arxiv.org/abs/1209.3756v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.3756v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.3913v1</id>
    <updated>2012-09-18T11:35:57Z</updated>
    <published>2012-09-18T11:35:57Z</published>
    <title>Keyspace: A Consistently Replicated, Highly-Available Key-Value Store</title>
    <summary>  This paper describes the design and architecture of Keyspace, a distributed
key-value store offering strong consistency, fault-tolerance and high
availability. The source code is available under the open-source AGPL license
for Linux, Windows and BSD-like platforms. As of 2012, Keyspace is no longer
undergoing active development.
</summary>
    <author>
      <name>Márton Trencséni</name>
    </author>
    <author>
      <name>Attila Gazsó</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.3913v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.3913v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.2354v1</id>
    <updated>2012-11-10T21:43:40Z</updated>
    <published>2012-11-10T21:43:40Z</published>
    <title>Privacy Preserving Web Query Log Publishing: A Survey on Anonymization
  Techniques</title>
    <summary>  Releasing Web query logs which contain valuable information for research or
marketing, can breach the privacy of search engine users. Therefore rendering
query logs to limit linking a query to an individual while preserving the data
usefulness for analysis, is an important research problem. This survey provides
an overview and discussion on the recent studies on this direction.
</summary>
    <author>
      <name>Amin Milani Fard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1211.2354v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.2354v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.1877v1</id>
    <updated>2013-04-06T11:20:24Z</updated>
    <published>2013-04-06T11:20:24Z</published>
    <title>Privacy-preserving Data Mining, Sharing and Publishing</title>
    <summary>  The goal of the paper is to present different approaches to
privacy-preserving data sharing and publishing in the context of e-health care
systems. In particular, the literature review on technical issues in privacy
assurance and current real-life high complexity implementation of medical
system that assumes proper data sharing mechanisms are presented in the paper.
</summary>
    <author>
      <name>Katarzyna Pasierb</name>
    </author>
    <author>
      <name>Tomasz Kajdanowicz</name>
    </author>
    <author>
      <name>Przemyslaw Kazienko</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Medical Informatics &amp; Technologies, Vol. 18, pp. 69-76,
  2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1304.1877v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.1877v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.2637v2</id>
    <updated>2013-06-19T18:47:43Z</updated>
    <published>2013-04-09T15:40:21Z</published>
    <title>Containment of Nested Regular Expressions</title>
    <summary>  Nested regular expressions (NREs) have been proposed as a powerful formalism
for querying RDFS graphs, but research in a more general graph database context
has been scarce, and static analysis results are currently lacking. In this
paper we investigate the problem of containment of NREs, and show that it can
be solved in PSPACE, i.e., the same complexity as the problem of containment of
regular expressions or regular path queries (RPQs).
</summary>
    <author>
      <name>Juan L. Reutter</name>
    </author>
    <link href="http://arxiv.org/abs/1304.2637v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.2637v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.5566v1</id>
    <updated>2013-04-20T00:25:50Z</updated>
    <published>2013-04-20T00:25:50Z</published>
    <title>A Markov Model for Ontology Alignment</title>
    <summary>  The explosion of available data along with the need to integrate and utilize
that data has led to a pressing interest in data integration techniques. In
terms of Semantic Web technologies, Ontology Alignment is a key step in the
process of integrating heterogeneous knowledge bases. In this paper, we present
the Edge Confidence technique, a modification and improvement over the popular
Similarity Flooding technique for Ontology Alignment.
</summary>
    <author>
      <name>Michael E. Cotterell</name>
    </author>
    <author>
      <name>Terrance Medina</name>
    </author>
    <link href="http://arxiv.org/abs/1304.5566v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.5566v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.1927v1</id>
    <updated>2013-07-07T22:20:27Z</updated>
    <published>2013-07-07T22:20:27Z</published>
    <title>Link Based Session Reconstruction: Finding All Maximal Paths</title>
    <summary>  This paper introduces a new method for the session construction problem,
which is the first main step of the web usage mining process. Through
experiments, it is shown that when our new technique is used, it outperforms
previous approaches in web usage mining applications such as next-page
prediction.
</summary>
    <author>
      <name>Murat Ali Bayir</name>
    </author>
    <author>
      <name>Ismail Hakki Toroslu</name>
    </author>
    <link href="http://arxiv.org/abs/1307.1927v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.1927v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.6985v3</id>
    <updated>2014-10-16T14:56:52Z</updated>
    <published>2014-03-27T11:54:27Z</published>
    <title>A Fast Minimal Infrequent Itemset Mining Algorithm</title>
    <summary>  A novel fast algorithm for finding quasi identifiers in large datasets is
presented. Performance measurements on a broad range of datasets demonstrate
substantial reductions in run-time relative to the state of the art and the
scalability of the algorithm to realistically-sized datasets up to several
million records.
</summary>
    <author>
      <name>Kostyantyn Demchuk</name>
    </author>
    <author>
      <name>Douglas J. Leith</name>
    </author>
    <link href="http://arxiv.org/abs/1403.6985v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.6985v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.5777v1</id>
    <updated>2014-05-22T14:51:13Z</updated>
    <published>2014-05-22T14:51:13Z</published>
    <title>An Analytical Survey of Provenance Sanitization</title>
    <summary>  Security is likely becoming a critical factor in the future adoption of
provenance technology, because of the risk of inadvertent disclosure of
sensitive information. In this survey paper we review the state of the art in
secure provenance, considering mechanisms for controlling access, and the
extent to which these mechanisms preserve provenance integrity. We examine
seven systems or approaches, comparing features and identifying areas for
future work.
</summary>
    <author>
      <name>James Cheney</name>
    </author>
    <author>
      <name>Roly Perera</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear, IPAW 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.5777v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.5777v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.5; D.4.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.2081v1</id>
    <updated>2014-08-09T11:09:00Z</updated>
    <published>2014-08-09T11:09:00Z</published>
    <title>On the BDD/FC Conjecture</title>
    <summary>  Bounded Derivation Depth property (BDD) and Finite Controllability (FC) are
two properties of sets of datalog rules and tuple generating dependencies
(known as Datalog +/- programs), which recently attracted some attention. We
conjecture that the first of these properties implies the second, and support
this conjecture by some evidence proving, among other results, that it holds
true for all theories over binary signature.
</summary>
    <author>
      <name>Tomasz Gogacz</name>
    </author>
    <author>
      <name>Jerzy Marcinkowski</name>
    </author>
    <link href="http://arxiv.org/abs/1408.2081v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.2081v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.5077v1</id>
    <updated>2014-10-19T14:10:50Z</updated>
    <published>2014-10-19T14:10:50Z</published>
    <title>On the Provenance of Linked Data Statistics</title>
    <summary>  As the amount of linked data published on the web grows, attempts are being
made to describe and measure it. However even basic statistics about a graph,
such as its size, are difficult to express in a uniform and predictable way. In
order to be able to sensibly interpret a statistic it is necessary to know how
it was calculate. In this paper we survey the nature of the problem and outline
a strategy for addressing it.
</summary>
    <author>
      <name>William Waites</name>
    </author>
    <link href="http://arxiv.org/abs/1410.5077v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.5077v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.1671v2</id>
    <updated>2014-12-10T10:20:23Z</updated>
    <published>2014-12-04T14:12:23Z</published>
    <title>Chases and Bag-Set Certain Answers</title>
    <summary>  In this paper we show that the chase technique is powerful enough to capture
the bag-set semantics of conjunctive queries over IDBs and IDs and TGDs. In
addition, we argue that in such cases it provides efficient (LogSpace) query
evaluation algorithms and that, moreover, it can serve as a basis for
evaluating some restricted classes of aggregate queries under incomplete
information.
</summary>
    <author>
      <name>Camilo Thorne</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4pp</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.1671v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.1671v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.05039v1</id>
    <updated>2015-01-21T02:41:55Z</updated>
    <published>2015-01-21T02:41:55Z</published>
    <title>Defining Data Science</title>
    <summary>  Data science is gaining more and more and widespread attention, but no
consensus viewpoint on what data science is has emerged. As a new science, its
objects of study and scientific issues should not be covered by established
sciences. Data in cyberspace have formed what we call datanature. In the
present paper, data science is defined as the science of exploring datanature.
</summary>
    <author>
      <name>Yangyong Zhu</name>
    </author>
    <author>
      <name>Yun Xiong</name>
    </author>
    <link href="http://arxiv.org/abs/1501.05039v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.05039v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.02781v1</id>
    <updated>2015-03-10T06:23:56Z</updated>
    <published>2015-03-10T06:23:56Z</published>
    <title>Unravelling Graph-Exchange File Formats</title>
    <summary>  A graph is used to represent data in which the relationships between the
objects in the data are at least as important as the objects themselves. Over
the last two decades nearly a hundred file formats have been proposed or used
to provide portable access to such data. This paper seeks to review these
formats, and provide some insight to both reduce the ongoing creation of
unnecessary formats, and guide the development of new formats where needed.
</summary>
    <author>
      <name>Matthew Roughan</name>
    </author>
    <author>
      <name>Jonathan Tuke</name>
    </author>
    <link href="http://arxiv.org/abs/1503.02781v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.02781v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.07950v1</id>
    <updated>2015-06-26T03:24:39Z</updated>
    <published>2015-06-26T03:24:39Z</published>
    <title>Bag-of-Features Image Indexing and Classification in Microsoft SQL
  Server Relational Database</title>
    <summary>  This paper presents a novel relational database architecture aimed to visual
objects classification and retrieval. The framework is based on the
bag-of-features image representation model combined with the Support Vector
Machine classification and is integrated in a Microsoft SQL Server database.
</summary>
    <author>
      <name>Marcin Korytkowski</name>
    </author>
    <author>
      <name>Rafal Scherer</name>
    </author>
    <author>
      <name>Pawel Staszewski</name>
    </author>
    <author>
      <name>Piotr Woldan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CYBConf.2015.7175981</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CYBConf.2015.7175981" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2015 IEEE 2nd International Conference on Cybernetics (CYBCONF),
  Gdynia, Poland, 24-26 June 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.07950v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.07950v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.01708v1</id>
    <updated>2015-07-07T08:43:59Z</updated>
    <published>2015-07-07T08:43:59Z</published>
    <title>Typing Regular Path Query Languages for Data Graphs</title>
    <summary>  Regular path query languages for data graphs are essentially \emph{untyped}.
The lack of type information greatly limits the optimization opportunities for
query engines and makes application development more complex. In this paper we
discuss a simple, yet expressive, schema language for edge-labelled data
graphs. This schema language is, then, used to define a query type inference
approach with good precision properties.
</summary>
    <author>
      <name>Dario Colazzo</name>
    </author>
    <author>
      <name>Carlo Sartiani</name>
    </author>
    <link href="http://arxiv.org/abs/1507.01708v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.01708v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.00384v1</id>
    <updated>2015-11-02T05:31:42Z</updated>
    <published>2015-11-02T05:31:42Z</published>
    <title>Z Specification for the W3C Editor's Draft Core SHACL Semantics</title>
    <summary>  This article provides a formalization of the W3C Draft Core SHACL Semantics
specification using Z notation. This formalization exercise has identified a
number of quality issues in the draft. It has also established that the
recursive definitions in the draft are well-founded. Further formal validation
of the draft will require the use of an executable specification technology.
</summary>
    <author>
      <name>Arthur Ryman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">57 pages, Invited Expert contribution to the W3C RDF Data Shapes
  Working Group</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.00384v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.00384v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.02650v1</id>
    <updated>2016-01-11T21:18:59Z</updated>
    <published>2016-01-11T21:18:59Z</published>
    <title>Inference rules for RDF(S) and OWL in N3Logic</title>
    <summary>  This paper presents inference rules for Resource Description Framework (RDF),
RDF Schema (RDFS) and Web Ontology Language (OWL). Our formalization is based
on Notation 3 Logic, which extended RDF by logical symbols and created Semantic
Web logic for deductive RDF graph stores. We also propose OWL-P that is a
lightweight formalism of OWL and supports soft inferences by omitting complex
language constructs.
</summary>
    <author>
      <name>Dominik Tomaszuk</name>
    </author>
    <link href="http://arxiv.org/abs/1601.02650v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.02650v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.01682v1</id>
    <updated>2016-03-05T05:39:46Z</updated>
    <published>2016-03-05T05:39:46Z</published>
    <title>Frequent-Itemset Mining using Locality-Sensitive Hashing</title>
    <summary>  The Apriori algorithm is a classical algorithm for the frequent itemset
mining problem. A significant bottleneck in Apriori is the number of I/O
operation involved, and the number of candidates it generates. We investigate
the role of LSH techniques to overcome these problems, without adding much
computational overhead. We propose randomized variations of Apriori that are
based on asymmetric LSH defined over Hamming distance and Jaccard similarity.
</summary>
    <author>
      <name>Debajyoti Bera</name>
    </author>
    <author>
      <name>Rameshwar Pratap</name>
    </author>
    <link href="http://arxiv.org/abs/1603.01682v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.01682v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07180v1</id>
    <updated>2016-04-25T09:39:57Z</updated>
    <published>2016-04-25T09:39:57Z</published>
    <title>Observing and Recommending from a Social Web with Biases</title>
    <summary>  The research question this report addresses is: how, and to what extent,
those directly involved with the design, development and employment of a
specific black box algorithm can be certain that it is not unlawfully
discriminating (directly and/or indirectly) against particular persons with
protected characteristics (e.g. gender, race and ethnicity)?
</summary>
    <author>
      <name>Steffen Staab</name>
    </author>
    <author>
      <name>Sophie Stalla-Bourdillon</name>
    </author>
    <author>
      <name>Laura Carmichael</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical Report, University of Southampton, March 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.07180v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07180v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.5.0; H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.00791v1</id>
    <updated>2016-09-03T04:35:57Z</updated>
    <published>2016-09-03T04:35:57Z</published>
    <title>Reprowd: Crowdsourced Data Processing Made Reproducible</title>
    <summary>  Crowdsourcing is a multidisciplinary research area including disciplines like
artificial intelligence, human-computer interaction, database, and social
science. To facilitate cooperation across disciplines, reproducibility is a
crucial factor, but unfortunately, it has not gotten enough attention in the
HCOMP community. In this paper, we present Reprowd, a system aiming to make it
easy to reproduce crowdsourced data processing research. We have open sourced
Reprowd at http://sfu-db.github.io/reprowd/.
</summary>
    <author>
      <name>Ruochen Jiang</name>
    </author>
    <author>
      <name>Jiannan Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">HCOMP 2016 Work in Progress</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.00791v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.00791v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.09062v1</id>
    <updated>2016-09-29T03:01:44Z</updated>
    <published>2016-09-29T03:01:44Z</published>
    <title>A Study on Altering PostgreSQL from Multi-Processes Structure to
  Multi-Threads Structure</title>
    <summary>  How to altering PostgreSQL database from multi-processes structure to
multi-threads structure is a difficult problem. In the paper, we bring forward
a comprehensive alteration scheme. Especially, put rational methods to account
for three difficult points: semaphores, signal processing and global variables.
At last, applied the scheme successfully to modify a famous open source DBMS.
</summary>
    <author>
      <name>Zhiyong Shan</name>
    </author>
    <link href="http://arxiv.org/abs/1609.09062v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.09062v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.04315v1</id>
    <updated>2016-10-14T03:19:54Z</updated>
    <published>2016-10-14T03:19:54Z</published>
    <title>The multiset semantics of SPARQL patterns</title>
    <summary>  The paper determines the algebraic and logic structure of the multiset
semantics of the core patterns of SPARQL. We prove that the fragment formed by
AND, UNION, OPTIONAL, FILTER, MINUS and SELECT corresponds precisely to both,
the intuitive multiset relational algebra (projection, selection, natural join,
arithmetic union and except), and the multiset non-recursive Datalog with safe
negation.
</summary>
    <author>
      <name>Renzo Angles</name>
    </author>
    <author>
      <name>Claudio Gutierrez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is an extended and updated version of the paper accepted at the
  International Semantic Web Conference 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.04315v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.04315v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.07514v1</id>
    <updated>2016-12-22T10:05:45Z</updated>
    <published>2016-12-22T10:05:45Z</published>
    <title>Getting Started with PATSTAT Register</title>
    <summary>  This paper provides a technical introduction to the PATSTAT Register
database, which contains bibliographical, procedural and legal status data on
patent applications handled by the European Patent Office. It presents eight
MySQL queries that cover some of the most relevant aspects of the database for
research purposes. It targets academic researchers and practitioners who are
familiar with the PATSTAT database and the MySQL language.
</summary>
    <author>
      <name>Gaetan de Rassenfosse</name>
    </author>
    <author>
      <name>Martin Kracker</name>
    </author>
    <author>
      <name>Gianluca Tarasconi</name>
    </author>
    <link href="http://arxiv.org/abs/1612.07514v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.07514v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.04182v1</id>
    <updated>2017-01-16T06:22:24Z</updated>
    <published>2017-01-16T06:22:24Z</published>
    <title>hMDAP: A Hybrid Framework for Multi-paradigm Data Analytical Processing
  on Spark</title>
    <summary>  We propose hMDAP, a hybrid framework for large-scale data analytical
processing on Spark, to support multi-paradigm process (incl. OLAP, machine
learning, and graph analysis etc.) in distributed environments. The framework
features a three-layer data process module and a business process module which
controls the former. We will demonstrate the strength of hMDAP by using traffic
scenarios in a real world.
</summary>
    <author>
      <name>Xiaowang Zhang</name>
    </author>
    <author>
      <name>Jiahui Zhang</name>
    </author>
    <author>
      <name>Zhiyong Feng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.04182v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.04182v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4; H.2.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.08028v1</id>
    <updated>2017-01-27T12:30:07Z</updated>
    <published>2017-01-27T12:30:07Z</published>
    <title>Biomedical Data Warehouses</title>
    <summary>  The aim of this article is to present an overview of the existing biomedical
data warehouses and to discuss the issues and future trends in this area. We
illustrate this topic by presenting the design of an innovative, complex data
warehouse for personal, anticipative medicine.
</summary>
    <author>
      <name>Jérôme Darmont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Emerson Olivier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:0809.2688</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Encyclopaedia of Healthcare Information Systems, IGI Publishing,
  pp.149-156, 2008</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1701.08028v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.08028v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.08052v1</id>
    <updated>2017-01-27T13:42:54Z</updated>
    <published>2017-01-27T13:42:54Z</published>
    <title>Database Benchmarks</title>
    <summary>  The aim of this article is to present an overview of the major families of
state-of-the-art data-base benchmarks, namely: relational benchmarks, object
and object-relational benchmarks, XML benchmarks, and decision-support
benchmarks, and to discuss the issues, tradeoffs and future trends in database
benchmarking. We particularly focus on XML and decision-support benchmarks,
which are currently the most innovative tools that are developed in this area.
</summary>
    <author>
      <name>Jérôme Darmont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Encyclopedia of Information Science and Technology, Second
  Edition, IGI Publishing, pp.950-954, 2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1701.08052v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.08052v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.08634v1</id>
    <updated>2017-01-30T15:10:07Z</updated>
    <published>2017-01-30T15:10:07Z</published>
    <title>Data Processing Benchmarks</title>
    <summary>  The aim of this article is to present an overview of the major families of
state-of-the-art data processing benchmarks, namely transaction processing
benchmarks and decision support benchmarks. We also address the newer trends in
cloud benchmarking. Finally, we discuss the issues, tradeoffs and future trends
for data processing benchmarks.
</summary>
    <author>
      <name>Jérôme Darmont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1701.08052</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Encyclopedia of Information Science and Technology, Third Edition,
  pp.146-152, 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1701.08634v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.08634v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.03447v1</id>
    <updated>2017-02-11T19:18:41Z</updated>
    <published>2017-02-11T19:18:41Z</published>
    <title>A Collective, Probabilistic Approach to Schema Mapping: Appendix</title>
    <summary>  In this appendix we provide additional supplementary material to "A
Collective, Probabilistic Approach to Schema Mapping." We include an additional
extended example, supplementary experiment details, and proof for the
complexity result stated in the main paper.
</summary>
    <author>
      <name>Angelika Kimmig</name>
    </author>
    <author>
      <name>Alex Memory</name>
    </author>
    <author>
      <name>Renee J. Miller</name>
    </author>
    <author>
      <name>Lise Getoor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is the appendix to the paper "A Collective, Probabilistic
  Approach to Schema Mapping" accepted to ICDE 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.03447v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.03447v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.01298v1</id>
    <updated>2017-03-03T18:17:53Z</updated>
    <published>2017-03-03T18:17:53Z</published>
    <title>Defining Domain-Independent Discovery Informatics</title>
    <summary>  This paper presents a personal account of the early legacy of discovery
informatics, especially surrounding the first published definition of
domain-independent DI. The state of DI is traced across various reference
sources and the literature on the fourth paradigm of the scientific method.
Observations are offered on DI, concluding that it will retain its appeal as a
highly apt descriptor for research and practice activities that are inherent in
our human nature.
</summary>
    <author>
      <name>William W. Agresti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages; no figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.01298v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.01298v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.08685v2</id>
    <updated>2017-04-01T07:27:18Z</updated>
    <published>2017-03-25T12:48:55Z</published>
    <title>Thespis: Actor-Based Middleware for Causal Consistency</title>
    <summary>  This paper provides a survey of the current state of the art in
Causally-Consistent data stores. Furthermore, we present the design of Thespis,
a middleware that innovatively leverages the Actor model to implement causal
consistency over an industry-standard data store.
</summary>
    <author>
      <name>Carl Camilleri</name>
    </author>
    <author>
      <name>Joseph Vella</name>
    </author>
    <author>
      <name>Vitezslav Nezval</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">need to withdraw for corrections</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.08685v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.08685v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.08470v1</id>
    <updated>2017-09-13T15:39:03Z</updated>
    <published>2017-09-13T15:39:03Z</published>
    <title>An efficient clustering algorithm from the measure of local Gaussian
  distribution</title>
    <summary>  In this paper, I will introduce a fast and novel clustering algorithm based
on Gaussian distribution and it can guarantee the separation of each cluster
centroid as a given parameter, $d_s$. The worst run time complexity of this
algorithm is approximately $\sim$O$(T\times N \times \log(N))$ where $T$ is the
iteration steps and $N$ is the number of features.
</summary>
    <author>
      <name>Yuan-Yen Tai</name>
    </author>
    <link href="http://arxiv.org/abs/1709.08470v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.08470v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.09003v1</id>
    <updated>2017-09-19T07:59:41Z</updated>
    <published>2017-09-19T07:59:41Z</published>
    <title>CASP-DM: Context Aware Standard Process for Data Mining</title>
    <summary>  We propose an extension of the Cross Industry Standard Process for Data
Mining (CRISPDM) which addresses specific challenges of machine learning and
data mining for context and model reuse handling. This new general
context-aware process model is mapped with CRISP-DM reference model proposing
some new or enhanced outputs.
</summary>
    <author>
      <name>Fernando Martínez-Plumed</name>
    </author>
    <author>
      <name>Lidia Contreras-Ochando</name>
    </author>
    <author>
      <name>Cèsar Ferri</name>
    </author>
    <author>
      <name>Peter Flach</name>
    </author>
    <author>
      <name>José Hernández-Orallo</name>
    </author>
    <author>
      <name>Meelis Kull</name>
    </author>
    <author>
      <name>Nicolas Lachiche</name>
    </author>
    <author>
      <name>María José Ramírez-Quintana</name>
    </author>
    <link href="http://arxiv.org/abs/1709.09003v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.09003v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9902017v2</id>
    <updated>1999-02-11T18:31:10Z</updated>
    <published>1999-02-09T04:02:55Z</published>
    <title>Not Available</title>
    <summary>  withdrawn by author
</summary>
    <author>
      <name>Not Available</name>
    </author>
    <link href="http://arxiv.org/abs/cs/9902017v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9902017v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="Not Available" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.0717v1</id>
    <updated>2014-10-02T21:31:12Z</updated>
    <published>2014-10-02T21:31:12Z</published>
    <title>Fast, memory efficient low-rank approximation of SimRank</title>
    <summary>  SimRank is a well-known similarity measure between graph vertices.
  In this paper novel low-rank approximation of SimRank is proposed.
</summary>
    <author>
      <name>I. V. Oseledets</name>
    </author>
    <author>
      <name>G. V. Ovchinnikov</name>
    </author>
    <link href="http://arxiv.org/abs/1410.0717v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.0717v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9809004v1</id>
    <updated>1998-09-02T00:49:25Z</updated>
    <published>1998-09-02T00:49:25Z</published>
    <title>Performance / Price Sort</title>
    <summary>  NTsort is an external sort on WindowsNT 5.0. It has minimal functionality but
excellent price performance. In particular, running on mail-order hardware it
can sort 1.5 GB for a penny. For commercially available sorts, Postman Sort
from Robert Ramey Software Development has elapsed time performance comparable
to NTsort, while using less processor time. It can sort 1.27 GB for a penny
(12.7 million records.) These sorts set new price-performance records. This
paper documents this and proposes that the PennySort benchmark be revised to
Performance/Price sort: a simple GB/$ sort metric based on a two-pass external
sort.
</summary>
    <author>
      <name>Jim Gray</name>
    </author>
    <author>
      <name>Joshua Coates</name>
    </author>
    <author>
      <name>Chris Nyberg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Original word file at:
  http://research.microsoft.com/~gray/PennySort.doc</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9809004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9809004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.5;H.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9809005v1</id>
    <updated>1998-09-02T01:49:32Z</updated>
    <published>1998-09-02T01:49:32Z</published>
    <title>The Five-Minute Rule Ten Years Later, and Other Computer Storage Rules
  of Thumb</title>
    <summary>  Simple economic and performance arguments suggest appropriate lifetimes for
main memory pages and suggest optimal page sizes. The fundamental tradeoffs are
the prices and bandwidths of RAMs and disks. The analysis indicates that with
today's technology, five minutes is a good lifetime for randomly accessed
pages, one minute is a good lifetime for two-pass sequentially accessed pages,
and 16 KB is a good size for index pages. These rules-of-thumb change in
predictable ways as technology ratios change. They also motivate the importance
of the new Kaps, Maps, Scans, and $/Kaps, $/Maps, $/TBscan metrics.
</summary>
    <author>
      <name>Jim Gray</name>
    </author>
    <author>
      <name>Goetz Graefe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Original document at:
  http://research.microsoft.com/~gray/5_min_rule_SIGMOD.doc</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM SIGMOD Record 26(4): 63-68 (1997)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9809005v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9809005v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9809033v2</id>
    <updated>1998-09-25T16:12:20Z</updated>
    <published>1998-09-18T21:24:23Z</published>
    <title>Efficient Retrieval of Similar Time Sequences Using DFT</title>
    <summary>  We propose an improvement of the known DFT-based indexing technique for fast
retrieval of similar time sequences. We use the last few Fourier coefficients
in the distance computation without storing them in the index since every
coefficient at the end is the complex conjugate of a coefficient at the
beginning and as strong as its counterpart. We show analytically that this
observation can accelerate the search time of the index by more than a factor
of two. This result was confirmed by our experiments, which were carried out on
real stock prices and synthetic data.
</summary>
    <author>
      <name>Davood Rafiei</name>
    </author>
    <author>
      <name>Alberto Mendelzon</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of 5th Intl. Conf. on Foundations of Data
  Organizations and Algorithms (FODO '98), November 1998, Kobe, Japan</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9809033v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9809033v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2;H.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9912015v1</id>
    <updated>1999-12-22T15:25:55Z</updated>
    <published>1999-12-22T15:25:55Z</published>
    <title>Comparative Analysis of Five XML Query Languages</title>
    <summary>  XML is becoming the most relevant new standard for data representation and
exchange on the WWW. Novel languages for extracting and restructuring the XML
content have been proposed, some in the tradition of database query languages
(i.e. SQL, OQL), others more closely inspired by XML. No standard for XML query
language has yet been decided, but the discussion is ongoing within the World
Wide Web Consortium and within many academic institutions and Internet-related
major companies. We present a comparison of five, representative query
languages for XML, highlighting their common features and differences.
</summary>
    <author>
      <name>Angela Bonifati</name>
    </author>
    <author>
      <name>Stefano Ceri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">TeX v3.1415, 17 pages, 6 figures, to be published in ACM Sigmod
  Record, March 2000</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9912015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9912015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2; H.2.3; I.7; I.7.1; I.7.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0110032v1</id>
    <updated>2001-10-16T19:05:24Z</updated>
    <published>2001-10-16T19:05:24Z</published>
    <title>A logic-based approach to data integration</title>
    <summary>  An important aspect of data integration involves answering queries using
various resources rather than by accessing database relations. The process of
transforming a query from the database relations to the resources is often
referred to as query folding or answering queries using views, where the views
are the resources. We present a uniform approach that includes as special cases
much of the previous work on this subject. Our approach is logic-based using
resolution. We deal with integrity constraints, negation, and recursion also
within this framework.
</summary>
    <author>
      <name>J. Grant</name>
    </author>
    <author>
      <name>J. Minker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">47 pages, Accepted for publication in the Theory and Practice of
  Logic Programming</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0110032v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0110032v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2, I.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0112007v2</id>
    <updated>2002-11-30T01:15:08Z</updated>
    <published>2001-12-07T15:40:11Z</published>
    <title>A Tight Upper Bound on the Number of Candidate Patterns</title>
    <summary>  In the context of mining for frequent patterns using the standard levelwise
algorithm, the following question arises: given the current level and the
current set of frequent patterns, what is the maximal number of candidate
patterns that can be generated on the next level? We answer this question by
providing a tight upper bound, derived from a combinatorial result from the
sixties by Kruskal and Katona. Our result is useful to reduce the number of
database scans.
</summary>
    <author>
      <name>Floris Geerts</name>
    </author>
    <author>
      <name>Bart Goethals</name>
    </author>
    <author>
      <name>Jan Van den Bussche</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0112007v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0112007v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0112011v2</id>
    <updated>2003-02-05T10:05:18Z</updated>
    <published>2001-12-10T15:50:47Z</published>
    <title>Interactive Constrained Association Rule Mining</title>
    <summary>  We investigate ways to support interactive mining sessions, in the setting of
association rule mining. In such sessions, users specify conditions (queries)
on the associations to be generated. Our approach is a combination of the
integration of querying conditions inside the mining phase, and the incremental
querying of already generated associations. We present several concrete
algorithms and compare their performance.
</summary>
    <author>
      <name>Bart Goethals</name>
    </author>
    <author>
      <name>Jan Van den Bussche</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A preliminary report on this work was presented at the Second
  International Conference on Knowledge Discovery and Data Mining (DaWaK 2000)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0112011v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0112011v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0202001v1</id>
    <updated>2002-02-01T05:00:24Z</updated>
    <published>2002-02-01T05:00:24Z</published>
    <title>The Deductive Database System LDL++</title>
    <summary>  This paper describes the LDL++ system and the research advances that have
enabled its design and development. We begin by discussing the new nonmonotonic
and nondeterministic constructs that extend the functionality of the LDL++
language, while preserving its model-theoretic and fixpoint semantics. Then, we
describe the execution model and the open architecture designed to support
these new constructs and to facilitate the integration with existing DBMSs and
applications. Finally, we describe the lessons learned by using LDL++ on
various tested applications, such as middleware and datamining.
</summary>
    <author>
      <name>Faiz Arni</name>
    </author>
    <author>
      <name>KayLiang Ong</name>
    </author>
    <author>
      <name>Shalom Tsur</name>
    </author>
    <author>
      <name>Haixun Wang</name>
    </author>
    <author>
      <name>Carlo Zaniolo</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0202001v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0202001v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0202037v2</id>
    <updated>2003-10-13T12:29:04Z</updated>
    <published>2002-02-25T19:35:12Z</published>
    <title>Towards practical meta-querying</title>
    <summary>  We describe a meta-querying system for databases containing queries in
addition to ordinary data. In the context of such databases, a meta-query is a
query about queries. Representing stored queries in XML, and using the standard
XML manipulation language XSLT as a sublanguage, we show that just a few
features need to be added to SQL to turn it into a fully-fledged meta-query
language. The good news is that these features can be directly supported by
extensible database technology.
</summary>
    <author>
      <name>Jan Van den Bussche</name>
    </author>
    <author>
      <name>Stijn Vansummeren</name>
    </author>
    <author>
      <name>Gottfried Vossen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.is.2004.04.001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.is.2004.04.001" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Includes a new section "Experimental performance evaluation"</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Information Systems, Volume 30, Issue 4 , June 2005, Pages 317-332</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0202037v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0202037v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0203027v1</id>
    <updated>2002-03-27T03:35:12Z</updated>
    <published>2002-03-27T03:35:12Z</published>
    <title>The Algorithms of Updating Sequential Patterns</title>
    <summary>  Because the data being mined in the temporal database will evolve with time,
many researchers have focused on the incremental mining of frequent sequences
in temporal database. In this paper, we propose an algorithm called IUS, using
the frequent and negative border sequences in the original database for
incremental sequence mining. To deal with the case where some data need to be
updated from the original database, we present an algorithm called DUS to
maintain sequential patterns in the updated database. We also define the
negative border sequence threshold: Min_nbd_supp to control the number of
sequences in the negative border.
</summary>
    <author>
      <name>Qingguo Zheng</name>
    </author>
    <author>
      <name>Ke Xu</name>
    </author>
    <author>
      <name>Shilong Ma</name>
    </author>
    <author>
      <name>Weifeng Lv</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The Second SIAM Data mining2002: workshop HPDM</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0203027v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0203027v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0204010v1</id>
    <updated>2002-04-05T22:24:33Z</updated>
    <published>2002-04-05T22:24:33Z</published>
    <title>On the Computational Complexity of Consistent Query Answers</title>
    <summary>  We consider here the problem of obtaining reliable, consistent information
from inconsistent databases -- databases that do not have to satisfy given
integrity constraints. We use the notion of consistent query answer -- a query
answer which is true in every (minimal) repair of the database. We provide a
complete classification of the computational complexity of consistent answers
to first-order queries w.r.t. functional dependencies and denial constraints.
We show how the complexity depends on the {\em type} of the constraints
considered, their {\em number}, and the {\em size} of the query. We obtain
several new PTIME cases, using new algorithms.
</summary>
    <author>
      <name>Jan Chomicki</name>
    </author>
    <author>
      <name>Jerzy Marcinkowski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0204010v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0204010v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.3; F.4.1; I.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0204038v1</id>
    <updated>2002-04-16T16:03:18Z</updated>
    <published>2002-04-16T16:03:18Z</published>
    <title>Technology For Information Engineering (TIE): A New Way of Storing,
  Retrieving and Analyzing Information</title>
    <summary>  The theoretical foundations of a new model and paradigm (called TIE) for data
storage and access are introduced. Associations between data elements are
stored in a single Matrix table, which is usually kept entirely in RAM for
quick access. The model ties together a very intuitive "guided" GUI to the
Matrix structure, allowing extremely easy complex searches through the data.
Although it is an "Associative Model" in that it stores the data associations
separately from the data itself, in contrast to other implementations of that
model TIE guides the user to only the available information ensuring that every
search is always fruitful. Very many diverse applications of the technology are
reviewed.
</summary>
    <author>
      <name>Jerzy Lewak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages. Introduces the theoretical foundation for associative
  information</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0204038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0204038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0206023v1</id>
    <updated>2002-06-15T12:08:12Z</updated>
    <published>2002-06-15T12:08:12Z</published>
    <title>Relational Association Rules: getting WARMeR</title>
    <summary>  In recent years, the problem of association rule mining in transactional data
has been well studied. We propose to extend the discovery of classical
association rules to the discovery of association rules of conjunctive queries
in arbitrary relational data, inspired by the WARMR algorithm, developed by
Dehaspe and Toivonen, that discovers association rules over a limited set of
conjunctive queries. Conjunctive query evaluation in relational databases is
well understood, but still poses some great challenges when approached from a
discovery viewpoint in which patterns are generated and evaluated with respect
to some well defined search space and pruning operators.
</summary>
    <author>
      <name>Bart Goethals</name>
    </author>
    <author>
      <name>Jan Van den Bussche</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0206023v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0206023v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0211042v1</id>
    <updated>2002-11-29T04:28:56Z</updated>
    <published>2002-11-29T04:28:56Z</published>
    <title>Database Repairs and Analytic Tableaux</title>
    <summary>  In this article, we characterize in terms of analytic tableaux the repairs of
inconsistent relational databases, that is databases that do not satisfy a
given set of integrity constraints. For this purpose we provide closing and
opening criteria for branches in tableaux that are built for database instances
and their integrity constraints. We use the tableaux based characterization as
a basis for consistent query answering, that is for retrieving from the
database answers to queries that are consistent wrt the integrity constraints.
</summary>
    <author>
      <name>Leopoldo Bertossi</name>
    </author>
    <author>
      <name>Camilla Schwind</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of paper appeared in Proc. FOIKS02. Submitted by
  invitation to AMAI journal. Uses packages: llncs.cls, amssymb.sty,
  parsetree.sty. 31 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0211042v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0211042v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H2; F4; I2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0212017v1</id>
    <updated>2002-12-09T17:29:12Z</updated>
    <published>2002-12-09T17:29:12Z</published>
    <title>Classes of Spatiotemporal Objects and Their Closure Properties</title>
    <summary>  We present a data model for spatio-temporal databases. In this model
spatio-temporal data is represented as a finite union of objects described by
means of a spatial reference object, a temporal object and a geometric
transformation function that determines the change or movement of the reference
object in time.
  We define a number of practically relevant classes of spatio-temporal
objects, and give complete results concerning closure under Boolean set
operators for these classes. Since only few classes are closed under all set
operators, we suggest an extension of the model, which leads to better closure
properties, and therefore increased practical applicability. We also discuss a
normal form for this extended data model.
</summary>
    <author>
      <name>Jan Chomicki</name>
    </author>
    <author>
      <name>Sofie Haesevoets</name>
    </author>
    <author>
      <name>Bart Kuijpers</name>
    </author>
    <author>
      <name>Peter Revesz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0212017v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0212017v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0301009v1</id>
    <updated>2003-01-13T05:34:39Z</updated>
    <published>2003-01-13T05:34:39Z</published>
    <title>A Script Language for Data Integration in Database</title>
    <summary>  A Script Language in this paper is designed to transform the original data
into the target data by the computing formula. The Script Language can be
translated into the corresponding SQL Language, and the computation is finally
implemented by the first type of dynamic SQL. The Script Language has the
operations of insert, update, delete, union, intersect, and minus for the table
in the database.The Script Language is edited by a text file and you can easily
modify the computing formula in the text file to deal with the situations when
the computing formula have been changed. So you only need modify the text of
the script language, but needn't change the programs that have complied.
</summary>
    <author>
      <name>Qingguo Zheng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0301009v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0301009v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0306066v1</id>
    <updated>2003-06-13T14:33:53Z</updated>
    <published>2003-06-13T14:33:53Z</published>
    <title>The COMPASS Event Store in 2002</title>
    <summary>  COMPASS, the fixed-target experiment at CERN studying the structure of the
nucleon and spectroscopy, collected over 260 TB during summer 2002 run. All
these data, together with reconstructed events information, were put from the
beginning in a database infrastructure based on Objectivity/DB and on the
hierarchical storage manager CASTOR. The experience in the usage of the
database is reviewed and the evolution of the system outlined.
</summary>
    <author>
      <name>Venicio Duic</name>
    </author>
    <author>
      <name>Massimo Lamanna</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TNS.2004.832645</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TNS.2004.832645" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Talk from the 2003 conference: "Computing in High Energy and Nuclear
  Physics" (CHEP03), La Jolla, Ca, USA, March 2003, 6 pages. PSN MOKT011</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0306066v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0306066v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2; H.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0307073v1</id>
    <updated>2003-07-31T11:18:51Z</updated>
    <published>2003-07-31T11:18:51Z</published>
    <title>Search and Navigation in Relational Databases</title>
    <summary>  We present a new application for keyword search within relational databases,
which uses a novel algorithm to solve the join discovery problem by finding
Memex-like trails through the graph of foreign key dependencies. It differs
from previous efforts in the algorithms used, in the presentation mechanism and
in the use of primary-key only database queries at query-time to maintain a
fast response for users. We present examples using the DBLP data set.
</summary>
    <author>
      <name>Richard Wheeldon</name>
    </author>
    <author>
      <name>Mark Levene</name>
    </author>
    <author>
      <name>Kevin Keenoy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0307073v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0307073v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3;H.4;H.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0308014v2</id>
    <updated>2004-03-03T13:43:33Z</updated>
    <published>2003-08-06T14:12:11Z</published>
    <title>On the expressive power of semijoin queries</title>
    <summary>  The semijoin algebra is the variant of the relational algebra obtained by
replacing the join operator by the semijoin operator. We provide an
Ehrenfeucht-Fraiss\'{e} game, characterizing the discerning power of the
semijoin algebra. This game gives a method for showing that queries are not
expressible in the semijoin algebra.
</summary>
    <author>
      <name>Dirk Leinders</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Limburgs Universitair Centrum, Belgium</arxiv:affiliation>
    </author>
    <author>
      <name>Jerzy Tyszkiewicz</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Warsaw University</arxiv:affiliation>
    </author>
    <author>
      <name>Jan Van den Bussche</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Limburgs Universitair Centrum, Belgium</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, to appear in Information Processing Letters; added results
  that more clearly delineate the expressive power of SA, added a section that
  discusses the impact of order on the expressive power of SA, deemphasized the
  discussion on the relationship with GF</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0308014v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0308014v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.3; F.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0312042v1</id>
    <updated>2003-12-18T17:43:43Z</updated>
    <published>2003-12-18T17:43:43Z</published>
    <title>Declarative Semantics for Active Rules</title>
    <summary>  In this paper we analyze declarative deterministic and non-deterministic
semantics for active rules. In particular we consider several (partial) stable
model semantics, previously defined for deductive rules, such as well-founded,
max deterministic, unique total stable model, total stable model, and maximal
stable model semantics. The semantics of an active program AP is given by first
rewriting it into a deductive program P, then computing a model M defining the
declarative semantics of P and, finally, applying `consistent' updates
contained in M to the source database. The framework we propose permits a
natural integration of deductive and active rules and can also be applied to
queries with function symbols or to queries over infinite databases.
</summary>
    <author>
      <name>Sergio Flesca</name>
    </author>
    <author>
      <name>Sergio Greco</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Theory and Practice of Logic Programming, 1(1): 43-69, 2001</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0312042v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0312042v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.1.6; F.3.1; F.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0401014v1</id>
    <updated>2004-01-18T03:09:04Z</updated>
    <published>2004-01-18T03:09:04Z</published>
    <title>Nested Intervals with Farey Fractions</title>
    <summary>  Relational Databases are universally conceived as an advance over their
predecessors Network and Hierarchical models. Superior in every querying
respect, they turned out to be surprisingly incomplete when modeling transitive
dependencies. Almost every couple of months a question how to model a tree in
the database surfaces at comp.database.theory newsgroup. This article completes
a series of articles exploring Nested Intervals Model. Previous articles
introduced tree encoding with Binary Rational Numbers. However, binary encoding
grows exponentially, both in breadth and in depth. In this article, we'll
leverage Farey fractions in order to overcome this problem. We'll also
demonstrate that our implementation scales to a tree with 1M nodes.
</summary>
    <author>
      <name>Vadim Tropashko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0401014v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0401014v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0401015v1</id>
    <updated>2004-01-20T19:26:35Z</updated>
    <published>2004-01-20T19:26:35Z</published>
    <title>Query Answering in Peer-to-Peer Data Exchange Systems</title>
    <summary>  The problem of answering queries posed to a peer who is a member of a
peer-to-peer data exchange system is studied. The answers have to be consistent
wrt to both the local semantic constraints and the data exchange constraints
with other peers; and must also respect certain trust relationships between
peers. A semantics for peer consistent answers under exchange constraints and
trust relationships is introduced and some techniques for obtaining those
answers are presented.
</summary>
    <author>
      <name>Leopoldo Bertossi</name>
    </author>
    <author>
      <name>Loreto Bravo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0401015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0401015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4;F.4.1;I.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0402016v1</id>
    <updated>2004-02-09T19:13:17Z</updated>
    <published>2004-02-09T19:13:17Z</published>
    <title>Perspects in astrophysical databases</title>
    <summary>  Astrophysics has become a domain extremely rich of scientific data. Data
mining tools are needed for information extraction from such large datasets.
This asks for an approach to data management emphasizing the efficiency and
simplicity of data access; efficiency is obtained using multidimensional access
methods and simplicity is achieved by properly handling metadata. Moreover,
clustering and classification techniques on large datasets pose additional
requirements in terms of computation and memory scalability and
interpretability of results. In this study we review some possible solutions.
</summary>
    <author>
      <name>M. Frailis</name>
    </author>
    <author>
      <name>A. De Angelis</name>
    </author>
    <author>
      <name>V. Roberto</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.physa.2004.02.024</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.physa.2004.02.024" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Physica A338 (2004) 54-59</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0402016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0402016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4; H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0403014v2</id>
    <updated>2004-03-12T14:33:23Z</updated>
    <published>2004-03-11T06:30:30Z</published>
    <title>Search Efficiency in Indexing Structures for Similarity Searching</title>
    <summary>  Similarity searching finds application in a wide variety of domains including
multilingual databases, computational biology, pattern recognition and text
retrieval. Similarity is measured in terms of a distance function, edit
distance, in general metric spaces, which is expensive to compute. Indexing
techniques can be used reduce the number of distance computations. We present
an analysis of various existing similarity indexing structures for the same.
The performance obtained using the index structures studied was found to be
unsatisfactory . We propose an indexing technique that combines the features of
clustering with M tree(MTB) and the results indicate that this gives better
performance.
</summary>
    <author>
      <name>Girish Motwani</name>
    </author>
    <author>
      <name>Sandhya G. Nair</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0403014v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0403014v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0406004v1</id>
    <updated>2004-06-02T12:55:04Z</updated>
    <published>2004-06-02T12:55:04Z</published>
    <title>Application of Business Intelligence In Banks (Pakistan)</title>
    <summary>  The financial services industry is rapidly changing. Factors such as
globalization, deregulation, mergers and acquisitions, competition from
non-financial institutions, and technological innovation, have forced companies
to re-think their business.Many large companies have been using Business
Intelligence (BI) computer software for some years to help them gain
competitive advantage. With the introduction of cheaper and more generalized
products to the market place BI is now in the reach of smaller and medium sized
companies. Business Intelligence is also known as knowledge management,
management information systems (MIS), Executive information systems (EIS) and
On-line analytical Processing (OLAP).
</summary>
    <author>
      <name>Muhammad Nadeem</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Szabist</arxiv:affiliation>
    </author>
    <author>
      <name>Syed Ata Hussain Jaffri</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Szabist</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 Pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0406004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0406004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0406060v1</id>
    <updated>2004-06-29T16:09:10Z</updated>
    <published>2004-06-29T16:09:10Z</published>
    <title>Well-Definedness and Semantic Type-Checking in the Nested Relational
  Calculus and XQuery</title>
    <summary>  Two natural decision problems regarding the XML query language XQuery are
well-definedness and semantic type-checking. We study these problems in the
setting of a relational fragment of XQuery. We show that well-definedness and
semantic type-checking are undecidable, even in the positive-existential case.
Nevertheless, for a ``pure'' variant of XQuery, in which no identification is
made between an item and the singleton containing that item, the problems
become decidable. We also consider the analogous problems in the setting of the
nested relational calculus.
</summary>
    <author>
      <name>Jan Van den Bussche</name>
    </author>
    <author>
      <name>Dirk Van Gucht</name>
    </author>
    <author>
      <name>Stijn Vansummeren</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0406060v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0406060v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0407007v1</id>
    <updated>2004-07-02T15:44:32Z</updated>
    <published>2004-07-02T15:44:32Z</published>
    <title>The semijoin algebra and the guarded fragment</title>
    <summary>  The semijoin algebra is the variant of the relational algebra obtained by
replacing the join operator by the semijoin operator. We discuss some
interesting connections between the semijoin algebra and the guarded fragment
of first-order logic. We also provide an Ehrenfeucht-Fraisse game,
characterizing the discerning power of the semijoin algebra. This game gives a
method for showing that certain queries are not expressible in the semijoin
algebra.
</summary>
    <author>
      <name>Dirk Leinders</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Limburgs Universitair Centrum, Diepenbeek, Belgium</arxiv:affiliation>
    </author>
    <author>
      <name>Jerzy Tyszkiewicz</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Institute of Informatics, Warsaw University, Warsaw, Poland</arxiv:affiliation>
    </author>
    <author>
      <name>Jan Van den Bussche</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Limburgs Universitair Centrum, Diepenbeek, Belgium</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0407007v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0407007v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.3;F.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0409020v1</id>
    <updated>2004-09-11T11:02:30Z</updated>
    <published>2004-09-11T11:02:30Z</published>
    <title>A Generalized Disjunctive Paraconsistent Data Model for Negative and
  Disjunctive Information</title>
    <summary>  This paper presents a generalization of the disjunctive paraconsistent
relational data model in which disjunctive positive and negative information
can be represented explicitly and manipulated. There are situations where the
closed world assumption to infer negative facts is not valid or undesirable and
there is a need to represent and reason with negation explicitly. We consider
explicit disjunctive negation in the context of disjunctive databases as there
is an interesting interplay between these two types of information. Generalized
disjunctive paraconsistent relation is introduced as the main structure in this
model. The relational algebra is appropriately generalized to work on
generalized disjunctive paraconsistent relations and their correctness is
established.
</summary>
    <author>
      <name>Haibin Wang</name>
    </author>
    <author>
      <name>Yuanchun He</name>
    </author>
    <author>
      <name>Rajshekhar Sunderraman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0409020v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0409020v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0410053v1</id>
    <updated>2004-10-20T08:50:22Z</updated>
    <published>2004-10-20T08:50:22Z</published>
    <title>An Extended Generalized Disjunctive Paraconsistent Data Model for
  Disjunctive Information</title>
    <summary>  This paper presents an extension of generalized disjunctive paraconsistent
relational data model in which pure disjunctive positive and negative
information as well as mixed disjunctive positive and negative information can
be represented explicitly and manipulated. We consider explicit mixed
disjunctive information in the context of disjunctive databases as there is an
interesting interplay between these two types of information. Extended
generalized disjunctive paraconsistent relation is introduced as the main
structure in this model. The relational algebra is appropriately generalized to
work on extended generalized disjunctive paraconsistent relations and their
correctness is established.
</summary>
    <author>
      <name>Haibin Wang</name>
    </author>
    <author>
      <name>Hao Tian</name>
    </author>
    <author>
      <name>Rajshekhar Sunderraman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0410053v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0410053v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0502009v1</id>
    <updated>2005-02-02T03:26:40Z</updated>
    <published>2005-02-02T03:26:40Z</published>
    <title>Performance Considerations for Gigabyte per Second Transcontinental
  Disk-to-Disk File Transfers</title>
    <summary>  Moving data from CERN to Pasadena at a gigabyte per second using the next
generation Internet requires good networking and good disk IO. Ten Gbps
Ethernet and OC192 links are in place, so now it is simply a matter of
programming. This report describes our preliminary work and measurements in
configuring the disk subsystem for this effort. Using 24 SATA disks at each
endpoint we are able to locally read and write an NTFS volume is striped across
24 disks at 1.2 GBps. A 32-disk stripe delivers 1.7 GBps. Experiments on higher
performance and higher-capacity systems deliver up to 3.5 GBps.
</summary>
    <author>
      <name>Peter Kukol</name>
    </author>
    <author>
      <name>Jim Gray</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0502009v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0502009v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0502011v1</id>
    <updated>2005-02-02T04:40:55Z</updated>
    <published>2005-02-02T04:40:55Z</published>
    <title>Where the Rubber Meets the Sky: Bridging the Gap between Databases and
  Science</title>
    <summary>  Scientists in all domains face a data avalanche - both from better
instruments and from improved simulations. We believe that computer science
tools and computer scientists are in a position to help all the sciences by
building tools and developing techniques to manage, analyze, and visualize
peta-scale scientific information. This article is summarizes our experiences
over the last seven years trying to bridge the gap between database technology
and the needs of the astronomy community in building the World-Wide Telescope.
</summary>
    <author>
      <name>Jim Gray</name>
    </author>
    <author>
      <name>Alexander S. Szalay</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Data Engineering Bulletin, Vol 27.4, Dec. 2004, pp. 3-11</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0502011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0502011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0503081v1</id>
    <updated>2005-03-29T13:31:01Z</updated>
    <published>2005-03-29T13:31:01Z</published>
    <title>An Optimization Model for Outlier Detection in Categorical Data</title>
    <summary>  The task of outlier detection is to find small groups of data objects that
are exceptional when compared with rest large amount of data. Detection of such
outliers is important for many applications such as fraud detection and
customer migration. Most existing methods are designed for numeric data. They
will encounter problems with real-life applications that contain categorical
data. In this paper, we formally define the problem of outlier detection in
categorical data as an optimization problem from a global viewpoint. Moreover,
we present a local-search heuristic based algorithm for efficiently finding
feasible solutions. Experimental results on real datasets and large synthetic
datasets demonstrate the superiority of our model and algorithm.
</summary>
    <author>
      <name>Zengyou He</name>
    </author>
    <author>
      <name>Xiaofei Xu</name>
    </author>
    <author>
      <name>Shengchun Deng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0503081v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0503081v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0503092v1</id>
    <updated>2005-03-31T19:43:31Z</updated>
    <published>2005-03-31T19:43:31Z</published>
    <title>Monotonic and Nonmonotonic Preference Revision</title>
    <summary>  We study here preference revision, considering both the monotonic case where
the original preferences are preserved and the nonmonotonic case where the new
preferences may override the original ones. We use a relational framework in
which preferences are represented using binary relations (not necessarily
finite). We identify several classes of revisions that preserve order axioms,
for example the axioms of strict partial or weak orders. We consider
applications of our results to preference querying in relational databases.
</summary>
    <author>
      <name>Jan Chomicki</name>
    </author>
    <author>
      <name>Joyce Song</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0503092v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0503092v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.3; F.4.1; I.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0505002v1</id>
    <updated>2005-04-29T22:28:31Z</updated>
    <published>2005-04-29T22:28:31Z</published>
    <title>Tight Lower Bounds for Query Processing on Streaming and External Memory
  Data</title>
    <summary>  We study a clean machine model for external memory and stream processing. We
show that the number of scans of the external data induces a strict hierarchy
(as long as work space is sufficiently small, e.g., polylogarithmic in the size
of the input). We also show that neither joins nor sorting are feasible if the
product of the number $r(n)$ of scans of the external memory and the size
$s(n)$ of the internal memory buffers is sufficiently small, e.g., of size
$o(\sqrt[5]{n})$. We also establish tight bounds for the complexity of XPath
evaluation and filtering.
</summary>
    <author>
      <name>Martin Grohe</name>
    </author>
    <author>
      <name>Christoph Koch</name>
    </author>
    <author>
      <name>Nicole Schweikardt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 4 figures, to appear in Proc. ICALP 2005; extended version
  with appendix</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0505002v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0505002v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.3; I.7.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0506063v1</id>
    <updated>2005-06-14T23:10:47Z</updated>
    <published>2005-06-14T23:10:47Z</published>
    <title>Priority-Based Conflict Resolution in Inconsistent Relational Databases</title>
    <summary>  We study here the impact of priorities on conflict resolution in inconsistent
relational databases. We extend the framework of repairs and consistent query
answers. We propose a set of postulates that an extended framework should
satisfy and consider two instantiations of the framework: (locally preferred)
l-repairs and (globally preferred) g-repairs. We study the relationships
between them and the impact each notion of repair has on the computational
complexity of repair checking and consistent query answers.
</summary>
    <author>
      <name>Slawomir Staworko</name>
    </author>
    <author>
      <name>Jan Chomicki</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0506063v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0506063v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0605124v1</id>
    <updated>2006-05-26T16:41:15Z</updated>
    <published>2006-05-26T16:41:15Z</published>
    <title>Semantics and Complexity of SPARQL</title>
    <summary>  SPARQL is the W3C candidate recommendation query language for RDF. In this
paper we address systematically the formal study of SPARQL, concentrating in
its graph pattern facility. We consider for this study a fragment without
literals and a simple version of filters which encompasses all the main issues
yet is simple to formalize. We provide a compositional semantics, prove there
are normal forms, prove complexity bounds, among others that the evaluation of
SPARQL patterns is PSPACE-complete, compare our semantics to an alternative
operational semantics, give simple and natural conditions when both semantics
coincide and discuss optimizations procedures.
</summary>
    <author>
      <name>Jorge Perez</name>
    </author>
    <author>
      <name>Marcelo Arenas</name>
    </author>
    <author>
      <name>Claudio Gutierrez</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0605124v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0605124v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0605127v1</id>
    <updated>2006-05-27T00:51:46Z</updated>
    <published>2006-05-27T00:51:46Z</published>
    <title>Analyzing Large Collections of Electronic Text Using OLAP</title>
    <summary>  Computer-assisted reading and analysis of text has various applications in
the humanities and social sciences. The increasing size of many electronic text
archives has the advantage of a more complete analysis but the disadvantage of
taking longer to obtain results. On-Line Analytical Processing is a method used
to store and quickly analyze multidimensional data. By storing text analysis
information in an OLAP system, a user can obtain solutions to inquiries in a
matter of seconds as opposed to minutes, hours, or even days. This analysis is
user-driven allowing various users the freedom to pursue their own direction of
research.
</summary>
    <author>
      <name>Steven Keith</name>
    </author>
    <author>
      <name>Owen Kaser</name>
    </author>
    <author>
      <name>Daniel Lemire</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0605127v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0605127v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0606094v1</id>
    <updated>2006-06-22T09:46:48Z</updated>
    <published>2006-06-22T09:46:48Z</published>
    <title>On Typechecking Top-Down XML Tranformations: Fixed Input or Output
  Schemas</title>
    <summary>  Typechecking consists of statically verifying whether the output of an XML
transformation always conforms to an output type for documents satisfying a
given input type. In this general setting, both the input and output schema as
well as the transformation are part of the input for the problem. However,
scenarios where the input or output schema can be considered to be fixed, are
quite common in practice. In the present work, we investigate the computational
complexity of the typechecking problem in the latter setting.
</summary>
    <author>
      <name>Wim Martens</name>
    </author>
    <author>
      <name>Frank Neven</name>
    </author>
    <author>
      <name>Marc Gyssens</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0606094v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0606094v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0607013v1</id>
    <updated>2006-07-05T18:37:12Z</updated>
    <published>2006-07-05T18:37:12Z</published>
    <title>Database Querying under Changing Preferences</title>
    <summary>  We present here a formal foundation for an iterative and incremental approach
to constructing and evaluating preference queries. Our main focus is on query
modification: a query transformation approach which works by revising the
preference relation in the query. We provide a detailed analysis of the cases
where the order-theoretic properties of the preference relation are preserved
by the revision. We consider a number of different revision operators: union,
prioritized and Pareto composition. We also formulate algebraic laws that
enable incremental evaluation of preference queries. Finally, we consider two
variations of the basic framework: finite restrictions of preference relations
and weak-order extensions of strict partial order preference relations.
</summary>
    <author>
      <name>Jan Chomicki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to a journal</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0607013v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0607013v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.3; F.4.1; I.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0610020v2</id>
    <updated>2006-10-06T23:03:01Z</updated>
    <published>2006-10-04T23:21:11Z</published>
    <title>XString: XML as a String</title>
    <summary>  Extensible markup language (XML) is a technology that has been much hyped, so
that XML has become an industry buzzword. Behind the hype is a powerful
technology for data representation in a platform independent manner. As a text
document, however, XML suffers from being too bloated, and requires an XML
parser to access and manipulate it. XString is an encoding method for XML, in
essence, a markup language's markup language. XString gives the benefit of
compressing XML, and allows for easy manipulation and processing of XML source
as a very long string.
</summary>
    <author>
      <name>William F. Gilreath</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27-pages, 2-tables</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0610020v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0610020v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0612114v1</id>
    <updated>2006-12-21T22:29:07Z</updated>
    <published>2006-12-21T22:29:07Z</published>
    <title>Demaq: A Foundation for Declarative XML Message Processing</title>
    <summary>  This paper gives an overview of Demaq, an XML message processing system
operating on the foundation of transactional XML message queues. We focus on
the syntax and semantics of its fully declarative, rule-based application
language and demonstrate our message-based programming paradigm in the context
of a case study. Further, we discuss optimization opportunities for executing
Demaq programs.
</summary>
    <author>
      <name>Alexander Böhm</name>
    </author>
    <author>
      <name>Carl-Christian Kanne</name>
    </author>
    <author>
      <name>Guido Moerkotte</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This article is published under a Creative Commons License Agreement
  (http://creativecommons.org/licenses/by/2.5/.) You may copy, distribute,
  display, and perform the work, make derivative works and make commercial use
  of the work, but, you must attribute the work to the author and CIDR 2007.
  3rd Biennial Conference on Innovative Data Systems Research (CIDR) January
  710, 2007, Asilomar, California, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0612114v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0612114v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0701157v1</id>
    <updated>2007-01-25T22:53:48Z</updated>
    <published>2007-01-25T22:53:48Z</published>
    <title>A Critique of ANSI SQL Isolation Levels</title>
    <summary>  ANSI SQL-92 defines Isolation Levels in terms of phenomena: Dirty Reads,
Non-Repeatable Reads, and Phantoms. This paper shows that these phenomena and
the ANSI SQL definitions fail to characterize several popular isolation levels,
including the standard locking implementations of the levels. Investigating the
ambiguities of the phenomena leads to clearer definitions; in addition new
phenomena that better characterize isolation types are introduced. An important
multiversion isolation type, Snapshot Isolation, is defined.
</summary>
    <author>
      <name>Hal Berenson</name>
    </author>
    <author>
      <name>Phil Bernstein</name>
    </author>
    <author>
      <name>Jim Gray</name>
    </author>
    <author>
      <name>Jim Melton</name>
    </author>
    <author>
      <name>Elizabeth O'Neil</name>
    </author>
    <author>
      <name>Patrick O'Neil</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. ACM SIGMOD 95, pp. 1-10, San Jose CA, June 1995</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0701157v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0701157v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0701160v1</id>
    <updated>2007-01-25T23:05:40Z</updated>
    <published>2007-01-25T23:05:40Z</published>
    <title>Supporting Finite Element Analysis with a Relational Database Backend,
  Part II: Database Design and Access</title>
    <summary>  This is Part II of a three article series on using databases for Finite
Element Analysis (FEA). It discusses (1) db design, (2) data loading, (3)
typical use cases during grid building, (4) typical use cases during simulation
(get and put), (5) typical use cases during analysis (also done in Part III)
and some performance measures of these cases. It argues that using a database
is simpler to implement than custom data schemas, has better performance
because it can use data parallelism, and better supports FEA modularity and
tool evolution because database schema evolution, data independence, and
self-defining data.
</summary>
    <author>
      <name>Gerd Heber</name>
    </author>
    <author>
      <name>Jim Gray</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0701160v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0701160v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0701163v1</id>
    <updated>2007-01-26T00:00:37Z</updated>
    <published>2007-01-26T00:00:37Z</published>
    <title>Using Table Valued Functions in SQL Server 2005 To Implement a Spatial
  Data Library</title>
    <summary>  This article explains how to add spatial search functions (point-near-point
and point in polygon) to Microsoft SQL Server 2005 using C# and table-valued
functions. It is possible to use this library to add spatial search to your
application without writing any special code. The library implements the
public-domain C# Hierarchical Triangular Mesh (HTM) algorithms from Johns
Hopkins University. That C# library is connected to SQL Server 2005 via a set
of scalar-valued and table-valued functions. These functions act as a spatial
index.
</summary>
    <author>
      <name>Jim Gray</name>
    </author>
    <author>
      <name>Alex Szalay</name>
    </author>
    <author>
      <name>Gyorgy Fekete</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0701163v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0701163v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0701171v1</id>
    <updated>2007-01-26T05:11:20Z</updated>
    <published>2007-01-26T05:11:20Z</published>
    <title>The Zones Algorithm for Finding Points-Near-a-Point or Cross-Matching
  Spatial Datasets</title>
    <summary>  Zones index an N-dimensional Euclidian or metric space to efficiently support
points-near-a-point queries either within a dataset or between two datasets.
The approach uses relational algebra and the B-Tree mechanism found in almost
all relational database systems. Hence, the Zones Algorithm gives a
portable-relational implementation of points-near-point, spatial cross-match,
and self-match queries. This article corrects some mistakes in an earlier
article we wrote on the Zones Algorithm and describes some algorithmic
improvements. The Appendix includes an implementation of point-near-point,
self-match, and cross-match using the USGS city and stream gauge database.
</summary>
    <author>
      <name>Jim Gray</name>
    </author>
    <author>
      <name>Maria A. Nieto-Santisteban</name>
    </author>
    <author>
      <name>Alexander S. Szalay</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0701171v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0701171v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0702012v1</id>
    <updated>2007-02-01T20:52:13Z</updated>
    <published>2007-02-01T20:52:13Z</published>
    <title>Plagiarism Detection in arXiv</title>
    <summary>  We describe a large-scale application of methods for finding plagiarism in
research document collections. The methods are applied to a collection of
284,834 documents collected by arXiv.org over a 14 year period, covering a few
different research disciplines. The methodology efficiently detects a variety
of problematic author behaviors, and heuristics are developed to reduce the
number of false positives. The methods are also efficient enough to implement
as a real-time submission screen for a collection many times larger.
</summary>
    <author>
      <name>Daria Sorokina</name>
    </author>
    <author>
      <name>Johannes Gehrke</name>
    </author>
    <author>
      <name>Simeon Warner</name>
    </author>
    <author>
      <name>Paul Ginsparg</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICDM.2006.126</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICDM.2006.126" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Sixth International Conference on Data Mining (ICDM'06), Dec 2006</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0702012v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0702012v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0702075v1</id>
    <updated>2007-02-13T11:13:29Z</updated>
    <published>2007-02-13T11:13:29Z</published>
    <title>Firebird Database Backup by Serialized Database Table Dump</title>
    <summary>  This paper presents a simple data dump and load utility for Firebird
databases which mimics mysqldump in MySQL. This utility, fb_dump and fb_load,
for dumping and loading respectively, retrieves each database table using
kinterbasdb and serializes the data using marshal module. This utility has two
advantages over the standard Firebird database backup utility, gbak. Firstly,
it is able to backup and restore single database tables which might help to
recover corrupted databases. Secondly, the output is in text-coded format (from
marshal module) making it more resilient than a compressed text backup, as in
the case of using gbak.
</summary>
    <author>
      <name>Maurice HT Ling</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Ling, Maurice HT. 2007. Firebird Database Backup by Serialized
  Database Table Dump. The Python Papers 2 (1): 10-14</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0702075v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0702075v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.7; E.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0707.1288v1</id>
    <updated>2007-07-09T15:52:02Z</updated>
    <published>2007-07-09T15:52:02Z</published>
    <title>Espaces de représentation multidimensionnels dédiés à la
  visualisation</title>
    <summary>  In decision-support systems, the visual component is important for On Line
Analysis Processing (OLAP). In this paper, we propose a new approach that faces
the visualization problem due to data sparsity. We use the results of a
Multiple Correspondence Analysis (MCA) to reduce the negative effect of
sparsity by organizing differently data cube cells. Our approach does not
reduce sparsity, however it tries to build relevant representation spaces where
facts are efficiently gathered. In order to evaluate our approach, we propose
an homogeneity criterion based on geometric neighborhood of cells. The obtained
experimental results have shown the efficiency of our method.
</summary>
    <author>
      <name>Riadh Ben Messaoud</name>
    </author>
    <author>
      <name>Kamel Aouiche</name>
    </author>
    <author>
      <name>Cécile Favre</name>
    </author>
    <link href="http://arxiv.org/abs/0707.1288v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0707.1288v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0707.1304v1</id>
    <updated>2007-07-09T16:58:14Z</updated>
    <published>2007-07-09T16:58:14Z</published>
    <title>Un index de jointure pour les entrepôts de données XML</title>
    <summary>  XML data warehouses form an interesting basis for decision-support
applications that exploit heterogeneous data from multiple sources. However,
XML-native database systems currently bear limited performances and it is
necessary to research ways to optimize them. In this paper, we propose a new
index that is specifically adapted to the multidimensional architecture of XML
warehouses and eliminates join operations, while preserving the information
contained in the original warehouse. A theoretical study and experimental
results demonstrate the efficiency of our index, even when queries are complex.
</summary>
    <author>
      <name>Hadj Mahboubi</name>
    </author>
    <author>
      <name>Kamel Aouiche</name>
    </author>
    <author>
      <name>Jérôme Darmont</name>
    </author>
    <link href="http://arxiv.org/abs/0707.1304v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0707.1304v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0707.1306v1</id>
    <updated>2007-07-09T17:23:31Z</updated>
    <published>2007-07-09T17:23:31Z</published>
    <title>Sélection simultanée d'index et de vues matérialisées</title>
    <summary>  Indices and materialized views are physical structures that accelerate data
access in data warehouses. However, these data structures generate some
maintenance overhead. They also share the same storage space. The existing
studies about index and materialized view selection consider these structures
separately. In this paper, we adopt the opposite stance and couple index and
materialized view selection to take into account the interactions between them
and achieve an efficient storage space sharing. We develop cost models that
evaluate the respective benefit of indexing and view materialization. These
cost models are then exploited by a greedy algorithm to select a relevant
configuration of indices and materialized views. Experimental results show that
our strategy performs better than the independent selection of indices and
materialized views.
</summary>
    <author>
      <name>Nora Maiz</name>
    </author>
    <author>
      <name>Kamel Aouiche</name>
    </author>
    <author>
      <name>Jérôme Darmont</name>
    </author>
    <link href="http://arxiv.org/abs/0707.1306v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0707.1306v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0708.2717v1</id>
    <updated>2007-08-20T20:08:53Z</updated>
    <published>2007-08-20T20:08:53Z</published>
    <title>Aggregation Languages for Moving Object and Places of Interest Data</title>
    <summary>  We address aggregate queries over GIS data and moving object data, where
non-spatial data are stored in a data warehouse. We propose a formal data model
and query language to express complex aggregate queries. Next, we study the
compression of trajectory data, produced by moving objects, using the notions
of stops and moves. We show that stops and moves are expressible in our query
language and we consider a fragment of this language, consisting of regular
expressions to talk about temporally ordered sequences of stops and moves. This
fragment can be used to efficiently express data mining and pattern matching
tasks over trajectory data.
</summary>
    <author>
      <name>Leticia Gomez</name>
    </author>
    <author>
      <name>Bart Kuijpers</name>
    </author>
    <author>
      <name>Alejandro Vaisman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0708.2717v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0708.2717v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.1404v1</id>
    <updated>2007-10-07T08:22:53Z</updated>
    <published>2007-10-07T08:22:53Z</published>
    <title>Performance Comparison of Persistence Frameworks</title>
    <summary>  One of the essential and most complex components in the software development
process is the database. The complexity increases when the "orientation" of the
interacting components differs. A persistence framework moves the program data
in its most natural form to and from a permanent data store, the database. Thus
a persistence framework manages the database and the mapping between the
database and the objects. This paper compares the performance of two
persistence frameworks ? Hibernate and iBatis?s SQLMaps using a banking
database. The performance of both of these tools in single and multi-user
environments are evaluated.
</summary>
    <author>
      <name>Sabu M. Thampi</name>
    </author>
    <author>
      <name>Ashwin a K</name>
    </author>
    <link href="http://arxiv.org/abs/0710.1404v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.1404v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.2604v1</id>
    <updated>2007-10-13T11:47:14Z</updated>
    <published>2007-10-13T11:47:14Z</published>
    <title>Efficient Skyline Querying with Variable User Preferences on Nominal
  Attributes</title>
    <summary>  Current skyline evaluation techniques assume a fixed ordering on the
attributes. However, dynamic preferences on nominal attributes are more
realistic in known applications. In order to generate online response for any
such preference issued by a user, we propose two methods of different
characteristics. The first one is a semi-materialization method and the second
is an adaptive SFS method. Finally, we conduct experiments to show the
efficiency of our proposed algorithms.
</summary>
    <author>
      <name>Raymond Chi-Wing Wong</name>
    </author>
    <author>
      <name>Ada Wai-chee Fu</name>
    </author>
    <author>
      <name>Jian Pei</name>
    </author>
    <author>
      <name>Yip Sing Ho</name>
    </author>
    <author>
      <name>Tai Wong</name>
    </author>
    <author>
      <name>Yubao Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0710.2604v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.2604v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0801.0131v1</id>
    <updated>2007-12-30T14:29:17Z</updated>
    <published>2007-12-30T14:29:17Z</published>
    <title>Two-Level Concept-Oriented Data Model</title>
    <summary>  In this paper we describe a new approach to data modelling called the
concept-oriented model (CoM). This model is based on the formalism of nested
ordered sets which uses inclusion relation to produce hierarchical structure of
sets and ordering relation to produce multi-dimensional structure among its
elements. Nested ordered set is defined as an ordered set where an each element
can be itself an ordered set. Ordering relation in CoM is used to define data
semantics and operations with data such as projection and de-projection. This
data model can be applied to very different problems and the paper describes
some its uses such grouping with aggregation and multi-dimensional analysis.
</summary>
    <author>
      <name>Alexandr Savinov</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Institute of Mathematics and Computer Science, Academy of Sciences
  of Moldova, Technical Report RT0006, 2007</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0801.0131v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.0131v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0802.0137v3</id>
    <updated>2009-03-31T14:41:43Z</updated>
    <published>2008-02-01T14:47:24Z</published>
    <title>Fault-Tolerant Partial Replication in Large-Scale Database Systems</title>
    <summary>  We investigate a decentralised approach to committing transactions in a
replicated database, under partial replication. Previous protocols either
re-execute transactions entirely and/or compute a total order of transactions.
In contrast, ours applies update values, and orders only conflicting
transactions. It results that transactions execute faster, and distributed
databases commit in small committees. Both effects contribute to preserve
scalability as the number of databases and transactions increase. Our algorithm
ensures serializability, and is live and safe in spite of faults.
</summary>
    <author>
      <name>Pierre Sutra</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt</arxiv:affiliation>
    </author>
    <author>
      <name>Marc Shapiro</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/0802.0137v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0802.0137v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0804.3171v2</id>
    <updated>2008-04-27T19:32:31Z</updated>
    <published>2008-04-20T03:23:38Z</published>
    <title>Optimization Approach for Detecting the Critical Data on a Database</title>
    <summary>  Through purposeful introduction of malicious transactions (tracking
transactions) into randomly select nodes of a (database) graph, soiled and
clean segments are identified. Soiled and clean measures corresponding those
segments are then computed. These measures are used to repose the problem of
critical database elements detection as an optimization problem over the graph.
This method is universally applicable over a large class of graphs (including
directed, weighted, disconnected, cyclic) that occur in several contexts of
databases. A generalization argument is presented which extends the critical
data problem to abstract settings.
</summary>
    <author>
      <name>Prashanth Alluvada</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 figure, 3 tables. corrected typos, added remarks</arxiv:comment>
    <link href="http://arxiv.org/abs/0804.3171v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0804.3171v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0806.0075v1</id>
    <updated>2008-05-31T14:49:00Z</updated>
    <published>2008-05-31T14:49:00Z</published>
    <title>An Experimental Investigation of XML Compression Tools</title>
    <summary>  This paper presents an extensive experimental study of the state-of-the-art
of XML compression tools. The study reports the behavior of nine XML
compressors using a large corpus of XML documents which covers the different
natures and scales of XML documents. In addition to assessing and comparing the
performance characteristics of the evaluated XML compression tools, the study
tries to assess the effectiveness and practicality of using these tools in the
real world. Finally, we provide some guidelines and recommen- dations which are
useful for helping developers and users for making an effective decision for
selecting the most suitable XML compression tool for their needs.
</summary>
    <author>
      <name>Sherif Sakr</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">http://xmlcompbench.sourceforge.net/</arxiv:comment>
    <link href="http://arxiv.org/abs/0806.0075v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0806.0075v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0806.3115v1</id>
    <updated>2008-06-19T02:06:14Z</updated>
    <published>2008-06-19T02:06:14Z</published>
    <title>Using rational numbers to key nested sets</title>
    <summary>  This report details the generation and use of tree node ordering keys in a
single relational database table. The keys for each node are calculated from
the keys of its parent, in such a way that the sort order places every node in
the tree before all of its descendants and after all siblings having a lower
index. The calculation from parent keys to child keys is simple, and reversible
in the sense that the keys of every ancestor of a node can be calculated from
that node's keys without having to consult the database.
  Proofs of the above properties of the key encoding process and of its
correspondence to a finite continued fraction form are provided.
</summary>
    <author>
      <name>Dan Hazel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Technology One</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0806.3115v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0806.3115v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0808.2083v3</id>
    <updated>2009-01-19T15:22:33Z</updated>
    <published>2008-08-15T03:14:55Z</published>
    <title>Histogram-Aware Sorting for Enhanced Word-Aligned Compression in Bitmap
  Indexes</title>
    <summary>  Bitmap indexes must be compressed to reduce input/output costs and minimize
CPU usage. To accelerate logical operations (AND, OR, XOR) over bitmaps, we use
techniques based on run-length encoding (RLE), such as Word-Aligned Hybrid
(WAH) compression. These techniques are sensitive to the order of the rows: a
simple lexicographical sort can divide the index size by 9 and make indexes
several times faster. We investigate reordering heuristics based on computed
attribute-value histograms. Simply permuting the columns of the table based on
these histograms can increase the sorting efficiency by 40%.
</summary>
    <author>
      <name>Owen Kaser</name>
    </author>
    <author>
      <name>Daniel Lemire</name>
    </author>
    <author>
      <name>Kamel Aouiche</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in proceedings of DOLAP 2008</arxiv:comment>
    <link href="http://arxiv.org/abs/0808.2083v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0808.2083v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.2; E.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.1971v1</id>
    <updated>2008-09-11T12:20:00Z</updated>
    <published>2008-09-11T12:20:00Z</published>
    <title>Knowledge and Metadata Integration for Warehousing Complex Data</title>
    <summary>  With the ever-growing availability of so-called complex data, especially on
the Web, decision-support systems such as data warehouses must store and
process data that are not only numerical or symbolic. Warehousing and analyzing
such data requires the joint exploitation of metadata and domain-related
knowledge, which must thereby be integrated. In this paper, we survey the types
of knowledge and metadata that are needed for managing complex data, discuss
the issue of knowledge and metadata integration, and propose a CWM-compliant
integration solution that we incorporate into an XML complex data warehousing
framework we previously designed.
</summary>
    <author>
      <name>Jean-Christian Ralaivao</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Jérôme Darmont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6th International Conference on Information Systems Technology and
  its Applications (ISTA 07), Kharkiv : Ukraine (2007)</arxiv:comment>
    <link href="http://arxiv.org/abs/0809.1971v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.1971v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.1981v1</id>
    <updated>2008-09-11T12:44:10Z</updated>
    <published>2008-09-11T12:44:10Z</published>
    <title>A Join Index for XML Data Warehouses</title>
    <summary>  XML data warehouses form an interesting basis for decision-support
applications that exploit complex data. However, native-XML database management
systems (DBMSs) currently bear limited performances and it is necessary to
research for ways to optimize them. In this paper, we propose a new join index
that is specifically adapted to the multidimensional architecture of XML
warehouses. It eliminates join operations while preserving the information
contained in the original warehouse. A theoretical study and experimental
results demonstrate the efficiency of our join index. They also show that
native XML DBMSs can compete with XML-compatible, relational DBMSs when
warehousing and analyzing XML data.
</summary>
    <author>
      <name>Hadj Mahboubi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Kamel Aouiche</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Jérôme Darmont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2008 International Conference on Information Resources Management
  (Conf-IRM 08), Niagra Falls : Canada (2008)</arxiv:comment>
    <link href="http://arxiv.org/abs/0809.1981v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.1981v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0810.5582v2</id>
    <updated>2008-11-03T23:33:20Z</updated>
    <published>2008-10-31T19:25:02Z</published>
    <title>Anonymizing Unstructured Data</title>
    <summary>  In this paper we consider the problem of anonymizing datasets in which each
individual is associated with a set of items that constitute private
information about the individual. Illustrative datasets include market-basket
datasets and search engine query logs. We formalize the notion of k-anonymity
for set-valued data as a variant of the k-anonymity model for traditional
relational datasets. We define an optimization problem that arises from this
definition of anonymity and provide O(klogk) and O(1)-approximation algorithms
for the same. We demonstrate applicability of our algorithms to the America
Online query log dataset.
</summary>
    <author>
      <name>Rajeev Motwani</name>
    </author>
    <author>
      <name>Shubha U. Nabar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/0810.5582v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0810.5582v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0811.2117v1</id>
    <updated>2008-11-13T14:12:57Z</updated>
    <published>2008-11-13T14:12:57Z</published>
    <title>Disjunctive Databases for Representing Repairs</title>
    <summary>  This paper addresses the problem of representing the set of repairs of a
possibly inconsistent database by means of a disjunctive database.
Specifically, the class of denial constraints is considered. We show that,
given a database and a set of denial constraints, there exists a (unique)
disjunctive database, called canonical, which represents the repairs of the
database w.r.t. the constraints and is contained in any other disjunctive
database with the same set of minimal models. We propose an algorithm for
computing the canonical disjunctive database. Finally, we study the size of the
canonical disjunctive database in the presence of functional dependencies for
both repairs and cardinality-based repairs.
</summary>
    <author>
      <name>Cristian Molinaro</name>
    </author>
    <author>
      <name>Jan Chomicki</name>
    </author>
    <author>
      <name>Jerzy Marcinkowski</name>
    </author>
    <link href="http://arxiv.org/abs/0811.2117v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0811.2117v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0811.3301v2</id>
    <updated>2009-06-10T17:37:32Z</updated>
    <published>2008-11-20T16:22:05Z</published>
    <title>Faster Retrieval with a Two-Pass Dynamic-Time-Warping Lower Bound</title>
    <summary>  The Dynamic Time Warping (DTW) is a popular similarity measure between time
series. The DTW fails to satisfy the triangle inequality and its computation
requires quadratic time. Hence, to find closest neighbors quickly, we use
bounding techniques. We can avoid most DTW computations with an inexpensive
lower bound (LB Keogh). We compare LB Keogh with a tighter lower bound (LB
Improved). We find that LB Improved-based search is faster. As an example, our
approach is 2-3 times faster over random-walk and shape time series.
</summary>
    <author>
      <name>Daniel Lemire</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.patcog.2008.11.030</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.patcog.2008.11.030" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in Pattern Recognition on November 20th, 2008</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Daniel Lemire, Faster Retrieval with a Two-Pass
  Dynamic-Time-Warping Lower Bound, Pattern Recognition 42(9): 2169-2180 (2009)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0811.3301v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0811.3301v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0812.3715v1</id>
    <updated>2008-12-19T07:52:46Z</updated>
    <published>2008-12-19T07:52:46Z</published>
    <title>Business processes integration and performance indicators in a PLM</title>
    <summary>  In an economic environment more and more competitive, the effective
management of information and knowledge is a strategic issue for industrial
enterprises. In the global marketplace, companies must use reactive strategies
and reduce their products development cycle. In this context, the PLM (Product
Lifecycle Management) is considered as a key component of the information
system. The aim of this paper is to present an approach to integrate Business
Processes in a PLM system. This approach is implemented in automotive sector
with second-tier subcontractor
</summary>
    <author>
      <name>Aurélie Bissay</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIESP</arxiv:affiliation>
    </author>
    <author>
      <name>Philippe Pernelle</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIESP</arxiv:affiliation>
    </author>
    <author>
      <name>Arnaud Lefebvre</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIESP</arxiv:affiliation>
    </author>
    <author>
      <name>Abdelaziz Bouras</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIESP</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">APMS'08, Espoo : Finlande (2008)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0812.3715v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0812.3715v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.3072v1</id>
    <updated>2009-03-18T04:54:54Z</updated>
    <published>2009-03-18T04:54:54Z</published>
    <title>Spatial Skyline Queries: An Efficient Geometric Algorithm</title>
    <summary>  As more data-intensive applications emerge, advanced retrieval semantics,
such as ranking or skylines, have attracted attention. Geographic information
systems are such an application with massive spatial data. Our goal is to
efficiently support skyline queries over massive spatial data. To achieve this
goal, we first observe that the best known algorithm VS2, despite its claim,
may fail to deliver correct results. In contrast, we present a simple and
efficient algorithm that computes the correct results. To validate the
effectiveness and efficiency of our algorithm, we provide an extensive
empirical comparison of our algorithm and VS2 in several aspects.
</summary>
    <author>
      <name>Wanbin Son</name>
    </author>
    <author>
      <name>Mu-Woong Lee</name>
    </author>
    <author>
      <name>Hee-Kap Ahn</name>
    </author>
    <author>
      <name>Seung-won Hwang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, uses LNCS format</arxiv:comment>
    <link href="http://arxiv.org/abs/0903.3072v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.3072v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.4305v2</id>
    <updated>2009-06-22T14:55:54Z</updated>
    <published>2009-03-25T11:39:31Z</published>
    <title>Evaluation d'une requete en SQL</title>
    <summary>  The objective of this paper is to show how the interrogation processor
responds to SQL interrogation. The interrogation processor is split into two
parts. The first, called the interrogation compiler translates an SQL query
into a plan of physical execution. The second, called evaluation query runs the
execution plan.
</summary>
    <author>
      <name>Diana Sophia Codat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, exposed on 4th International Conferences "Actualities and
  Perspectives on Hardware and Software" - APHS2007, Timisoara, Romania</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Ann. Univ. Tibiscus Comp. Sci. Series V (2007), 99-104</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0903.4305v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.4305v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0904.3312v1</id>
    <updated>2009-04-21T18:38:25Z</updated>
    <published>2009-04-21T18:38:25Z</published>
    <title>HybridMiner: Mining Maximal Frequent Itemsets Using Hybrid Database
  Representation Approach</title>
    <summary>  In this paper we present a novel hybrid (arraybased layout and vertical
bitmap layout) database representation approach for mining complete Maximal
Frequent Itemset (MFI) on sparse and large datasets. Our work is novel in terms
of scalability, item search order and two horizontal and vertical projection
techniques. We also present a maximal algorithm using this hybrid database
representation approach. Different experimental results on real and sparse
benchmark datasets show that our approach is better than previous state of art
maximal algorithms.
</summary>
    <author>
      <name>Shariq Bashir</name>
    </author>
    <author>
      <name>Abdul Rauf Baig</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/INMIC.2005.334484</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/INMIC.2005.334484" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 Pages In the proceedings of 9th IEEE-INMIC 2005, Karachi, Pakistan,
  2005</arxiv:comment>
    <link href="http://arxiv.org/abs/0904.3312v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0904.3312v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0905.4138v1</id>
    <updated>2009-05-26T08:52:42Z</updated>
    <published>2009-05-26T08:52:42Z</published>
    <title>Faster estimation of the correlation fractal dimension using
  box-counting</title>
    <summary>  Fractal dimension is widely adopted in spatial databases and data mining,
among others as a measure of dataset skewness. State-of-the-art algorithms for
estimating the fractal dimension exhibit linear runtime complexity whether
based on box-counting or approximation schemes. In this paper, we revisit a
correlation fractal dimension estimation algorithm that redundantly rescans the
dataset and, extending that work, we propose another linear, yet faster and as
accurate method, which completes in a single pass.
</summary>
    <author>
      <name>Christos Attikos</name>
    </author>
    <author>
      <name>Michael Doumpos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, to appear in BCI 2009 - 4th Balkan Conference in Informatics</arxiv:comment>
    <link href="http://arxiv.org/abs/0905.4138v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0905.4138v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0905.4605v1</id>
    <updated>2009-05-28T10:29:20Z</updated>
    <published>2009-05-28T10:29:20Z</published>
    <title>Techniques for Securing Data Exchange between a Database Server and a
  Client Program</title>
    <summary>  The goal of the presented work is to illustrate a method by which the data
exchange between a standalone computer software and a shared database server
can be protected of unauthorized interceptation of the traffic in Internet
network, a transport network for data managed by those two systems,
interceptation by which an attacker could gain illegetimate access to the
database, threatening this way the data integrity and compromising the
database.
</summary>
    <author>
      <name>Ovidiu Crista</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, exposed on 5th International Conference "Actualities and
  Perspectives on Hardware and Software" - APHS2009, Timisoara, Romania</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Ann. Univ. Tibiscus Comp. Sci. Series VII(2009),95-100</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0905.4605v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0905.4605v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.0910v1</id>
    <updated>2009-06-04T13:28:51Z</updated>
    <published>2009-06-04T13:28:51Z</published>
    <title>On the Challenges of Collaborative Data Processing</title>
    <summary>  The last 30 years have seen the creation of a variety of electronic
collaboration tools for science and business. Some of the best-known
collaboration tools support text editing (e.g., wikis). Wikipedia's success
shows that large-scale collaboration can produce highly valuable content.
Meanwhile much structured data is being collected and made publicly available.
We have never had access to more powerful databases and statistical packages.
Is large-scale collaborative data analysis now possible? Using a quantitative
analysis of Web 2.0 data visualization sites, we find evidence that at least
moderate open collaboration occurs. We then explore some of the limiting
factors of collaboration over data.
</summary>
    <author>
      <name>Sylvie Noel</name>
    </author>
    <author>
      <name>Daniel Lemire</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear as a chapter in an upcoming book (Collaborative Information
  Behavior)</arxiv:comment>
    <link href="http://arxiv.org/abs/0906.0910v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.0910v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.4927v1</id>
    <updated>2009-06-26T13:24:57Z</updated>
    <published>2009-06-26T13:24:57Z</published>
    <title>Fast Probabilistic Ranking under x-Relation Model</title>
    <summary>  The probabilistic top-k queries based on the interplay of score and
probability, under the possible worlds semantic, become an important research
issue that considers both score and uncertainty on the same basis. In the
literature, many different probabilistic top-k queries are proposed. Almost all
of them need to compute the probability of a tuple t_i to be ranked at the j-th
position across the entire set of possible worlds. The cost of such computing
is the dominant cost and is known as O(kn^2), where n is the size of dataset.
In this paper, we propose a new novel algorithm that computes such probability
in O(kn).
</summary>
    <author>
      <name>Lijun Chang</name>
    </author>
    <author>
      <name>Jeffrey Xu Yu</name>
    </author>
    <author>
      <name>Lu Qin</name>
    </author>
    <link href="http://arxiv.org/abs/0906.4927v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.4927v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.1763v1</id>
    <updated>2009-09-09T18:09:06Z</updated>
    <published>2009-09-09T18:09:06Z</published>
    <title>Remembrance: The Unbearable Sentience of Being Digital</title>
    <summary>  We introduce a world vision in which data is endowed with memory. In this
data-centric systems paradigm, data items can be enabled to retain all or some
of their previous values. We call this ability "remembrance" and posit that it
empowers significant leaps in the security, availability, and general
operational dimensions of systems. With the explosion in cheap, fast memories
and storage, large-scale remembrance will soon become practical. Here, we
introduce and explore the advantages of such a paradigm and the challenges in
making it a reality.
</summary>
    <author>
      <name>Ragib Hasan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Illinois</arxiv:affiliation>
    </author>
    <author>
      <name>Radu Sion</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Stony Brook University</arxiv:affiliation>
    </author>
    <author>
      <name>Marianne Winslett</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Illinois</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CIDR 2009</arxiv:comment>
    <link href="http://arxiv.org/abs/0909.1763v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.1763v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.1782v1</id>
    <updated>2009-09-09T18:10:33Z</updated>
    <published>2009-09-09T18:10:33Z</published>
    <title>Principles for Inconsistency</title>
    <summary>  Data consistency is very desirable because strong semantic properties make it
easier to write correct programs that perform as users expect. However, there
are good reasons why consistency may have to be weakened to achieve other
business goals. In this CIDR 2009 Perspectives paper, we present real-world
reasons inconsistency may be necessary, offer principles for managing
inconsistency coherently, and describe implementation approaches we are
investigating for sustainably scalable systems that offer comprehensible user
experiences despite inconsistency.
</summary>
    <author>
      <name>Shel Finkelstein</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">SAP</arxiv:affiliation>
    </author>
    <author>
      <name>Dean Jacobs</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">SAP</arxiv:affiliation>
    </author>
    <author>
      <name>Rainer Brendle</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">SAP</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CIDR 2009</arxiv:comment>
    <link href="http://arxiv.org/abs/0909.1782v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.1782v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.4412v1</id>
    <updated>2009-09-24T11:09:15Z</updated>
    <published>2009-09-24T11:09:15Z</published>
    <title>Algorithm for Spatial Clustering with Obstacles</title>
    <summary>  In this paper, we propose an efficient clustering technique to solve the
problem of clustering in the presence of obstacles. The proposed algorithm
divides the spatial area into rectangular cells. Each cell is associated with
statistical information that enables us to label the cell as dense or
non-dense. We also label each cell as obstructed (i.e. intersects any obstacle)
or non-obstructed. Then the algorithm finds the regions (clusters) of
connected, dense, non-obstructed cells. Finally, the algorithm finds a center
for each such region and returns those centers as centers of the relatively
dense regions (clusters) in the spatial area.
</summary>
    <author>
      <name>Mohamed E. El-Sharkawi</name>
    </author>
    <author>
      <name>Mohamed A. El-Zawawy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proc. 2002 ICICIS Int. Conference on Intelligent Computing and
  Information Systems (ICICIS02), Cairo, Egypt, June 2002</arxiv:comment>
    <link href="http://arxiv.org/abs/0909.4412v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.4412v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0911.3600v1</id>
    <updated>2009-11-18T16:59:38Z</updated>
    <published>2009-11-18T16:59:38Z</published>
    <title>"Almost automatic" and semantic integration of XML Schemas at various
  "severity" levels</title>
    <summary>  This paper presents a novel approach for the integration of a set of XML
Schemas. The proposed approach is specialized for XML, is almost automatic,
semantic and "light". As a further, original, peculiarity, it is parametric
w.r.t. a "severity" level against which the integration task is performed. The
paper describes the approach in all details, illustrates various theoretical
results, presents the experiments we have performed for testing it and,
finally, compares it with various related approaches already proposed in the
literature.
</summary>
    <author>
      <name>P. De Meo</name>
    </author>
    <author>
      <name>G. Quattrone</name>
    </author>
    <author>
      <name>G. Terracina</name>
    </author>
    <author>
      <name>D. Ursino</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 3 Figures, 3 Tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. of the International Conference on Cooperative Information
  Systems (CoopIS 2003), pages 4 -21, Taormina, Italy, 2003. Lecture Notes in
  Computer Science, Springer</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0911.3600v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.3600v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.2134v1</id>
    <updated>2009-12-11T00:58:32Z</updated>
    <published>2009-12-11T00:58:32Z</published>
    <title>Enterprise Multi-Branch Database Synchronization with MSMQ</title>
    <summary>  When we talk about databases there have always been problems concerning data
synchronization. The latter is a technique for maintaining consistency among
different copies of data (often called replicas). In general, there is no
universal solution to this problem and often a particular situation requires a
particular approach driven by specific conditions. This paper presents an
approach tackling the issue of data synchronization in a distributed
multi-branch enterprise database. The proposed solution is based on MSMQ
(Microsoft Message Queue), a mechanism for asynchronous messaging.
</summary>
    <author>
      <name>Emil Vassev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0912.2134v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.2134v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.2822v1</id>
    <updated>2009-12-15T08:55:12Z</updated>
    <published>2009-12-15T08:55:12Z</published>
    <title>Data management in Systems biology II - Outlook towards the semantic web</title>
    <summary>  The benefit of using ontologies, defined by the respective data standards, is
shown. It is presented how ontologies can be used for the semantic enrichment
of data and how this can contribute to the vision of the semantic web to become
true. The problems existing today on the way to a true semantic web are
pinpointed, different semantic web standards, tools and development frameworks
are overlooked and an outlook towards artificial intelligence and agents for
searching and mining the data in the semantic web are given, paving the way
from data management to information and in the end true knowledge management
systems.
</summary>
    <author>
      <name>Gerhard Mayer</name>
    </author>
    <link href="http://arxiv.org/abs/0912.2822v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.2822v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.3494v1</id>
    <updated>2010-01-20T08:02:20Z</updated>
    <published>2010-01-20T08:02:20Z</published>
    <title>Proposing a New Method for Query Processing Adaption in DataBase</title>
    <summary>  This paper proposes a multi agent system by compiling two technologies, query
processing optimization and agents which contains features of personalized
queries and adaption with changing of requirements. This system uses a new
algorithm based on modeling of users' long-term requirements and also GA to
gather users' query data. Experimented Result shows more adaption capability
for presented algorithm in comparison with classic algorithms.
</summary>
    <author>
      <name>Mohammad-Reza Feizi-Derakhshi</name>
    </author>
    <author>
      <name>Hasan Asil</name>
    </author>
    <author>
      <name>Amir Asil</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Computing, Vol. 2, Issue 1, January 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1001.3494v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.3494v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.4892v1</id>
    <updated>2010-01-27T10:38:51Z</updated>
    <published>2010-01-27T10:38:51Z</published>
    <title>Janus: Automatic Ontology Builder from XSD Files</title>
    <summary>  The construction of a reference ontology for a large domain still remains an
hard human task. The process is sometimes assisted by software tools that
facilitate the information extraction from a textual corpus. Despite of the
great use of XML Schema files on the internet and especially in the B2B domain,
tools that offer a complete semantic analysis of XML schemas are really rare.
In this paper we introduce Janus, a tool for automatically building a reference
knowledge base starting from XML Schema files. Janus also provides different
useful views to simplify B2B application integration.
</summary>
    <author>
      <name>Ivan Bedini</name>
    </author>
    <author>
      <name>Benjamin Nguyen</name>
    </author>
    <author>
      <name>Georges Gardarin</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the World Wide Web Conference (WWW), Beijin, China,
  April 2008 (Developper Track), 2008</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1001.4892v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.4892v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.1010v1</id>
    <updated>2010-03-04T10:31:01Z</updated>
    <published>2010-03-04T10:31:01Z</published>
    <title>Verifying Recursive Active Documents with Positive Data Tree Rewriting</title>
    <summary>  This paper proposes a data tree-rewriting framework for modeling evolving
documents. The framework is close to Guarded Active XML, a platform used for
handling XML repositories evolving through web services. We focus on automatic
verification of properties of evolving documents that can contain data from an
infinite domain. We establish the boundaries of decidability, and show that
verification of a {\em positive} fragment that can handle recursive service
calls is decidable. We also consider bounded model-checking in our data
tree-rewriting framework and show that it is $\nexptime$-complete.
</summary>
    <author>
      <name>Blaise Genest</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA - Irisa</arxiv:affiliation>
    </author>
    <author>
      <name>Anca Muscholl</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LaBRI</arxiv:affiliation>
    </author>
    <author>
      <name>Zhilin Wu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LaBRI</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1003.1010v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.1010v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.2682v1</id>
    <updated>2010-03-13T06:22:07Z</updated>
    <published>2010-03-13T06:22:07Z</published>
    <title>Table manipulation in simplicial databases</title>
    <summary>  In \cite{Spi}, we developed a category of databases in which the schema of a
database is represented as a simplicial set. Each simplex corresponds to a
table in the database. There, our main concern was to find a categorical
formulation of databases; the simplicial nature of the schemas was to some
degree unexpected and unexploited.
  In the present note, we show how to use this geometric formulation
effectively on a computer. If we think of each simplex as a polygonal tile, we
can imagine assembling custom databases by mixing and matching tiles. Queries
on this database can be performed by drawing paths through the resulting tile
formations, selecting records at the start-point of this path and retrieving
corresponding records at its end-point.
</summary>
    <author>
      <name>David I. Spivak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages.</arxiv:comment>
    <link href="http://arxiv.org/abs/1003.2682v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.2682v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2; H.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.4418v1</id>
    <updated>2010-03-23T13:55:40Z</updated>
    <published>2010-03-23T13:55:40Z</published>
    <title>Evaluation of Query Generators for Entity Search Engines</title>
    <summary>  Dynamic web applications such as mashups need efficient access to web data
that is only accessible via entity search engines (e.g. product or publication
search engines). However, most current mashup systems and applications only
support simple keyword searches for retrieving data from search engines. We
propose the use of more powerful search strategies building on so-called query
generators. For a given set of entities query generators are able to
automatically determine a set of search queries to retrieve these entities from
an entity search engine. We demonstrate the usefulness of query generators for
on-demand web data integration and evaluate the effectiveness and efficiency of
query generators for a challenging real-world integration scenario.
</summary>
    <author>
      <name>Stefan Endrullis</name>
    </author>
    <author>
      <name>Andreas Thor</name>
    </author>
    <author>
      <name>Erhard Rahm</name>
    </author>
    <link href="http://arxiv.org/abs/1003.4418v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.4418v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; H.3.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.4827v1</id>
    <updated>2010-03-25T09:23:18Z</updated>
    <published>2010-03-25T09:23:18Z</published>
    <title>Tuple-based abstract data types: full parallelism</title>
    <summary>  Commutativity has the same inherent limitations as compatibility. Then, it is
worth conceiving simple concurrency control techniques. We propose a restricted
form of commutativity which increases parallelism without incurring a higher
overhead than compatibility. Advantages of our proposition are: (1)
commutativity of operations is determined at compile-time, (2) run-time
checking is as efficient as for compatibility, (3) neither commutativity
relations, (4) nor inverse operations, need to be specified, and (5) log space
utilization is reduced.
</summary>
    <author>
      <name>José Martinez</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LINA</arxiv:affiliation>
    </author>
    <author>
      <name>Carmelo Malta</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Symposium on Computer and Information Sciences
  (ISCIS'92), Antalya : Turkey (1992)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1003.4827v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.4827v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.4828v1</id>
    <updated>2010-03-25T09:33:35Z</updated>
    <published>2010-03-25T09:33:35Z</published>
    <title>A framework for designing concurrent and recoverable abstract data types
  based on commutativity</title>
    <summary>  In this paper, we try to focus the reader's interest on the problems that
transactional systems have to resolve for taking advantage of commutativity in
a serializable and recoverable way. Our framework is, (as others), based on the
use of conditional commutativity on abstract date types. We present new
features that have not been found in the literature hitherto, that both
increase concurrency and simplify recovery.
</summary>
    <author>
      <name>Carmelo Malta</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LINA</arxiv:affiliation>
    </author>
    <author>
      <name>José Martinez</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LINA</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Symposium on Computer and Information Sciences
  (ISCIS'91), Side : Turkey (1991)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1003.4828v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.4828v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.4830v1</id>
    <updated>2010-03-25T09:52:39Z</updated>
    <published>2010-03-25T09:52:39Z</published>
    <title>Limits of Commutativity on Abstract Data Types</title>
    <summary>  We present some formal properties of (symmetrical) commutativity, the major
criterion used in transactional systems, which allow us to fully understand its
advantages and disadvantages. The main result is that commutativity is subject
to the same limitation as compatibility for arbitrary objects. However,
commutativity has also a number of attracting properties, one of which is
related to recovery and, to our knowledge, has not been exploited in the
literature. Advantages and disadvantages are illustrated on abstract data types
of interest. We also show how limits of commutativity have been circumvented,
which gives guidelines for doing so (or not!).
</summary>
    <author>
      <name>Carmelo Malta</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LINA</arxiv:affiliation>
    </author>
    <author>
      <name>José Martinez</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LINA</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">5th International Conference on Information Systems and Management
  of Data (CISMOD'92), Bangalore : India (1992)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1003.4830v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.4830v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.5350v1</id>
    <updated>2010-03-28T08:11:05Z</updated>
    <published>2010-03-28T08:11:05Z</published>
    <title>An Improved Algorithm for Generating Database Transactions from
  Relational Algebra Specifications</title>
    <summary>  Alloy is a lightweight modeling formalism based on relational algebra. In
prior work with Fisler, Giannakopoulos, Krishnamurthi, and Yoo, we have
presented a tool, Alchemy, that compiles Alloy specifications into
implementations that execute against persistent databases. The foundation of
Alchemy is an algorithm for rewriting relational algebra formulas into code for
database transactions. In this paper we report on recent progress in improving
the robustness and efficiency of this transformation.
</summary>
    <author>
      <name>Daniel J. Dougherty</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.21.7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.21.7" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 21, 2010, pp. 77-89</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1003.5350v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.5350v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.1747v1</id>
    <updated>2010-04-10T22:23:27Z</updated>
    <published>2010-04-10T22:23:27Z</published>
    <title>Mobile Database System: Role of Mobility on the Query Processing</title>
    <summary>  The rapidly expanding technology of mobile communication will give mobile
users capability of accessing information from anywhere and any time. The
wireless technology has made it possible to achieve continuous connectivity in
mobile environment. When the query is specified as continuous, the requesting
mobile user can obtain continuously changing result. In order to provide
accurate and timely outcome to requesting mobile user, the locations of moving
object has to be closely monitored. The objective of paper is to discuss the
problem related to the role of personal and terminal mobility and query
processing in the mobile environment.
</summary>
    <author>
      <name>Samidha Dwivedi Sharma</name>
    </author>
    <author>
      <name>Dr. R. S. Kasana</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Publication format, ISSN 1947 5500,
  http://sites.google.com/site/ijcsis/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSIS, Vol. 7 No. 3, March 2010, 211-216</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1004.1747v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.1747v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.3272v1</id>
    <updated>2010-04-19T18:18:43Z</updated>
    <published>2010-04-19T18:18:43Z</published>
    <title>Database Reverse Engineering based on Association Rule Mining</title>
    <summary>  Maintaining a legacy database is a difficult task especially when system
documentation is poor written or even missing. Database reverse engineering is
an attempt to recover high-level conceptual design from the existing database
instances. In this paper, we propose a technique to discover conceptual schema
using the association mining technique. The discovered schema corresponds to
the normalization at the third normal form, which is a common practice in many
business organizations. Our algorithm also includes the rule filtering
heuristic to solve the problem of exponential growth of discovered rules
inherited with the association mining technique.
</summary>
    <author>
      <name>Nattapon Pannurat</name>
    </author>
    <author>
      <name>Nittaya Kerdprasop</name>
    </author>
    <author>
      <name>Kittisak Kerdprasop</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science Issues online at
  http://ijcsi.org/articles/Database-Reverse-Engineering-based-on-Association-Rule-Mining.php</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSI, Volume 7, Issue 2, March 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1004.3272v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.3272v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.4216v1</id>
    <updated>2010-04-23T20:04:57Z</updated>
    <published>2010-04-23T20:04:57Z</published>
    <title>Symmetric M-tree</title>
    <summary>  The M-tree is a paged, dynamically balanced metric access method that
responds gracefully to the insertion of new objects. To date, no algorithm has
been published for the corresponding Delete operation. We believe this to be
non-trivial because of the design of the M-tree's Insert algorithm. We propose
a modification to Insert that overcomes this problem and give the corresponding
Delete algorithm. The performance of the tree is comparable to the M-tree and
offers additional benefits in terms of supported operations, which we briefly
discuss.
</summary>
    <author>
      <name>Alan P. Sexton</name>
    </author>
    <author>
      <name>Richard Swinbank</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1004.4216v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.4216v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.0201v1</id>
    <updated>2010-05-03T06:40:36Z</updated>
    <published>2010-05-03T06:40:36Z</published>
    <title>Personnalisation de bases de données multidimensionnelles</title>
    <summary>  This paper deals with decision support systems resting on multidimensional
modelling of data. Moreover, we intend to offer a set of concepts and
mechanisms for personalized multidimensional database specifications. This
personalization consists in associating weights to different components of a
multidimensional schema. Personalization specifications are specified through
the use of a language based on the principle of Event Condition Action. This
personalisation determines multidimensional data display as well as their
analyses (with the use of drilling or rotating operations).
</summary>
    <author>
      <name>Franck Ravat</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <author>
      <name>Olivier Teste</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <author>
      <name>Gilles Zurfluh</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Congr\`es Informatique des Organisations et Syst\`emes
  d'Information et de D\'ecision - INFORSID'07, Perros-Guirec : France (2007)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1005.0201v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.0201v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.0212v1</id>
    <updated>2010-05-03T07:43:22Z</updated>
    <published>2010-05-03T07:43:22Z</published>
    <title>Construction graphique d'entrepôts et de magasins de données</title>
    <summary>  Nowadays, decisional systems have became a significant research topic in
databases. Data warehouses and data marts are the main elements of such
systems. This paper presents our decisional support system. We present
graphical interfaces which help the administrator to build data warehouses and
data marts. We present a data warehouse building interface based on an
object-oriented conceptual model. This model allows the warehouse data
historisation at three levels: attribute, class and environment. Also, we
present a data mart building interface which allows warehouse data to be
reorganised through a multidimensional object-oriented model.
</summary>
    <author>
      <name>Frédéric Bret</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <author>
      <name>Olivier Teste</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Congr\`es INFormatique des ORganisations et Syst\`emes
  d'Information et de D\'ecision - INFORSID'99, La Garde : France (1999)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1005.0212v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.0212v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.0213v1</id>
    <updated>2010-05-03T07:43:25Z</updated>
    <published>2010-05-03T07:43:25Z</published>
    <title>Algèbre OLAP et langage graphique</title>
    <summary>  This article deals with OLAP systems based on multidimensional model. The
conceptual model we provide, represents data through a constellation
(multi-facts) composed of several multi-hierarchy dimensions. In this model,
data are displayed through multidimensional tables. We define a query algebra
handling these tables. This user oriented algebra is composed of a closure core
of OLAP operators as soon as advanced operators dedicated to complex analysis.
Finally, we specify a graphical OLAP language based on this algebra. This
language facilitates analyses of decision makers.
</summary>
    <author>
      <name>Franck Ravat</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <author>
      <name>Olivier Teste</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <author>
      <name>Gilles Zurfluh</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Congr\`es Informatique des Organisations et Syst\`emes
  d'Information et de D\'ecision - INFORSID'06, Hammamet : Tunisie (2006)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1005.0213v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.0213v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.0214v1</id>
    <updated>2010-05-03T07:43:45Z</updated>
    <published>2010-05-03T07:43:45Z</published>
    <title>Modélisation et extraction de données pour un entrepôt objet</title>
    <summary>  This paper describes an object-oriented model for designing complex and
time-variant data warehouse data. The main contribution is the warehouse class
concept, which extends the class concept by temporal and archive filters as
well as a mapping function. Filters allow the keeping of relevant data changes
whereas the mapping function defines the warehouse class schema from a global
data source schema. The approach take into account static properties as well as
dynamic properties. The behaviour extraction is based on the use-matrix
concept.
</summary>
    <author>
      <name>Franck Ravat</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <author>
      <name>Olivier Teste</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <author>
      <name>Zurfluh Gilles</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Bases de donn\'ees avanc\'ees (BDA 2000), Blois : France (2000)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1005.0214v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.0214v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.0217v1</id>
    <updated>2010-05-03T07:47:20Z</updated>
    <published>2010-05-03T07:47:20Z</published>
    <title>Analyse multigraduelle OLAP</title>
    <summary>  Decisional systems are based on multidimensional databases improving OLAP
analyses. The paper describes a new OLAP operator named "BLEND" to perform
multigradual analyses. The operation transforms multidimensional structures
during querying in order to analyse measures according to various granularity
levels, which are reorganised into a single parameter. We study valid
combinations of the operation in the context of strict hierarchies. First
experimentations implement the operation in an R-OLAP framework showing the
slight cost of this operation.
</summary>
    <author>
      <name>Gilles Hubert</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <author>
      <name>Olivier Teste</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Revue des nouvelles technologies - RNTI, E-15 (2009) 253-258</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1005.0217v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.0217v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.0220v1</id>
    <updated>2010-05-03T07:49:14Z</updated>
    <published>2010-05-03T07:49:14Z</published>
    <title>Elaboration d'entrepôts de données complexes</title>
    <summary>  In this paper, we study the data warehouse modelling used in decision support
systems. We provide an object-oriented data warehouse model allowing data
warehouse description as a central repository of relevant, complex and temporal
data. Our model integrates three concepts such as warehouse object, environment
and warehouse class. Each warehouse object is composed of one current state,
several past states (modelling its detailed evolutions) and several archive
states (modelling its evolutions within a summarised form). The environment
concept defines temporal parts in the data warehouse schema with significant
granularities (attribute, class, graph). Finally, we provide five functions
aiming at defining the data warehouse structures and two functions allowing the
warehouse class inheritance hierarchy organisation.
</summary>
    <author>
      <name>Olivier Teste</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">congr\`es INFormatique des ORganisations et Syst\`emes
  d'Information et de D\'ecision - INFORSID'00, Lyon : France (2000)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1005.0220v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.0220v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.0224v1</id>
    <updated>2010-05-03T07:55:33Z</updated>
    <published>2010-05-03T07:55:33Z</published>
    <title>Towards Conceptual Multidimensional Design in Decision Support Systems</title>
    <summary>  Multidimensional databases support efficiently on-line analytical processing
(OLAP). In this paper, we depict a model dedicated to multidimensional
databases. The approach we present designs decisional information through a
constellation of facts and dimensions. Each dimension is possibly shared
between several facts and it is organised according to multiple hierarchies. In
addition, we define a comprehensive query algebra regrouping the more popular
multidimensional operations in current commercial systems and research
approaches. We introduce new operators dedicated to a constellation. Finally,
we describe a prototype that allows managers to query constellations of facts,
dimensions and multiple hierarchies.
</summary>
    <author>
      <name>Olivier Teste</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">5th East-European Conference on Advances in Databases and
  Information Systems - ADBIS'01, Vilnius : Lithuania (2001)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1005.0224v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.0224v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.5433v1</id>
    <updated>2010-05-29T07:35:23Z</updated>
    <published>2010-05-29T07:35:23Z</published>
    <title>A Data Warehouse Assistant Design System Based on Clover Model</title>
    <summary>  Nowadays, Data Warehouse (DW) plays a crucial role in the process of decision
making. However, their design remains a very delicate and difficult task either
for expert or users. The goal of this paper is to propose a new approach based
on the clover model, destined to assist users to design a DW. The proposed
approach is based on two main steps. The first one aims to guide users in their
choice of DW schema model. The second one aims to finalize the chosen model by
offering to the designer views related to former successful DW design
experiences.
</summary>
    <author>
      <name>Nouha Arfaoui</name>
    </author>
    <author>
      <name>Jalel Akaichi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijdms.2010.2204</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijdms.2010.2204" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 Pages, IJDMS</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Database Management Systems 2.2 (2010)
  57-71</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1005.5433v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.5433v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.5514v1</id>
    <updated>2010-05-30T11:13:08Z</updated>
    <published>2010-05-30T11:13:08Z</published>
    <title>Managing Semantic Loss during Query Reformulation in Peer Data
  Management Systems</title>
    <summary>  In this paper we deal with the notion of semantic loss in Peer Data
Management Systems (PDMS) queries. We define such a notion and we give a
mechanism that discovers semantic loss in a PDMS network. Next, we propose an
algorithm that addresses the problem of restoring such a loss. Further
evaluation of our proposed algorithm is an ongoing work
</summary>
    <author>
      <name>Yannis Delveroudis</name>
    </author>
    <author>
      <name>Paraskevas V. Lekeas</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/SWOD.2007.353199</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/SWOD.2007.353199" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SWOD '07 Proceedings of the 2007 IEEE International Workshop on
  Databases for Next Generation Researchers</arxiv:comment>
    <link href="http://arxiv.org/abs/1005.5514v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.5514v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.1309v1</id>
    <updated>2010-06-07T17:47:30Z</updated>
    <published>2010-06-07T17:47:30Z</published>
    <title>Using Grid Files for a Relational Database Management System</title>
    <summary>  This paper describes our experience with using Grid files as the main storage
organization for a relational database management system. We primarily focus on
the following two aspects. (i) Strategies for implementing grid files
efficiently. (ii) Methods for efficiency evaluating queries posed to a database
organized using grid files.
</summary>
    <author>
      <name>S. M. Joshi</name>
    </author>
    <author>
      <name>S. Sanyal</name>
    </author>
    <author>
      <name>S. Banerjee</name>
    </author>
    <author>
      <name>S. Srikumar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 Pages, 5 Figures, 2 tables, This Paper was referred to in the
  seminal Paper by J. Nievergelt, H. Hinterberger,K.C. Sevcik, The Grid File:
  An Adaptable, Symmetric Multikey File Structure ACM Transactions on Database
  Systems (TODS), Volume 9, Issue 1, March, 1984. Pages: 38-71, ISSN:
  0362-5915, as [Reference 12]</arxiv:comment>
    <link href="http://arxiv.org/abs/1006.1309v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.1309v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.4833v1</id>
    <updated>2010-06-24T16:47:28Z</updated>
    <published>2010-06-24T16:47:28Z</published>
    <title>A Generic Storage API</title>
    <summary>  We present a generic API suitable for provision of highly generic storage
facilities that can be tailored to produce various individually customised
storage infrastructures. The paper identifies a candidate set of minimal
storage system building blocks, which are sufficiently simple to avoid
encapsulating policy where it cannot be customised by applications, and
composable to build highly flexible storage architectures. Four main generic
components are defined: the store, the namer, the caster and the interpreter.
It is hypothesised that these are sufficiently general that they could act as
building blocks for any information storage and retrieval system. The essential
characteristics of each are defined by an interface, which may be implemented
by multiple implementing classes.
</summary>
    <author>
      <name>Graham Kirby</name>
    </author>
    <author>
      <name>Evangelos Zirintsis</name>
    </author>
    <author>
      <name>Alan Dearle</name>
    </author>
    <author>
      <name>Ron Morrison</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ACSC 2004</arxiv:comment>
    <link href="http://arxiv.org/abs/1006.4833v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.4833v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1008.1337v1</id>
    <updated>2010-08-07T12:55:26Z</updated>
    <published>2010-08-07T12:55:26Z</published>
    <title>PDM based I-SOAS Data Warehouse Design</title>
    <summary>  This research paper briefly describes the industrial contributions of Product
Data Management in any organization's technical and managerial data management.
Then focusing on some current major PDM based problems i.e. Static and
Unintelligent Search, Platform Independent System and Successful PDM System
Implementation, briefly presents a semantic based solution i.e. I-SOAS. Majorly
this research paper is about to present and discuss the contributions of I-SOAS
in any organization's technical and system data management.
</summary>
    <author>
      <name>Zeeshan Ahmed</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">", In the proceedings of FIFTH International Conference on
  Statistical Sciences: Mathematics, Paper ID 125, ISBN 978-969-8858-04-9, Vol.
  17, 23-25 January 2009</arxiv:comment>
    <link href="http://arxiv.org/abs/1008.1337v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1008.1337v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1008.1715v5</id>
    <updated>2011-11-24T15:10:52Z</updated>
    <published>2010-08-10T14:05:28Z</published>
    <title>The universality of iterated hashing over variable-length strings</title>
    <summary>  Iterated hash functions process strings recursively, one character at a time.
At each iteration, they compute a new hash value from the preceding hash value
and the next character. We prove that iterated hashing can be pairwise
independent, but never 3-wise independent. We show that it can be almost
universal over strings much longer than the number of hash values; we bound the
maximal string length given the collision probability.
</summary>
    <author>
      <name>Daniel Lemire</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.dam.2011.11.009</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.dam.2011.11.009" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Discrete Applied Mathematics 160 (4-5), 604--617 (2012)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1008.1715v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1008.1715v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.0397v1</id>
    <updated>2010-09-02T11:41:29Z</updated>
    <published>2010-09-02T11:41:29Z</published>
    <title>Mobile Information Collectors' Trajectory Data Warehouse Design</title>
    <summary>  To analyze complex phenomena which involve moving objects, Trajectory Data
Warehouse (TDW) seems to be an answer for many recent decision problems related
to various professions (physicians, commercial representatives, transporters,
ecologists ...) concerned with mobility. This work aims to make trajectories as
a first class concept in the trajectory data conceptual model and to design a
TDW, in which data resulting from mobile information collectors' trajectory are
gathered. These data will be analyzed, according to trajectory characteristics,
for decision making purposes, such as new products commercialization, new
commerce implementation, etc.
</summary>
    <author>
      <name>wided oueslati</name>
    </author>
    <author>
      <name>jalel akaichi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 9 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">international journal of managing information technology august
  2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1009.0397v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.0397v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.0827v1</id>
    <updated>2010-09-04T11:29:55Z</updated>
    <published>2010-09-04T11:29:55Z</published>
    <title>A Novel Watermarking Scheme for Detecting and Recovering Distortions in
  Database Tables</title>
    <summary>  In this paper a novel fragile watermarking scheme is proposed to detect,
localize and recover malicious modifications in relational databases. In the
proposed scheme, all tuples in the database are first securely divided into
groups. Then watermarks are embedded and verified group-by-group independently.
By using the embedded watermark, we are able to detect and localize the
modification made to the database and even we recover the true data from the
database modified locations. Our experimental results show that this scheme is
so qualified; i.e. distortion detection and true data recovery both are
performed successfully.
</summary>
    <author>
      <name>Hamed khataeimaragheh</name>
    </author>
    <author>
      <name>Hassan Rashidi</name>
    </author>
    <link href="http://arxiv.org/abs/1009.0827v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.0827v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.5149v1</id>
    <updated>2010-09-27T03:10:28Z</updated>
    <published>2010-09-27T03:10:28Z</published>
    <title>Towards an incremental maintenance of cyclic association rules</title>
    <summary>  Recently, the cyclic association rules have been introduced in order to
discover rules from items characterized by their regular variation over time.
In real life situations, temporal databases are often appended or updated.
Rescanning the whole database every time is highly expensive while existing
incremental mining techniques can efficiently solve such a problem. In this
paper, we propose an incremental algorithm for cyclic association rules
maintenance. The carried out experiments of our proposal stress on its
efficiency and performance.
</summary>
    <author>
      <name>Eya ben Ahmed</name>
    </author>
    <author>
      <name>Mohamed Salah Gouider</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Database Management Systems (IJDMS),
  November 2010, Volume 2, Number 4</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1009.5149v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.5149v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1010.1147v1</id>
    <updated>2010-10-06T13:03:18Z</updated>
    <published>2010-10-06T13:03:18Z</published>
    <title>XML Query Processing and Query Languges: A Survey</title>
    <summary>  Today's database is associated with interoperability between different
domains and applications. This consequently results in the importance of data
portability in database. XML format fits the requirements and it has been
increasingly used for serving applications across different domains and
purposes. However, querying XML document effectively and efficiently is still a
challenging issue. This paper discusses query processing issues on XML and
reviews proposed solutions for querying XML databases by various authors.
</summary>
    <author>
      <name>Mikael Fernandus Simalango</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures, 2 tables, written in 2008</arxiv:comment>
    <link href="http://arxiv.org/abs/1010.1147v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1010.1147v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1010.3615v1</id>
    <updated>2010-10-18T14:38:31Z</updated>
    <published>2010-10-18T14:38:31Z</published>
    <title>Scalable XML Collaborative Editing with Undo short paper</title>
    <summary>  Commutative Replicated Data-Type (CRDT) is a new class of algorithms that
ensures scalable consistency of replicated data. It has been successfully
applied to collaborative editing of texts without complex concurrency control.
In this paper, we present a CRDT to edit XML data. Compared to existing
approaches for XML collaborative editing, our approach is more scalable and
handles all the XML editing aspects : elements, contents, attributes and undo.
Indeed, undo is recognized as an important feature for collaborative editing
that allows to overcome system complexity through error recovery or
collaborative conflict resolution.
</summary>
    <author>
      <name>Stéphane Martin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIF</arxiv:affiliation>
    </author>
    <author>
      <name>Pascal Urso</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Stéphane Weiss</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1010.3615v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1010.3615v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.1660v1</id>
    <updated>2010-12-08T00:51:37Z</updated>
    <published>2010-12-08T00:51:37Z</published>
    <title>Provenance and evidence in UniProtKB</title>
    <summary>  The primary mission of UniProt is to support biological research by
maintaining a stable, comprehensive, fully classified, richly and accurately
annotated protein sequence knowledgebase, with extensive cross-references to
external resources, that is freely available to the scientific community. To
enable users of the knowledgebase to accurately assess the reliability of the
information contained in this resource, the evidence for and provenance of the
information must be recorded. This paper discusses the user requirements for
this kind of metadata and the manner in which UniProtKB records it.
</summary>
    <author>
      <name>Jerven Bolleman</name>
    </author>
    <author>
      <name>Alain Gateau</name>
    </author>
    <author>
      <name>Sebastien Gehant</name>
    </author>
    <author>
      <name>Nicole Redaschi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in Adrian Paschke, Albert Burger, Andrea Splendiani, M. Scott
  Marshall, Paolo Romano: Proceedings of the 3rd International Workshop on
  Semantic Web Applications and Tools for the Life Sciences, Berlin,Germany,
  December 8-10, 2010</arxiv:comment>
    <link href="http://arxiv.org/abs/1012.1660v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.1660v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.1898v1</id>
    <updated>2010-12-09T00:03:31Z</updated>
    <published>2010-12-09T00:03:31Z</published>
    <title>Ontology Usage at ZFIN</title>
    <summary>  The Zebrafish Model Organism Database (ZFIN) provides a Web resource of
zebrafish genomic, genetic, developmental, and phenotypic data. Four different
ontologies are currently used to annotate data to the most specific term
available facilitating a better comparison between inter-species data. In
addition, ontologies are used to help users find and cluster data more quickly
without the need of knowing the exact technical name for a term.
</summary>
    <author>
      <name>Doug Howe</name>
    </author>
    <author>
      <name>Christian Pich</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in Adrian Paschke, Albert Burger, Andrea Splendiani, M. Scott
  Marshall, Paolo Romano: Proceedings of the 3rd International Workshop on
  Semantic Web Applications and Tools for the Life Sciences, Berlin,Germany,
  December 8-10, 2010</arxiv:comment>
    <link href="http://arxiv.org/abs/1012.1898v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.1898v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.2858v1</id>
    <updated>2010-12-13T20:48:12Z</updated>
    <published>2010-12-13T20:48:12Z</published>
    <title>Relational transducers for declarative networking</title>
    <summary>  Motivated by a recent conjecture concerning the expressiveness of declarative
networking, we propose a formal computation model for "eventually consistent"
distributed querying, based on relational transducers. A tight link has been
conjectured between coordination-freeness of computations, and monotonicity of
the queries expressed by such computations. Indeed, we propose a formal
definition of coordination-freeness and confirm that the class of monotone
queries is captured by coordination-free transducer networks.
Coordination-freeness is a semantic property, but the syntactic class that we
define of "oblivious" transducers also captures the same class of monotone
queries. Transducer networks that are not coordination-free are much more
powerful.
</summary>
    <author>
      <name>Tom Ameloot</name>
    </author>
    <author>
      <name>Frank Neven</name>
    </author>
    <author>
      <name>Jan Van den Bussche</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/1989284.1989321</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/1989284.1989321" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">30th ACM Symposium on Principles of Database Systems, 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1012.2858v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.2858v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1102.0952v1</id>
    <updated>2011-02-04T15:59:12Z</updated>
    <published>2011-02-04T15:59:12Z</published>
    <title>Pattern tree-based XOLAP rollup operator for XML complex hierarchies</title>
    <summary>  With the rise of XML as a standard for representing business data, XML data
warehousing appears as a suitable solution for decision-support applications.
In this context, it is necessary to allow OLAP analyses on XML data cubes.
Thus, XQuery extensions are needed. To define a formal framework and allow
much-needed performance optimizations on analytical queries expressed in
XQuery, defining an algebra is desirable. However, XML-OLAP (XOLAP) algebras
from the literature still largely rely on the relational model. Hence, we
propose in this paper a rollup operator based on a pattern tree in order to
handle multidimensional XML data expressed within complex hierarchies.
</summary>
    <author>
      <name>Marouane Hachicha</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Jérôme Darmont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Machine and Web Intelligence (ICMWI
  10), Algiers : Algeria (2010)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1102.0952v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1102.0952v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.0686v1</id>
    <updated>2011-03-03T14:09:24Z</updated>
    <published>2011-03-03T14:09:24Z</published>
    <title>Querying and Manipulating Temporal Databases</title>
    <summary>  Many works have focused, for over twenty five years, on the integration of
the time dimension in databases (DB). However, the standard SQL3 does not yet
allow easy definition, manipulation and querying of temporal DBs. In this
paper, we study how we can simplify querying and manipulating temporal facts in
SQL3, using a model that integrates time in a native manner. To do this, we
propose new keywords and syntax to define different temporal versions for many
relational operators and functions used in SQL. It then becomes possible to
perform various queries and updates appropriate to temporal facts. We
illustrate the use of these proposals on many examples from a real application.
</summary>
    <author>
      <name>Mohamed Mkaouar</name>
    </author>
    <author>
      <name>Rafik Bouaziz</name>
    </author>
    <author>
      <name>Mohamed Moalla</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Database Management Systems (IJDMS),
  February 2011, Volume 3, Number 1</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1103.0686v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.0686v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.1367v1</id>
    <updated>2011-03-07T20:22:38Z</updated>
    <published>2011-03-07T20:22:38Z</published>
    <title>Efficient Batch Query Answering Under Differential Privacy</title>
    <summary>  Differential privacy is a rigorous privacy condition achieved by randomizing
query answers. This paper develops efficient algorithms for answering multiple
queries under differential privacy with low error. We pursue this goal by
advancing a recent approach called the matrix mechanism, which generalizes
standard differentially private mechanisms. This new mechanism works by first
answering a different set of queries (a strategy) and then inferring the
answers to the desired workload of queries. Although a few strategies are known
to work well on specific workloads, finding the strategy which minimizes error
on an arbitrary workload is intractable. We prove a new lower bound on the
optimal error of this mechanism, and we propose an efficient algorithm that
approaches this bound for a wide range of workloads.
</summary>
    <author>
      <name>Chao Li</name>
    </author>
    <author>
      <name>Gerome Miklau</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 figues, 22 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1103.1367v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.1367v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.2406v1</id>
    <updated>2011-03-12T01:05:38Z</updated>
    <published>2011-03-12T01:05:38Z</published>
    <title>Automatic Wrappers for Large Scale Web Extraction</title>
    <summary>  We present a generic framework to make wrapper induction algorithms tolerant
to noise in the training data. This enables us to learn wrappers in a
completely unsupervised manner from automatically and cheaply obtained noisy
training data, e.g., using dictionaries and regular expressions. By removing
the site-level supervision that wrapper-based techniques require, we are able
to perform information extraction at web-scale, with accuracy unattained with
existing unsupervised extraction techniques. Our system is used in production
at Yahoo! and powers live applications.
</summary>
    <author>
      <name>Nilesh Dalvi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Yahoo! Research</arxiv:affiliation>
    </author>
    <author>
      <name>Ravi Kumar</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Yahoo! Research</arxiv:affiliation>
    </author>
    <author>
      <name>Mohamed Soliman</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">U. of Waterloo</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">VLDB2011</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the VLDB Endowment (PVLDB), Vol. 4, No. 4, pp.
  219-230 (2011)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1103.2406v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.2406v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.2410v1</id>
    <updated>2011-03-12T01:09:30Z</updated>
    <published>2011-03-12T01:09:30Z</published>
    <title>Large-Scale Collective Entity Matching</title>
    <summary>  There have been several recent advancements in Machine Learning community on
the Entity Matching (EM) problem. However, their lack of scalability has
prevented them from being applied in practical settings on large real-life
datasets. Towards this end, we propose a principled framework to scale any
generic EM algorithm. Our technique consists of running multiple instances of
the EM algorithm on small neighborhoods of the data and passing messages across
neighborhoods to construct a global solution. We prove formal properties of our
framework and experimentally demonstrate the effectiveness of our approach in
scaling EM algorithms.
</summary>
    <author>
      <name>Vibhor Rastogi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Yahoo! Research</arxiv:affiliation>
    </author>
    <author>
      <name>Nilesh Dalvi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Yahoo! Research</arxiv:affiliation>
    </author>
    <author>
      <name>Minos Garofalakis</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Technical University of Crete</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">VLDB2011</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the VLDB Endowment (PVLDB), Vol. 4, No. 4, pp.
  208-218 (2011)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1103.2410v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.2410v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.3857v2</id>
    <updated>2011-04-26T20:17:21Z</updated>
    <published>2011-03-20T15:35:03Z</published>
    <title>Difference Sequence Compression of Multidimensional Databases</title>
    <summary>  The multidimensional databases often use compression techniques in order to
decrease the size of the database. This paper introduces a new method called
difference sequence compression. Under some conditions, this new technique is
able to create a smaller size multidimensional database than others like single
count header compression, logical position compression or base-offset
compression. Keywords: compression, multidimensional database, On-line
Analytical Processing, OLAP.
</summary>
    <author>
      <name>István Szépkúti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 4 figures, 5 tables. Paper presented at the Third
  Conference of PhD Students in Computer Science, Szeged, Hungary, 1 - 4 July
  2002. For further details, please refer to
  http://www.inf.u-szeged.hu/~szepkuti/papers.html#differencesequence</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Periodica Polytechnica Electrical Engineering, Vol. 48, Number
  3-4, pp. 197-218, 2004</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1103.3857v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.3857v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.4979v3</id>
    <updated>2016-03-01T04:55:06Z</updated>
    <published>2011-03-25T14:24:18Z</published>
    <title>An Introduction to Functional dependency in Relational Databases</title>
    <summary>  This write-up is the suggested lecture notes for a second level course on
advanced topics in database systems for master's students of Computer Science
with a theoretical focus. A prerequisite in algorithms and an exposure to
database systems are required. Additional reading may require exposure to
mathematical logic. The starting point for these notes are from M.Y.Vardi's
survey listed herein as a reference - some of the proofs are presented as such
. This select rewrite on functional dependency is intended to provide a few
clarifications even though radically new design approaches are now being
proposed.
</summary>
    <author>
      <name>K. Viswanathan Iyer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Revised 2nd version</arxiv:comment>
    <link href="http://arxiv.org/abs/1103.4979v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.4979v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1104.4163v1</id>
    <updated>2011-04-21T03:50:34Z</updated>
    <published>2011-04-21T03:50:34Z</published>
    <title>Data Mining : A prediction of performer or underperformer using
  classification</title>
    <summary>  Now a day's students have a large set of data having precious information
hidden. Data mining technique can help to find this hidden information. In this
paper, data mining techniques name Byes classification method is used on these
data to help an institution. Institutions can find those students who are
consistently perform well. This study will help to institution reduce the drop
put ratio to a significant level and improve the performance level of the
institution.
</summary>
    <author>
      <name>Umesh Kumar Pandey</name>
    </author>
    <author>
      <name>Saurabh Pal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">(IJCSIT) International Journal of Computer Science and Information
  Technology, Vol. 2(2), 2011, 686-690</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1104.4163v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.4163v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1104.4899v1</id>
    <updated>2011-04-26T12:25:56Z</updated>
    <published>2011-04-26T12:25:56Z</published>
    <title>Data Base Mappings and Theory of Sketches</title>
    <summary>  In this paper we will present the two basic operations for database schemas
used in database mapping systems (separation and Data Federation), and we will
explain why the functorial semantics for database mappings needed a new base
category instead of usual Set category. Successively, it is presented a
definition of the graph G for a schema database mapping system, and the
definition of its sketch category Sch(G). Based on this framework we presented
functorial semantics for database mapping systems with the new base category
DB.
</summary>
    <author>
      <name>Zoran Majkic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1104.4899v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.4899v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.5951v1</id>
    <updated>2011-05-30T11:58:42Z</updated>
    <published>2011-05-30T11:58:42Z</published>
    <title>Performance of Short-Commit in Extreme Database Environment</title>
    <summary>  Atomic commit protocols are used where data integrity is more important than
data availability. Two-Phase commit (2PC) is a standard commit protocol for
commercial database management systems. To reduce certain drawbacks in 2PC
protocol people have suggested different variance of this protocol.
Short-Commit protocol is developed with an objective to achieve low cost
transaction commitment cost with non-blocking capability. In this paper we have
briefly explained short-commit protocol executing pattern. Experimental
analysis and results are presented to support the claim that short-commit can
work efficiently in extreme database environment.
</summary>
    <author>
      <name>Muhammad Tayyab Shahzad</name>
    </author>
    <author>
      <name>Muhammad Rizwan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages. International Journal of Database Management Systems, ISSN
  : 0975-5705 (Online); International Journal of Database Management Systems
  (IJDMS)2011, 0975-5985 (Print)</arxiv:comment>
    <link href="http://arxiv.org/abs/1105.5951v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.5951v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.0831v1</id>
    <updated>2011-08-03T12:36:23Z</updated>
    <published>2011-08-03T12:36:23Z</published>
    <title>Towards Spatio-Temporal SOLAP</title>
    <summary>  The integration of Geographic Information Systems (GIS) and On-Line
Analytical Processing (OLAP), denoted SOLAP, is aimed at exploring and
analyzing spatial data. In real-world SOLAP applications, spatial and
non-spatial data are subject to changes. In this paper we present a temporal
query language for SOLAP, called TPiet-QL, supporting so-called discrete
changes (for example, in land use or cadastral applications there are
situations where parcels are merged or split). TPiet-QL allows expressing
integrated GIS-OLAP queries in an scenario where spatial objects change across
time.
</summary>
    <author>
      <name>Pablo Bisceglia</name>
    </author>
    <author>
      <name>Leticia Gomez</name>
    </author>
    <author>
      <name>Alejandro Vaisman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1108.0831v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.0831v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.0617v1</id>
    <updated>2011-09-03T12:04:00Z</updated>
    <published>2011-09-03T12:04:00Z</published>
    <title>Metadata Challenge for Query Processing Over Heterogeneous Wireless
  Sensor Network</title>
    <summary>  Wireless sensor networks become integral part of our life. These networks can
be used for monitoring the data in various domain due to their flexibility and
functionality. Query processing and optimization in the WSN is a very
challenging task because of their energy and memory constraint. In this paper,
first our focus is to review the different approaches that have significant
impacts on the development of query processing techniques for WSN. Finally, we
aim to illustrate the existing approach in popular query processing engines
with future research challenges in query optimization.
</summary>
    <author>
      <name>C. Komalavalli</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Jagan Institute of Management Studies, Rohini, New Delhi</arxiv:affiliation>
    </author>
    <author>
      <name>Chetna Laroiya</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Jagan Insitute of Management Studies, Rohini, New Delhi</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 Pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Wireless &amp; Mobile Networks (IJWMN) Vol.
  3, No. 4, August 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1109.0617v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.0617v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.1202v1</id>
    <updated>2011-09-06T14:32:05Z</updated>
    <published>2011-09-06T14:32:05Z</published>
    <title>Data Mining Techniques: A Source for Consumer Behavior Analysis</title>
    <summary>  Various studies on consumer purchasing behaviors have been presented and used
in real problems. Data mining techniques are expected to be a more effective
tool for analyzing consumer behaviors. However, the data mining method has
disadvantages as well as advantages. Therefore, it is important to select
appropriate techniques to mine databases. The objective of this paper is to
know consumer behavior, his psychological condition at the time of purchase and
how suitable data mining method apply to improve conventional method. Moreover,
in an experiment, association rule is employed to mine rules for trusted
customers using sales data in a super market industry
</summary>
    <author>
      <name>Abhijit Raorane</name>
    </author>
    <author>
      <name>R. V. Kulkarni</name>
    </author>
    <link href="http://arxiv.org/abs/1109.1202v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.1202v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.6299v1</id>
    <updated>2011-09-28T19:04:18Z</updated>
    <published>2011-09-28T19:04:18Z</published>
    <title>Sensitivity Analysis for Declarative Relational Query Languages with
  Ordinal Ranks</title>
    <summary>  We present sensitivity analysis for results of query executions in a
relational model of data extended by ordinal ranks. The underlying model of
data results from the ordinary Codd's model of data in which we consider
ordinal ranks of tuples in data tables expressing degrees to which tuples match
queries. In this setting, we show that ranks assigned to tuples are insensitive
to small changes, i.e., small changes in the input data do not yield large
changes in the results of queries.
</summary>
    <author>
      <name>Radim Belohlavek</name>
    </author>
    <author>
      <name>Lucie Urbanova</name>
    </author>
    <author>
      <name>Vilem Vychodil</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The paper will appear in Proceedings of the 19th International
  Conference on Applications of Declarative Programming and Knowledge
  Management (INAP 2011)</arxiv:comment>
    <link href="http://arxiv.org/abs/1109.6299v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.6299v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.3017v1</id>
    <updated>2011-10-13T18:01:39Z</updated>
    <published>2011-10-13T18:01:39Z</published>
    <title>Towards a Query Language for the Web of Data (A Vision Paper)</title>
    <summary>  Research on querying the Web of Data is still in its infancy. In this paper,
we provide an initial set of general features that we envision should be
considered in order to define a query language for the Web of Data.
Furthermore, for each of these features, we pose questions that have not been
addressed before in the context of querying the Web of Data. We believe that
addressing these questions and studying these features may guide the next 10
years of research on the Web of Data.
</summary>
    <author>
      <name>Juan Sequeda</name>
    </author>
    <author>
      <name>Olaf Hartig</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1110.3017v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.3017v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.5639v1</id>
    <updated>2011-11-23T22:28:38Z</updated>
    <published>2011-11-23T22:28:38Z</published>
    <title>A New Technique to Backup and Restore DBMS using XML and .NET
  Technologies</title>
    <summary>  In this paper, we proposed a new technique for backing up and restoring
different Database Management Systems (DBMS). The technique is enabling to
backup and restore a part of or the whole database using a unified interface
using ASP.NET and XML technologies. It presents a Web Solution allowing the
administrators to do their jobs from everywhere, locally or remotely. To show
the importance of our solution, we have taken two case studies, oracle 11g and
SQL Server 2008.
</summary>
    <author>
      <name>Seifedine Kadry</name>
    </author>
    <author>
      <name>Mohamad Smaili</name>
    </author>
    <author>
      <name>Hussam Kassem</name>
    </author>
    <author>
      <name>Hassan Hayek</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal on Computer Science and Engineering Vol. 02,
  No. 04, 2010, 1092-1102</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1111.5639v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.5639v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.5687v1</id>
    <updated>2011-11-24T07:52:59Z</updated>
    <published>2011-11-24T07:52:59Z</published>
    <title>Coron : Plate-forme d'extraction de connaissances dans les bases de
  données</title>
    <summary>  Coron is a domain and platform independent, multi-purposed data mining
toolkit, which incorporates not only a rich collection of data mining
algorithms, but also allows a number of auxiliary operations. To the best of
our knowledge, a data mining toolkit designed specifically for itemset
extraction and association rule generation like Coron does not exist elsewhere.
Coron also provides support for preparing and filtering data, and for
interpreting the extracted units of knowledge.
</summary>
    <author>
      <name>Baptiste Ducatel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Mehdi Kaytoue</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Florent Marcuola</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Amedeo Napoli</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Laszlo Szathmary</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">17\`eme conf\'erence en Reconnaissance des Formes et Intelligence
  Artificielle (2010) 883-884</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1111.5687v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.5687v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.5690v1</id>
    <updated>2011-11-24T07:56:18Z</updated>
    <published>2011-11-24T07:56:18Z</published>
    <title>The Coron System</title>
    <summary>  Coron is a domain and platform independent, multi-purposed data mining
toolkit, which incorporates not only a rich collection of data mining
algorithms, but also allows a number of auxiliary operations. To the best of
our knowledge, a data mining toolkit designed specifically for itemset
extraction and association rule generation like Coron does not exist elsewhere.
Coron also provides support for preparing and filtering data, and for
interpreting the extracted units of knowledge.
</summary>
    <author>
      <name>Mehdi Kaytoue</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Florent Marcuola</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Amedeo Napoli</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Laszlo Szathmary</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Jean Villerd</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">8th International Conference on Formal Concept Analsis (ICFCA)
  (2010) 55--58</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1111.5690v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.5690v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1112.3134v1</id>
    <updated>2011-12-14T07:12:31Z</updated>
    <published>2011-12-14T07:12:31Z</published>
    <title>Proposing Cluster_Similarity Method in Order to Find as Much Better
  Similarities in Databases</title>
    <summary>  Different ways of entering data into databases result in duplicate records
that cause increasing of databases' size. This is a fact that we cannot ignore
it easily. There are several methods that are used for this purpose. In this
paper, we have tried to increase the accuracy of operations by using cluster
similarity instead of direct similarity of fields. So that clustering is done
on fields of database and according to accomplished clustering on fields,
similarity degree of records is obtained. In this method by using present
information in database, more logical similarity is obtained for deficient
information that in general, the method of cluster similarity could improve
operations 24% compared with previous methods.
</summary>
    <author>
      <name>Mohammad-Reza Feizi-Derakhshi</name>
    </author>
    <author>
      <name>Azade Roohany</name>
    </author>
    <link href="http://arxiv.org/abs/1112.3134v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1112.3134v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.3807v1</id>
    <updated>2012-02-16T22:00:09Z</updated>
    <published>2012-02-16T22:00:09Z</published>
    <title>An Adaptive Mechanism for Accurate Query Answering under Differential
  Privacy</title>
    <summary>  We propose a novel mechanism for answering sets of count- ing queries under
differential privacy. Given a workload of counting queries, the mechanism
automatically selects a different set of "strategy" queries to answer
privately, using those answers to derive answers to the workload. The main
algorithm proposed in this paper approximates the optimal strategy for any
workload of linear counting queries. With no cost to the privacy guarantee, the
mechanism improves significantly on prior approaches and achieves near-optimal
error for many workloads, when applied under (\epsilon, \delta)-differential
privacy. The result is an adaptive mechanism which can help users achieve good
utility without requiring that they reason carefully about the best formulation
of their task.
</summary>
    <author>
      <name>Chao Li</name>
    </author>
    <author>
      <name>Gerome Miklau</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">VLDB2012. arXiv admin note: substantial text overlap with
  arXiv:1103.1367</arxiv:comment>
    <link href="http://arxiv.org/abs/1202.3807v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.3807v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.0056v1</id>
    <updated>2012-03-01T00:17:13Z</updated>
    <published>2012-03-01T00:17:13Z</published>
    <title>SharedDB: Killing One Thousand Queries With One Stone</title>
    <summary>  Traditional database systems are built around the query-at-a-time model. This
approach tries to optimize performance in a best-effort way. Unfortunately,
best effort is not good enough for many modern applications. These applications
require response time guarantees in high load situations. This paper describes
the design of a new database architecture that is based on batching queries and
shared computation across possibly hundreds of concurrent queries and updates.
Performance experiments with the TPC-W benchmark show that the performance of
our implementation, SharedDB, is indeed robust across a wide range of dynamic
workloads.
</summary>
    <author>
      <name>Georgios Giannikis</name>
    </author>
    <author>
      <name>Gustavo Alonso</name>
    </author>
    <author>
      <name>Donald Kossmann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">VLDB2012</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 6, pp.
  526-537 (2012)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1203.0056v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.0056v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.2672v1</id>
    <updated>2012-03-12T23:19:09Z</updated>
    <published>2012-03-12T23:19:09Z</published>
    <title>FDB: A Query Engine for Factorised Relational Databases</title>
    <summary>  Factorised databases are relational databases that use compact factorised
representations at the physical layer to reduce data redundancy and boost query
performance. This paper introduces FDB, an in-memory query engine for
select-project-join queries on factorised databases. Key components of FDB are
novel algorithms for query optimisation and evaluation that exploit the
succinctness brought by data factorisation. Experiments show that for data sets
with many-to-many relationships FDB can outperform relational engines by orders
of magnitude.
</summary>
    <author>
      <name>Nurzhan Bakibayev</name>
    </author>
    <author>
      <name>Dan Olteanu</name>
    </author>
    <author>
      <name>Jakub Závodný</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1203.2672v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.2672v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.4380v3</id>
    <updated>2014-06-16T12:06:27Z</updated>
    <published>2012-03-20T10:41:29Z</published>
    <title>Analyzing closed frequent itemsets with convex polytopes</title>
    <summary>  Computing frequent itemsets in transactional databases is a vital but
computationally expensive task. Measuring the difference of two datasets is
often done by computing their respective frequent itemsets despite high
computational cost. This paper proposes a linear programming-based approach to
this problem and shows that there exists a distance measure for transactional
database that relies on closed frequent itemsets but does not require their
generation.
</summary>
    <author>
      <name>Natalia Vanetik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as another paper with different data model</arxiv:comment>
    <link href="http://arxiv.org/abs/1203.4380v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.4380v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.6; H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.6406v1</id>
    <updated>2012-03-29T00:07:19Z</updated>
    <published>2012-03-29T00:07:19Z</published>
    <title>An Analysis of Structured Data on the Web</title>
    <summary>  In this paper, we analyze the nature and distribution of structured data on
the Web. Web-scale information extraction, or the problem of creating
structured tables using extraction from the entire web, is gathering lots of
research interest. We perform a study to understand and quantify the value of
Web-scale extraction, and how structured information is distributed amongst top
aggregator websites and tail sites for various interesting domains. We believe
this is the first study of its kind, and gives us new insights for information
extraction over the Web.
</summary>
    <author>
      <name>Nilesh Dalvi</name>
    </author>
    <author>
      <name>Ashwin Machanavajjhala</name>
    </author>
    <author>
      <name>Bo Pang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">VLDB2012</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 7, pp.
  680-691 (2012)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1203.6406v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.6406v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.1598v1</id>
    <updated>2012-04-07T06:24:53Z</updated>
    <published>2012-04-07T06:24:53Z</published>
    <title>Improving Seek Time for Column Store Using MMH Algorithm</title>
    <summary>  Hash based search has, proven excellence on large data warehouses stored in
column store. Data distribution has significant impact on hash based search. To
reduce impact of data distribution, we have proposed Memory Managed Hash (MMH)
algorithm that uses shift XOR group for Queries and Transactions in column
store. Our experiments show that MMH improves read and write throughput by 22%
for TPC-H distribution.
</summary>
    <author>
      <name>Tejaswini Apte</name>
    </author>
    <author>
      <name>Dr. Maya Ingle</name>
    </author>
    <author>
      <name>Dr. A. K. Goyal</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">(IJACSA) International Journal of Advanced Computer Science and
  Applications Vol. 3, No.2, 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1204.1598v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.1598v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.2610v1</id>
    <updated>2012-04-12T03:49:26Z</updated>
    <published>2012-04-12T03:49:26Z</published>
    <title>A Novel Framework using Elliptic Curve Cryptography for Extremely Secure
  Transmission in Distributed Privacy Preserving Data Mining</title>
    <summary>  Privacy Preserving Data Mining is a method which ensures privacy of
individual information during mining. Most important task involves retrieving
information from multiple data bases which is distributed. The data once in the
data warehouse can be used by mining algorithms to retrieve confidential
information. The proposed framework has two major tasks, secure transmission
and privacy of confidential information during mining. Secure transmission is
handled by using elliptic curve cryptography and data distortion for privacy
preservation ensuring highly secure environment.
</summary>
    <author>
      <name>P. Kiran</name>
    </author>
    <author>
      <name>S Sathish Kumar</name>
    </author>
    <author>
      <name>N. P. Kavya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Advanced Computing: An International Journal ( ACIJ ), Vol.3,
  No.2, March 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1204.2610v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.2610v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.2731v1</id>
    <updated>2012-04-12T14:05:37Z</updated>
    <published>2012-04-12T14:05:37Z</published>
    <title>How do Ontology Mappings Change in the Life Sciences?</title>
    <summary>  Mappings between related ontologies are increasingly used to support data
integration and analysis tasks. Changes in the ontologies also require the
adaptation of ontology mappings. So far the evolution of ontology mappings has
received little attention albeit ontologies change continuously especially in
the life sciences. We therefore analyze how mappings between popular life
science ontologies evolve for different match algorithms. We also evaluate
which semantic ontology changes primarily affect the mappings. We further
investigate alternatives to predict or estimate the degree of future mapping
changes based on previous ontology and mapping transitions.
</summary>
    <author>
      <name>Anika Gross</name>
    </author>
    <author>
      <name>Michael Hartung</name>
    </author>
    <author>
      <name>Andreas Thor</name>
    </author>
    <author>
      <name>Erhard Rahm</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Keywords: mapping evolution, ontology matching, ontology evolution</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.2731v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.2731v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.3223v1</id>
    <updated>2012-04-14T22:30:15Z</updated>
    <published>2012-04-14T22:30:15Z</published>
    <title>Intelligent Database Flexible Querying System by Approximate Query
  Processing</title>
    <summary>  Database flexible querying is an alternative to the classic one for users.
The use of Formal Concepts Analysis (FCA) makes it possible to make approximate
answers that those turned over by a classic DataBase Management System (DBMS).
Some applications do not need exact answers. However, flexible querying can be
expensive in response time. This time is more significant when the flexible
querying require the calculation of aggregate functions ("Sum", "Avg", "Count",
"Var" etc.). In this paper, we propose an approach which tries to solve this
problem by using Approximate Query Processing (AQP).
</summary>
    <author>
      <name>Oussama Tlili</name>
    </author>
    <author>
      <name>Minyar Sassi</name>
    </author>
    <author>
      <name>Habib Ounelli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures, 9 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The Third International Conference on Advances in Databases,
  Knowledge, and Data Applications (DBKDA 2011), January 23-28, 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1204.3223v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.3223v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.3432v3</id>
    <updated>2014-11-04T16:17:12Z</updated>
    <published>2012-04-16T10:13:43Z</published>
    <title>Converging to the Chase - a Tool for Finite Controllability</title>
    <summary>  We solve a problem, stated in [CGP10], showing that Sticky Datalog, defined
in the cited paper as an element of the Datalog\pm project, has the finite
controllability property. In order to do that, we develop a technique, which we
believe can have further applications, of approximating Chase(D, T), for a
database instance D and some sets of tuple generating dependencies T, by an
infinite sequence of finite structures, all of them being models of T.
</summary>
    <author>
      <name>T. Gogacz</name>
    </author>
    <author>
      <name>J. Marcinkowski</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/LICS.2013.61</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/LICS.2013.61" rel="related"/>
    <link href="http://arxiv.org/abs/1204.3432v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.3432v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P15" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.4948v2</id>
    <updated>2012-04-28T18:28:23Z</updated>
    <published>2012-04-22T23:47:28Z</published>
    <title>On Injective Embeddings of Tree Patterns</title>
    <summary>  We study three different kinds of embeddings of tree patterns:
weakly-injective, ancestor-preserving, and lca-preserving. While each of them
is often referred to as injective embedding, they form a proper hierarchy and
their computational properties vary (from P to NP-complete). We present a
thorough study of the complexity of the model checking problem i.e., is there
an embedding of a given tree pattern in a given tree, and we investigate the
impact of various restrictions imposed on the tree pattern: bound on the degree
of a node, bound on the height, and type of allowed labels and edges.
</summary>
    <author>
      <name>Jakub Michaliszyn</name>
    </author>
    <author>
      <name>Anca Muscholl</name>
    </author>
    <author>
      <name>Sławek Staworko</name>
    </author>
    <author>
      <name>Piotr Wieczorek</name>
    </author>
    <author>
      <name>Zhilin Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Under conference submission</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.4948v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.4948v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.6081v1</id>
    <updated>2012-04-26T23:25:50Z</updated>
    <published>2012-04-26T23:25:50Z</published>
    <title>Optimizing I/O for Big Array Analytics</title>
    <summary>  Big array analytics is becoming indispensable in answering important
scientific and business questions. Most analysis tasks consist of multiple
steps, each making one or multiple passes over the arrays to be analyzed and
generating intermediate results. In the big data setting, I/O optimization is a
key to efficient analytics. In this paper, we develop a framework and
techniques for capturing a broad range of analysis tasks expressible in
nested-loop forms, representing them in a declarative way, and optimizing their
I/O by identifying sharing opportunities. Experiment results show that our
optimizer is capable of finding execution plans that exploit nontrivial I/O
sharing opportunities with significant savings.
</summary>
    <author>
      <name>Yi Zhang</name>
    </author>
    <author>
      <name>Jun Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">VLDB2012</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 8, pp.
  764-775 (2012)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1204.6081v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.6081v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.0724v1</id>
    <updated>2011-12-14T11:46:13Z</updated>
    <published>2011-12-14T11:46:13Z</published>
    <title>Using Data Warehouse to Support Building Strategy or Forecast Business
  Tend</title>
    <summary>  The data warehousing is becoming increasingly important in terms of strategic
decision making through their capacity to integrate heterogeneous data from
multiple information sources in a common storage space, for querying and
analysis. So it can evolve into a multi-tier structure where parts of the
organization take information from the main data warehouse into their own
systems. These may include analysis databases or dependent data marts. As the
data warehouse evolves and the organization gets better at capturing
information on all interactions with the customer. Data warehouse can track
customer interactions over the whole of the customer's lifetime.
</summary>
    <author>
      <name>Phuc V. Nguyen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Hung Vuong Univesity, Ho Chi Minh City</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1205.0724v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.0724v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.1126v1</id>
    <updated>2012-05-05T12:29:35Z</updated>
    <published>2012-05-05T12:29:35Z</published>
    <title>A Comprehensive Study of CRM through Data Mining Techniques</title>
    <summary>  In today's competitive scenario in corporate world, "Customer Retention"
strategy in Customer Relationship Management (CRM) is an increasingly pressed
issue. Data mining techniques play a vital role in better CRM. This paper
attempts to bring a new perspective by focusing the issue of data mining
applications, opportunities and challenges in CRM. It covers the topic such as
customer retention, customer services, risk assessment, fraud detection and
some of the data mining tools which are widely used in CRM.
</summary>
    <author>
      <name>Md. Rashid Farooqi</name>
    </author>
    <author>
      <name>Khalid Raza</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the National Conference; NCCIST-2011, September 09,
  2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1205.1126v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.1126v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.1796v1</id>
    <updated>2012-05-08T19:58:28Z</updated>
    <published>2012-05-08T19:58:28Z</published>
    <title>Moving Object Trajectories Meta-Model And Spatio-Temporal Queries</title>
    <summary>  In this paper, a general moving object trajectories framework is put forward
to allow independent applications processing trajectories data benefit from a
high level of interoperability, information sharing as well as an efficient
answer for a wide range of complex trajectory queries. Our proposed meta-model
is based on ontology and event approach, incorporates existing presentations of
trajectory and integrates new patterns like space-time path to describe
activities in geographical space-time. We introduce recursive Region of
Interest concepts and deal mobile objects trajectories with diverse
spatio-temporal sampling protocols and different sensors available that
traditional data model alone are incapable for this purpose.
</summary>
    <author>
      <name>Azedine Boulmakoul</name>
    </author>
    <author>
      <name>Lamia Karim</name>
    </author>
    <author>
      <name>Ahmed Lbath</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijdms.2012.4203</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijdms.2012.4203" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Database Management Systems (IJDMS) Vol.4,
  No.2, April 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1205.1796v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.1796v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.2320v1</id>
    <updated>2012-05-10T17:29:05Z</updated>
    <published>2012-05-10T17:29:05Z</published>
    <title>Publishing Life Science Data as Linked Open Data: the Case Study of
  miRBase</title>
    <summary>  This paper presents our Linked Open Data (LOD) infrastructures for genomic
and experimental data related to microRNA biomolecules. Legacy data from two
well-known microRNA databases with experimental data and observations, as well
as change and version information about microRNA entities, are fused and
exported as LOD. Our LOD server assists biologists to explore biological
entities and their evolution, and provides a SPARQL endpoint for applications
and services to query historical miRNA data and track changes, their causes and
effects.
</summary>
    <author>
      <name>Theodore Dalamagas</name>
    </author>
    <author>
      <name>Nikos Bikakis</name>
    </author>
    <author>
      <name>George Papastefanatos</name>
    </author>
    <author>
      <name>Yannis Stavrakas</name>
    </author>
    <author>
      <name>Artemis G. Hatzigeorgiou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the First International Workshop On Open Data, WOD-2012
  (arXiv:1204.3726)</arxiv:comment>
    <link href="http://arxiv.org/abs/1205.2320v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.2320v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.5353v1</id>
    <updated>2012-05-24T07:37:28Z</updated>
    <published>2012-05-24T07:37:28Z</published>
    <title>A hybrid clustering algorithm for data mining</title>
    <summary>  Data clustering is a process of arranging similar data into groups. A
clustering algorithm partitions a data set into several groups such that the
similarity within a group is better than among groups. In this paper a hybrid
clustering algorithm based on K-mean and K-harmonic mean (KHM) is described.
The proposed algorithm is tested on five different datasets. The research is
focused on fast and accurate clustering. Its performance is compared with the
traditional K-means &amp; KHM algorithm. The result obtained from proposed hybrid
algorithm is much better than the traditional K-mean &amp; KHM algorithm.
</summary>
    <author>
      <name>Ravindra Jain</name>
    </author>
    <link href="http://arxiv.org/abs/1205.5353v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.5353v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.5921v1</id>
    <updated>2012-05-26T22:37:10Z</updated>
    <published>2012-05-26T22:37:10Z</published>
    <title>Robust representation for conversion UML class into XML Document using
  DOM</title>
    <summary>  This paper presents a Framework for converting a class diagram into an XML
structure and shows how to use Web files for the design of data warehouses
based on the classification UML. Extensible Markup Language (XML) has become a
standard for representing data over the Internet. We use XSD schema for define
the structure of XML documents and validate XML documents.
  A prototype has been developed, which migrates successfully UML Class into
XML document based on the formulation mathematics model. The experimental
results were very encouraging, demonstrating that the proposed approach is
feasible efficient and correct.
</summary>
    <author>
      <name>Noreddine GHERABI</name>
    </author>
    <author>
      <name>Mohamed BAHAJ</name>
    </author>
    <link href="http://arxiv.org/abs/1205.5921v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.5921v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.5922v1</id>
    <updated>2012-05-26T22:41:44Z</updated>
    <published>2012-05-26T22:41:44Z</published>
    <title>Mapping relational database into OWL Structure with data semantic
  preservation</title>
    <summary>  This paper proposes a solution for migrating an RDB into Web semantic. The
solution takes an existing RDB as input, and extracts its metadata
representation (MTRDB). Based on the MTRDB, a Canonical Data Model (CDM) is
generated. Finally, the structure of the classification scheme in the CDM model
is converted into OWL ontology and the recordsets of database are stored in owl
document. A prototype has been implemented, which migrates a RDB into OWL
structure, for demonstrate the practical applicability of our approach by
showing how the results of reasoning of this technique can help improve the Web
systems.
</summary>
    <author>
      <name>Noreddine Gherabi</name>
    </author>
    <author>
      <name>Khaoula Addakiri</name>
    </author>
    <author>
      <name>Mohamed Bahaj</name>
    </author>
    <link href="http://arxiv.org/abs/1205.5922v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.5922v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.6698v1</id>
    <updated>2012-05-30T14:33:10Z</updated>
    <published>2012-05-30T14:33:10Z</published>
    <title>Type-Based Detection of XML Query-Update Independence</title>
    <summary>  This paper presents a novel static analysis technique to detect XML
query-update independence, in the presence of a schema. Rather than types, our
system infers chains of types. Each chain represents a path that can be
traversed on a valid document during query/update evaluation. The resulting
independence analysis is precise, although it raises a challenging issue:
recursive schemas may lead to infer infinitely many chains. A sound and
complete approximation technique ensuring a finite analysis in any case is
presented, together with an efficient implementation performing the chain-based
analysis in polynomial space and time.
</summary>
    <author>
      <name>Nicole Bidoit-Tollu</name>
    </author>
    <author>
      <name>Dario Colazzo</name>
    </author>
    <author>
      <name>Federico Ulliana</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">VLDB2012</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 9, pp.
  872-883 (2012)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1205.6698v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.6698v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.0084v1</id>
    <updated>2012-08-01T03:51:05Z</updated>
    <published>2012-08-01T03:51:05Z</published>
    <title>Fundamentals of Order Dependencies</title>
    <summary>  Dependencies have played a significant role in database design for many
years. They have also been shown to be useful in query optimization. In this
paper, we discuss dependencies between lexicographically ordered sets of
tuples. We introduce formally the concept of order dependency and present a set
of axioms (inference rules) for them. We show how query rewrites based on these
axioms can be used for query optimization. We present several interesting
theorems that can be derived using the inference rules. We prove that
functional dependencies are subsumed by order dependencies and that our set of
axioms for order dependencies is sound and complete.
</summary>
    <author>
      <name>Jaroslaw Szlichta</name>
    </author>
    <author>
      <name>Parke Godfrey</name>
    </author>
    <author>
      <name>Jarek Gryz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">VLDB2012</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.
  1220-1231 (2012)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1208.0084v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.0084v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.1932v1</id>
    <updated>2012-08-09T14:52:19Z</updated>
    <published>2012-08-09T14:52:19Z</published>
    <title>Statistical Distortion: Consequences of Data Cleaning</title>
    <summary>  We introduce the notion of statistical distortion as an essential metric for
measuring the effectiveness of data cleaning strategies. We use this metric to
propose a widely applicable yet scalable experimental framework for evaluating
data cleaning strategies along three dimensions: glitch improvement,
statistical distortion and cost-related criteria. Existing metrics focus on
glitch improvement and cost, but not on the statistical impact of data cleaning
strategies. We illustrate our framework on real world data, with a
comprehensive suite of experiments and analyses.
</summary>
    <author>
      <name>Tamraparni Dasu</name>
    </author>
    <author>
      <name>Ji Meng Loh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">VLDB2012</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 11, pp.
  1674-1683 (2012)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1208.1932v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.1932v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.3307v3</id>
    <updated>2013-04-21T22:21:40Z</updated>
    <published>2012-08-16T07:43:28Z</published>
    <title>Impedance mismatch is not an "Objects vs. Relations" problem</title>
    <summary>  A problem of impedance mismatch between applications written in OO languages
and relational DB is not a problem of discrepancy between object-oriented and
relational approaches themselves. Its real causes can be found in usual
implementation of the OO approach. Direct comparison of the two approaches
cannot be used as a base for the conclusion that they are discrepant or
mismatched. Experimental proof of absence of contradiction between
object-oriented paradigm and relational data model is also presented
</summary>
    <author>
      <name>Grigoriev Evgeny</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1208.3307v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.3307v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.1; H.2.2; H.2.4; C.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.4173v1</id>
    <updated>2012-08-21T02:53:52Z</updated>
    <published>2012-08-21T02:53:52Z</published>
    <title>The Vertica Analytic Database: C-Store 7 Years Later</title>
    <summary>  This paper describes the system architecture of the Vertica Analytic Database
(Vertica), a commercialization of the design of the C-Store research prototype.
Vertica demonstrates a modern commercial RDBMS system that presents a classical
relational interface while at the same time achieving the high performance
expected from modern "web scale" analytic systems by making appropriate
architectural choices. Vertica is also an instructive lesson in how academic
systems research can be directly commercialized into a successful product.
</summary>
    <author>
      <name>Andrew Lamb</name>
    </author>
    <author>
      <name>Matt Fuller</name>
    </author>
    <author>
      <name>Ramakrishna Varadarajan</name>
    </author>
    <author>
      <name>Nga Tran</name>
    </author>
    <author>
      <name>Ben Vandier</name>
    </author>
    <author>
      <name>Lyric Doshi</name>
    </author>
    <author>
      <name>Chuck Bear</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">VLDB2012</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 12, pp.
  1790-1801 (2012)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1208.4173v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.4173v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.1011v1</id>
    <updated>2012-09-05T15:18:35Z</updated>
    <published>2012-09-05T15:18:35Z</published>
    <title>Kleisli Database Instances</title>
    <summary>  We use monads to relax the atomicity requirement for data in a database.
Depending on the choice of monad, the database fields may contain generalized
values such as lists or sets of values, or they may contain exceptions such as
various types of nulls. The return operation for monads ensures that any
ordinary database instance will count as one of these generalized instances,
and the bind operation ensures that generalized values behave well under joins
of foreign key sequences. Different monads allow for vastly different types of
information to be stored in the database. For example, we show that classical
concepts like Markov chains, graphs, and finite state automata are each
perfectly captured by a different monad on the same schema.
</summary>
    <author>
      <name>David I. Spivak</name>
    </author>
    <link href="http://arxiv.org/abs/1209.1011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.1011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="18C20, 68P15, 68Q65" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; H.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.2647v1</id>
    <updated>2012-09-12T15:48:33Z</updated>
    <published>2012-09-12T15:48:33Z</published>
    <title>Shadow Theory, data model design for data integration</title>
    <summary>  For data integration in information ecosystems, semantic heterogeneity is a
known difficulty. In this paper, we propose Shadow Theory as the philosophical
foundation to address this issue. It is based on the notion of shadows in
Plato's Allegory of the Cave. What we can observe are just shadows, and
meanings of shadows are mental entities that only exist in viewers' cognitive
structures. With enterprise customer data integration example, we proposed six
design principles and algebra to support required operations.
</summary>
    <author>
      <name>Jason T. Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">85 pages, 31 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.2647v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.2647v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.3944v1</id>
    <updated>2012-09-18T13:09:24Z</updated>
    <published>2012-09-18T13:09:24Z</published>
    <title>Cyclic Association Rules Mining under Constraints</title>
    <summary>  Several researchers have explored the temporal aspect of association rules
mining. In this paper, we focus on the cyclic association rules, in order to
discover correlations among items characterized by regular cyclic variation
overtime. The overview of the state of the art has revealed the drawbacks of
proposed algorithm literatures, namely the excessive number of generated rules
which are not meeting the expert's expectations. To overcome these
restrictions, we have introduced our approach dedicated to generate the cyclic
association rules under constraints through a new method called
Constraint-Based Cyclic Association Rules CBCAR. The carried out experiments
underline the usefulness and the performance of our new approach.
</summary>
    <author>
      <name>Wafa Tebourski Wahiba Ben Abdessalem Karaa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.3944v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.3944v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.4169v1</id>
    <updated>2012-09-19T07:23:02Z</updated>
    <published>2012-09-19T07:23:02Z</published>
    <title>Hybrid Data Mining Technique for Knowledge Discovery from Engineering
  Materials' Data sets</title>
    <summary>  Studying materials informatics from a data mining perspective can be
beneficial for manufacturing and other industrial engineering applications.
Predictive data mining technique and machine learning algorithm are combined to
design a knowledge discovery system for the selection of engineering materials
that meet the design specifications. Predictive method-Naive Bayesian
classifier and Machine learning Algorithm - Pearson correlation coefficient
method were implemented respectively for materials classification and
selection. The knowledge extracted from the engineering materials data sets is
proposed for effective decision making in advanced engineering materials design
applications.
</summary>
    <author>
      <name> Doreswamy</name>
    </author>
    <author>
      <name>Hemanth K. S</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 8 figures; International Journal of Database Management
  Systems (IJDMS), Vol.3, No.1, February 2011. arXiv admin note: text overlap
  with arXiv:1206.3078 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.4169v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.4169v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.4891v1</id>
    <updated>2012-10-16T17:46:50Z</updated>
    <published>2012-10-16T17:46:50Z</published>
    <title>Hokusai - Sketching Streams in Real Time</title>
    <summary>  We describe Hokusai, a real time system which is able to capture frequency
information for streams of arbitrary sequences of symbols. The algorithm uses
the CountMin sketch as its basis and exploits the fact that sketching is
linear. It provides real time statistics of arbitrary events, e.g. streams of
queries as a function of time. We use a factorizing approximation to provide
point estimates at arbitrary (time, item) combinations. Queries can be answered
in constant time.
</summary>
    <author>
      <name>Sergiy Matusevych</name>
    </author>
    <author>
      <name>Alex Smola</name>
    </author>
    <author>
      <name>Amr Ahmed</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</arxiv:comment>
    <link href="http://arxiv.org/abs/1210.4891v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.4891v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.6242v1</id>
    <updated>2012-10-23T14:22:02Z</updated>
    <published>2012-10-23T14:22:02Z</published>
    <title>Enhancing Algebraic Query Relaxation with Semantic Similarity</title>
    <summary>  Cooperative database systems support a database user by searching for answers
that are closely related to his query and hence are informative answers. Common
operators to relax the user query are Dropping Condition, Anti-Instantiation
and Goal Replacement. In this article, we provide an algebraic version of these
operators. Moreover we propose some heuristics to assign a degree of similarity
to each tuple of an answer table; this degree can help the user to determine
whether this answer is relevant for him or not.
</summary>
    <author>
      <name>Lena Wiese</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appeared in Proceedings of IADIS Information Systems 2012 (10-12
  March 2012, Berlin, Germany)</arxiv:comment>
    <link href="http://arxiv.org/abs/1210.6242v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.6242v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.0176v1</id>
    <updated>2012-11-01T13:45:04Z</updated>
    <published>2012-11-01T13:45:04Z</published>
    <title>Joining relations under discrete uncertainty</title>
    <summary>  In this paper we introduce and experimentally compare alternative algorithms
to join uncertain relations. Different algorithms are based on specific
principles, e.g., sorting, indexing, or building intermediate relational tables
to apply traditional approaches. As a consequence their performance is affected
by different features of the input data, and each algorithm is shown to be more
efficient than the others in specific cases. In this way statistics explicitly
representing the amount and kind of uncertainty in the input uncertain
relations can be used to choose the most efficient algorithm.
</summary>
    <author>
      <name>Matteo Magnani</name>
    </author>
    <author>
      <name>Danilo Montesi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">database join operator, uncertain relations with discrete
  uncertainty, algorithms and experimental evaluation (28 pages)</arxiv:comment>
    <link href="http://arxiv.org/abs/1211.0176v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.0176v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.4414v1</id>
    <updated>2012-11-19T13:43:39Z</updated>
    <published>2012-11-19T13:43:39Z</published>
    <title>Towards a Scalable Dynamic Spatial Database System</title>
    <summary>  With the rise of GPS-enabled smartphones and other similar mobile devices,
massive amounts of location data are available. However, no scalable solutions
for soft real-time spatial queries on large sets of moving objects have yet
emerged. In this paper we explore and measure the limits of actual algorithms
and implementations regarding different application scenarios. And finally we
propose a novel distributed architecture to solve the scalability issues.
</summary>
    <author>
      <name>Joaquín Keller</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIP6</arxiv:affiliation>
    </author>
    <author>
      <name>Raluca Diaconu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIP6</arxiv:affiliation>
    </author>
    <author>
      <name>Mathieu Valero</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIP6, INRIA Rocquencourt</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">(2012)</arxiv:comment>
    <link href="http://arxiv.org/abs/1211.4414v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.4414v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.3501v1</id>
    <updated>2012-12-14T15:28:45Z</updated>
    <published>2012-12-14T15:28:45Z</published>
    <title>On optimum left-to-right strategies for active context-free games</title>
    <summary>  Active context-free games are two-player games on strings over finite
alphabets with one player trying to rewrite the input string to match a target
specification. These games have been investigated in the context of exchanging
Active XML (AXML) data. While it was known that the rewriting problem is
undecidable in general, it is shown here that it is EXPSPACE-complete to decide
for a given context-free game, whether all safely rewritable strings can be
safely rewritten in a left-to-right manner, a problem that was previously
considered by Abiteboul et al. Furthermore, it is shown that the corresponding
problem for games with finite replacement languages is EXPTIME-complete.
</summary>
    <author>
      <name>Henrik Björklund</name>
    </author>
    <author>
      <name>Martin Schuster</name>
    </author>
    <author>
      <name>Thomas Schwentick</name>
    </author>
    <author>
      <name>Joscha Kulbatzki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in ICDT 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.3501v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.3501v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.6051v1</id>
    <updated>2012-12-25T14:07:44Z</updated>
    <published>2012-12-25T14:07:44Z</published>
    <title>Automatic approach for generating ETL operators</title>
    <summary>  This article addresses the generation of the ETL
operators(Extract-Transform-Load) for supplying a Data Warehouse from a
relational data source. As a first step, we add new rules to those proposed by
the authors of [1], these rules deal with the combination of ETL operators. In
a second step, we propose an automatic approach based on model transformations
to generate the ETL operations needed for loading a data warehouse. This
approach offers the possibility to set some designer requirements for loading.
</summary>
    <author>
      <name>Wided Bakari</name>
    </author>
    <author>
      <name>Mouez Ali</name>
    </author>
    <author>
      <name>Hanene Ben-Abdallah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in French</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.6051v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.6051v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.6640v1</id>
    <updated>2012-12-29T15:29:26Z</updated>
    <published>2012-12-29T15:29:26Z</published>
    <title>Exploring mutexes, the Oracle RDBMS retrial spinlocks</title>
    <summary>  Spinlocks are widely used in database engines for processes synchronization.
KGX mutexes is new retrial spinlocks appeared in contemporary Oracle versions
for submicrosecond synchronization. The mutex contention is frequently observed
in highly concurrent OLTP environments.
  This work explores how Oracle mutexes operate, spin, and sleep. It develops
predictive mathematical model and discusses parameters and statistics related
to mutex performance tuning, as well as results of contention experiments.
</summary>
    <author>
      <name>Andrey Nikolaev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of International Conference on Informatics MEDIAS2012.
  Cyprus, Limassol, May 7--14, 2012. ISBN 978-5-88835-023-2. 12 pages, 15
  figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.6640v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.6640v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.4200v1</id>
    <updated>2013-01-17T19:42:55Z</updated>
    <published>2013-01-17T19:42:55Z</published>
    <title>Enabling Operator Reordering in Data Flow Programs Through Static Code
  Analysis</title>
    <summary>  In many massively parallel data management platforms, programs are
represented as small imperative pieces of code connected in a data flow. This
popular abstraction makes it hard to apply algebraic reordering techniques
employed by relational DBMSs and other systems that use an algebraic
programming abstraction. We present a code analysis technique based on reverse
data and control flow analysis that discovers a set of properties from user
code, which can be used to emulate algebraic optimizations in this setting.
</summary>
    <author>
      <name>Fabian Hueske</name>
    </author>
    <author>
      <name>Aljoscha Krettek</name>
    </author>
    <author>
      <name>Kostas Tzoumas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, accepted and presented at the First International Workshop
  on Cross-model Language Design and Implementation (XLDI), affiliated with
  ICFP 2012, Copenhagen</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.4200v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.4200v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.0337v1</id>
    <updated>2013-02-02T03:36:37Z</updated>
    <published>2013-02-02T03:36:37Z</published>
    <title>Perancangan basisdata sistem informasi penggajian</title>
    <summary>  The purpose of this research is to design database scheme of information
system at XYZ University. By using database design methods (conceptual scheme,
logical scheme, &amp; physical scheme) the writer designs payroll information
system. The physical scheme is compatible with Borland Delphi Database Engine
Scheme to support the implementation of the I.S. After 3 (three) steps we get 7
(seven) tables, dan 6 (six) forms. By using this shemce, the system can produce
several reports quickly, accurately, efficiently, and effectively.
</summary>
    <author>
      <name>Leon Andretti Abdillah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">MATRIK. 8 (2006) 135-152</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1302.0337v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.0337v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.1923v1</id>
    <updated>2013-02-08T01:22:54Z</updated>
    <published>2013-02-08T01:22:54Z</published>
    <title>Update XML Views</title>
    <summary>  View update is the problem of translating an update to a view to some updates
to the source data of the view. In this paper, we show the factors determining
XML view update translation, propose a translation procedure, and propose
translated updates to the source document for different types of views. We
further show that the translated updates are precise. The proposed solution
makes it possible for users who do not have access privileges to the source
data to update the source data via a view.
</summary>
    <author>
      <name>Jixue Liu</name>
    </author>
    <author>
      <name>Chengfei Liu</name>
    </author>
    <author>
      <name>Theo Haerder</name>
    </author>
    <author>
      <name>Jeffery Xu Yu</name>
    </author>
    <link href="http://arxiv.org/abs/1302.1923v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.1923v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.3860v1</id>
    <updated>2013-02-15T20:04:10Z</updated>
    <published>2013-02-15T20:04:10Z</published>
    <title>ScalienDB: Designing and Implementing a Distributed Database using Paxos</title>
    <summary>  ScalienDB is a scalable, replicated database built on top of the Paxos
algorithm. It was developed from 2010 to 2012, when the startup backing it
failed. This paper discusses the design decisions of the distributed database,
describes interesting parts of the C++ codebase and enumerates lessons learned
putting ScalienDB into production at a handful of clients. The source code is
available on Github under the AGPL license, but it is no longer developed or
maintained.
</summary>
    <author>
      <name>Márton Trencséni</name>
    </author>
    <author>
      <name>Attila Gazsó</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1302.3860v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.3860v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.5549v1</id>
    <updated>2013-02-22T11:01:11Z</updated>
    <published>2013-02-22T11:01:11Z</published>
    <title>On Graph Deltas for Historical Queries</title>
    <summary>  In this paper, we address the problem of evaluating historical queries on
graphs. To this end, we investigate the use of graph deltas, i.e., a log of
time-annotated graph operations. Our storage model maintains the current graph
snapshot and the delta. We reconstruct past snapshots by applying appropriate
parts of the graph delta on the current snapshot. Query evaluation proceeds on
the reconstructed snapshots but we also propose algorithms based mostly on
deltas for efficiency. We introduce various techniques for improving
performance, including materializing intermediate snapshots, partial
reconstruction and indexing deltas.
</summary>
    <author>
      <name>Georgia Koloniari</name>
    </author>
    <author>
      <name>Dimitris Souravlias</name>
    </author>
    <author>
      <name>Evaggelia Pitoura</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 figure, WOSS 2012, Istanbul, Turkey</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of 1st Workshop on Online Social Systems (WOSS) 2012,
  in conjunction with VLDB 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1302.5549v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.5549v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.0418v1</id>
    <updated>2013-03-02T19:15:38Z</updated>
    <published>2013-03-02T19:15:38Z</published>
    <title>Transparent Data Encryption -- Solution for Security of Database
  Contents</title>
    <summary>  The present study deals with Transparent Data Encryption which is a
technology used to solve the problems of security of data. Transparent Data
Encryption means encrypting databases on hard disk and on any backup media.
Present day global business environment presents numerous security threats and
compliance challenges. To protect against data thefts and frauds we require
security solutions that are transparent by design.
</summary>
    <author>
      <name>Dr. Anwar Pasha Deshmukh</name>
    </author>
    <author>
      <name>Dr. Riyazuddin Qureshi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 Pages 2 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Advanced Computer Science and
  Applications, Volume 2 No. 3, March 2011, pp 25-28. ISSN: 2156-5570(Online) &amp;
  ISSN: 2158-107X(Print)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1303.0418v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.0418v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.5175v1</id>
    <updated>2013-03-21T06:14:05Z</updated>
    <published>2013-03-21T06:14:05Z</published>
    <title>Discovery of Convoys in Network Proximity Log</title>
    <summary>  This paper describes an algorithm for discovery of convoys in database with
proximity log. Traditionally, discovery of convoys covers trajectories
databases. This paper presents a model for context-aware browsing application
based on the network proximity. Our model uses mobile phone as proximity sensor
and proximity data replaces location information. As per our concept, any
existing or even especially created wireless network node could be used as
presence sensor that can discover access to some dynamic or user-generated
content. Content revelation in this model depends on rules based on the
proximity. Discovery of convoys in historical user's logs provides a new class
of rules for delivering local content to mobile subscribers.
</summary>
    <author>
      <name>Dmitry Namiot</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1303.5175v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.5175v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.0959v1</id>
    <updated>2013-04-03T14:11:04Z</updated>
    <published>2013-04-03T14:11:04Z</published>
    <title>Conditional Tables in practice</title>
    <summary>  Due to the ever increasing importance of the internet, interoperability of
heterogeneous data sources is as well of ever increasing importance.
Interoperability can be achieved e.g. through data integration and data
exchange. Common to both approaches is the need for the DBMS to be able to
store and query incomplete databases. In this report we present PossDB, a DBMS
capable of storing and querying incomplete databases. The system is wrapper
over PostgreSQL, and the query language is an extension of a subset of standard
SQL. Our experimental results show that our system scales well, actually better
than comparable systems.
</summary>
    <author>
      <name>Gosta Grahne</name>
    </author>
    <author>
      <name>Adrian Onet</name>
    </author>
    <author>
      <name>Nihat Tartal</name>
    </author>
    <link href="http://arxiv.org/abs/1304.0959v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.0959v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.7285v1</id>
    <updated>2013-04-25T09:27:18Z</updated>
    <published>2013-04-25T09:27:18Z</published>
    <title>Traitement approximatif des requêtes flexibles avec groupement
  d'attributs et jointure</title>
    <summary>  This paper addresses the problem of approximate processing for flexible
queries in the form SELECT-FROM-WHERE-GROUP BY with join condition. It offers a
flexible framework for online aggregation while promoting response time at the
expense of result accuracy.
</summary>
    <author>
      <name>Minyar Sassi-Hidri</name>
    </author>
    <author>
      <name>Soukaina Ben Bdira</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in French. The 13\`eme Conf\'erence Francophone sur l'Extraction et
  la Gestion des Connaissances (EGC), pp. 29-30, 2013</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The 3rd International Conference on Advances in Databases,
  Knowledge, and Data Applications (DBKDA), pp. 128-135, 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1304.7285v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.7285v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.3058v1</id>
    <updated>2013-05-14T08:31:10Z</updated>
    <published>2013-05-14T08:31:10Z</published>
    <title>Rule-Based Application Development using Webdamlog</title>
    <summary>  We present the WebdamLog system for managing distributed data on the Web in a
peer-to-peer manner. We demonstrate the main features of the system through an
application called Wepic for sharing pictures between attendees of the sigmod
conference. Using Wepic, the attendees will be able to share, download, rate
and annotate pictures in a highly decentralized manner. We show how WebdamLog
handles heterogeneity of the devices and services used to share data in such a
Web setting. We exhibit the simple rules that define the Wepic application and
show how to easily modify the Wepic application.
</summary>
    <author>
      <name>Serge Abiteboul</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LSV</arxiv:affiliation>
    </author>
    <author>
      <name>Émilien Antoine</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LSV</arxiv:affiliation>
    </author>
    <author>
      <name>Gerome Miklau</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LSV, UMASS</arxiv:affiliation>
    </author>
    <author>
      <name>Julia Stoyanovich</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LSV, McGill</arxiv:affiliation>
    </author>
    <author>
      <name>Jules Testard</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LSV, McGill</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SIGMOD - Special Interest Group on Management Of Data (2013)</arxiv:comment>
    <link href="http://arxiv.org/abs/1305.3058v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.3058v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.5653v1</id>
    <updated>2013-05-24T08:54:46Z</updated>
    <published>2013-05-24T08:54:46Z</published>
    <title>Geographica: A Benchmark for Geospatial RDF Stores</title>
    <summary>  Geospatial extensions of SPARQL like GeoSPARQL and stSPARQL have recently
been defined and corresponding geospatial RDF stores have been implemented.
However, there is no widely used benchmark for evaluating geospatial RDF stores
which takes into account recent advances to the state of the art in this area.
In this paper, we develop a benchmark, called Geographica, which uses both
real-world and synthetic data to test the offered functionality and the
performance of some prominent geospatial RDF stores.
</summary>
    <author>
      <name>George Garbis</name>
    </author>
    <author>
      <name>Kostis Kyzirakos</name>
    </author>
    <author>
      <name>Manolis Koubarakis</name>
    </author>
    <link href="http://arxiv.org/abs/1305.5653v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.5653v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.6506v1</id>
    <updated>2013-05-28T14:16:11Z</updated>
    <published>2013-05-28T14:16:11Z</published>
    <title>Notes on Physical &amp; Logical Data Layouts</title>
    <summary>  In this short note I review and discuss fundamental options for physical and
logical data layouts as well as the impact of the choices on data processing. I
should say in advance that these notes offer no new insights, that is,
everything stated here has already been published elsewhere. In fact, it has
been published in so many different places, such as blog posts, in the
literature, etc. that the main contribution is to bring it all together in one
place.
</summary>
    <author>
      <name>Michael Hausenblas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1305.6506v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.6506v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.1689v1</id>
    <updated>2013-06-07T11:17:41Z</updated>
    <published>2013-06-07T11:17:41Z</published>
    <title>Verification of Query Completeness over Processes [Extended Version]</title>
    <summary>  Data completeness is an essential aspect of data quality, and has in turn a
huge impact on the effective management of companies. For example, statistics
are computed and audits are conducted in companies by implicitly placing the
strong assumption that the analysed data are complete. In this work, we are
interested in studying the problem of completeness of data produced by business
processes, to the aim of automatically assessing whether a given database query
can be answered with complete information in a certain state of the process. We
formalize so-called quality-aware processes that create data in the real world
and store it in the company's information system possibly at a later point.
</summary>
    <author>
      <name>Simon Razniewski</name>
    </author>
    <author>
      <name>Marco Montali</name>
    </author>
    <author>
      <name>Werner Nutt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of a paper that was submitted to BPM 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1306.1689v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.1689v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.2460v1</id>
    <updated>2013-06-11T09:24:06Z</updated>
    <published>2013-06-11T09:24:06Z</published>
    <title>StreamWorks - A system for Dynamic Graph Search</title>
    <summary>  Acting on time-critical events by processing ever growing social media, news
or cyber data streams is a major technical challenge. Many of these data
sources can be modeled as multi-relational graphs. Mining and searching for
subgraph patterns in a continuous setting requires an efficient approach to
incremental graph search. The goal of our work is to enable real-time search
capabilities for graph databases. This demonstration will present a dynamic
graph query system that leverages the structural and semantic characteristics
of the underlying multi-relational graph.
</summary>
    <author>
      <name>Sutanay Choudhury</name>
    </author>
    <author>
      <name>Lawrence Holder</name>
    </author>
    <author>
      <name>George Chin</name>
    </author>
    <author>
      <name>Abhik Ray</name>
    </author>
    <author>
      <name>Sherman Beus</name>
    </author>
    <author>
      <name>John Feo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SIGMOD 2013: International Conference on Management of Data</arxiv:comment>
    <link href="http://arxiv.org/abs/1306.2460v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.2460v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.6734v1</id>
    <updated>2013-06-28T07:12:23Z</updated>
    <published>2013-06-28T07:12:23Z</published>
    <title>A novel ER model to relational model transformation algorithm for
  semantically clear high quality database design</title>
    <summary>  Conceptual modelling using the entity relationship (ER) model has been widely
used for database design for a long period of time. However, studies indicate
that creating a satisfactory relational model representation from an ER model
is uncertain due to the insufficiencies both in the transformation methods used
and in the relational model itself. In an effort to solve the issue the
original ER notation has been modified, and accordingly, a new transformation
algorithm has been developed. This paper presents the proposed transformation
algorithm. Using a real world example it shows how the algorithm can be applied
in practice. The paper also discusses how to validate the resulted database and
reclaim the information that it represents.
</summary>
    <author>
      <name>Dhammika Pieris</name>
    </author>
    <link href="http://arxiv.org/abs/1306.6734v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.6734v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.0193v1</id>
    <updated>2013-06-30T10:07:22Z</updated>
    <published>2013-06-30T10:07:22Z</published>
    <title>A Sampling Algebra for Aggregate Estimation</title>
    <summary>  As of 2005, sampling has been incorporated in all major database systems.
While efficient sampling techniques are realizable, determining the accuracy of
an estimate obtained from the sample is still an unresolved problem. In this
paper, we present a theoretical framework that allows an elegant treatment of
the problem. We base our work on generalized uniform sampling (GUS), a class of
sampling methods that subsumes a wide variety of sampling techniques. We
introduce a key notion of equivalence that allows GUS sampling operators to
commute with selection and join, and derivation of confidence intervals. We
illustrate the theory through extensive examples and give indications on how to
use it to provide meaningful estimations in database systems.
</summary>
    <author>
      <name>Supriya Nirkhiwale</name>
    </author>
    <author>
      <name>Alin Dobra</name>
    </author>
    <author>
      <name>Chris Jermaine</name>
    </author>
    <link href="http://arxiv.org/abs/1307.0193v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.0193v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.3448v1</id>
    <updated>2013-07-12T13:16:21Z</updated>
    <published>2013-07-12T13:16:21Z</published>
    <title>Evaluating a healthcare data warehouse for cancer diseases</title>
    <summary>  This paper presents the evaluation of the architecture of healthcare data
warehouse specific to cancer diseases. This data warehouse containing relevant
cancer medical information and patient data. The data warehouse provides the
source for all current and historical health data to help executive manager and
doctors to improve the decision making process for cancer patients. The
evaluation model based on Bill Inmon's definition of data warehouse is proposed
to evaluate the Cancer data warehouse.
</summary>
    <author>
      <name>Dr. Osama E. Sheta</name>
    </author>
    <author>
      <name>Ahmed Nour Eldeen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.3448v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.3448v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.4519v1</id>
    <updated>2013-07-17T07:09:30Z</updated>
    <published>2013-07-17T07:09:30Z</published>
    <title>Extending the ER Model to relational Model novel transformation
  Algorithm: transforming relationship Types among Subtypes</title>
    <summary>  A novel approach for creating ER conceptual models and an algorithm for
transforming them to the relational model has been developed by modifying and
extending the existing methods. A part of the new algorithm has previously been
presented. This paper presents the rest of the algorithm. One of the objectives
of this paper is to use it as a supportive document for ongoing empirical
evaluations of the new approach being conducted using the cognitive engagement
method and with the participation of different segments of the field as
respondents.
</summary>
    <author>
      <name>Dhammika Pieris</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.4519v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.4519v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.7328v1</id>
    <updated>2013-07-28T03:50:16Z</updated>
    <published>2013-07-28T03:50:16Z</published>
    <title>Data Warehouse Success and Strategic Oriented Business Intelligence: A
  Theoretical Framework</title>
    <summary>  With the proliferation of the data warehouses as supportive decision making
tools, organizations are increasingly looking forward for a complete data
warehouse success model that would manage the enormous amounts of growing data.
It is therefore important to measure the success of these massive projects.
While general IS success models have received great deals of attention, few
research has been conducted to assess the success of data warehouses for
strategic business intelligence purposes. The framework developed in this study
consists of the following nine measures: Vendors and Consultants, Management
Actions, System Quality, Information Quality, Data Warehouse Usage, Perceived
utility, Individual Decision Making Impact, Organizational Decision Making
Impact, and Corporate Strategic Goals Attainment.
</summary>
    <author>
      <name>Eiad Basher Alhyasat</name>
    </author>
    <author>
      <name>Mahmoud Al-Dalahmeh</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Management Research 5(3), 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1307.7328v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.7328v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.8269v1</id>
    <updated>2013-07-31T10:21:33Z</updated>
    <published>2013-07-31T10:21:33Z</published>
    <title>Introducing Access Control in Webdamlog</title>
    <summary>  We survey recent work on the specification of an access control mechanism in
a collaborative environment. The work is presented in the context of the
WebdamLog language, an extension of datalog to a distributed context. We
discuss a fine-grained access control mechanism for intentional data based on
provenance as well as a control mechanism for delegation, i.e., for deploying
rules at remote peers.
</summary>
    <author>
      <name>Serge Abiteboul</name>
    </author>
    <author>
      <name>Émilien Antoine</name>
    </author>
    <author>
      <name>Gerome Miklau</name>
    </author>
    <author>
      <name>Julia Stoyanovich</name>
    </author>
    <author>
      <name>Vera Zaychik Moffitt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 14th International Symposium on Database
  Programming Languages (DBPL 2013), August 30, 2013, Riva del Garda, Trento,
  Italy</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.8269v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.8269v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.1471v1</id>
    <updated>2013-08-07T04:12:50Z</updated>
    <published>2013-08-07T04:12:50Z</published>
    <title>Application of Inventory Management Principles for Efficient Data
  Placement in Storage Networks</title>
    <summary>  The principles and strategies found in material management are comparable and
analogue with the data management. This paper concentrates on the conversion of
product inventory management principles into data inventory management
principles. Efforts were made to enumerate various impacting parameters that
would be appropriate to consider if any data inventory model could be plotted.
</summary>
    <author>
      <name>R. Arokia Paul Rajan</name>
    </author>
    <author>
      <name>F. Sagayaraj Francis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSI International Journal of Computer Science Issues, Vol. 9, Issue
  6, No 2, November 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1308.1471v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.1471v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.2147v1</id>
    <updated>2013-08-09T15:04:09Z</updated>
    <published>2013-08-09T15:04:09Z</published>
    <title>Exploiting Locality in Lease-Based Replicated Transactional Memory via
  Task Migration</title>
    <summary>  We present Lilac-TM, the first locality-aware Distributed Software
Transactional Memory (DSTM) implementation. Lilac-TM is a fully decentralized
lease-based replicated DSTM. It employs a novel self- optimizing lease
circulation scheme based on the idea of dynamically determining whether to
migrate transactions to the nodes that own the leases required for their
validation, or to demand the acquisition of these leases by the node that
originated the transaction. Our experimental evaluation establishes that
Lilac-TM provides significant performance gains for distributed workloads
exhibiting data locality, while typically incurring no overhead for non-data
local workloads.
</summary>
    <author>
      <name>Danny Hendler</name>
    </author>
    <author>
      <name>Alex Naiman</name>
    </author>
    <author>
      <name>Sebastiano Peluso</name>
    </author>
    <author>
      <name>Francesco Quaglia</name>
    </author>
    <author>
      <name>Paolo Romano</name>
    </author>
    <author>
      <name>Adi Suissa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1308.2147v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.2147v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.2310v1</id>
    <updated>2013-08-10T13:21:20Z</updated>
    <published>2013-08-10T13:21:20Z</published>
    <title>Mining Positive and Negative Association Rules Using CoherentApproach</title>
    <summary>  In the data mining field, association rules are discovered having domain
knowledge specified as a minimum support threshold. The accuracy in setting up
this threshold directly influences the number and the quality of association
rules discovered. Typically, before association rules are mined, a user needs
to determine a support threshold in order to obtain only the frequent item
sets. Having users to determine a support threshold attracts a number of
issues. We propose an association rule mining framework that does not require a
per-set support threshold. Often, the number of association rules, even though
large in number, misses some interesting rules and the rules quality
necessitates further analysis. As a result, decision making using these rules
could lead to risky actions.
</summary>
    <author>
      <name>Rakesh Duggirala</name>
    </author>
    <author>
      <name>P. Narayana</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IJCTT-2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1308.2310v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.2310v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="97Pxx" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.1334v2</id>
    <updated>2013-09-10T09:48:21Z</updated>
    <published>2013-09-05T12:48:03Z</published>
    <title>Proceedings of the 14th International Symposium on Database Programming
  Languages (DBPL 2013), August 30, 2013, Riva del Garda, Trento, Italy</title>
    <summary>  This volume contains the papers presented at the 14th Symposium on Database
Programming Languages (DBPL 2013) held on August 30th, 2013, in Riva del Garda,
co-located with the 39th International Conference on Very Large Databases (VLDB
2013). They cover a wide range of topics including the application of
programming language techniques to further the expressiveness of database
languages, schema management, and the practical use of XPath. To complement
this technical program, DBPL 2013 featured three invited talks by Serge
Abiteboul (Inria), J\'er\^ome Sim\'eon (IBM), and Soren Lassen (Facebook).
</summary>
    <author>
      <name>Todd J. Green</name>
    </author>
    <author>
      <name>Alan Schmitt</name>
    </author>
    <link href="http://arxiv.org/abs/1309.1334v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.1334v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.2371v1</id>
    <updated>2013-09-10T04:52:58Z</updated>
    <published>2013-09-10T04:52:58Z</published>
    <title>Performance analysis of modified algorithm for finding multilevel
  association rules</title>
    <summary>  Multilevel association rules explore the concept hierarchy at multiple levels
which provides more specific information. Apriori algorithm explores the single
level association rules. Many implementations are available of Apriori
algorithm. Fast Apriori implementation is modified to develop new algorithm for
finding multilevel association rules. In this study the performance of this new
algorithm is analyzed in terms of running time in seconds.
</summary>
    <author>
      <name>Arpna Shrivastava</name>
    </author>
    <author>
      <name>R. C. Jain</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/cseij.2013.3401</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/cseij.2013.3401" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computer Science &amp; Engineering: An International Journal (CSEIJ),
  Vol. 3, No. 4, August 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1309.2371v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.2371v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.2687v1</id>
    <updated>2013-09-10T23:06:57Z</updated>
    <published>2013-09-10T23:06:57Z</published>
    <title>CrowdPlanner: A Crowd-Based Route Recommendation System</title>
    <summary>  CrowdPlanner -- a novel crowd-based route recommendation system has been
developed, which requests human workers to evaluate candidates routes
recommended by different sources and methods, and determine the best route
based on the feedbacks of these workers. Our system addresses two critical
issues in its core components: a) task generation component generates a series
of informative and concise questions with optimized ordering for a given
candidate route set so that workers feel comfortable and easy to answer; and b)
worker selection component utilizes a set of selection criteria and an
efficient algorithm to find the most eligible workers to answer the questions
with high accuracy.
</summary>
    <author>
      <name>Han Su</name>
    </author>
    <link href="http://arxiv.org/abs/1309.2687v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.2687v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.5821v1</id>
    <updated>2013-09-20T13:51:18Z</updated>
    <published>2013-09-20T13:51:18Z</published>
    <title>Undefined By Data: A Survey of Big Data Definitions</title>
    <summary>  The term big data has become ubiquitous. Owing to a shared origin between
academia, industry and the media there is no single unified definition, and
various stakeholders provide diverse and often contradictory definitions. The
lack of a consistent definition introduces ambiguity and hampers discourse
relating to big data. This short paper attempts to collate the various
definitions which have gained some degree of traction and to furnish a clear
and concise definition of an otherwise ambiguous term.
</summary>
    <author>
      <name>Jonathan Stuart Ward</name>
    </author>
    <author>
      <name>Adam Barker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Big data definition paper, 2 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.5821v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.5821v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.0141v4</id>
    <updated>2015-02-25T18:42:57Z</updated>
    <published>2013-10-01T05:02:14Z</published>
    <title>Hopping over Big Data: Accelerating Ad-hoc OLAP Queries with Grasshopper
  Algorithms</title>
    <summary>  This paper presents a family of algorithms for fast subset filtering within
ordered sets of integers representing composite keys. Applications include
significant acceleration of (ad-hoc) analytic queries against a data warehouse
without any additional indexing. The algorithms work for point, range and set
restrictions on multiple attributes, in any combination, and are inherently
multidimensional. The main idea consists in intelligent combination of
sequential crawling with jumps over large portions of irrelevant keys. The way
to combine them is adaptive to characteristics of the underlying data store.
</summary>
    <author>
      <name>Alexander Russakovsky</name>
    </author>
    <link href="http://arxiv.org/abs/1310.0141v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.0141v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.0229v2</id>
    <updated>2014-03-26T10:36:22Z</updated>
    <published>2013-10-01T10:31:31Z</published>
    <title>Evolutionary Algorithm for Graph Anonymization</title>
    <summary>  In recent years there has been a significant increase in the use of graphs as
a tool for representing information. It is very important to preserve the
privacy of users when one wants to publish this information, especially in the
case of social graphs. In this case, it is essential to implement an
anonymization process in the data in order to preserve users' privacy. In this
paper we present an algorithm for graph anonymization, called Evolutionary
Algorithm for Graph Anonymization (EAGA), based on edge modifications to
preserve the k-anonymity model.
</summary>
    <author>
      <name>Jordi Casas-Roma</name>
    </author>
    <author>
      <name>Jordi Herrera-Joancomartí</name>
    </author>
    <author>
      <name>Vicenç Torra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1310.0229v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.0229v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.2375v1</id>
    <updated>2013-10-09T07:19:40Z</updated>
    <published>2013-10-09T07:19:40Z</published>
    <title>Web Usage Mining: Pattern Discovery and Forecasting</title>
    <summary>  Web usage mining: automatic discovery of patterns in clickstreams and
associated data collected or generated as a result of user interactions with
one or more Web sites. This paper describes web usage mining for our college
log files to analyze the behavioral patterns and profiles of users interacting
with a Web site. The discovered patterns are represented as clusters that are
frequently accessed by groups of visitors with common interests. In this paper,
the visitors and hits were forecasted to predict the further access statistics.
</summary>
    <author>
      <name>Dhanamma Jagli</name>
    </author>
    <author>
      <name>Sangeeta Oswal</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IFRSA International Journal of Data Warehousing &amp; Mining |Vol
  2|issue4|November 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1310.2375v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.2375v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.4647v1</id>
    <updated>2013-10-17T10:21:58Z</updated>
    <published>2013-10-17T10:21:58Z</published>
    <title>Census Data Mining and Data Analysis using WEKA</title>
    <summary>  Data mining (also known as knowledge discovery from databases) is the process
of extraction of hidden, previously unknown and potentially useful information
from databases. The outcome of the extracted data can be analyzed for the
future planning and development perspectives. In this paper, we have made an
attempt to demonstrate how one can extract the local (district) level census,
socio-economic and population related other data for knowledge discovery and
their analysis using the powerful data mining tool Weka.
</summary>
    <author>
      <name>Sudhir B Jagtap</name>
    </author>
    <author>
      <name>Kodge B. G</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">06 pages, 03 figures. International Conference in Emerging Trends in
  Science, Technology and Management-2013, Singapore</arxiv:comment>
    <link href="http://arxiv.org/abs/1310.4647v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.4647v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.5254v1</id>
    <updated>2013-10-19T17:30:48Z</updated>
    <published>2013-10-19T17:30:48Z</published>
    <title>Real Time Data Warehouse</title>
    <summary>  Data Warehouse (DW) is an essential part of Business Intelligence. DW emerged
as a fast growing reporting and analysis technique in early 1980s. Today, it
has almost replaced relational databases. However, with passage of time, static
and historic data of DWs could not produce Real Time reporting and analysis,
thus giving a way to emerge the Idea of Real Time Data Warehouse (RTDW).
Although, there are problems with RTDWs, but with advancement in technology and
researchers focus, RTDWs will be able to generate real time reports, analysis
and forecasting.
</summary>
    <author>
      <name>Syed Ijaz Ahmad Bukhari</name>
    </author>
    <link href="http://arxiv.org/abs/1310.5254v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.5254v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.7829v1</id>
    <updated>2013-10-29T15:17:39Z</updated>
    <published>2013-10-29T15:17:39Z</published>
    <title>About Summarization in Large Fuzzy Databases</title>
    <summary>  Moved by the need increased for modeling of the fuzzy data, the success of
the systems of exact generation of summary of data, we propose in this paper, a
new approach of generation of summary from fuzzy data called Fuzzy-SaintEtiQ.
This approach is an extension of the SaintEtiQ model to support the fuzzy data.
It presents the following optimizations such as 1) the minimization of the
expert risk; 2) the construction of a more detailed and more precise summaries
hierarchy, and 3) the co-operation with the user by giving him fuzzy summaries
in different hierarchical levels
</summary>
    <author>
      <name>Ines Benali Sougui</name>
    </author>
    <author>
      <name>Minyar Sassi Hidri</name>
    </author>
    <author>
      <name>Amel Grissa-Touzi</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The 5th International Conference on Advances in Databases,
  Knowledge, and Data Applications (DBKDA), pp. 87-94, 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1310.7829v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.7829v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.0350v1</id>
    <updated>2013-11-02T06:55:10Z</updated>
    <published>2013-11-02T06:55:10Z</published>
    <title>Sequential Mining: Patterns and Algorithms Analysis</title>
    <summary>  This paper presents and analysis the common existing sequential pattern
mining algorithms. It presents a classifying study of sequential pattern-mining
algorithms into five extensive classes. First, on the basis of Apriori-based
algorithm, second on Breadth First Search-based strategy, third on Depth First
Search strategy, fourth on sequential closed-pattern algorithm and five on the
basis of incremental pattern mining algorithms. At the end, a comparative
analysis is done on the basis of important key features supported by various
algorithms. This study gives an enhancement in the understanding of the
approaches of sequential pattern mining.
</summary>
    <author>
      <name>Thabet Slimani</name>
    </author>
    <author>
      <name>Amor Lazzez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer and Electronics Research, Volume
  2, Issue 5, October 2013, pp 639-647</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1311.0350v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.0350v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.0156v1</id>
    <updated>2013-11-30T21:33:49Z</updated>
    <published>2013-11-30T21:33:49Z</published>
    <title>Datom: Towards modular data management</title>
    <summary>  Recent technology breakthroughs have enabled data collection of unprecedented
scale, rate, variety and complexity that has led to an explosion in data
management requirements. Existing theories and techniques are not adequate to
fulfil these requirements. We endeavour to rethink the way data management
research is being conducted and we propose to work towards modular data
management that will allow for unification of the expression of data management
problems and systematization of their solution. The core of such an approach is
the novel notion of a datom, i.e. a data management atom, which encapsulates
generic data management provision. The datom is the foundation for comparison,
customization and re-usage of data management problems and solutions. The
proposed approach can signal a revolution in data management research and a
long anticipated evolution in data management engineering.
</summary>
    <author>
      <name>Verena Kantere</name>
    </author>
    <link href="http://arxiv.org/abs/1312.0156v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.0156v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.2353v1</id>
    <updated>2013-12-09T09:34:05Z</updated>
    <published>2013-12-09T09:34:05Z</published>
    <title>On the difference between checking integrity constraints before or after
  updates</title>
    <summary>  Integrity checking is a crucial issue, as databases change their instance all
the time and therefore need to be checked continuously and rapidly. Decades of
research have produced a plethora of methods for checking integrity constraints
of a database in an incremental manner. However, not much has been said about
when to check integrity. In this paper, we study the differences and
similarities between checking integrity before an update (a.k.a. pre-test) or
after (a.k.a. post-test) in order to assess the respective convenience and
properties.
</summary>
    <author>
      <name>Davide Martinenghi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.2353v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.2353v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.5148v2</id>
    <updated>2014-04-14T05:08:00Z</updated>
    <published>2013-12-18T14:17:09Z</published>
    <title>Object Selection under Team Context</title>
    <summary>  Context-aware database has drawn increasing attention from both industry and
academia recently by taking users' current situation and environment into
consideration. However, most of the literature focus on individual context,
overlooking the team users. In this paper, we investigate how to integrate team
context into database query process to help the users' get top-ranked database
tuples and make the team more competitive. We introduce naive and optimized
query algorithm to select the suitable records and show that they output the
same results while the latter is more computational efficient. Extensive
empirical studies are conducted to evaluate the query approaches and
demonstrate their effectiveness and efficiency.
</summary>
    <author>
      <name>Xiaolu Lu</name>
    </author>
    <author>
      <name>Dongxu Li</name>
    </author>
    <author>
      <name>Xiang Li</name>
    </author>
    <author>
      <name>Ling Feng</name>
    </author>
    <link href="http://arxiv.org/abs/1312.5148v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.5148v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.0494v1</id>
    <updated>2014-01-02T18:14:33Z</updated>
    <published>2014-01-02T18:14:33Z</published>
    <title>Flexible SQLf query based on fuzzy linguistic summaries</title>
    <summary>  Data is often partially known, vague or ambiguous in many real world
applications. To deal with such imprecise information, fuzziness is introduced
in the classical model. SQLf is one of the practical language to deal with
flexible fuzzy querying in Fuzzy DataBases (FDB). However, with a huge amount
of fuzzy data, the necessity to work with synthetic views became a challenge
for many DB community researchers. The present work deals with Flexible SQLf
query based on fuzzy linguistic summaries. We use the fuzzy summaries produced
by our Fuzzy-SaintEtiq approach. It provides a description of objects depending
on the fuzzy linguistic labels specified as selection criteria.
</summary>
    <author>
      <name>Ines Benali-Sougui</name>
    </author>
    <author>
      <name>Minyar Sassi-Hidri</name>
    </author>
    <author>
      <name>Amel Grissa-Touzi</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Control, Engineering &amp; Information
  Technology (CEIT), Proceedings Engineering &amp; Technology, Vol. 1, pp. 175-180,
  2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1401.0494v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.0494v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.7733v1</id>
    <updated>2014-01-30T04:43:22Z</updated>
    <published>2014-01-30T04:43:22Z</published>
    <title>Security Implications of Distributed Database Management System Models</title>
    <summary>  Security features must be addressed when escalating a distributed database.
The choice between the object oriented and the relational data model, several
factors should be considered. The most important of these factors are single
and multilevel access controls (MAC), protection and integrity maintenance.
While determining which distributed database replica will be more secure for a
particular function, the choice should not be made exclusively on the basis of
available security features. One should also query the effectiveness and
efficiency of the delivery of these characteristics. In this paper, the
security strengths and weaknesses of both database models and the thorough
problems initiate in the distributed environment are conversed.
</summary>
    <author>
      <name>C. Sunil Kumar</name>
    </author>
    <author>
      <name>J. Seetha</name>
    </author>
    <author>
      <name>S. R. Vinotha</name>
    </author>
    <link href="http://arxiv.org/abs/1401.7733v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.7733v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.1327v1</id>
    <updated>2014-02-06T11:37:45Z</updated>
    <published>2014-02-06T11:37:45Z</published>
    <title>A Survey on Spatial Co-location Patterns Discovery from Spatial Datasets</title>
    <summary>  Spatial data mining or Knowledge discovery in spatial database is the
extraction of implicit knowledge, spatial relations and spatial patterns that
are not explicitly stored in databases. Co-location patterns discovery is the
process of finding the subsets of features that are frequently located together
in the same geographic area. In this paper, we discuss the different approaches
like Rule based approach, Join-less approach, Partial Join approach and
Constraint neighborhood based approach for finding co-location patterns.
</summary>
    <author>
      <name>Mr. Rushirajsinh L. Zala</name>
    </author>
    <author>
      <name>Mr. Brijesh B. Mehta</name>
    </author>
    <author>
      <name>Mr. Mahipalsinh R. Zala</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.14445/22312803/IJCTT-V7P140</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.14445/22312803/IJCTT-V7P140" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages,8 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCTT 7(3):137-142, January 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1402.1327v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.1327v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.3495v1</id>
    <updated>2014-03-14T07:26:33Z</updated>
    <published>2014-03-14T07:26:33Z</published>
    <title>Analyzing Large Biological Datasets with an Improved Algorithm for MIC</title>
    <summary>  A computational framework utilizes the traditional similarity measures for
mining the significant relationships in biological annotations is recently
proposed by Tatiana V. Karpinets et al. [2]. In this paper, an improved
approximation algorithm for MIC (maximal information coefficient) named IAMIC
is suggested to perfect this framework for discovering the hidden regularities
between biological annotations. Further, IAMIC is the enhanced algorithm for
approximating a novel similarity coefficient MIC with generality and
equitability, which makes it more appropriate for data exploration. Here it is
shown that IAMIC is also applicable for identify the associations between
biological annotations.
</summary>
    <author>
      <name>Shuliang Wang</name>
    </author>
    <author>
      <name>Yiping Zhao</name>
    </author>
    <link href="http://arxiv.org/abs/1403.3495v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.3495v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.3461v1</id>
    <updated>2014-04-14T05:20:48Z</updated>
    <published>2014-04-14T05:20:48Z</published>
    <title>A 2D based Partition Strategy for Solving Ranking under Team Context
  (RTP)</title>
    <summary>  In this paper, we propose a 2D based partition method for solving the problem
of Ranking under Team Context(RTC) on datasets without a priori. We first map
the data into 2D space using its minimum and maximum value among all
dimensions. Then we construct window queries with consideration of current team
context. Besides, during the query mapping procedure, we can pre-prune some
tuples which are not top ranked ones. This pre-classified step will defer
processing those tuples and can save cost while providing solutions for the
problem. Experiments show that our algorithm performs well especially on large
datasets with correctness.
</summary>
    <author>
      <name>Xiaolu Lu</name>
    </author>
    <author>
      <name>Dongxu Li</name>
    </author>
    <author>
      <name>Xiang Li</name>
    </author>
    <author>
      <name>Ling Feng</name>
    </author>
    <link href="http://arxiv.org/abs/1404.3461v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.3461v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.6857v2</id>
    <updated>2014-06-28T23:32:35Z</updated>
    <published>2014-04-28T02:58:31Z</published>
    <title>Causality in Databases: The Diagnosis and Repair Connections</title>
    <summary>  In this work we establish and investigate the connections between causality
for query answers in databases, database repairs wrt. denial constraints, and
consistency-based diagnosis. The first two are relatively new problems in
databases, and the third one is an established subject in knowledge
representation. We show how to obtain database repairs from causes and the
other way around. The vast body of research on database repairs can be applied
to the newer problem of determining actual causes for query answers. By
formulating a causality problem as a diagnosis problem, we manage to
characterize causes in terms of a system's diagnoses.
</summary>
    <author>
      <name>Babak Salimi</name>
    </author>
    <author>
      <name>Leopoldo Bertossi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. 15th International Workshop on Non-Monotonic Reasoning (NMR
  2014)</arxiv:comment>
    <link href="http://arxiv.org/abs/1404.6857v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.6857v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.1339v1</id>
    <updated>2014-05-06T16:22:38Z</updated>
    <published>2014-05-06T16:22:38Z</published>
    <title>General upper bounds for well-behaving goodness measures on dependency
  rules</title>
    <summary>  In the search for statistical dependency rules, a crucial task is to restrict
the search space by estimating upper bounds for the goodness of yet
undiscovered rules. In this paper, we show that all well-behaving goodness
measures achieve their maximal values in the same points. Therefore, the same
generic search strategy can be applied with any of these measures. The notion
of well-behaving measures is based on the classical axioms for any proper
goodness measures, and extended to negative dependencies, as well. As an
example, we show that several commonly used goodness measures are
well-behaving.
</summary>
    <author>
      <name>Wilhelmiina Hämäläinen</name>
    </author>
    <link href="http://arxiv.org/abs/1405.1339v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.1339v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.1360v1</id>
    <updated>2014-05-06T16:54:20Z</updated>
    <published>2014-05-06T16:54:20Z</published>
    <title>Assessing the statistical significance of association rules</title>
    <summary>  An association rule is statistically significant, if it has a small
probability to occur by chance. It is well-known that the traditional
frequency-confidence framework does not produce statistically significant
rules. It can both accept spurious rules (type 1 error) and reject significant
rules (type 2 error). The same problem concerns other commonly used
interestingness measures and pruning heuristics.
  In this paper, we inspect the most common measure functions - frequency,
confidence, degree of dependence, $\chi^2$, correlation coefficient, and
$J$-measure - and redundancy reduction techniques. For each technique, we
analyze whether it can make type 1 or type 2 error and the conditions under
which the error occurs. In addition, we give new theoretical results which can
be use to guide the search for statistically significant association rules.
</summary>
    <author>
      <name>Wilhelmiina Hämäläinen</name>
    </author>
    <link href="http://arxiv.org/abs/1405.1360v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.1360v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.1851v1</id>
    <updated>2014-05-08T09:26:52Z</updated>
    <published>2014-05-08T09:26:52Z</published>
    <title>Output Privacy Protection With Pattern-Based Heuristic Algorithm</title>
    <summary>  Privacy Preserving Data Mining(PPDM) is an ongoing research area aimed at
bridging the gap between the collaborative data mining and data confidentiality
There are many different approaches which have been adopted for PPDM, of them
the rule hiding approach is used in this article. This approach ensures output
privacy that prevent the mined patterns(itemsets) from malicious inference
problems. An efficient algorithm named as Pattern-based Maxcover Algorithm is
proposed with experimental results. This algorithm minimizes the dissimilarity
between the source and the released database; Moreover the patterns protected
cannot be retrieved from the released database by an adversary or counterpart
even with an arbitrarily low support threshold.
</summary>
    <author>
      <name>P. Cynthia Selvi</name>
    </author>
    <author>
      <name>A. R. Mohammed Shanavas</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijcsit.2014.6210</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijcsit.2014.6210" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.1851v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.1851v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.1912v1</id>
    <updated>2014-05-08T13:06:34Z</updated>
    <published>2014-05-08T13:06:34Z</published>
    <title>The Efficiency Examination of Teaching of Different Normalization
  Methods</title>
    <summary>  Normalization is an important database design method, in the course of the
teaching of data modeling the understanding and applying of this method cause
problems for students the most. For improving the efficiency of learning
normalization we looked for alternative normalization methods and introduced
them into education. We made a survey among engineer students how efficient
could they execute the normalization with different methods. We executed
statistical and data mining examinations to decide whether any of the methods
resulted significantly better solutions.
</summary>
    <author>
      <name>Márta Czenky</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijdms.2014.6201</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijdms.2014.6201" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Database Management Systems ( IJDMS )
  Vol.6, No.2, April 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1405.1912v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.1912v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.1; K.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.4607v1</id>
    <updated>2014-05-19T05:09:50Z</updated>
    <published>2014-05-19T05:09:50Z</published>
    <title>$Υ$-DB: Managing scientific hypotheses as uncertain data</title>
    <summary>  In view of the paradigm shift that makes science ever more data-driven, we
consider deterministic scientific hypotheses as uncertain data. This vision
comprises a probabilistic database (p-DB) design methodology for the systematic
construction and management of U-relational hypothesis DBs, viz.,
$\Upsilon$-DBs. It introduces hypothesis management as a promising new class of
applications for p-DBs. We illustrate the potential of $\Upsilon$-DB as a tool
for deep predictive analytics.
</summary>
    <author>
      <name>Bernardo Gonçalves</name>
    </author>
    <author>
      <name>Fabio Porto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in PVLDB 2014</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">PVLDB 7(11):959-62, 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1405.4607v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.4607v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.5671v1</id>
    <updated>2014-05-22T08:56:01Z</updated>
    <published>2014-05-22T08:56:01Z</published>
    <title>A Logical Formalization of a Secure XML Database</title>
    <summary>  In this paper, we first define a logical theory representing an XML database
supporting XPath as query language and XUpdate as modification language. We
then extend our theory with predicates allowing us to specify the security
policy protecting the database. The security policy includes rules addressing
the read and write privileges. We propose axioms to derive the database view
each user is permitted to see. We also propose axioms to derive the new
database content after an update.
</summary>
    <author>
      <name>Alban Gabillon</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">GePaSUD</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1405.5671v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.5671v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.5905v2</id>
    <updated>2015-05-17T23:01:31Z</updated>
    <published>2014-05-22T20:43:51Z</published>
    <title>Managing large-scale scientific hypotheses as uncertain and
  probabilistic data with support for predictive analytics</title>
    <summary>  The sheer scale of high-resolution raw data generated by simulation has
motivated non-conventional approaches for data exploration referred as
`immersive' and `in situ' query processing of the raw simulation data. Another
step towards supporting scientific progress is to enable data-driven hypothesis
management and predictive analytics out of simulation results. We present a
synthesis method and tool for encoding and managing competing hypotheses as
uncertain data in a probabilistic database that can be conditioned in the
presence of observations.
</summary>
    <author>
      <name>Bernardo Gonçalves</name>
    </author>
    <author>
      <name>Fabio Porto</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MCSE.2015.102</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MCSE.2015.102" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 9 figures, 1 table</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Computing in Science and Eng. 17(5):35-43, 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1405.5905v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.5905v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.2125v1</id>
    <updated>2014-06-09T10:33:41Z</updated>
    <published>2014-06-09T10:33:41Z</published>
    <title>From XML Schema to JSON Schema: Translation with CHR</title>
    <summary>  Despite its rising popularity as data format especially for web services, the
software ecosystem around the JavaScript Object Notation (JSON) is not as
widely distributed as that of XML. For both data formats there exist schema
languages to specify the structure of instance documents, but there is
currently no opportunity to translate already existing XML Schema documents
into equivalent JSON Schemas.
  In this paper we introduce an implementation of a language translator. It
takes an XML Schema and creates its equivalent JSON Schema document. Our
approach is based on Prolog and CHR. By unfolding the XML Schema document into
CHR constraints, it is possible to specify the concrete translation rules in a
declarative way.
</summary>
    <author>
      <name>Falco Nogatz</name>
    </author>
    <author>
      <name>Thom Frühwirth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Part of CHR 2014 proceedings (arXiv:1406.1510)</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.2125v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.2125v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.5917v1</id>
    <updated>2014-06-23T14:21:13Z</updated>
    <published>2014-06-23T14:21:13Z</published>
    <title>BSTree: an Incremental Indexing Structure for Similarity Search and Real
  Time Monitoring of Data Streams</title>
    <summary>  In this work, a new indexing technique of data streams called BSTree is
proposed. This technique uses the method of data discretization, SAX [4], to
reduce online the dimensionality of data streams. It draws on Btree to build
the index and finally uses an LRV (least Recently visited) pruning technique to
rid the index structure from data whose last visit time exceeds a threshold
value and thus minimizes response time for similarity search queries.
</summary>
    <author>
      <name>Abdelwaheb Ferchichi</name>
    </author>
    <author>
      <name>Mohamed Salah Gouider</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Future Information Technology Lecture Notes in Electrical
  Engineering Volume 276, 2014, pp 185-190</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1406.5917v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.5917v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.7371v1</id>
    <updated>2014-06-28T08:18:57Z</updated>
    <published>2014-06-28T08:18:57Z</published>
    <title>Using Apriori with WEKA for Frequent Pattern Mining</title>
    <summary>  Knowledge exploration from the large set of data,generated as a result of the
various data processing activities due to data mining only. Frequent Pattern
Mining is a very important undertaking in data mining. Apriori approach applied
to generate frequent item set generally espouse candidate generation and
pruning techniques for the satisfaction of the desired objective. This paper
shows how the different approaches achieve the objective of frequent mining
along with the complexities required to perform the job. This paper
demonstrates the use of WEKA tool for association rule mining using Apriori
algorithm.
</summary>
    <author>
      <name>Paresh Tanna</name>
    </author>
    <author>
      <name>Yogesh Ghodasara</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.14445/22315381/IJETT-V12P223</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.14445/22315381/IJETT-V12P223" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 Pages, 4 Figures, "Published with International Journal of
  Engineering Trends and Technology (IJETT)"</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Engineering Trends and Technology
  (IJETT), V12(3), 127-131, June 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1406.7371v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.7371v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.0116v1</id>
    <updated>2014-07-01T07:03:22Z</updated>
    <published>2014-07-01T07:03:22Z</published>
    <title>Differential privacy for counting queries: can Bayes estimation help
  uncover the true value?</title>
    <summary>  Differential privacy is achieved by the introduction of Laplacian noise in
the response to a query, establishing a precise trade-off between the level of
differential privacy and the accuracy of the database response (via the amount
of noise introduced). Multiple queries may improve the accuracy but erode the
privacy budget. We examine the case where we submit just a single counting
query. We show that even in that case a Bayesian approach may be used to
improve the accuracy for the same amount of noise injected, if we know the size
of the database and the probability of a positive response to the query.
</summary>
    <author>
      <name>Maurizio Naldi</name>
    </author>
    <author>
      <name>Giuseppe D'Acquisto</name>
    </author>
    <link href="http://arxiv.org/abs/1407.0116v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.0116v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8; H.2.4; K.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.3191v1</id>
    <updated>2014-07-11T15:06:03Z</updated>
    <published>2014-07-11T15:06:03Z</published>
    <title>A Comparison of Blocking Methods for Record Linkage</title>
    <summary>  Record linkage seeks to merge databases and to remove duplicates when unique
identifiers are not available. Most approaches use blocking techniques to
reduce the computational complexity associated with record linkage. We review
traditional blocking techniques, which typically partition the records
according to a set of field attributes, and consider two variants of a method
known as locality sensitive hashing, sometimes referred to as "private
blocking." We compare these approaches in terms of their recall, reduction
ratio, and computational complexity. We evaluate these methods using different
synthetic datafiles and conclude with a discussion of privacy-related issues.
</summary>
    <author>
      <name>Rebecca C. Steorts</name>
    </author>
    <author>
      <name>Samuel L. Ventura</name>
    </author>
    <author>
      <name>Mauricio Sadinle</name>
    </author>
    <author>
      <name>Stephen E. Fienberg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 2 tables, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.3191v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.3191v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.6812v1</id>
    <updated>2014-07-25T08:33:12Z</updated>
    <published>2014-07-25T08:33:12Z</published>
    <title>Aber-OWL: a framework for ontology-based data access in biology</title>
    <summary>  Many ontologies have been developed in biology and these ontologies
increasingly contain large volumes of formalized knowledge commonly expressed
in the Web Ontology Language (OWL). Computational access to the knowledge
contained within these ontologies relies on the use of automated reasoning. We
have developed the Aber-OWL infrastructure that provides reasoning services for
bio-ontologies. Aber-OWL consists of an ontology repository, a set of web
services and web interfaces that enable ontology-based semantic access to
biological data and literature. Aber-OWL is freely available at
http://aber-owl.net.
</summary>
    <author>
      <name>Robert Hoehndorf</name>
    </author>
    <author>
      <name>Luke Slater</name>
    </author>
    <author>
      <name>Paul N. Schofield</name>
    </author>
    <author>
      <name>Georgios V. Gkoutos</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1186/s12859-015-0456-9</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1186/s12859-015-0456-9" rel="related"/>
    <link href="http://arxiv.org/abs/1407.6812v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.6812v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.2800v2</id>
    <updated>2014-08-27T12:33:33Z</updated>
    <published>2014-08-12T18:53:23Z</published>
    <title>Supporting SPARQL Update Queries in RDF-XML Integration</title>
    <summary>  The Web of Data encourages organizations and companies to publish their data
according to the Linked Data practices and offer SPARQL endpoints. On the other
hand, the dominant standard for information exchange is XML. The SPARQL2XQuery
Framework focuses on the automatic translation of SPARQL queries in XQuery
expressions in order to access XML data across the Web. In this paper, we
outline our ongoing work on supporting update queries in the RDF-XML
integration scenario.
</summary>
    <author>
      <name>Nikos Bikakis</name>
    </author>
    <author>
      <name>Chrisa Tsinaraki</name>
    </author>
    <author>
      <name>Ioannis Stavrakantonakis</name>
    </author>
    <author>
      <name>Stavros Christodoulakis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13th International Semantic Web Conference (ISWC '14)</arxiv:comment>
    <link href="http://arxiv.org/abs/1408.2800v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.2800v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.4793v1</id>
    <updated>2014-08-19T22:57:41Z</updated>
    <published>2014-08-19T22:57:41Z</published>
    <title>Restpark: Minimal RESTful API for Retrieving RDF Triples</title>
    <summary>  How do RDF datasets currently get published on the Web? They are either
available as large RDF files, which need to be downloaded and processed
locally, or they exist behind complex SPARQL endpoints. By providing a RESTful
API that can access triple data, we allow users to query a dataset through a
simple interface based on just a couple of HTTP parameters. If RDF resources
were published this way we could quickly build applications that depend on
these datasets, without having to download and process them locally. This is
what Restpark is: a set of HTTP GET parameters that servers need to handle, and
respond with JSON-LD.
</summary>
    <author>
      <name>Luca Matteis</name>
    </author>
    <link href="http://arxiv.org/abs/1408.4793v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.4793v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.6395v2</id>
    <updated>2014-09-03T19:09:38Z</updated>
    <published>2014-08-27T12:31:17Z</published>
    <title>Bridging the Semantic Gap between RDF and SPARQL using Completeness
  Statements [Extended Version]</title>
    <summary>  RDF data is often treated as incomplete, following the Open-World Assumption.
On the other hand, SPARQL, the standard query language over RDF, usually
follows the Closed-World Assumption, assuming RDF data to be complete. This
gives rise to a semantic gap between RDF and SPARQL. In this paper, we address
how to close the semantic gap between RDF and SPARQL in terms of certain
answers and possible answers using completeness statements.
</summary>
    <author>
      <name>Fariz Darari</name>
    </author>
    <author>
      <name>Simon Razniewski</name>
    </author>
    <author>
      <name>Werner Nutt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is an extended version with proofs of a poster paper at
  ISWC 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1408.6395v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.6395v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.0651v1</id>
    <updated>2014-09-02T10:07:27Z</updated>
    <published>2014-09-02T10:07:27Z</published>
    <title>An LSH Index for Computing Kendall's Tau over Top-k Lists</title>
    <summary>  We consider the problem of similarity search within a set of top-k lists
under the Kendall's Tau distance function. This distance describes how related
two rankings are in terms of concordantly and discordantly ordered items. As
top-k lists are usually very short compared to the global domain of possible
items to be ranked, creating an inverted index to look up overlapping lists is
possible but does not capture tight enough the similarity measure. In this
work, we investigate locality sensitive hashing schemes for the Kendall's Tau
distance and evaluate the proposed methods using two real-world datasets.
</summary>
    <author>
      <name>Koninika Pal</name>
    </author>
    <author>
      <name>Sebastian Michel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 8 subfigures, presented in Seventeenth International
  Workshop on the Web and Databases (WebDB 2014) co-located with ACM SIGMOD2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.0651v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.0651v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.0798v1</id>
    <updated>2014-09-02T17:16:47Z</updated>
    <published>2014-09-02T17:16:47Z</published>
    <title>DataHub: Collaborative Data Science &amp; Dataset Version Management at
  Scale</title>
    <summary>  Relational databases have limited support for data collaboration, where teams
collaboratively curate and analyze large datasets. Inspired by software version
control systems like git, we propose (a) a dataset version control system,
giving users the ability to create, branch, merge, difference and search large,
divergent collections of datasets, and (b) a platform, DataHub, that gives
users the ability to perform collaborative data analysis building on this
version control system. We outline the challenges in providing dataset version
control at scale.
</summary>
    <author>
      <name>Anant Bhardwaj</name>
    </author>
    <author>
      <name>Souvik Bhattacherjee</name>
    </author>
    <author>
      <name>Amit Chavan</name>
    </author>
    <author>
      <name>Amol Deshpande</name>
    </author>
    <author>
      <name>Aaron J. Elmore</name>
    </author>
    <author>
      <name>Samuel Madden</name>
    </author>
    <author>
      <name>Aditya G. Parameswaran</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.0798v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.0798v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.0980v2</id>
    <updated>2015-07-03T20:17:42Z</updated>
    <published>2014-09-03T08:06:22Z</published>
    <title>Monoidal functional dependencies</title>
    <summary>  We present a complete logic for reasoning with functional dependencies (FDs)
with semantics defined over classes of commutative integral partially ordered
monoids and complete residuated lattices. The dependencies allow us to express
stronger relationships between attribute values than the ordinary FDs. In our
setting, the dependencies not only express that certain values are determined
by others but also express that similar values of attributes imply similar
values of other attributes. We show complete axiomatization using a system of
Armstrong-like rules, comment on related computational issues, and the
relational vs. propositional semantics of the dependencies.
</summary>
    <author>
      <name>Vilem Vychodil</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.jcss.2015.03.006</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.jcss.2015.03.006" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Computer and System Sciences 81(7) (2015) 1357-1372</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1409.0980v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.0980v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P15, 03B52, 03G10" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4; F.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.3682v1</id>
    <updated>2014-09-12T08:57:24Z</updated>
    <published>2014-09-12T08:57:24Z</published>
    <title>A novel recovery mechanism enabling fine-granularity locking and fast,
  REDO-only recovery</title>
    <summary>  We present a series of novel techniques and algorithms for transaction
commit, logging, recovery, and propagation control. In combination, they
provide a recovery component that maintains the persistent state of the
database (both log and data pages) always in a committed state. Recovery from
system and media failures only requires only REDO operations, which can happen
concurrently with the processing of new transactions. The mechanism supports
fine-granularity locking, partial rollbacks, and snapshot isolation for reader
transactions. Our design does not assume a specific hardware configuration such
as non-volatile RAM or flash---it is designed for traditional disk
environments. Nevertheless, it can exploit modern I/O devices for higher
transaction throughput and reduced recovery time with a high degree of
flexibility.
</summary>
    <author>
      <name>Caetano Sauer</name>
    </author>
    <author>
      <name>Theo Härder</name>
    </author>
    <link href="http://arxiv.org/abs/1409.3682v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.3682v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.6848v1</id>
    <updated>2014-09-24T07:20:20Z</updated>
    <published>2014-09-24T07:20:20Z</published>
    <title>A New Clustering Algorithm Based on Near Neighbor Influence</title>
    <summary>  This paper presents Clustering based on Near Neighbor Influence (CNNI), a new
clustering algorithm which is inspired by the idea of near neighbor and the
superposition principle of influence. In order to clearly describe this
algorithm, it introduces some important concepts, such as near neighbor point
set, near neighbor influence, and similarity measure. By simulated experiments
of some artificial data sets and seven real data sets, we observe that this
algorithm can often get good clustering quality when making proper value of
some parameters. At last, it gives some research expectations to popularize
this algorithm.
</summary>
    <author>
      <name>Xinquan Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 9 figures, and 8 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.6848v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.6848v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.7311v2</id>
    <updated>2014-09-30T15:22:51Z</updated>
    <published>2014-09-25T16:08:14Z</published>
    <title>Estimating the pattern frequency spectrum inside the browser</title>
    <summary>  We present a browser application for estimating the number of frequent
patterns, in particular itemsets, as well as the pattern frequency spectrum.
The pattern frequency spectrum is defined as the function that shows for every
value of the frequency threshold $\sigma$ the number of patterns that are
frequent in a given dataset. Our demo implements a recent algorithm proposed by
the authors for finding the spectrum. The demo is 100% JavaScript, and runs in
all modern browsers. We observe that modern JavaScript engines can deliver
performance that makes it viable to run non-trivial data analysis algorithms in
browser applications.
</summary>
    <author>
      <name>Matthijs van Leeuwen</name>
    </author>
    <author>
      <name>Antti Ukkonen</name>
    </author>
    <link href="http://arxiv.org/abs/1409.7311v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.7311v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.1343v1</id>
    <updated>2014-10-06T12:28:22Z</updated>
    <published>2014-10-06T12:28:22Z</published>
    <title>Combined Algorithm for Data Mining using Association rules</title>
    <summary>  Association Rule mining is one of the most important fields in data mining
and knowledge discovery. This paper proposes an algorithm that combines the
simple association rules derived from basic Apriori Algorithm with the multiple
minimum support using maximum constraints. The algorithm is implemented, and is
compared to its predecessor algorithms using a novel proposed comparison
algorithm. Results of applying the proposed algorithm show faster performance
than other algorithms without scarifying the accuracy.
</summary>
    <author>
      <name>Walaa Medhat</name>
    </author>
    <author>
      <name>Ahmed Hassan Yousef</name>
    </author>
    <author>
      <name>Hoda Korashy Mohamed</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Ain Shams Journal of Electrical Engineering, 2008</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.1343v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.1343v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.7990v1</id>
    <updated>2014-10-27T22:17:19Z</updated>
    <published>2014-10-27T22:17:19Z</published>
    <title>Linked Data Integration with Conflicts</title>
    <summary>  Linked Data have emerged as a successful publication format and one of its
main strengths is its fitness for integration of data from multiple sources.
This gives them a great potential both for semantic applications and the
enterprise environment where data integration is crucial. Linked Data
integration poses new challenges, however, and new algorithms and tools
covering all steps of the integration process need to be developed. This paper
explores Linked Data integration and its specifics. We focus on data fusion and
conflict resolution: two novel algorithms for Linked Data fusion with
provenance tracking and quality assessment of fused data are proposed. The
algorithms are implemented as part of the ODCleanStore framework and evaluated
on real Linked Open Data.
</summary>
    <author>
      <name>Jan Michelfeit</name>
    </author>
    <author>
      <name>Tomáš Knap</name>
    </author>
    <author>
      <name>Martin Nečaský</name>
    </author>
    <link href="http://arxiv.org/abs/1410.7990v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.7990v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.3622v1</id>
    <updated>2014-11-13T17:30:03Z</updated>
    <published>2014-11-13T17:30:03Z</published>
    <title>Handling owl:sameAs via Rewriting</title>
    <summary>  Rewriting is widely used to optimise owl:sameAs reasoning in materialisation
based OWL 2 RL systems. We investigate issues related to both the correctness
and efficiency of rewriting, and present an algorithm that guarantees
correctness, improves efficiency, and can be effectively parallelised. Our
evaluation shows that our approach can reduce reasoning times on practical data
sets by orders of magnitude.
</summary>
    <author>
      <name>Boris Motik</name>
    </author>
    <author>
      <name>Yavor Nenov</name>
    </author>
    <author>
      <name>Robert Piro</name>
    </author>
    <author>
      <name>Ian Horrocks</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is the technical report supporting the AAAI 2015 Conference
  submission with the same title</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.3622v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.3622v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.5014v1</id>
    <updated>2014-11-18T14:19:28Z</updated>
    <published>2014-11-18T14:19:28Z</published>
    <title>Music Data Analysis: A State-of-the-art Survey</title>
    <summary>  Music accounts for a significant chunk of interest among various online
activities. This is reflected by wide array of alternatives offered in music
related web/mobile apps, information portals, featuring millions of artists,
songs and events attracting user activity at similar scale. Availability of
large scale structured and unstructured data has attracted similar level of
attention by data science community. This paper attempts to offer current
state-of-the-art in music related analysis. Various approaches involving
machine learning, information theory, social network analysis, semantic web and
linked open data are represented in the form of taxonomy along with data
sources and use cases addressed by the research community.
</summary>
    <author>
      <name>Shubhanshu Gupta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.5014v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.5014v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="97M80" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5; J.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.6562v1</id>
    <updated>2014-11-12T23:50:43Z</updated>
    <published>2014-11-12T23:50:43Z</published>
    <title>Evaluating the Crowd with Confidence</title>
    <summary>  Worker quality control is a crucial aspect of crowdsourcing systems;
typically occupying a large fraction of the time and money invested on
crowdsourcing. In this work, we devise techniques to generate confidence
intervals for worker error rate estimates, thereby enabling a better evaluation
of worker quality. We show that our techniques generate correct confidence
intervals on a range of real-world datasets, and demonstrate wide applicability
by using them to evict poorly performing workers, and provide confidence
intervals on the accuracy of the answers.
</summary>
    <author>
      <name>Manas Joglekar</name>
    </author>
    <author>
      <name>Hector Garcia-Molina</name>
    </author>
    <author>
      <name>Aditya Parameswaran</name>
    </author>
    <link href="http://arxiv.org/abs/1411.6562v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.6562v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.05546v1</id>
    <updated>2015-01-22T15:58:42Z</updated>
    <published>2015-01-22T15:58:42Z</published>
    <title>Using a Big Data Database to Identify Pathogens in Protein Data Space</title>
    <summary>  Current metagenomic analysis algorithms require significant computing
resources, can report excessive false positives (type I errors), may miss
organisms (type II errors / false negatives), or scale poorly on large
datasets. This paper explores using big data database technologies to
characterize very large metagenomic DNA sequences in protein space, with the
ultimate goal of rapid pathogen identification in patient samples. Our approach
uses the abilities of a big data databases to hold large sparse associative
array representations of genetic data to extract statistical patterns about the
data that can be used in a variety of ways to improve identification
algorithms.
</summary>
    <author>
      <name>Ashley Mae Conard</name>
    </author>
    <author>
      <name>Stephanie Dodson</name>
    </author>
    <author>
      <name>Jeremy Kepner</name>
    </author>
    <author>
      <name>Darrell Ricke</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1501.05546v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.05546v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.06964v1</id>
    <updated>2015-01-27T11:03:12Z</updated>
    <published>2015-01-27T11:03:12Z</published>
    <title>Learning Analytics: A Survey</title>
    <summary>  Learning analytics is a research topic that is gaining increasing popularity
in recent time. It analyzes the learning data available in order to make aware
or improvise the process itself and/or the outcome such as student performance.
In this survey paper, we look at the recent research work that has been
conducted around learning analytics, framework and integrated models, and
application of various models and data mining techniques to identify students
at risk and to predict student performance.
</summary>
    <author>
      <name>Usha Keshavamurthy</name>
    </author>
    <author>
      <name>H. S. Guruprasad</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.14445/22312803/IJCTT-V18P155</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.14445/22312803/IJCTT-V18P155" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Trends and Technology (IJCTT)
  Volume 18 Number 6 Dec 2014 Page 260 - 264</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1501.06964v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.06964v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.05947v2</id>
    <updated>2015-05-12T22:47:57Z</updated>
    <published>2015-02-20T17:41:40Z</published>
    <title>Functorial Data Migration: From Theory to Practice</title>
    <summary>  In this paper we describe a functorial data migration scenario about the
manufacturing service capability of a distributed supply chain. The scenario is
a category-theoretic analog of an OWL ontology-based semantic enrichment
scenario developed at the National Institute of Standards and Technology
(NIST). The scenario is presented using, and is included with, the open-source
FQL tool, available for download at categoricaldata.net/fql.html.
</summary>
    <author>
      <name>Ryan Wisnesky</name>
    </author>
    <author>
      <name>David I. Spivak</name>
    </author>
    <author>
      <name>Patrick Schultz</name>
    </author>
    <author>
      <name>Eswaran Subrahmanian</name>
    </author>
    <link href="http://arxiv.org/abs/1502.05947v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.05947v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.00245v1</id>
    <updated>2015-03-01T09:53:15Z</updated>
    <published>2015-03-01T09:53:15Z</published>
    <title>Novel Metaknowledge-based Processing Technique for Multimedia Big Data
  clustering challenges</title>
    <summary>  Past research has challenged us with the task of showing relational patterns
between text-based data and then clustering for predictive analysis using Golay
Code technique. We focus on a novel approach to extract metaknowledge in
multimedia datasets. Our collaboration has been an on-going task of studying
the relational patterns between datapoints based on metafeatures extracted from
metaknowledge in multimedia datasets. Those selected are significant to suit
the mining technique we applied, Golay Code algorithm. In this research paper
we summarize findings in optimization of metaknowledge representation for
23-bit representation of structured and unstructured multimedia data in order
to
</summary>
    <author>
      <name>Nima Bari</name>
    </author>
    <author>
      <name>Roman Vichr</name>
    </author>
    <author>
      <name>Kamran Kowsari</name>
    </author>
    <author>
      <name>Simon Y. Berkovich</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/BigMM.2015.78</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/BigMM.2015.78" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Multimedia Big Data (BigMM 2015)</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.00245v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.00245v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.00301v1</id>
    <updated>2015-03-01T16:40:56Z</updated>
    <published>2015-03-01T16:40:56Z</published>
    <title>On Defining SPARQL with Boolean Tensor Algebra</title>
    <summary>  The Resource Description Framework (RDF) represents information as
subject-predicate-object triples. These triples are commonly interpreted as a
directed labelled graph. We propose an alternative approach, interpreting the
data as a 3-way Boolean tensor. We show how SPARQL queries - the standard
queries for RDF - can be expressed as elementary operations in Boolean algebra,
giving us a complete re-interpretation of RDF and SPARQL. We show how the
Boolean tensor interpretation allows for new optimizations and analyses of the
complexity of SPARQL queries. For example, estimating the size of the results
for different join queries becomes much simpler.
</summary>
    <author>
      <name>Saskia Metzler</name>
    </author>
    <author>
      <name>Pauli Miettinen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2740908.2742738</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2740908.2742738" rel="related"/>
    <link href="http://arxiv.org/abs/1503.00301v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.00301v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.00503v2</id>
    <updated>2015-03-22T20:30:36Z</updated>
    <published>2015-03-02T12:52:53Z</published>
    <title>A Next-Generation Data Language Proposal</title>
    <summary>  This paper attempts to explain consequences of the relational calculus not
allowing relations to be domains of relations, and to suggest a solution for
the issue. On the example of SQL we describe the consequent problem of the
multitude of different representations for relations; analyze in detail the
disadvantages of the notions "TABLE" and "FOREIGN KEY"; and propose a complex
solution which includes brand new data language, abandonment of tables as a
representation for relations, and relatively small yet very significant
alteration of the data storage concept, called "multitable index".
</summary>
    <author>
      <name>Eugene Panferov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.00503v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.00503v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.00650v1</id>
    <updated>2015-03-02T18:14:20Z</updated>
    <published>2015-03-02T18:14:20Z</published>
    <title>Consistent Answers of Conjunctive Queries on Graphs</title>
    <summary>  During the past decade, there has been an extensive investigation of the
computational complexity of the consistent answers of Boolean conjunctive
queries under primary key constraints. Much of this investigation has focused
on self-join-free Boolean conjunctive queries. In this paper, we study the
consistent answers of Boolean conjunctive queries involving a single binary
relation, i.e., we consider arbitrary Boolean conjunctive queries on directed
graphs. In the presence of a single key constraint, we show that for each such
Boolean conjunctive query, either the problem of computing its consistent
answers is expressible in first-order logic, or it is polynomial-time solvable,
but not expressible in first-order logic.
</summary>
    <author>
      <name>Foto N. Afrati</name>
    </author>
    <author>
      <name>Phokion G. Kolaitis</name>
    </author>
    <author>
      <name>Angelos Vasilakopoulos</name>
    </author>
    <link href="http://arxiv.org/abs/1503.00650v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.00650v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.02940v1</id>
    <updated>2015-03-10T14:57:26Z</updated>
    <published>2015-03-10T14:57:26Z</published>
    <title>Efficient Query Processing for SPARQL Federations with Replicated
  Fragments</title>
    <summary>  Low reliability and availability of public SPARQL endpoints prevent
real-world applications from exploiting all the potential of these querying
infras-tructures. Fragmenting data on servers can improve data availability but
degrades performance. Replicating fragments can offer new tradeoff between
performance and availability. We propose FEDRA, a framework for querying Linked
Data that takes advantage of client-side data replication, and performs a
source selection algorithm that aims to reduce the number of selected public
SPARQL endpoints, execution time, and intermediate results. FEDRA has been
implemented on the state-of-the-art query engines ANAPSID and FedX, and
empirically evaluated on a variety of real-world datasets.
</summary>
    <author>
      <name>Gabriela Montoya</name>
    </author>
    <author>
      <name>Hala Skaf-Molli</name>
    </author>
    <author>
      <name>Pascal Molli</name>
    </author>
    <author>
      <name>Maria-Esther Vidal</name>
    </author>
    <link href="http://arxiv.org/abs/1503.02940v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.02940v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.03208v1</id>
    <updated>2015-03-11T08:13:51Z</updated>
    <published>2015-03-11T08:13:51Z</published>
    <title>Fraudulent Electronic transaction detection using KDA Model</title>
    <summary>  Clustering analysis and Datamining methodologies were applied to the problem
of identifying illegal and fraud transactions. The researchers independently
developed model and software using data provided by a bank and using Rapidminer
modeling tool. The research objectives are to propose dynamic model and
mechanism to cover fraud detection system limitations. KDA model as proposed
model can detect 68.75% of fraudulent transactions with online dynamic modeling
and 81.25% in offline mode and the Fraud Detection System &amp; Decision Support
System. Software propose a good supporting procedure to detect fraudulent
transaction dynamically.
</summary>
    <author>
      <name>M. Vadoodparast</name>
    </author>
    <author>
      <name>A. Razak Hamdan</name>
    </author>
    <author>
      <name> Hafiz</name>
    </author>
    <link href="http://arxiv.org/abs/1503.03208v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.03208v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.04385v1</id>
    <updated>2015-03-15T04:09:26Z</updated>
    <published>2015-03-15T04:09:26Z</published>
    <title>Design and Implementation of Database Independent Auto Sequence Numbers</title>
    <summary>  Developers across the world use autonumber or auto sequences field of the
backend databases for developing both the desktop and web based data centric
applications which is easier to use at the development and deployment purpose
but can create a lot of problems under varied situations. This paper examines
how a database independent autonumber could be developed and reused solving all
the problems as well as providing the same degree of easy to use features of
autonumber offered by modern Relational Database Systems.
</summary>
    <author>
      <name>Kisor Ray</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.14445/22312803/IJCTT-V20P111</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.14445/22312803/IJCTT-V20P111" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">03 pages, 02 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Trends and Technology,Volume-20
  Number-2,2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1503.04385v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.04385v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.8.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.05157v1</id>
    <updated>2015-03-17T18:39:22Z</updated>
    <published>2015-03-17T18:39:22Z</published>
    <title>Quality Assessment of Linked Datasets using Probabilistic Approximation</title>
    <summary>  With the increasing application of Linked Open Data, assessing the quality of
datasets by computing quality metrics becomes an issue of crucial importance.
For large and evolving datasets, an exact, deterministic computation of the
quality metrics is too time consuming or expensive. We employ probabilistic
techniques such as Reservoir Sampling, Bloom Filters and Clustering Coefficient
estimation for implementing a broad set of data quality metrics in an
approximate but sufficiently accurate way. Our implementation is integrated in
the comprehensive data quality assessment framework Luzzu. We evaluated its
performance and accuracy on Linked Open Datasets of broad relevance.
</summary>
    <author>
      <name>Jeremy Debattista</name>
    </author>
    <author>
      <name>Santiago Londoño</name>
    </author>
    <author>
      <name>Christoph Lange</name>
    </author>
    <author>
      <name>Sören Auer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 2 figures, To appear in ESWC 2015 proceedings</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.05157v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.05157v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.06548v1</id>
    <updated>2015-03-23T08:15:46Z</updated>
    <published>2015-03-23T08:15:46Z</published>
    <title>Using MongoDB for Social Networking Website</title>
    <summary>  Social media is a biggest successful buzzword used in the recent time. Its
success opened various opportunities for the developers. Developing any
application requires storage of large data into databases. Many databases are
available for the developers, Choosing the right one make development easier.
MongoDB is a cross platform document oriented, schema-less database eschewed
the traditional table based relational database structure in favor of JSON like
documents. This article discusses various pros and cons encountered with the
use of the MongoDB so that developers would be helped while choosing it wisely.
</summary>
    <author>
      <name>Sumitkumar Kanoje</name>
    </author>
    <author>
      <name>Varsha Powar</name>
    </author>
    <author>
      <name>Debajyoti Mukhopadhyay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.06548v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.06548v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.08636v1</id>
    <updated>2015-03-30T11:00:35Z</updated>
    <published>2015-03-30T11:00:35Z</published>
    <title>Design &amp; Implementation Approach for Error Free Clinical Data Repository
  for the Medical Practitioners</title>
    <summary>  The modern treatment of any disease is heavily dependent on the medical
diagnosis. Clinical data obtained through the diagnostics tests need to be
collected and entered into the computer database in order to make a clinical
data repository. In most of the cases, manual entry is an absolute necessity.
However, manual entry can cause errors also, leading to wrong diagnosis. This
paper explains how data could be entered free of error to reduce the chances of
wrong diagnosis by designing and implementation of a simple database driven
application.
</summary>
    <author>
      <name>Kisor Ray</name>
    </author>
    <author>
      <name>Santanu Ghosh</name>
    </author>
    <author>
      <name>Mridul Das</name>
    </author>
    <author>
      <name>Bhaswati Ray</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.14445/22312803/IJCTT-V21P113</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.14445/22312803/IJCTT-V21P113" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">04 pages, 04 Figures, International Journal of Computer Trends and
  Technology, Volume-21 Number-2,2015, ISSN 2231-2803</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.08636v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.08636v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.4.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.03247v1</id>
    <updated>2015-04-13T16:29:10Z</updated>
    <published>2015-04-13T16:29:10Z</published>
    <title>Handling Skew in Multiway Joins in Parallel Processing</title>
    <summary>  Handling skew is one of the major challenges in query processing. In
distributed computational environments such as MapReduce, uneven distribution
of the data to the servers is not desired. One of the dominant measures that we
want to optimize in distributed environments is communication cost. In a
MapReduce job this is the amount of data that is transferred from the mappers
to the reducers. In this paper we will introduce a novel technique for handling
skew when we want to compute a multiway join in one MapReduce round with
minimum communication cost. This technique is actually an adaptation of the
Shares algorithm [Afrati et. al, TKDE 2011].
</summary>
    <author>
      <name>Foto N. Afrati</name>
    </author>
    <author>
      <name>Jeffrey D. Ullman</name>
    </author>
    <author>
      <name>Angelos Vasilakopoulos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.03247v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.03247v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.03386v1</id>
    <updated>2015-04-13T23:01:58Z</updated>
    <published>2015-04-13T23:01:58Z</published>
    <title>Tractable Query Answering and Optimization for Extensions of
  Weakly-Sticky Datalog+-</title>
    <summary>  We consider a semantic class, weakly-chase-sticky (WChS), and a syntactic
subclass, jointly-weakly-sticky (JWS), of Datalog+- programs. Both extend that
of weakly-sticky (WS) programs, which appear in our applications to data
quality. For WChS programs we propose a practical, polynomial-time query
answering algorithm (QAA). We establish that the two classes are closed under
magic-sets rewritings. As a consequence, QAA can be applied to the optimized
programs. QAA takes as inputs the program (including the query) and semantic
information about the "finiteness" of predicate positions. For the syntactic
subclasses JWS and WS of WChS, this additional information is computable.
</summary>
    <author>
      <name>Mostafa Milani</name>
    </author>
    <author>
      <name>Leopoldo Bertossi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Proc. Alberto Mendelzon WS on Foundations of Data
  Management (AMW15)</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.03386v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.03386v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.04031v1</id>
    <updated>2015-04-15T20:06:33Z</updated>
    <published>2015-04-15T20:06:33Z</published>
    <title>Mining Semi-structured Data</title>
    <summary>  The need for discovering knowledge from XML documents according to both
structure and content features has become challenging, due to the increase in
application contexts for which handling both structure and content information
in XML data is essential. So, the challenge is to find an hierarchical
structure which ensure a combination of data levels and their representative
structures. In this work, we will be based on the Formal Concept Analysis-based
views to index and query both content and structure. We evaluate given
structure in a querying process which allows the searching of user query
answers.
</summary>
    <author>
      <name>Olfa Arfaoui</name>
    </author>
    <author>
      <name>Minyar Sassi Hidri</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The 5th International Conference on Web and Information
  Technologies (ICWIT), pp. 51-60, 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1504.04031v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.04031v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.07597v1</id>
    <updated>2015-04-27T11:53:01Z</updated>
    <published>2015-04-27T11:53:01Z</published>
    <title>Duplicate Detection with Efficient Language Models for Automatic
  Bibliographic Heterogeneous Data Integration</title>
    <summary>  We present a new method to detect duplicates used to merge different
bibliographic record corpora with the help of lexical and social information.
As we show, a trivial key is not available to delete useless documents. Merging
heteregeneous document databases to get a maximum of information can be of
interest. In our case we try to build a document corpus about the TOR molecule
so as to extract relationships with other gene components from PubMed and
WebOfScience document databases. Our approach makes key fingerprints based on
n-grams. We made two documents gold standards using this corpus to make an
evaluation. Comparison with other well-known methods in deduplication gives
best scores of recall (95\%) and precision (100\%).
</summary>
    <author>
      <name>Nicolas Turenne</name>
    </author>
    <link href="http://arxiv.org/abs/1504.07597v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.07597v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.00307v1</id>
    <updated>2015-05-31T23:37:58Z</updated>
    <published>2015-05-31T23:37:58Z</published>
    <title>Efficient Iterative Processing in the SciDB Parallel Array Engine</title>
    <summary>  Many scientific data-intensive applications perform iterative computations on
array data. There exist multiple engines specialized for array processing.
These engines efficiently support various types of operations, but none
includes native support for iterative processing. In this paper, we develop a
model for iterative array computations and a series of optimizations. We
evaluate the benefits of an optimized, native support for iterative array
processing on the SciDB engine and real workloads from the astronomy domain.
</summary>
    <author>
      <name>Emad Soroush</name>
    </author>
    <author>
      <name>Magdalena Balazinska</name>
    </author>
    <author>
      <name>Simon Krughoff</name>
    </author>
    <author>
      <name>Andrew Connolly</name>
    </author>
    <link href="http://arxiv.org/abs/1506.00307v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.00307v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.05158v1</id>
    <updated>2015-06-16T21:54:12Z</updated>
    <published>2015-06-16T21:54:12Z</published>
    <title>An Entropy Maximizing Geohash for Distributed Spatiotemporal Database
  Indexing</title>
    <summary>  We present a modification of the standard geohash algorithm based on maximum
entropy encoding in which the data volume is approximately constant for a given
hash prefix length. Distributed spatiotemporal databases, which typically
require interleaving spatial and temporal elements into a single key, reap
large benefits from a balanced geohash by creating a consistent ratio between
spatial and temporal precision even across areas of varying data density. This
property is also useful for indexing purely spatial datasets, where the load
distribution of large range scans is an important aspect of query performance.
We apply our algorithm to data generated proportional to population as given by
census block population counts provided from the US Census Bureau.
</summary>
    <author>
      <name>Taylor Arnold</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.05158v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.05158v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.04180v1</id>
    <updated>2015-07-15T11:59:07Z</updated>
    <published>2015-07-15T11:59:07Z</published>
    <title>Wikidata through the Eyes of DBpedia</title>
    <summary>  DBpedia is one of the first and most prominent nodes of the Linked Open Data
cloud. It provides structured data for more than 100 Wikipedia language
editions as well as Wikimedia Commons, has a mature ontology and a stable and
thorough Linked Data publishing lifecycle. Wikidata, on the other hand, has
recently emerged as a user curated source for structured information which is
included in Wikipedia. In this paper, we present how Wikidata is incorporated
in the DBpedia ecosystem. Enriching DBpedia with structured information from
Wikidata provides added value for a number of usage scenarios. We outline those
scenarios and describe the structure and conversion process of the
DBpediaWikidata dataset.
</summary>
    <author>
      <name>Ali Ismayilov</name>
    </author>
    <author>
      <name>Dimitris Kontokostas</name>
    </author>
    <author>
      <name>Sören Auer</name>
    </author>
    <author>
      <name>Jens Lehmann</name>
    </author>
    <author>
      <name>Sebastian Hellmann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.04180v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.04180v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.04955v1</id>
    <updated>2015-07-17T13:04:32Z</updated>
    <published>2015-07-17T13:04:32Z</published>
    <title>Structurally Tractable Uncertain Data</title>
    <summary>  Many data management applications must deal with data which is uncertain,
incomplete, or noisy. However, on existing uncertain data representations, we
cannot tractably perform the important query evaluation tasks of determining
query possibility, certainty, or probability: these problems are hard on
arbitrary uncertain input instances. We thus ask whether we could restrict the
structure of uncertain data so as to guarantee the tractability of exact query
evaluation. We present our tractability results for tree and tree-like
uncertain data, and a vision for probabilistic rule reasoning. We also study
uncertainty about order, proposing a suitable representation, and study
uncertain data conditioned by additional observations.
</summary>
    <author>
      <name>Antoine Amarilli</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2744680.2744690</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2744680.2744690" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 1 figure, 1 table. To appear in SIGMOD/PODS PhD Symposium
  2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.04955v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.04955v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.00703v1</id>
    <updated>2015-08-04T08:42:41Z</updated>
    <published>2015-08-04T08:42:41Z</published>
    <title>Parameter Database : Data-centric Synchronization for Scalable Machine
  Learning</title>
    <summary>  We propose a new data-centric synchronization framework for carrying out of
machine learning (ML) tasks in a distributed environment. Our framework
exploits the iterative nature of ML algorithms and relaxes the application
agnostic bulk synchronization parallel (BSP) paradigm that has previously been
used for distributed machine learning. Data-centric synchronization complements
function-centric synchronization based on using stale updates to increase the
throughput of distributed ML computations. Experiments to validate our
framework suggest that we can attain substantial improvement over BSP while
guaranteeing sequential correctness of ML tasks.
</summary>
    <author>
      <name>Naman Goel</name>
    </author>
    <author>
      <name>Divyakant Agrawal</name>
    </author>
    <author>
      <name>Sanjay Chawla</name>
    </author>
    <author>
      <name>Ahmed Elmagarmid</name>
    </author>
    <link href="http://arxiv.org/abs/1508.00703v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.00703v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.00100v1</id>
    <updated>2015-09-01T00:15:42Z</updated>
    <published>2015-09-01T00:15:42Z</published>
    <title>Decidability of Equivalence of Aggregate Count-Distinct Queries</title>
    <summary>  We address the problem of equivalence of count-distinct aggregate queries,
prove that the problem is decidable, and can be decided in the third level of
Polynomial hierarchy. We introduce the notion of core for conjunctive queries
with comparisons as an extension of the classical notion for relational
queries, and prove that the existence of isomorphism among cores of queries is
a sufficient and necessary condition for equivalence of conjunctive queries
with comparisons similar to the classical relational setting. However, it is
not a necessary condition for equivalence of count-distinct queries. We
introduce a relaxation of this condition based on a new notion, which is a
potentially new query equivalent to the initial query, introduced to capture
the behavior of count-distinct operator.
</summary>
    <author>
      <name>Babak Bagheri Hariri</name>
    </author>
    <author>
      <name>Val Tannen</name>
    </author>
    <link href="http://arxiv.org/abs/1509.00100v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.00100v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.04238v1</id>
    <updated>2015-09-14T18:57:02Z</updated>
    <published>2015-09-14T18:57:02Z</published>
    <title>A Practioner's Guide to Evaluating Entity Resolution Results</title>
    <summary>  Entity resolution (ER) is the task of identifying records belonging to the
same entity (e.g. individual, group) across one or multiple databases.
Ironically, it has multiple names: deduplication and record linkage, among
others. In this paper we survey metrics used to evaluate ER results in order to
iteratively improve performance and guarantee sufficient quality prior to
deployment. Some of these metrics are borrowed from multi-class classification
and clustering domains, though some key differences exist differentiating
entity resolution from general clustering. Menestrina et al. empirically showed
rankings from these metrics often conflict with each other, thus our primary
motivation for studying them. This paper provides practitioners the basic
knowledge to begin evaluating their entity resolution results.
</summary>
    <author>
      <name>Matt Barnes</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical report</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.04238v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.04238v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.03935v1</id>
    <updated>2015-11-12T15:54:49Z</updated>
    <published>2015-11-12T15:54:49Z</published>
    <title>Fast Data Management with Distributed Streaming SQL</title>
    <summary>  To stay competitive in today's data driven economy, enterprises large and
small are turning to stream processing platforms to process high volume, high
velocity, and diverse streams of data (fast data) as they arrive. Low-level
programming models provided by the popular systems of today suffer from lack of
responsiveness to change: enhancements require code changes with attendant
large turn-around times. Even though distributed SQL query engines have been
available for Big Data, we still lack support for SQL-based stream querying
capabilities in distributed stream processing systems. In this white paper, we
identify a set of requirements and propose a standard SQL based streaming query
model for management of what has been referred to as Fast Data.
</summary>
    <author>
      <name>Milinda Pathirage</name>
    </author>
    <author>
      <name>Beth Plale</name>
    </author>
    <link href="http://arxiv.org/abs/1511.03935v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.03935v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.08915v2</id>
    <updated>2016-02-11T16:12:55Z</updated>
    <published>2015-11-28T17:16:55Z</published>
    <title>Column-Oriented Datalog Materialization for Large Knowledge Graphs
  (Extended Technical Report)</title>
    <summary>  The evaluation of Datalog rules over large Knowledge Graphs (KGs) is
essential for many applications. In this paper, we present a new method of
materializing Datalog inferences, which combines a column-based memory layout
with novel optimization methods that avoid redundant inferences at runtime. The
pro-active caching of certain subqueries further increases efficiency. Our
empirical evaluation shows that this approach can often match or even surpass
the performance of state-of-the-art systems, especially under restricted
resources.
</summary>
    <author>
      <name>Jacopo Urbani</name>
    </author>
    <author>
      <name>Ceriel Jacobs</name>
    </author>
    <author>
      <name>Markus Krötzsch</name>
    </author>
    <link href="http://arxiv.org/abs/1511.08915v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.08915v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.3; H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.09059v1</id>
    <updated>2015-11-29T18:46:06Z</updated>
    <published>2015-11-29T18:46:06Z</published>
    <title>Analysis Traceability and Provenance for HEP</title>
    <summary>  This paper presents the use of the CRISTAL software in the N4U project.
CRISTAL was used to create a set of provenance aware analysis tools for the
Neuroscience domain. This paper advocates that the approach taken in N4U to
build the analysis suite is sufficiently generic to be able to be applied to
the HEP domain. A mapping to the PROV model for provenance interoperability is
also presented and how this can be applied to the HEP domain for the
interoperability of HEP analyses.
</summary>
    <author>
      <name>Jetendr Shamdasani</name>
    </author>
    <author>
      <name>Richard McClatchey</name>
    </author>
    <author>
      <name>Andrew Branson</name>
    </author>
    <author>
      <name>Zsolt Kovacs</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/1742-6596/664/3/032028</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/1742-6596/664/3/032028" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pagesd, 4 figures. Presented at 21st Int Conf on Computing in High
  Energy and Nuclear Physics (CHEP15). Okinawa, Japan. April 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.09059v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.09059v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.00196v2</id>
    <updated>2015-12-03T12:06:50Z</updated>
    <published>2015-12-01T09:44:05Z</published>
    <title>SQL Queries for Declarative Process Mining on Event Logs of Relational
  Databases</title>
    <summary>  Flexible business processes can often be modelled more easily using a
declarative rather than a procedural modelling approach. Process mining aims at
automating the discovery of business process models. Existing declarative
process mining approaches either suffer performance issues with real-life event
logs or limit their expressiveness to a specific set of constaint types.
Lately, with RelationalXES a relational database architecture for storing event
log data has been introduced. In this technical report, we introduce a mining
approach that directly works on relational event data by querying the log with
conventional SQL. We provide a list of SQL queries for discovering a set of
commonly used and mined process constraints.
</summary>
    <author>
      <name>Stefan Schönig</name>
    </author>
    <link href="http://arxiv.org/abs/1512.00196v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.00196v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.01681v1</id>
    <updated>2015-12-05T15:54:19Z</updated>
    <published>2015-12-05T15:54:19Z</published>
    <title>Red Spider Meets a Rainworm: Conjunctive Query Finite Determinacy Is
  Undecidable</title>
    <summary>  We solve a well known and long-standing open problem in database theory,
proving that Conjunctive Query Finite Determinacy Problem is undecidable. The
technique we use builds on the top of our Red Spider method which we developed
in our paper [GM15] to show undecidability of the same problem in the
"unrestricted case" -- when database instances are allowed to be infinite. We
also show a specific instance $Q_0$, ${\cal Q}= \{Q_1, Q_2, \ldots Q_k\}$ such
that the set $\cal Q$ of CQs does not determine CQ $Q_0$ but finitely
determines it. Finally, we claim that while $Q_0$ is finitely determined by
$\cal Q$, there is no FO-rewriting of $Q_0$, with respect to $\cal Q$, and we
outline a proof of this claim
</summary>
    <author>
      <name>Tomasz Gogacz</name>
    </author>
    <author>
      <name>Jerzy Marcinkowski</name>
    </author>
    <link href="http://arxiv.org/abs/1512.01681v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.01681v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.01808v1</id>
    <updated>2015-12-06T17:15:38Z</updated>
    <published>2015-12-06T17:15:38Z</published>
    <title>Entropy bounds for conjunctive queries with functional dependencies</title>
    <summary>  We study the problem of finding the worst-case bound for the size of the
result $Q(\mathbb{ D})$ of a fixed conjunctive query $Q$ applied to a database
$\mathbb{ D}$ satisfying given functional dependencies. We provide a precise
characterization of this bound in terms of entropy vectors, and in terms of
finite groups. In particular, we show that an upper bound provided by Gottlob,
Lee, Valiant and Valiant is tight, answering a question from their paper. Our
result generalizes the bound due to Atserias, Grohe and Marx, who consider the
case without functional dependencies. Our result shows that the problem of
computing the worst-case size bound, in the general case, is closely related to
difficult problems from information theory.
</summary>
    <author>
      <name>Tomasz Gogacz</name>
    </author>
    <author>
      <name>Szymon Toruńczyk</name>
    </author>
    <link href="http://arxiv.org/abs/1512.01808v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.01808v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.06246v1</id>
    <updated>2015-12-19T13:18:24Z</updated>
    <published>2015-12-19T13:18:24Z</published>
    <title>Parallel-Correctness and Containment for Conjunctive Queries with Union
  and Negation</title>
    <summary>  Single-round multiway join algorithms first reshuffle data over many servers
and then evaluate the query at hand in a parallel and communication-free way. A
key question is whether a given distribution policy for the reshuffle is
adequate for computing a given query, also referred to as parallel-correctness.
This paper extends the study of the complexity of parallel-correctness and its
constituents, parallel-soundness and parallel-completeness, to unions of
conjunctive queries with and without negation. As a by-product it is shown that
the containment problem for conjunctive queries with negation is
coNEXPTIME-complete.
</summary>
    <author>
      <name>Gaetano Geck</name>
    </author>
    <author>
      <name>Bas Ketsman</name>
    </author>
    <author>
      <name>Frank Neven</name>
    </author>
    <author>
      <name>Thomas Schwentick</name>
    </author>
    <link href="http://arxiv.org/abs/1512.06246v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.06246v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.00524v1</id>
    <updated>2015-12-30T22:45:56Z</updated>
    <published>2015-12-30T22:45:56Z</published>
    <title>Ideal Databases</title>
    <summary>  From algebraic geometry perspective database relations are succinctly defined
as Finite Varieties. After establishing basic framework, we give analytic proof
of Heath theorem from Database Dependency theory. Next, we leverage
Algebra/Geometry dictionary and focus on algebraic counterparts of finite
varieties, polynomial ideals. It is well known that intersection and sum of
ideals are lattice operations. We generalize this fact to ideals from different
rings, therefore establishing that algebra of ideals is Relational Lattice. The
final stop is casting the framework into Linear Algebra, and traversing to
Quantum Theory.
</summary>
    <author>
      <name>Vadim Tropashko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.00524v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.00524v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.03240v2</id>
    <updated>2016-04-20T13:42:55Z</updated>
    <published>2016-01-13T13:40:55Z</published>
    <title>Counting Answers to Existential Positive Queries: A Complexity
  Classification</title>
    <summary>  Existential positive formulas form a fragment of first-order logic that
includes and is semantically equivalent to unions of conjunctive queries, one
of the most important and well-studied classes of queries in database theory.
We consider the complexity of counting the number of answers to existential
positive formulas on finite structures and give a trichotomy theorem on query
classes, in the setting of bounded arity. This theorem generalizes and unifies
several known results on the complexity of conjunctive queries and unions of
conjunctive queries.
</summary>
    <author>
      <name>Hubie Chen</name>
    </author>
    <author>
      <name>Stefan Mengel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1501.07195</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.03240v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.03240v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.04980v1</id>
    <updated>2016-01-19T16:24:43Z</updated>
    <published>2016-01-19T16:24:43Z</published>
    <title>Integrity Constraints for General-Purpose Knowledge Bases</title>
    <summary>  Integrity constraints in databases have been studied extensively since the
1980s, and they are considered essential to guarantee database integrity. In
recent years, several authors have studied how the same notion can be adapted
to reasoning frameworks, in such a way that they achieve the purpose of
guaranteeing a system's consistency, but are kept separate from the reasoning
mechanisms.
  In this paper we focus on multi-context systems, a general-purpose framework
for combining heterogeneous reasoning systems, enhancing them with a notion of
integrity constraints that generalizes the corresponding concept in the
database world.
</summary>
    <author>
      <name>Luís Cruz-Filipe</name>
    </author>
    <author>
      <name>Isabel Nunes</name>
    </author>
    <author>
      <name>Peter Schneider-Kamp</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-30024-5_13</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-30024-5_13" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">FoIKS 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.04980v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.04980v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.00503v1</id>
    <updated>2016-02-01T12:49:25Z</updated>
    <published>2016-02-01T12:49:25Z</published>
    <title>GRAD: On Graph Database Modeling</title>
    <summary>  Graph databases have emerged as the fundamental technology underpinning
trendy application domains where traditional databases are not well-equipped to
handle complex graph data. However, current graph databases support basic graph
structures and integrity constraints with no standard algebra. In this paper,
we introduce GRAD, a native and generic graph database model. GRAD goes beyond
traditional graph database models, which support simple graph structures and
constraints. Instead, GRAD presents a complete graph database model supporting
advanced graph structures, a set of well-defined constraints over these
structures and a powerful graph analysis-oriented algebra.
</summary>
    <author>
      <name>Amine Ghrab</name>
    </author>
    <author>
      <name>Oscar Romero</name>
    </author>
    <author>
      <name>Sabri Skhiri</name>
    </author>
    <author>
      <name>Alejandro Vaisman</name>
    </author>
    <author>
      <name>Esteban Zimányi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.00503v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.00503v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.06458v1</id>
    <updated>2016-02-20T20:57:59Z</updated>
    <published>2016-02-20T20:57:59Z</published>
    <title>Causes for Query Answers from Databases, Datalog Abduction and
  View-Updates: The Presence of Integrity Constraints</title>
    <summary>  Causality has been recently introduced in databases, to model, characterize
and possibly compute causes for query results (answers). Connections between
queryanswer causality, consistency-based diagnosis, database repairs (wrt.
integrity constraint violations), abductive diagnosis and the view-update
problem have been established. In this work we further investigate connections
between query-answer causality and abductive diagnosis and the view-update
problem. In this context, we also define and investigate the notion of
query-answer causality in the presence of integrity constraints.
</summary>
    <author>
      <name>Babak Salimi</name>
    </author>
    <author>
      <name>Leopoldo Bertossi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Proceedings Flairs, 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.06458v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.06458v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.07064v1</id>
    <updated>2016-02-23T07:33:02Z</updated>
    <published>2016-02-23T07:33:02Z</published>
    <title>SIFT: An Algorithm for Extracting Structural Information From Taxonomies</title>
    <summary>  In this work we present SIFT, a 3-step algorithm for the analysis of the
structural information represented by means of a taxonomy. The major advantage
of this algorithm is the capability to leverage the information inherent to the
hierarchical structures of taxonomies to infer correspondences which can allow
to merge them in a later step. This method is particular relevant in scenarios
where taxonomy alignment techniques exploiting textual information from
taxonomy nodes cannot operate successfully.
</summary>
    <author>
      <name>Jorge Martinez-Gil</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.07064v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.07064v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.08791v1</id>
    <updated>2016-02-29T00:49:11Z</updated>
    <published>2016-02-29T00:49:11Z</published>
    <title>The BigDAWG Architecture</title>
    <summary>  BigDAWG is a polystore system designed to work on complex problems that
naturally span across different processing or storage engines. BigDAWG provides
an architecture that supports diverse database systems working with different
data models, support for the competing notions of location transparency and
semantic completeness via islands of information and a middleware that provides
a uniform multi-island interface. In this article, we describe the current
architecture of BigDAWG, its application on the MIMIC II medical dataset, and
our plans for the mechanics of cross-system queries. During the presentation,
we will also deliver a brief demonstration of the current version of BigDAWG.
</summary>
    <author>
      <name>Vijay Gadepally</name>
    </author>
    <author>
      <name>Jennie Duggan</name>
    </author>
    <author>
      <name>Aaron Elmore</name>
    </author>
    <author>
      <name>Jeremy Kepner</name>
    </author>
    <author>
      <name>Samuel Madden</name>
    </author>
    <author>
      <name>Tim Mattson</name>
    </author>
    <author>
      <name>Michael Stonebraker</name>
    </author>
    <link href="http://arxiv.org/abs/1602.08791v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.08791v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.06053v4</id>
    <updated>2016-06-06T11:38:47Z</updated>
    <published>2016-03-19T06:37:28Z</published>
    <title>Negation in SPARQL</title>
    <summary>  This paper presents a thorough study of negation in SPARQL. The types of
negation supported in SPARQL are identified and their main features discussed.
Then, we study the expressive power of the corresponding negation operators. At
this point, we identify a core SPARQL algebra which could be used instead of
the W3C SPARQL algebra. Finally, we analyze the negation operators in terms of
their compliance with elementary axioms of set theory.
</summary>
    <author>
      <name>Renzo Angles</name>
    </author>
    <author>
      <name>Claudio Gutierrez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. of the Alberto Mendelzon International Workshop on Foundations
  of Data Management (AMW'2016)</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.06053v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.06053v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.08568v2</id>
    <updated>2016-05-02T13:13:22Z</updated>
    <published>2016-04-28T19:34:43Z</published>
    <title>Towards Temporal Graph Databases</title>
    <summary>  In spite of the extensive literature on graph databases (GDBs), temporal GDBs
have not received too much attention so far. Temporal GBDs can capture, for
example, the evolution of social networks across time, a relevant topic in data
analysis nowadays. In this paper we propose a data model and query language
(denoted TEG-QL) for temporal GDBs, based on the notion of attribute graphs.
This allows a straightforward translation to Neo4J, a well-known GBD. We
present extensive examples of the use of TEG-QL, and comment our
implementation.
</summary>
    <author>
      <name>Alexander Campos</name>
    </author>
    <author>
      <name>Jorge Mozzino</name>
    </author>
    <author>
      <name>Alejandro Vaisman</name>
    </author>
    <link href="http://arxiv.org/abs/1604.08568v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.08568v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07865v2</id>
    <updated>2016-11-05T20:34:28Z</updated>
    <published>2016-05-25T12:59:37Z</published>
    <title>Constructing Data Graphs for Keyword Search</title>
    <summary>  A data graph is a convenient paradigm for supporting keyword search that
takes into account available semantic structure and not just textual relevance.
However, the problem of constructing data graphs that facilitate both
efficiency and effectiveness of the underlying system has hardly been
addressed. A conceptual model for this task is proposed. Principles for
constructing good data graphs are explained. Transformations for generating
data graphs from RDB and XML are developed. The results obtained from these
transformations are analyzed. It is shown that XML is a better starting point
for getting a good data graph.
</summary>
    <author>
      <name>Konstantin Golenberg</name>
    </author>
    <author>
      <name>Yehoshua Sagiv</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-44406-2_33</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-44406-2_33" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of DEXA'16 paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.07865v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07865v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.09753v1</id>
    <updated>2016-05-31T18:20:36Z</updated>
    <published>2016-05-31T18:20:36Z</published>
    <title>PerfEnforce: A Dynamic Scaling Engine for Analytics with Performance
  Guarantees</title>
    <summary>  In this paper, we present PerfEnforce, a scaling engine designed to enable
cloud providers to sell performance levels for data analytics cloud services.
PerfEnforce scales a cluster of virtual machines allocated to a user in a way
that minimizes cost while probabilistically meeting the query runtime
guarantees offered by a service level agreement. With PerfEnforce, we show how
to scale a cluster in a way that minimally disrupts a user's query session. We
further show when to scale the cluster using one of three methods: feedback
control, reinforcement learning, or perceptron learning. We find that
perceptron learning outperforms the other two methods when making cluster
scaling decisions.
</summary>
    <author>
      <name>Jennifer Ortiz</name>
    </author>
    <author>
      <name>Brendan Lee</name>
    </author>
    <author>
      <name>Magdalena Balazinska</name>
    </author>
    <author>
      <name>Joseph L. Hellerstein</name>
    </author>
    <link href="http://arxiv.org/abs/1605.09753v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.09753v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.01742v1</id>
    <updated>2016-06-06T13:48:34Z</updated>
    <published>2016-06-06T13:48:34Z</published>
    <title>Adaptive Distributed Top-k Query Processing</title>
    <summary>  ADiT is an adaptive approach for processing distributed top-$k$ queries over
peer-to-peer networks optimizing both system load and query response time. This
approach considers the size of the peer to peer network, the amount $k$ of
searched objects, the network capabilities of a connected peer, i.e. the
transmission rate, the amount of objects stored on each peer, and the speed of
a peer in processing a local top-$k$ query. In extensive experiments with a
variety of scenarios we could show that ADiT outperforms state of the art
distributed query processing techniques.
</summary>
    <author>
      <name>Claus Dabringer</name>
    </author>
    <author>
      <name>Johann Eder</name>
    </author>
    <link href="http://arxiv.org/abs/1606.01742v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.01742v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02208v1</id>
    <updated>2016-06-07T16:46:53Z</updated>
    <published>2016-06-07T16:46:53Z</published>
    <title>Initialization Errors in Quantum Data Base Recall</title>
    <summary>  This paper analyzes the relationship between initialization error and recall
of a specific memory in the Grover algorithm for quantum database search. It is
shown that the correct memory is obtained with high probability even when the
initial state is far removed from the correct one. The analysis is done by
relating the variance of error in the initial state to the recovery of the
correct memory and the surprising result is obtained that the relationship
between the two is essentially linear.
</summary>
    <author>
      <name>Kalyani Natu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.02208v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02208v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02237v1</id>
    <updated>2016-06-07T18:05:50Z</updated>
    <published>2016-06-07T18:05:50Z</published>
    <title>Concept-Oriented Model: the Functional View</title>
    <summary>  The plethora of existing data models and specific data modeling techniques is
not only confusing but leads to complex, eclectic and inefficient designs of
systems for data management and analytics. The main goal of this paper is to
describe a unified approach to data modeling, called the concept-oriented model
(COM), by using functions as a basis for its formalization. COM tries to answer
the question what is data and to rethink basic assumptions underlying this and
related notions. Its main goal is to unify major existing views on data
(generality), using only a few main notions (simplicity) which are very close
to how data is used in real life (naturalness).
</summary>
    <author>
      <name>Alexandr Savinov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.02237v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02237v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.08657v1</id>
    <updated>2016-06-28T11:33:55Z</updated>
    <published>2016-06-28T11:33:55Z</published>
    <title>RDF Graph Alignment with Bisimulation</title>
    <summary>  We investigate the problem of aligning two RDF databases, an essential
problem in understanding the evolution of ontologies. Our approaches address
three fundamental challenges: 1) the use of "blank" (null) names, 2) ontology
changes in which different names are used to identify the same entity, and 3)
small changes in the data values as well as small changes in the graph
structure of the RDF database. We propose approaches inspired by the classical
notion of graph bisimulation and extend them to capture the natural metrics of
edit distance on the data values and the graph structure. We evaluate our
methods on three evolving curated data sets. Overall, our results show that the
proposed methods perform well and are scalable.
</summary>
    <author>
      <name>Peter Buneman</name>
    </author>
    <author>
      <name>Sławek Staworko</name>
    </author>
    <link href="http://arxiv.org/abs/1606.08657v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.08657v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.00813v1</id>
    <updated>2016-07-04T10:39:58Z</updated>
    <published>2016-07-04T10:39:58Z</published>
    <title>Query Answering with Transitive and Linear-Ordered Data</title>
    <summary>  We consider entailment problems involving powerful constraint languages such
as guarded existential rules, in which additional semantic restrictions are put
on a set of distinguished relations. We consider restricting a relation to be
transitive, restricting a relation to be the transitive closure of another
relation, and restricting a relation to be a linear order. We give some natural
generalizations of guardedness that allow inference to be decidable in each
case, and isolate the complexity of the corresponding decision problems.
Finally we show that slight changes in our conditions lead to undecidability.
</summary>
    <author>
      <name>Antoine Amarilli</name>
    </author>
    <author>
      <name>Michael Benedikt</name>
    </author>
    <author>
      <name>Pierre Bourhis</name>
    </author>
    <author>
      <name>Michael Vanden Boom</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">36 pages. To appear in IJCAI 2016. Extended version with proofs</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.00813v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.00813v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.05538v1</id>
    <updated>2016-07-19T12:15:37Z</updated>
    <published>2016-07-19T12:15:37Z</published>
    <title>Challenges for Efficient Query Evaluation on Structured Probabilistic
  Data</title>
    <summary>  Query answering over probabilistic data is an important task but is generally
intractable. However, a new approach for this problem has recently been
proposed, based on structural decompositions of input databases, following,
e.g., tree decompositions. This paper presents a vision for a database
management system for probabilistic data built following this structural
approach. We review our existing and ongoing work on this topic and highlight
many theoretical and practical challenges that remain to be addressed.
</summary>
    <author>
      <name>Antoine Amarilli</name>
    </author>
    <author>
      <name>Silviu Maniu</name>
    </author>
    <author>
      <name>Mikaël Monet</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 1 figure, 23 references. Accepted for publication at SUM
  2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.05538v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.05538v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.05702v1</id>
    <updated>2016-07-19T19:18:08Z</updated>
    <published>2016-07-19T19:18:08Z</published>
    <title>Integration of Probabilistic Uncertain Information</title>
    <summary>  We study the problem of data integration from sources that contain
probabilistic uncertain information. Data is modeled by possible-worlds with
probability distribution, compactly represented in the probabilistic relation
model. Integration is achieved efficiently using the extended probabilistic
relation model. We study the problem of determining the probability
distribution of the integration result. It has been shown that, in general,
only probability ranges can be determined for the result of integration. In
this paper we concentrate on a subclass of extended probabilistic relations,
those that are obtainable through integration. We show that under intuitive and
reasonable assumptions we can determine the exact probability distribution of
the result of integration.
</summary>
    <author>
      <name>Fereidoon Sadri</name>
    </author>
    <author>
      <name>Gayatri Tallur</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.05702v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.05702v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.06063v1</id>
    <updated>2016-07-20T19:10:48Z</updated>
    <published>2016-07-20T19:10:48Z</published>
    <title>Fragment Allocation Configuration in Distributed Database Systems</title>
    <summary>  In distributed database (DDB) management systems, fragment allocation is one
of the most important components that can directly affect the performance of
DDB. In this research work, we will show that declarative programming
languages, e.g. logic programming languages, can be used to represent different
data fragment allocation techniques. Results indicate that, using declarative
programming language significantly simplifies the representation of fragment
allocation algorithm, thus opens door for any further developments and
optimizations. The under consideration case study also show that our approach
can be extended to be used in different areas of distributed systems.
</summary>
    <author>
      <name>Mohammad Reza Abbasifard</name>
    </author>
    <author>
      <name>Omid Isfahani Alamdari</name>
    </author>
    <link href="http://arxiv.org/abs/1607.06063v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.06063v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.00117v1</id>
    <updated>2016-09-01T05:55:02Z</updated>
    <published>2016-09-01T05:55:02Z</published>
    <title>Group Rotation Type Crowdsourcing</title>
    <summary>  A common workflow to perform a continuous human task stream is to divide
workers into groups, have one group perform the newly-arrived task, and rotate
the groups. We call this type of workflow the group rotation. This paper
addresses the problem of how to manage Group Rotation Type Crowdsourcing, the
group rotation in a crowdsourcing setting. In the group-rotation type
crowdsourcing, we must change the group structure dynamically because workers
come in and leave frequently. This paper proposes an approach to explore a
design space of methods for group restructuring in the group rotation type
crowdsourcing.
</summary>
    <author>
      <name>Katsumi Kumai</name>
    </author>
    <author>
      <name>Yuhki Shiraishi</name>
    </author>
    <author>
      <name>Jianwei Zhang</name>
    </author>
    <author>
      <name>Hiroyuki Kitagawa</name>
    </author>
    <author>
      <name>Atsuyuki Morishima</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 non-reference pages + reference-only page, HCOMP2016, WiP paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.00117v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.00117v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.03346v1</id>
    <updated>2016-09-12T11:16:35Z</updated>
    <published>2016-09-12T11:16:35Z</published>
    <title>A Meaning-oriented Approach to Semantic Data Modeling</title>
    <summary>  Semantic information is often represented as the entities and the
relationships among them with conventional semantic models. This approach is
straightforward but is not suitable for many posteriori requests in semantic
data modeling. In this paper, we propose a meaning-oriented approach to
modeling semantic data and establish a graph-based semantic data model. In this
approach we use the meanings, i.e., the subjective views of the entities and
relationships, to describe the semantic information, and use the semantic
graphs containing the meaning nodes and the meta-meaning relations to specify
the taxonomy and the compound construction of the semantic concepts. We
demonstrate how this meaning-oriented approach can address many important
semantic representation issues, including dynamic specialization and natural
join.
</summary>
    <author>
      <name>Xuhui Li</name>
    </author>
    <link href="http://arxiv.org/abs/1609.03346v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.03346v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.06019v1</id>
    <updated>2016-09-20T05:03:58Z</updated>
    <published>2016-09-20T05:03:58Z</published>
    <title>Active Integrity Constraints for Multi-Context Systems</title>
    <summary>  We introduce a formalism to couple integrity constraints over general-purpose
knowledge bases with actions that can be executed to restore consistency. This
formalism generalizes active integrity constraints over databases. In the more
general setting of multi-context systems, adding repair suggestions to
integrity constraints allows defining simple iterative algorithms to find all
possible grounded repairs - repairs for the global system that follow the
suggestions given by the actions in the individual rules. We apply our
methodology to ontologies, and show that it can express most relevant types of
integrity constraints in this domain.
</summary>
    <author>
      <name>Luís Cruz-Filipe</name>
    </author>
    <author>
      <name>Graça Gaspar</name>
    </author>
    <author>
      <name>Isabel Nunes</name>
    </author>
    <author>
      <name>Peter Schneider-Kamp</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-49004-5_7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-49004-5_7" rel="related"/>
    <link href="http://arxiv.org/abs/1609.06019v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.06019v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.06084v1</id>
    <updated>2016-10-19T16:07:30Z</updated>
    <published>2016-10-19T16:07:30Z</published>
    <title>Portable Ontological Expressions in NoSQL Queries</title>
    <summary>  A significant barrier to the portability of queries across di- verse physical
implementations of large data stores, espe- cially NoSQL data stores, is that
the queries reference the physical storage attributes, such as the table and
column names. In this paper, we describe a technique for embed- ding
ontological expressions called Address Expressions, or A-Expressions, in NoSQL
queries to improve their portability across diverse physical implementations.
We discuss an implementation of such queries over a MongoDB data store of the
Enron email corpus with examples, and conduct a preliminary performance
assessment.
</summary>
    <author>
      <name>Suresh K. Damodaran</name>
    </author>
    <author>
      <name>Pedro A. Colon-Hernandez</name>
    </author>
    <link href="http://arxiv.org/abs/1610.06084v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.06084v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.07649v1</id>
    <updated>2016-10-24T20:52:57Z</updated>
    <published>2016-10-24T20:52:57Z</published>
    <title>ALPINE: Anytime Mining with Definite Guarantees</title>
    <summary>  ALPINE is to our knowledge the first anytime algorithm to mine frequent
itemsets and closed frequent itemsets. It guarantees that all itemsets with
support exceeding the current checkpoint's support have been found before it
proceeds further. Thus, it is very attractive for extremely long mining tasks
with very high dimensional data (for example in genetics) because it can offer
intermediate meaningful and complete results. This ANYTIME feature is the most
important contribution of ALPINE, which is also fast but not necessarily the
fastest algorithm around. Another critical advantage of ALPINE is that it does
not require the apriori decided minimum support value.
</summary>
    <author>
      <name>Qiong Hu</name>
    </author>
    <author>
      <name>Tomasz Imielinski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.07649v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.07649v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.03680v1</id>
    <updated>2016-11-11T12:39:41Z</updated>
    <published>2016-11-11T12:39:41Z</published>
    <title>DB-Nets: on The Marriage of Colored Petri Nets and Relational Databases</title>
    <summary>  The integrated management of business processes and mas- ter data is being
increasingly considered as a fundamental problem, by both the academia and the
industry. In this position paper, we focus on the foundations of the problem,
arguing that contemporary approaches struggle to find a suitable equilibrium
between data- and process-related aspects. We then propose db-nets, a new
formal model that balances such two pillars through the marriage of colored
Petri nets and relational databases. We invite the research community to build
on this model, discussing its potential in modeling, formal verification, and
simulation.
</summary>
    <author>
      <name>Marco Montali</name>
    </author>
    <author>
      <name>Andrey Rivkin</name>
    </author>
    <link href="http://arxiv.org/abs/1611.03680v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.03680v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.04977v1</id>
    <updated>2016-10-16T12:29:38Z</updated>
    <published>2016-10-16T12:29:38Z</published>
    <title>Query Data With Fuzzy Information In Object-Oriented Databases An
  Approach Interval Values</title>
    <summary>  In this paper, we propose methods of handling attributive values of object
classes in object oriented database with fuzzy information and uncertainty
based on quantitatively semantics based hedge algebraic. In this approach we
consider to attributive values (as well as methods) object class is interval
values and the interval values are converted into sub interval in [0, 1]
respectively. That its the fuzziness of the elements in the hedge algebra is
also sub interval in [0,1]. So, we present an algorithm allows the comparison
of two sub interval [0,1] helping the requirements of the query data
</summary>
    <author>
      <name>Doan Van Thang</name>
    </author>
    <author>
      <name>Doan Van Ban</name>
    </author>
    <link href="http://arxiv.org/abs/1611.04977v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.04977v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.06417v1</id>
    <updated>2016-11-19T19:37:41Z</updated>
    <published>2016-11-19T19:37:41Z</published>
    <title>Discover Aggregates Exceptions over Hidden Web Databases</title>
    <summary>  Nowadays, many web databases "hidden" behind their restrictive search
interfaces (e.g., Amazon, eBay) contain rich and valuable information that is
of significant interests to various third parties. Recent studies have
demonstrated the possibility of estimating/tracking certain aggregate queries
over dynamic hidden web databases. Nonetheless, tracking all possible aggregate
query answers to report interesting findings (i.e., exceptions), while still
adhering to the stringent query-count limitations enforced by many hidden web
databases providers, is very challenging. In this paper, we develop a novel
technique for tracking and discovering exceptions (in terms of sudden changes
of aggregates) over dynamic hidden web databases. Extensive real-world
experiments demonstrate the superiority of our proposed algorithms over
baseline solutions.
</summary>
    <author>
      <name>Saad Bin Suhaim</name>
    </author>
    <author>
      <name>Weimo Liu</name>
    </author>
    <author>
      <name>Nan Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1611.06417v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.06417v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.08269v1</id>
    <updated>2016-11-24T17:43:48Z</updated>
    <published>2016-11-24T17:43:48Z</published>
    <title>On measuring performances of C-SPARQL and CQELS</title>
    <summary>  To cope with the massive growth of semantic data streams, several RDF Stream
Processing (RSP) engines have been implemented. The efficiency of their
throughput, latency and memory consumption can be evaluated using available
benchmarks such as LSBench and City- Bench. Nevertheless, these benchmarks lack
an in-depth performance evaluation as some measurement metrics have not been
considered. The main goal of this paper is to analyze the performance of two
popular RSP engines, namely C-SPARQL and CQELS, when varying a set of
performance metrics. More precisely, we evaluate the impact of stream rate,
number of streams and window size on execution time as well as on memory
consumption.
</summary>
    <author>
      <name>Xiangnan Ren</name>
    </author>
    <author>
      <name>Houda Khrouf</name>
    </author>
    <author>
      <name>Zakia Kazi-Aoul</name>
    </author>
    <author>
      <name>Yousra Chabchoub</name>
    </author>
    <author>
      <name>Olivier Curé</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.08269v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.08269v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.09170v1</id>
    <updated>2016-11-28T15:16:42Z</updated>
    <published>2016-11-28T15:16:42Z</published>
    <title>DESP-C++: A Discrete-Event Simulation Package for C++</title>
    <summary>  DESP-C++ is a C++ discrete-event random simulation engine that has been
designed to be fast, very easy to use and expand, and valid. DESP-C++ is based
on the resource view. Its complete architecture is presented in detail, as well
as a short " user manual ". The validity of DESP-C++ is demonstrated by the
simulation of three significant models. In each case, the simulation results
obtained with DESP-C++ match those obtained with a validated simulation
software: QNAP2. The versatility of DESP-C++ is also illustrated this way,
since the modelled systems are very different from each other: a simple
production system, the dining philosopher classical deadlock problem, and a
complex object-oriented database management system.
</summary>
    <author>
      <name>Jérôme Darmont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIMOS</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Software: Practice and Experience, Wiley, 2000, 30 (1), pp.37-60</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1611.09170v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.09170v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.09691v1</id>
    <updated>2016-11-29T16:05:56Z</updated>
    <published>2016-11-29T16:05:56Z</published>
    <title>Data Partitioning View of Mining Big Data</title>
    <summary>  There are two main approximations of mining big data in memory. One is to
partition a big dataset to several subsets, so as to mine each subset in
memory. By this way, global patterns can be obtained by synthesizing all local
patterns discovered from these subsets. Another is the statistical sampling
method. This indicates that data partitioning should be an important strategy
for mining big data. This paper recalls our work on mining big data with a data
partitioning and shows some interesting findings among the local patterns
discovered from subsets of a dataset.
</summary>
    <author>
      <name>Shichao Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1611.09691v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.09691v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.00623v1</id>
    <updated>2016-12-02T10:38:49Z</updated>
    <published>2016-12-02T10:38:49Z</published>
    <title>Density Based Algorithm With Automatic Parameters Generation</title>
    <summary>  The traditional algorithms do not meet the latest multiple requirements
simultaneously for objects. Density-based method is one of the methodologies,
which can detect arbitrary shaped clusters where clusters are defined as dense
regions separated by low density regions. In this paper, we present a new
clustering algorithm to enhance the density-based algorithm DBSCAN. This
enables an automatic parameter generation strategy to create clusters with
different densities and enables noises recognition, and generates arbitrary
shaped clusters. The kdtree is used for increasing the memory efficiency.
Experimental result shows that proposed algorithm is capable of handling
complex objects with good memory efficiency and accuracy.
</summary>
    <author>
      <name>Singh Vijendra</name>
    </author>
    <author>
      <name>Priyanka Trikha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2011 IEEE 3rd International Conference on Machine Learning and
  Computing (ICMLC 2011), Singapore</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.00623v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.00623v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.05786v1</id>
    <updated>2016-12-17T16:08:04Z</updated>
    <published>2016-12-17T16:08:04Z</published>
    <title>Predicting Completeness in Knowledge Bases</title>
    <summary>  Knowledge bases such as Wikidata, DBpedia, or YAGO contain millions of
entities and facts. In some knowledge bases, the correctness of these facts has
been evaluated. However, much less is known about their completeness, i.e., the
proportion of real facts that the knowledge bases cover. In this work, we
investigate different signals to identify the areas where a knowledge base is
complete. We show that we can combine these signals in a rule mining approach,
which allows us to predict where facts may be missing. We also show that
completeness predictions can help other applications such as fact prediction.
</summary>
    <author>
      <name>Luis Galárraga</name>
    </author>
    <author>
      <name>Simon Razniewski</name>
    </author>
    <author>
      <name>Antoine Amarilli</name>
    </author>
    <author>
      <name>Fabian M. Suchanek</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3018661.3018739</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3018661.3018739" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 19 references, 1 figure, 5 tables. Complete version of the
  article accepted at WSDM'17</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.05786v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.05786v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.02221v1</id>
    <updated>2017-01-09T15:53:41Z</updated>
    <published>2017-01-09T15:53:41Z</published>
    <title>JSON: data model, query languages and schema specification</title>
    <summary>  Despite the fact that JSON is currently one of the most popular formats for
exchanging data on the Web, there are very few studies on this topic and there
are no agreement upon theoretical framework for dealing with JSON. There- fore
in this paper we propose a formal data model for JSON documents and, based on
the common features present in available systems using JSON, we define a
lightweight query language allowing us to navigate through JSON documents. We
also introduce a logic capturing the schema proposal for JSON and study the
complexity of basic computational tasks associated with these two formalisms.
</summary>
    <author>
      <name>Pierre Bourhis</name>
    </author>
    <author>
      <name>Juan L. Reutter</name>
    </author>
    <author>
      <name>Fernando Suárez</name>
    </author>
    <author>
      <name>Domagoj Vrgoč</name>
    </author>
    <link href="http://arxiv.org/abs/1701.02221v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.02221v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.05799v1</id>
    <updated>2017-01-19T00:29:31Z</updated>
    <published>2017-01-19T00:29:31Z</published>
    <title>BigDAWG Polystore Release and Demonstration</title>
    <summary>  The Intel Science and Technology Center for Big Data is developing a
reference implementation of a Polystore database. The BigDAWG (Big Data Working
Group) system supports "many sizes" of database engines, multiple programming
languages and complex analytics for a variety of workloads. Our recent efforts
include application of BigDAWG to an ocean metagenomics problem and
containerization of BigDAWG. We intend to release an open source BigDAWG v1.0
in the Spring of 2017. In this article, we will demonstrate a number of
polystore applications developed with oceanographic researchers at MIT and
describe our forthcoming open source release of the BigDAWG system.
</summary>
    <author>
      <name>Kyle OBrien</name>
    </author>
    <author>
      <name>Vijay Gadepally</name>
    </author>
    <author>
      <name>Jennie Duggan</name>
    </author>
    <author>
      <name>Adam Dziedzic</name>
    </author>
    <author>
      <name>Aaron Elmore</name>
    </author>
    <author>
      <name>Jeremy Kepner</name>
    </author>
    <author>
      <name>Samuel Madden</name>
    </author>
    <author>
      <name>Tim Mattson</name>
    </author>
    <author>
      <name>Zuohao She</name>
    </author>
    <author>
      <name>Michael Stonebraker</name>
    </author>
    <link href="http://arxiv.org/abs/1701.05799v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.05799v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.08029v1</id>
    <updated>2017-01-27T12:30:49Z</updated>
    <published>2017-01-27T12:30:49Z</published>
    <title>Index and Materialized View Selection in Data Warehouses</title>
    <summary>  The aim of this article is to present an overview of the major families of
state-of-the-art index and materialized view selection methods, and to discuss
the issues and future trends in data warehouse performance optimization. We
particularly focus on data mining-based heuristics we developed to reduce the
selection problem complexity and target the most pertinent candidate indexes
and materialized views.
</summary>
    <author>
      <name>Kamel Aouiche</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Centre LICEF - TÉLUQ</arxiv:affiliation>
    </author>
    <author>
      <name>Jérôme Darmont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Handbook of Research on Innovations in Database Technologies and
  Applications, II, pp.693-700, 2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1701.08029v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.08029v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.08054v1</id>
    <updated>2017-01-27T14:00:53Z</updated>
    <published>2017-01-27T14:00:53Z</published>
    <title>Indices in XML Databases</title>
    <summary>  With XML becoming a standard for business information representation and
exchange, stor-ing, indexing, and querying XML documents have rapidly become
major issues in database research. In this context, query processing and
optimization are primordial, native-XML data-bases not being mature yet. Data
structures such as indices, which help enhance performances substantially, are
extensively researched, especially since XML data bear numerous specifici-ties
with respect to relational data. In this paper, we survey state-of-the-art XML
indices and discuss the main issues, tradeoffs and future trends in XML
indexing. We also present an in-dex that we specifically designed for the
particular architecture of XML data warehouses.
</summary>
    <author>
      <name>Hadj Mahboubi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Jérôme Darmont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4018/978-1-60566-242-8.ch072</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4018/978-1-60566-242-8.ch072" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Handbook of Research on Innovations in Database Technologies and
  Applications, II, IGI Global, pp.674-681, 2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1701.08054v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.08054v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.08612v1</id>
    <updated>2017-01-30T14:27:07Z</updated>
    <published>2017-01-30T14:27:07Z</published>
    <title>XML Warehousing and OLAP</title>
    <summary>  The aim of this article is to present an overview of the major XML
warehousing approaches from the literature, as well as the existing approaches
for performing OLAP analyses over XML data (which is termed XML-OLAP or XOLAP;
Wang et al., 2005). We also discuss the issues and future trends in this area
and illustrate this topic by presenting the design of a unified, XML data
warehouse architecture and a set of XOLAP operators expressed in an XML
algebra.
</summary>
    <author>
      <name>Hadj Mahboubi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Marouane Hachicha</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Jérôme Darmont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1701.08033</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Encyclopedia of Data Warehousing and Mining, Second Edition, IV,
  IGI Publishing, pp.2109-2116, 2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1701.08612v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.08612v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.09049v1</id>
    <updated>2017-01-31T14:19:18Z</updated>
    <published>2017-01-31T14:19:18Z</published>
    <title>Batch Incremental Shared Nearest Neighbor Density Based Clustering
  Algorithm for Dynamic Datasets</title>
    <summary>  Incremental data mining algorithms process frequent updates to dynamic
datasets efficiently by avoiding redundant computation. Existing incremental
extension to shared nearest neighbor density based clustering (SNND) algorithm
cannot handle deletions to dataset and handles insertions only one point at a
time. We present an incremental algorithm to overcome both these bottlenecks by
efficiently identifying affected parts of clusters while processing updates to
dataset in batch mode. We show effectiveness of our algorithm by performing
experiments on large synthetic as well as real world datasets. Our algorithm is
up to four orders of magnitude faster than SNND and requires up to 60% extra
memory than SNND while providing output identical to SNND.
</summary>
    <author>
      <name>Panthadeep Bhattacharjee</name>
    </author>
    <author>
      <name>Amit Awekar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, Accepted at ECIR 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.09049v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.09049v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.03484v1</id>
    <updated>2017-02-12T03:06:25Z</updated>
    <published>2017-02-12T03:06:25Z</published>
    <title>MapSQ: A MapReduce-based Framework for SPARQL Queries on GPU</title>
    <summary>  In this paper, we present a MapReduce-based framework for evaluating SPARQL
queries on GPU (named MapSQ) to large-scale RDF datesets efficiently by
applying both high performance. Firstly, we develop a MapReduce-based Join
algorithm to handle SPARQL queries in a parallel way. Secondly, we present a
coprocessing strategy to manage the process of evaluating queries where CPU is
used to assigns subqueries and GPU is used to compute the join of subqueries.
Finally, we implement our proposed framework and evaluate our proposal by
comparing with two popular and latest SPARQL query engines gStore and gStoreD
on the LUBM benchmark. The experiments demonstrate that our proposal MapSQ is
highly efficient and effective (up to 50% speedup).
</summary>
    <author>
      <name>Jiaying Feng</name>
    </author>
    <author>
      <name>Xiaowang Zhang</name>
    </author>
    <author>
      <name>Zhiyong Feng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.03484v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.03484v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W10" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.06379v1</id>
    <updated>2017-02-21T13:41:35Z</updated>
    <published>2017-02-21T13:41:35Z</published>
    <title>Probabilistic Complex Event Recognition: A Survey</title>
    <summary>  Complex Event Recognition applications exhibit various types of uncertainty,
ranging from incomplete and erroneous data streams to imperfect complex event
patterns. We review Complex Event Recognition techniques that handle, to some
extent, uncertainty. We examine techniques based on automata, probabilistic
graphical models and first-order logic, which are the most common ones, and
approaches based on Petri Nets and Grammars, which are less frequently used. A
number of limitations are identified with respect to the employed languages,
their probabilistic models and their performance, as compared to the purely
deterministic cases. Based on those limitations, we highlight promising
directions for future work.
</summary>
    <author>
      <name>Elias Alevizos</name>
    </author>
    <author>
      <name>Anastasios Skarlatidis</name>
    </author>
    <author>
      <name>Alexander Artikis</name>
    </author>
    <author>
      <name>George Paliouras</name>
    </author>
    <link href="http://arxiv.org/abs/1702.06379v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.06379v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.03524v2</id>
    <updated>2017-05-04T01:01:48Z</updated>
    <published>2017-03-10T02:48:29Z</published>
    <title>The Ontological Multidimensional Data Model</title>
    <summary>  In this extended abstract we describe, mainly by examples, the main elements
of the Ontological Multidimensional Data Model, which considerably extends a
relational reconstruction of the multidimensional data model proposed by
Hurtado and Mendelzon by means of tuple-generating dependencies,
equality-generating dependencies, and negative constraints as found in
Datalog+-. We briefly mention some good computational properties of the model.
</summary>
    <author>
      <name>Leopoldo Bertossi</name>
    </author>
    <author>
      <name>Mostafa Milani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended abstract. This version with minor revisions and slightly
  extended. To appear in Proc. AMW'17</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.03524v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.03524v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.04206v1</id>
    <updated>2017-03-13T00:32:14Z</updated>
    <published>2017-03-13T00:32:14Z</published>
    <title>Application of Bitcoin Data-Structures &amp; Design Principles to Supply
  Chain Management</title>
    <summary>  Heretofore the concept of "blockchain" has not been precisely defined.
Accordingly the potential useful applications of this technology have been
largely inflated. This work sidesteps the question of what constitutes a
blockchain as such and focuses on the architectural components of the Bitcoin
cryptocurrency, insofar as possible, in isolation. We consider common problems
inherent in the design of effective supply chain management systems. With each
identified problem we propose a solution that utilizes one or more component
aspects of Bitcoin. This culminates in five design principles for increased
efficiency in supply chain management systems through the application of
incentive mechanisms and data structures native to the Bitcoin cryptocurrency
protocol.
</summary>
    <author>
      <name>S. Matthew English</name>
    </author>
    <author>
      <name>Ehsan Nezhadian</name>
    </author>
    <link href="http://arxiv.org/abs/1703.04206v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.04206v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.06348v1</id>
    <updated>2017-03-18T20:31:37Z</updated>
    <published>2017-03-18T20:31:37Z</published>
    <title>Application of Information Centric Networking to NoSQL Databases: the
  Spatio-Temporal use case</title>
    <summary>  This paper explores methodologies, advantages and challenges related to the
use of the Information Centric Network technology for developing NoSQL
distributed databases, which are expected to play a central role in the
forthcoming IoT and BigData era. ICN services make possible to simplify the
development of the database software, improve performance, and provide
data-level access control. We use our findings for devising a NoSQL
spatio-temporal database, named OpenGeoBase, and evaluate its performance with
a real data set related to Intelligent Transport System applications.
</summary>
    <author>
      <name>Andrea Detti</name>
    </author>
    <author>
      <name>Michele Orru</name>
    </author>
    <author>
      <name>Riccardo Paolillo</name>
    </author>
    <author>
      <name>Giulio Rossi</name>
    </author>
    <author>
      <name>Pierpaolo Loreti</name>
    </author>
    <author>
      <name>Lorenzo Bracciale</name>
    </author>
    <author>
      <name>Nicola Blefari Melazzi</name>
    </author>
    <link href="http://arxiv.org/abs/1703.06348v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.06348v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.07371v1</id>
    <updated>2017-03-21T18:06:29Z</updated>
    <published>2017-03-21T18:06:29Z</published>
    <title>Multi Agent Driven Data Mining For Knowledge Discovery in Cloud
  Computing</title>
    <summary>  Today, huge amount of data is available on the web. Now there is a need to
convert that data in knowledge which can be useful for different purposes. This
paper depicts the use of data mining process, OLAP with the combination of
multi agent system to find the knowledge from data in cloud computing. For
this, I am also trying to explain one case study of online shopping of one
Bakery Shop. May be we can increase the sale of items by using the model, which
I am trying to represent.
</summary>
    <author>
      <name>Vishal Jain</name>
    </author>
    <author>
      <name>Mahesh Kumar Madan</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science &amp; Information Technology
  Research Excellence Vol. 2, Issue 1, Jan-Feb 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1703.07371v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.07371v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.07484v1</id>
    <updated>2017-03-22T01:39:00Z</updated>
    <published>2017-03-22T01:39:00Z</published>
    <title>Incremental Maintenance of Regression Models over Joins</title>
    <summary>  This paper introduces a principled incremental view maintenance (IVM)
mechanism for in-database computation described by rings. We exemplify our
approach by introducing the covariance matrix ring that we use for learning
linear regression models over arbitrary equi-join queries.
  Our approach is a higher-order IVM algorithm that exploits the factorized
structure of joins and aggregates to avoid redundant computation and improve
performance. We implemented it in DBToaster, which uses program synthesis to
generate high-performance maintenance code. We experimentally show that it can
outperform first-order and fully recursive higher-order IVM as well as
recomputation by orders of magnitude while using less memory.
</summary>
    <author>
      <name>Milos Nikolic</name>
    </author>
    <author>
      <name>Dan Olteanu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.07484v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.07484v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.08425v1</id>
    <updated>2017-03-24T14:35:08Z</updated>
    <published>2017-03-24T14:35:08Z</published>
    <title>Redynis: Traffic-aware dynamic repartitioning for a distributed
  key-value store</title>
    <summary>  Most modern data stores tend to be distributed, to enable the scaling of the
data across multiple instances of commodity hardware. Although this ensures a
near unlimited potential for storage, the data itself is not always ideally
partitioned, and the cost of a network round-trip may cause a degradation of
end-user experience with respect to response latency. The problem being solved
is bringing the data objects closer to the frequent sources of requests using a
dynamic repartitioning algorithm. This is important if the objective is to
mitigate the overhead of network latency, and especially so if the partitions
are widely geo-distributed. The intention is to bring these features to an
existing distributed key-value store product, Redis.
</summary>
    <author>
      <name>Vineet John</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.13140/RG.2.2.12252.39048</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.13140/RG.2.2.12252.39048" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.08425v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.08425v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.09574v1</id>
    <updated>2017-03-28T13:52:03Z</updated>
    <published>2017-03-28T13:52:03Z</published>
    <title>Stored and Inherited Relations</title>
    <summary>  The universally applied Codd's relational model has two constructs: a stored
relation, with stored attributes only and a view, only with the inherited ones.
In 1992, we have proposed third construct, mixing both types of attributes.
Examples showed the idea attractive. No one followed however. We now revisit
our proposal. We show that a relational database scheme using also our
construct may be more faithful to reality. It may spare the logical navigation
or complex value expressions to queries. It may also avoid auxiliary views,
often necessary in practice at present. Better late than never, existing DBSs
should easily accommodate our proposal, with almost no storage and processing
overhead.
</summary>
    <author>
      <name>Witold Litwin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.09574v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.09574v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.01355v1</id>
    <updated>2017-04-05T10:30:18Z</updated>
    <published>2017-04-05T10:30:18Z</published>
    <title>Posterior Snapshot Isolation</title>
    <summary>  Snapshot Isolation (SI) is a widely adopted concurrency control mechanism in
database systems, which utilizes timestamps to resolve conflicts between
transactions. However, centralized allocation of timestamps is a potential
bottleneck for parallel transaction management. This bottleneck is becoming
increasingly visible with the rapidly growing degree of parallelism of today's
computing platforms. This paper introduces Posterior Snapshot Isolation
(PostSI), an SI mechanism that allows transactions to determine their
timestamps autonomously, without relying on centralized coordination. As such,
PostSI can scale well, rendering it suitable for various multi-core and MPP
platforms. Extensive experiments are conducted to demonstrate its advantage
over existing approaches.
</summary>
    <author>
      <name>Xuan Zhou</name>
    </author>
    <author>
      <name>Xin Zhou</name>
    </author>
    <author>
      <name>Zhengtai Yu</name>
    </author>
    <author>
      <name>Kian-Lee Tan</name>
    </author>
    <link href="http://arxiv.org/abs/1704.01355v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.01355v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.04302v1</id>
    <updated>2017-04-13T23:34:43Z</updated>
    <published>2017-04-13T23:34:43Z</published>
    <title>On a Distributed Approach for Density-based Clustering</title>
    <summary>  Efficient extraction of useful knowledge from these data is still a
challenge, mainly when the data is distributed, heterogeneous and of different
quality depending on its corresponding local infrastructure. To reduce the
overhead cost, most of the existing distributed clustering approaches generate
global models by aggregating local results obtained on each individual node.
The complexity and quality of solutions depend highly on the quality of the
aggregation. In this respect, we proposed for distributed density-based
clustering that both reduces the communication overheads due to the data
exchange and improves the quality of the global models by considering the
shapes of local clusters. From preliminary results we show that this algorithm
is very promising.
</summary>
    <author>
      <name>Nhien-An Le-Khac</name>
    </author>
    <author>
      <name>M-Tahar Kechadi</name>
    </author>
    <link href="http://arxiv.org/abs/1704.04302v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.04302v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.05730v2</id>
    <updated>2017-10-03T07:52:26Z</updated>
    <published>2017-04-19T13:41:50Z</published>
    <title>On Measuring Bias in Online Information</title>
    <summary>  Bias in online information has recently become a pressing issue, with search
engines, social networks and recommendation services being accused of
exhibiting some form of bias. In this vision paper, we make the case for a
systematic approach towards measuring bias. To this end, we discuss formal
measures for quantifying the various types of bias, we outline the system
components necessary for realizing them, and we highlight the related research
challenges and open problems.
</summary>
    <author>
      <name>Evaggelia Pitoura</name>
    </author>
    <author>
      <name>Panayiotis Tsaparas</name>
    </author>
    <author>
      <name>Giorgos Flouris</name>
    </author>
    <author>
      <name>Irini Fundulaki</name>
    </author>
    <author>
      <name>Panagiotis Papadakos</name>
    </author>
    <author>
      <name>Serge Abiteboul</name>
    </author>
    <author>
      <name>Gerhard Weikum</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.05730v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.05730v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.04915v1</id>
    <updated>2017-05-14T04:03:15Z</updated>
    <published>2017-05-14T04:03:15Z</published>
    <title>Discovering Multiple Truths with a Hybrid Model</title>
    <summary>  Many data management applications require integrating information from
multiple sources. The sources may not be accurate and provide erroneous values.
We thus have to identify the true values from conflicting observations made by
the sources. The problem is further complicated when there may exist multiple
truths (e.g., a book written by several authors). In this paper we propose a
model called Hybrid that jointly makes two decisions: how many truths there
are, and what they are. It considers the conflicts between values as important
evidence for ruling out wrong values, while keeps the flexibility of allowing
multiple truths. In this way, Hybrid is able to achieve both high precision and
high recall.
</summary>
    <author>
      <name>Furong Li</name>
    </author>
    <author>
      <name>Xin Luna Dong</name>
    </author>
    <author>
      <name>Anno Langen</name>
    </author>
    <author>
      <name>Yang Li</name>
    </author>
    <link href="http://arxiv.org/abs/1705.04915v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.04915v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.04928v1</id>
    <updated>2017-05-14T08:44:28Z</updated>
    <published>2017-05-14T08:44:28Z</published>
    <title>Big Data: Challenges, Opportunities and Realities</title>
    <summary>  With the advent of Internet of Things (IoT) and Web 2.0 technologies, there
has been a tremendous growth in the amount of data generated. This chapter
emphasizes on the need for big data, technological advancements, tools and
techniques being used to process big data are discussed. Technological
improvements and limitations of existing storage techniques are also presented.
Since, the traditional technologies like Relational Database Management System
(RDBMS) have their own limitations to handle big data, new technologies have
been developed to handle them and to derive useful insights. This chapter
presents an overview of big data analytics, its application, advantages, and
limitations. Few research issues and future directions are presented in this
chapter.
</summary>
    <author>
      <name>Abhay Bhadani</name>
    </author>
    <author>
      <name>Dhanya Jothimani</name>
    </author>
    <link href="http://arxiv.org/abs/1705.04928v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.04928v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.01762v1</id>
    <updated>2017-06-06T13:43:08Z</updated>
    <published>2017-06-06T13:43:08Z</published>
    <title>Specifying Transaction Control to Serialize Concurrent Program
  Executions</title>
    <summary>  We define a programming language independent transaction controller and an
operator which when applied to concurrent programs with shared locations turns
their behavior with respect to some abstract termination criterion into a
transactional behavior. We prove the correctness property that concurrent runs
under the transaction controller are serialisable. We specify the transaction
controller TaCtl and the operator TA in terms of Abstract State Machines. This
makes TaCtl applicable to a wide range of programs and in particular provides
the possibility to use it as a plug-in when specifying concurrent system
components in terms of Abstract State Machines.
</summary>
    <author>
      <name>Egon Börger</name>
    </author>
    <author>
      <name>Klaus-Dieter Schewe</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-662-43652-3_13</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-662-43652-3_13" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 1 figure</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">LNCS, vol. 8477, Springer 2014, pp. 142-157</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1706.01762v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.01762v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P15, 68Q60, 68Q85" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.00721v1</id>
    <updated>2017-07-03T18:24:24Z</updated>
    <published>2017-07-03T18:24:24Z</published>
    <title>Version 0.1 of the BigDAWG Polystore System</title>
    <summary>  A polystore system is a database management system (DBMS) composed of
integrated heterogeneous database engines and multiple programming languages.
By matching data to the storage engine best suited to its needs, complex
analytics run faster and flexible storage choices helps improve data
organization. BigDAWG (Big Data Working Group) is our reference implementation
of a polystore system. In this paper, we describe the current BigDAWG software
release which supports PostgreSQL, Accumulo and SciDB. We describe the overall
architecture, API and initial results of applying BigDAWG to the MIMIC II
medical dataset.
</summary>
    <author>
      <name>Vijay Gadepally</name>
    </author>
    <author>
      <name>Kyle OBrien</name>
    </author>
    <author>
      <name>Adam Dziedzic</name>
    </author>
    <author>
      <name>Aaron Elmore</name>
    </author>
    <author>
      <name>Jeremy Kepner</name>
    </author>
    <author>
      <name>Samuel Madden</name>
    </author>
    <author>
      <name>Tim Mattson</name>
    </author>
    <author>
      <name>Jennie Rogers</name>
    </author>
    <author>
      <name>Zuohao She</name>
    </author>
    <author>
      <name>Michael Stonebraker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to IEEE HPEC 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.00721v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.00721v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.09140v1</id>
    <updated>2017-08-30T07:03:58Z</updated>
    <published>2017-08-30T07:03:58Z</published>
    <title>The Complexity of Computing a Cardinality Repair for Functional
  Dependencies</title>
    <summary>  For a relation that violates a set of functional dependencies, we consider
the task of finding a maximum number of pairwise-consistent tuples, or what is
known as a "cardinality repair." We present a polynomial-time algorithm that,
for certain fixed relation schemas (with functional dependencies), computes a
cardinality repair. Moreover, we prove that on any of the schemas not covered
by the algorithm, finding a cardinality repair is, in fact, an NP-hard problem.
In particular, we establish a dichotomy in the complexity of computing a
cardinality repair, and we present an efficient algorithm to determine whether
a given schema belongs to the positive side or the negative side of the
dichotomy.
</summary>
    <author>
      <name>Ester Livshits</name>
    </author>
    <author>
      <name>Benny Kimelfeld</name>
    </author>
    <link href="http://arxiv.org/abs/1708.09140v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.09140v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.09171v1</id>
    <updated>2017-08-30T08:47:20Z</updated>
    <published>2017-08-30T08:47:20Z</published>
    <title>Enforcing Privacy in Cloud Databases</title>
    <summary>  Outsourcing databases, i.e., resorting to Database-as-a-Service (DBaaS), is
nowadays a popular choice due to the elasticity, availability, scalability and
pay-as-you-go features of cloud computing. However, most data are sensitive to
some extent, and data privacy remains one of the top concerns to DBaaS users,
for obvious legal and competitive reasons.In this paper, we survey the
mechanisms that aim at making databases secure in a cloud environment, and
discuss current pitfalls and related research challenges.
</summary>
    <author>
      <name>Somayeh Sobati Moghadam</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Jérôme Darmont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Gérald Gavin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">19th International Conference on Big Data Analytics and Knowledge
  Discovery (DaWaK 2017), Aug 2017, Lyon, France. Springer, Lecture Notes in
  Computer Science, 10440, pp.53-73, 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1708.09171v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.09171v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.03188v1</id>
    <updated>2017-09-10T22:25:13Z</updated>
    <published>2017-09-10T22:25:13Z</published>
    <title>The Ubiquity of Large Graphs and Surprising Challenges of Graph
  Processing: A User Survey</title>
    <summary>  Graph processing is becoming increasingly prevalent across many application
domains. In spite of this prevalence, there is little research about how graphs
are actually used in practice. We conducted an online survey aimed at
understanding: (i) the types of graphs users have; (ii) the graph computations
users run; (iii) the types of graph software users use; and (iv) the major
challenges users face when processing their graphs. We describe the responses
of the participants to our questions, highlighting common patterns and
challenges. The participants' responses revealed surprising facts about graph
processing in practice, which we hope can guide future research.
</summary>
    <author>
      <name>Siddhartha Sahu</name>
    </author>
    <author>
      <name>Amine Mhedhbi</name>
    </author>
    <author>
      <name>Semih Salihoglu</name>
    </author>
    <author>
      <name>Jimmy Lin</name>
    </author>
    <author>
      <name>M. Tamer Özsu</name>
    </author>
    <link href="http://arxiv.org/abs/1709.03188v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.03188v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.07493v1</id>
    <updated>2017-09-21T18:50:32Z</updated>
    <published>2017-09-21T18:50:32Z</published>
    <title>Big Data Systems Meet Machine Learning Challenges: Towards Big Data
  Science as a Service</title>
    <summary>  Recently, we have been witnessing huge advancements in the scale of data we
routinely generate and collect in pretty much everything we do, as well as our
ability to exploit modern technologies to process, analyze and understand this
data. The intersection of these trends is what is called, nowadays, as Big Data
Science. Cloud computing represents a practical and cost-effective solution for
supporting Big Data storage, processing and for sophisticated analytics
applications. We analyze in details the building blocks of the software stack
for supporting big data science as a commodity service for data scientists. We
provide various insights about the latest ongoing developments and open
challenges in this domain.
</summary>
    <author>
      <name>Radwa Elshawi</name>
    </author>
    <author>
      <name>Sherif Sakr</name>
    </author>
    <link href="http://arxiv.org/abs/1709.07493v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.07493v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.08880v1</id>
    <updated>2017-09-26T08:18:15Z</updated>
    <published>2017-09-26T08:18:15Z</published>
    <title>An enhanced method to compute the similarity between concepts of
  ontology</title>
    <summary>  With the use of ontologies in several domains such as semantic web,
information retrieval, artificial intelligence, the concept of similarity
measuring has become a very important domain of research. Therefore, in the
current paper, we propose our method of similarity measuring which uses the
Dijkstra algorithm to define and compute the shortest path. Then, we use this
one to compute the semantic distance between two concepts defined in the same
hierarchy of ontology. Afterward, we base on this result to compute the
semantic similarity. Finally, we present an experimental comparison between our
method and other methods of similarity measuring.
</summary>
    <author>
      <name>Noreddine Gherabi</name>
    </author>
    <author>
      <name>Abdelhadi Daoui</name>
    </author>
    <author>
      <name>Abderrahim Marzouk</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-64719-7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-64719-7" rel="related"/>
    <link href="http://arxiv.org/abs/1709.08880v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.08880v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.00813v1</id>
    <updated>2017-09-28T20:38:47Z</updated>
    <published>2017-09-28T20:38:47Z</published>
    <title>A Practical Python API for Querying AFLOWLIB</title>
    <summary>  Large databases such as aflowlib.org provide valuable data sources for
discovering material trends through machine learning. Although a REST API and
query language are available, there is a learning curve associated with the
AFLUX language that acts as a barrier for new users. Additionally, the data is
stored using non-standard serialization formats. Here we present a high-level
API that allows immediate access to the aflowlib data using standard python
operators and language features. It provides an easy way to integrate aflowlib
data with other python materials packages such as ase and quippy, and provides
automatic deserialization into numpy arrays and python objects. This package is
available via "pip install aflow".
</summary>
    <author>
      <name>Conred W. Rosenbrock</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 code listings</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.00813v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.00813v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.08978v2</id>
    <updated>2015-12-02T21:30:41Z</updated>
    <published>2015-06-30T07:55:46Z</published>
    <title>Big Data Technology Literature Review</title>
    <summary>  A short overview of various algorithms and technologies that are helpful for
big data storage and manipulation. Includes pointers to papers for further
reading, and, where applicable, pointers to open source projects implementing a
described storage type.
</summary>
    <author>
      <name>Michael Bar-Sinai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.08978v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.08978v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.08524v1</id>
    <updated>2017-07-26T16:21:32Z</updated>
    <published>2017-07-26T16:21:32Z</published>
    <title>The Shape Metric for Clustering Algorithms</title>
    <summary>  We construct a method by which we can calculate the precision with which an
algorithm identifies the shape of a cluster. We present our results for several
well known clustering algorithms and suggest ways to improve performance for
newer algorithms.
</summary>
    <author>
      <name>Clark Alexander</name>
    </author>
    <author>
      <name>Sofya Akhmametyeva</name>
    </author>
    <link href="http://arxiv.org/abs/1707.08524v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.08524v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0106021v1</id>
    <updated>2001-06-11T11:46:14Z</updated>
    <published>2001-06-11T11:46:14Z</published>
    <title>Object-oriented solutions</title>
    <summary>  In this paper are briefly outlined the motivations, mathematical ideas in
use, pre-formalization and assumptions, object-as-functor construction, `soft'
types and concept constructions, case study for concepts based on variable
domains, extracting a computational background, and examples of evaluations.
</summary>
    <author>
      <name>Viacheslav Wolfengagen</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 2-nd International Workshop on Advances in
  Databases and Information Systems, ADBIS'95, Moscow, June 27 --30, 1995, Vol.
  1, pp. 235--246</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0106021v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0106021v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.3.1; F.1; F.4.1; D.1.1; H.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0406058v2</id>
    <updated>2004-07-14T12:47:52Z</updated>
    <published>2004-06-29T11:05:29Z</published>
    <title>Proofs of Zero Knowledge</title>
    <summary>  We present a protocol for verification of ``no such entry'' replies from
databases. We introduce a new cryptographic primitive as the underlying
structure, the keyed hash tree, which is an extension of Merkle's hash tree. We
compare our scheme to Buldas et al.'s Undeniable Attesters and Micali et al.'s
Zero Knowledge Sets.
</summary>
    <author>
      <name>Matthias Bauer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0406058v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0406058v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.2; H.2.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/hep-lat/0003009v1</id>
    <updated>2000-03-14T15:36:30Z</updated>
    <published>2000-03-14T15:36:30Z</published>
    <title>Data storage issues in lattice QCD calculations</title>
    <summary>  I describe some of the data management issues in lattice Quantum
Chromodynamics calculations. I focus on the experience of the UKQCD
collaboration. I describe an attempt to use a relational database to store part
of the data produced by a lattice QCD calculation.
</summary>
    <author>
      <name>Craig McNeile</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Invited talk at: Workshop on Advanced Data Storage / Management
  Techniques for HPC, 23rd - 25th February 2000, Daresbury Laboratory,
  Warrington, UK. 10 pages html</arxiv:comment>
    <link href="http://arxiv.org/abs/hep-lat/0003009v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/hep-lat/0003009v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-lat" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-lat" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0705.3949v1</id>
    <updated>2007-05-27T12:36:58Z</updated>
    <published>2007-05-27T12:36:58Z</published>
    <title>Translating a first-order modal language to relational algebra</title>
    <summary>  This paper is about Kripke structures that are inside a relational database
and queried with a modal language. At first the modal language that is used is
introduced, followed by a definition of the database and relational algebra.
Based on these definitions two things are presented: a mapping from components
of the modal structure to a relational database schema and instance, and a
translation from queries in the modal language to relational algebra queries.
</summary>
    <author>
      <name>Yeb Havinga</name>
    </author>
    <link href="http://arxiv.org/abs/0705.3949v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0705.3949v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.3; I.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0806.1816v1</id>
    <updated>2008-06-11T09:05:21Z</updated>
    <published>2008-06-11T09:05:21Z</published>
    <title>Cardinality heterogeneities in Web service composition: Issues and
  solutions</title>
    <summary>  Data exchanges between Web services engaged in a composition raise several
heterogeneities. In this paper, we address the problem of data cardinality
heterogeneity in a composition. Firstly, we build a theoretical framework to
describe different aspects of Web services that relate to data cardinality, and
secondly, we solve this problem by developing a solution for cardinality
mediation based on constraint logic programming.
</summary>
    <author>
      <name>M. Mrissa</name>
    </author>
    <author>
      <name>Ph. Thiran</name>
    </author>
    <author>
      <name>J-M. Jacquet</name>
    </author>
    <author>
      <name>D. Benslimane</name>
    </author>
    <author>
      <name>Z. Maamar</name>
    </author>
    <link href="http://arxiv.org/abs/0806.1816v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0806.1816v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0808.0518v1</id>
    <updated>2008-08-04T22:11:45Z</updated>
    <published>2008-08-04T22:11:45Z</published>
    <title>Building a terminology network for search: the KoMoHe project</title>
    <summary>  The paper reports about results on the GESIS-IZ project "Competence Center
Modeling and Treatment of Semantic Heterogeneity" (KoMoHe). KoMoHe supervised a
terminology mapping effort, in which 'cross-concordances' between major
controlled vocabularies were organized, created and managed. In this paper we
describe the establishment and implementation of cross-concordances for search
in a digital library (DL).
</summary>
    <author>
      <name>Philipp Mayr</name>
    </author>
    <author>
      <name>Vivien Petras</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figure, Dublin Core Conference 2008</arxiv:comment>
    <link href="http://arxiv.org/abs/0808.0518v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0808.0518v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.1413v3</id>
    <updated>2011-06-21T17:05:53Z</updated>
    <published>2009-07-09T06:51:54Z</published>
    <title>Privacy constraints in regularized convex optimization</title>
    <summary>  This paper is withdrawn due to some errors, which are corrected in
arXiv:0912.0071v4 [cs.LG].
</summary>
    <author>
      <name>Kamalika Chaudhuri</name>
    </author>
    <author>
      <name>Anand D. Sarwate</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by the authors due to some errors.
  Corrections have been included in arXiv:0912.0071v4</arxiv:comment>
    <link href="http://arxiv.org/abs/0907.1413v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.1413v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.2299v1</id>
    <updated>2010-12-10T15:50:36Z</updated>
    <published>2010-12-10T15:50:36Z</published>
    <title>A Simple Correctness Proof for Magic Transformation</title>
    <summary>  The paper presents a simple and concise proof of correctness of the magic
transformation. We believe it may provide a useful example of formal reasoning
about logic programs.
  The correctness property concerns the declarative semantics. The proof,
however, refers to the operational semantics (LD-resolution) of the source
programs. Its conciseness is due to applying a suitable proof method.
</summary>
    <author>
      <name>Wlodzimierz Drabent</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to "Theory and Practice of Logic Programming"</arxiv:comment>
    <link href="http://arxiv.org/abs/1012.2299v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.2299v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.1125v1</id>
    <updated>2012-05-05T12:19:33Z</updated>
    <published>2012-05-05T12:19:33Z</published>
    <title>Application Of Data Mining In Bioinformatics</title>
    <summary>  This article highlights some of the basic concepts of bioinformatics and data
mining. The major research areas of bioinformatics are highlighted. The
application of data mining in the domain of bioinformatics is explained. It
also highlights some of the current challenges and opportunities of data mining
in bioinformatics.
</summary>
    <author>
      <name>Khalid Raza</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Indian Journal of Computer Science and Engineering 1(2):114-118
  2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1205.1125v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.1125v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.5745v1</id>
    <updated>2012-05-25T16:18:29Z</updated>
    <published>2012-05-25T16:18:29Z</published>
    <title>Generic Expression Hardness Results for Primitive Positive Formula
  Comparison</title>
    <summary>  We study the expression complexity of two basic problems involving the
comparison of primitive positive formulas: equivalence and containment. In
particular, we study the complexity of these problems relative to finite
relational structures. We present two generic hardness results for the studied
problems, and discuss evidence that they are optimal and yield, for each of the
problems, a complexity trichotomy.
</summary>
    <author>
      <name>Simone Bova</name>
    </author>
    <author>
      <name>Hubie Chen</name>
    </author>
    <author>
      <name>Matthew Valeriote</name>
    </author>
    <link href="http://arxiv.org/abs/1205.5745v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.5745v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.4187v1</id>
    <updated>2012-09-19T08:58:52Z</updated>
    <published>2012-09-19T08:58:52Z</published>
    <title>PaxosLease: Diskless Paxos for Leases</title>
    <summary>  This paper describes PaxosLease, a distributed algorithm for lease
negotiation. PaxosLease is based on Paxos, but does not require disk writes or
clock synchrony. PaxosLease is used for master lease negotation in the
open-source Keyspace and ScalienDB replicated key-value stores.
</summary>
    <author>
      <name>Márton Trencséni</name>
    </author>
    <author>
      <name>Attila Gazsó</name>
    </author>
    <author>
      <name>Holger Reinhardt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.4187v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.4187v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.7544v1</id>
    <updated>2013-04-29T00:30:36Z</updated>
    <published>2013-04-29T00:30:36Z</published>
    <title>Monoidify! Monoids as a Design Principle for Efficient MapReduce
  Algorithms</title>
    <summary>  It is well known that since the sort/shuffle stage in MapReduce is costly,
local aggregation is one important principle to designing efficient algorithms.
This short paper represents an attempt to more clearly articulate this design
principle in terms of monoids, which generalizes the use of combiners and the
in-mapper combining pattern.
</summary>
    <author>
      <name>Jimmy Lin</name>
    </author>
    <link href="http://arxiv.org/abs/1304.7544v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.7544v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.4927v2</id>
    <updated>2013-09-20T07:58:30Z</updated>
    <published>2013-09-19T10:59:05Z</published>
    <title>A finite axiomatization of conditional independence and inclusion
  dependencies</title>
    <summary>  We present a complete finite axiomatization of the unrestricted implication
problem for inclusion and conditional independence atoms in the context of
dependence logic. For databases, our result implies a finite axiomatization of
the unrestricted implication problem for inclusion, functional, and embedded
multivalued dependencies in the unirelational case.
</summary>
    <author>
      <name>Miika Hannula</name>
    </author>
    <author>
      <name>Juha Kontinen</name>
    </author>
    <link href="http://arxiv.org/abs/1309.4927v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.4927v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="03C80" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.1316v1</id>
    <updated>2013-10-04T15:21:25Z</updated>
    <published>2013-10-04T15:21:25Z</published>
    <title>A note on monadic datalog on unranked trees</title>
    <summary>  In the article 'Recursive queries on trees and data trees' (ICDT'13),
Abiteboul et al., asked whether the containment problem for monadic datalog
over unordered unranked labeled trees using the child relation and the
descendant relation is decidable. This note gives a positive answer to this
question, as well as an overview of the relative expressive power of monadic
datalog on various representations of unranked trees.
</summary>
    <author>
      <name>André Frochaux</name>
    </author>
    <author>
      <name>Nicole Schweikardt</name>
    </author>
    <link href="http://arxiv.org/abs/1310.1316v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.1316v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.5685v1</id>
    <updated>2013-11-22T09:22:03Z</updated>
    <published>2013-11-22T09:22:03Z</published>
    <title>Data Challenges in High-Performance Risk Analytics</title>
    <summary>  Risk Analytics is important to quantify, manage and analyse risks from the
manufacturing to the financial setting. In this paper, the data challenges in
the three stages of the high-performance risk analytics pipeline, namely risk
modelling, portfolio risk management and dynamic financial analysis is
presented.
</summary>
    <author>
      <name>Blesson Varghese</name>
    </author>
    <author>
      <name>Andrew Rau-Chaplin</name>
    </author>
    <link href="http://arxiv.org/abs/1311.5685v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.5685v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.3690v1</id>
    <updated>2014-01-15T18:04:36Z</updated>
    <published>2014-01-15T18:04:36Z</published>
    <title>FindStat - the combinatorial statistics database</title>
    <summary>  The FindStat project at www.FindStat.org provides an online platform for
mathematicians, particularly for combinatorialists, to gather information about
combinatorial statistics and their relations. This outline provides an overview
over the project.
</summary>
    <author>
      <name>Chris Berg</name>
    </author>
    <author>
      <name>Viviane Pons</name>
    </author>
    <author>
      <name>Travis Scrimshaw</name>
    </author>
    <author>
      <name>Jessica Striker</name>
    </author>
    <author>
      <name>Christian Stump</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, project description</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.3690v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.3690v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.4872v1</id>
    <updated>2014-01-20T11:58:23Z</updated>
    <published>2014-01-20T11:58:23Z</published>
    <title>Classification of IDS Alerts with Data Mining Techniques</title>
    <summary>  A data mining technique to reduce the amount of false alerts within an IDS
system is proposed. The new technique achieves an accuracy of 99% compared to
97% by the current systems.
</summary>
    <author>
      <name>Hany Nashat Gabra</name>
    </author>
    <author>
      <name>Ayman Mohammad Bahaa-Eldin</name>
    </author>
    <author>
      <name>Huda Korashy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2012 International Conference on Internet Study (NETs2012), Bangkok,
  Thailand</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.4872v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.4872v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.05730v1</id>
    <updated>2015-02-19T21:25:11Z</updated>
    <published>2015-02-19T21:25:11Z</published>
    <title>Designing Applications with Distributed Databases in a Hybrid Cloud</title>
    <summary>  Designing applications for use in a hybrid cloud has many features. These
include dynamic virtualization management and an unknown route switching
customers. This makes it impossible to evaluate the query and hence the optimal
distribution of data. In this paper, we formulate the main challenges of
designing and simulation offer installation for processing.
</summary>
    <author>
      <name>Evgeniy Pluzhnik</name>
    </author>
    <author>
      <name>Oleg Lukyanchikov</name>
    </author>
    <author>
      <name>Evgeny Nikulchev</name>
    </author>
    <author>
      <name>Simon Payain</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in WIT Transactions of Information and Communication Technologies,
  2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.05730v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.05730v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.03086v1</id>
    <updated>2015-11-10T12:30:42Z</updated>
    <published>2015-11-10T12:30:42Z</published>
    <title>The CTU Prague Relational Learning Repository</title>
    <summary>  The aim of the CTU Prague Relational Learning Repository is to support
machine learning research with multi-relational data. The repository currently
contains 50 SQL databases hosted on a public MySQL server located at
relational.fit.cvut.cz. A searchable meta-database provides metadata (e.g., the
number of tables in the database, the number of rows and columns in the tables,
the number of foreign key constraints between tables).
</summary>
    <author>
      <name>Jan Motl</name>
    </author>
    <author>
      <name>Oliver Schulte</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.03086v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.03086v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.03327v1</id>
    <updated>2017-07-05T19:02:22Z</updated>
    <published>2017-07-05T19:02:22Z</published>
    <title>PlumX As a Potential Tool to Assess the Macroscopic Multidimensional
  Impact of Books</title>
    <summary>  The main purpose of this macro-study is to shed light on the broad impact of
books. For this purpose, the impact of a very large collection of books has
been analyzed by using PlumX, an analytical tool providing a great number of
different metrics provided by various tools.
</summary>
    <author>
      <name>Daniel Torres-Salinas</name>
    </author>
    <author>
      <name>Christian Gumpenberger</name>
    </author>
    <author>
      <name>Juan Gorraiz</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3389/frma.2017.00005</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3389/frma.2017.00005" rel="related"/>
    <link href="http://arxiv.org/abs/1707.03327v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.03327v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9809011v1</id>
    <updated>1998-09-05T00:29:54Z</updated>
    <published>1998-09-05T00:29:54Z</published>
    <title>Microsoft TerraServer</title>
    <summary>  The Microsoft TerraServer stores aerial and satellite images of the earth in
a SQL Server Database served to the public via the Internet. It is the world's
largest atlas, combining five terabytes of image data from the United States
Geodetic Survey, Sovinformsputnik, and Encarta Virtual Globe. Internet browsers
provide intuitive spatial and gazetteer interfaces to the data. The TerraServer
is also an E-Commerce application. Users can buy the right to use the imagery
using Microsoft Site Servers managed by the USGS and Aerial Images. This paper
describes the TerraServer's design and implementation.
</summary>
    <author>
      <name>Tom Barclay</name>
    </author>
    <author>
      <name>Robert Eberl</name>
    </author>
    <author>
      <name>Jim Gray</name>
    </author>
    <author>
      <name>John Nordlinger</name>
    </author>
    <author>
      <name>Guru Raghavendran</name>
    </author>
    <author>
      <name>Don Slutz</name>
    </author>
    <author>
      <name>Greg Smith</name>
    </author>
    <author>
      <name>Phil Smoot</name>
    </author>
    <author>
      <name>John Hoffman</name>
    </author>
    <author>
      <name>Natt Robb III</name>
    </author>
    <author>
      <name>Hedy Rossmeissl</name>
    </author>
    <author>
      <name>Beth Duff</name>
    </author>
    <author>
      <name>George Lee</name>
    </author>
    <author>
      <name>Theresa Mathesmier</name>
    </author>
    <author>
      <name>Randall Sunne</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Original file at
  http://research.microsoft.com/~gray/TerraServer_TR.doc</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9809011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9809011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4;H.2.8;H.3.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9809023v2</id>
    <updated>1998-09-18T19:04:42Z</updated>
    <published>1998-09-17T14:41:07Z</published>
    <title>Similarity-Based Queries for Time Series Data</title>
    <summary>  We study a set of linear transformations on the Fourier series representation
of a sequence that can be used as the basis for similarity queries on
time-series data. We show that our set of transformations is rich enough to
formulate operations such as moving average and time warping. We present a
query processing algorithm that uses the underlying R-tree index of a
multidimensional data set to answer similarity queries efficiently. Our
experiments show that the performance of this algorithm is competitive to that
of processing ordinary (exact match) queries using the index, and much faster
than sequential scanning. We relate our transformations to the general
framework for similarity queries of Jagadish et al.
</summary>
    <author>
      <name>Davood Rafiei</name>
    </author>
    <author>
      <name>Alberto Mendelzon</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the ACM SIGMOD Intl. Conf. on Management of
  Data, pages 13-24, Tucson, Arizona, May 1997</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9809023v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9809023v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9811013v1</id>
    <updated>1998-11-09T05:09:36Z</updated>
    <published>1998-11-09T05:09:36Z</published>
    <title>The Asilomar Report on Database Research</title>
    <summary>  The database research community is rightly proud of success in basic
research, and its remarkable record of technology transfer. Now the field needs
to radically broaden its research focus to attack the issues of capturing,
storing, analyzing, and presenting the vast array of online data. The database
research community should embrace a broader research agenda -- broadening the
definition of database management to embrace all the content of the Web and
other online data stores, and rethinking our fundamental assumptions in light
of technology shifts. To accelerate this transition, we recommend changing the
way research results are evaluated and presented. In particular, we advocate
encouraging more speculative and long-range work, moving conferences to a
poster format, and publishing all research literature on the Web.
</summary>
    <author>
      <name>Phil Bernstein</name>
    </author>
    <author>
      <name>Michael Brodie</name>
    </author>
    <author>
      <name>Stefano Ceri</name>
    </author>
    <author>
      <name>David DeWitt</name>
    </author>
    <author>
      <name>Mike Franklin</name>
    </author>
    <author>
      <name>Hector Garcia-Molina</name>
    </author>
    <author>
      <name>Jim Gray</name>
    </author>
    <author>
      <name>Jerry Held</name>
    </author>
    <author>
      <name>Joe Hellerstein</name>
    </author>
    <author>
      <name>H. V. Jagadish</name>
    </author>
    <author>
      <name>Michael Lesk</name>
    </author>
    <author>
      <name>Dave Maier</name>
    </author>
    <author>
      <name>Jeff Naughton</name>
    </author>
    <author>
      <name>Hamid Pirahesh</name>
    </author>
    <author>
      <name>Mike Stonebraker</name>
    </author>
    <author>
      <name>Jeff Ullman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages in HTML; an original in MSword at
  http://research.microsoft.com/~gray/Asilomar_DB_98.doc</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM SIGMOD Record, December 1998</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9811013v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9811013v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.0;H.2;H.3;H.4;H.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9907016v1</id>
    <updated>1999-07-09T21:30:11Z</updated>
    <published>1999-07-09T21:30:11Z</published>
    <title>Microsoft TerraServer: A Spatial Data Warehouse</title>
    <summary>  The TerraServer stores aerial, satellite, and topographic images of the earth
in a SQL database available via the Internet. It is the world's largest online
atlas, combining five terabytes of image data from the United States Geological
Survey (USGS) and SPIN-2. This report describes the system-redesign based on
our experience over the last year. It also reports usage and operations results
over the last year -- over 2 billion web hits and over 20 Terabytes of imagry
served over the Internet. Internet browsers provide intuitive spatial and text
interfaces to the data. Users need no special hardware, software, or knowledge
to locate and browse imagery. This paper describes how terabytes of "Internet
unfriendly" geo-spatial images were scrubbed and edited into hundreds of
millions of "Internet friendly" image tiles and loaded into a SQL data
warehouse. Microsoft TerraServer demonstrates that general-purpose relational
database technology can manage large scale image repositories, and shows that
web browsers can be a good geospatial image presentation system.
</summary>
    <author>
      <name>Tom Barclay Jim Gray Don Slutz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Original MSword format at
  http://research.microsoft.com/~gray/papers/MS_TR_99_30_TerraServer.doc</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9907016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9907016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2, H.2.4, H.2.8, H.3.5, H.5.1,J.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9909016v1</id>
    <updated>1999-09-21T21:20:20Z</updated>
    <published>1999-09-21T21:20:20Z</published>
    <title>Least expected cost query optimization: an exercise in utility</title>
    <summary>  We identify two unreasonable, though standard, assumptions made by database
query optimizers that can adversely affect the quality of the chosen evaluation
plans. One assumption is that it is enough to optimize for the expected
case---that is, the case where various parameters (like available memory) take
on their expected value. The other assumption is that the parameters are
constant throughout the execution of the query. We present an algorithm based
on the ``System R''-style query optimization algorithm that does not rely on
these assumptions. The algorithm we present chooses the plan of the least
expected cost instead of the plan of least cost given some fixed value of the
parameters. In execution environments that exhibit a high degree of
variability, our techniques should result in better performance.
</summary>
    <author>
      <name>Francis C. Chu</name>
    </author>
    <author>
      <name>Joseph Y. Halpern</name>
    </author>
    <author>
      <name>Praveen Seshadri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper appears in Proceedings of the Eighteenth Annual ACM
  Symposium on Principles of Database Systems, 1999, pp. 138--147</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9909016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9909016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9910019v1</id>
    <updated>1999-10-22T15:17:29Z</updated>
    <published>1999-10-22T15:17:29Z</published>
    <title>Consistent Checkpointing in Distributed Databases: Towards a Formal
  Approach</title>
    <summary>  Whether it is for audit or for recovery purposes, data checkpointing is an
important problem of distributed database systems. Actually, transactions
establish dependence relations on data checkpoints taken by data object
managers. So, given an arbitrary set of data checkpoints (including at least a
single data checkpoint from a data manager, and at most a data checkpoint from
each data manager), an important question is the following one: ``Can these
data checkpoints be members of a same consistent global checkpoint?''. This
paper answers this question by providing a necessary and sufficient condition
suited for database systems. Moreover, to show the usefulness of this
condition, two {\em non-intrusive} data checkpointing protocols are derived
from this condition. It is also interesting to note that this paper, by
exhibiting ``correspondences'', establishes a bridge between the data
object/transaction model and the process/message-passing model.
</summary>
    <author>
      <name>R. Baldoni</name>
    </author>
    <author>
      <name>F. Quaglia</name>
    </author>
    <author>
      <name>M. Raynal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9910019v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9910019v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2.4; H.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9910021v1</id>
    <updated>1999-10-25T16:30:20Z</updated>
    <published>1999-10-25T16:30:20Z</published>
    <title>Efficient and Extensible Algorithms for Multi Query Optimization</title>
    <summary>  Complex queries are becoming commonplace, with the growing use of decision
support systems. These complex queries often have a lot of common
sub-expressions, either within a single query, or across multiple such queries
run as a batch. Multi-query optimization aims at exploiting common
sub-expressions to reduce evaluation cost. Multi-query optimization has
hither-to been viewed as impractical, since earlier algorithms were exhaustive,
and explore a doubly exponential search space.
  In this paper we demonstrate that multi-query optimization using heuristics
is practical, and provides significant benefits. We propose three cost-based
heuristic algorithms: Volcano-SH and Volcano-RU, which are based on simple
modifications to the Volcano search strategy, and a greedy heuristic. Our
greedy heuristic incorporates novel optimizations that improve efficiency
greatly. Our algorithms are designed to be easily added to existing optimizers.
We present a performance study comparing the algorithms, using workloads
consisting of queries from the TPC-D benchmark. The study shows that our
algorithms provide significant benefits over traditional optimization, at a
very acceptable overhead in optimization time.
</summary>
    <author>
      <name>Prasan Roy</name>
    </author>
    <author>
      <name>S. Seshadri</name>
    </author>
    <author>
      <name>S. Sudarshan</name>
    </author>
    <author>
      <name>Siddhesh Bhobe</name>
    </author>
    <link href="http://arxiv.org/abs/cs/9910021v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9910021v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4;H.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0003006v1</id>
    <updated>2000-03-02T08:05:24Z</updated>
    <published>2000-03-02T08:05:24Z</published>
    <title>Materialized View Selection and Maintenance Using Multi-Query
  Optimization</title>
    <summary>  Because the presence of views enhances query performance, materialized views
are increasingly being supported by commercial database/data warehouse systems.
Whenever the data warehouse is updated, the materialized views must also be
updated. However, whereas the amount of data entering a warehouse, the query
loads, and the need to obtain up-to-date responses are all increasing, the time
window available for making the warehouse up-to-date is shrinking. These trends
necessitate efficient techniques for the maintenance of materialized views.
  In this paper, we show how to find an efficient plan for maintenance of a
{\em set} of views, by exploiting common subexpressions between different view
maintenance expressions. These common subexpressions may be materialized
temporarily during view maintenance. Our algorithms also choose
subexpressions/indices to be materialized permanently (and maintained along
with other materialized views), to speed up view maintenance. While there has
been much work on view maintenance in the past, our novel contributions lie in
exploiting a recently developed framework for multiquery optimization to
efficiently find good view maintenance plans as above. In addition to faster
view maintenance, our algorithms can also be used to efficiently select
materialized views to speed up workloads containing queries.
</summary>
    <author>
      <name>Hoshi Mistry</name>
    </author>
    <author>
      <name>Prasan Roy</name>
    </author>
    <author>
      <name>Krithi Ramamritham</name>
    </author>
    <author>
      <name>S. Sudarshan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0003006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0003006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4;H.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0003043v1</id>
    <updated>2000-03-09T04:01:22Z</updated>
    <published>2000-03-09T04:01:22Z</published>
    <title>Automatic Classification of Text Databases through Query Probing</title>
    <summary>  Many text databases on the web are "hidden" behind search interfaces, and
their documents are only accessible through querying. Search engines typically
ignore the contents of such search-only databases. Recently, Yahoo-like
directories have started to manually organize these databases into categories
that users can browse to find these valuable resources. We propose a novel
strategy to automate the classification of search-only text databases. Our
technique starts by training a rule-based document classifier, and then uses
the classifier's rules to generate probing queries. The queries are sent to the
text databases, which are then classified based on the number of matches that
they produce for each query. We report some initial exploratory experiments
that show that our approach is promising to automatically characterize the
contents of text databases accessible on the web.
</summary>
    <author>
      <name>Panagiotis Ipeirotis</name>
    </author>
    <author>
      <name>Luis Gravano</name>
    </author>
    <author>
      <name>Mehran Sahami</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0003043v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0003043v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0007044v2</id>
    <updated>2001-06-11T08:13:05Z</updated>
    <published>2000-07-31T20:15:08Z</published>
    <title>Managing Periodically Updated Data in Relational Databases: A Stochastic
  Modeling Approach</title>
    <summary>  Recent trends in information management involve the periodic transcription of
data onto secondary devices in a networked environment, and the proper
scheduling of these transcriptions is critical for efficient data management.
To assist in the scheduling process, we are interested in modeling the
reduction of consistency over time between a relation and its replica, termed
obsolescence of data. The modeling is based on techniques from the field of
stochastic processes, and provides several stochastic models for content
evolution in the base relations of a database, taking referential integrity
constraints into account. These models are general enough to accommodate most
of the common scenarios in databases, including batch insertions and life spans
both with and without memory. As an initial "proof of concept" of the
applicability of our approach, we validate the insertion portion of our model
framework via experiments with real data feeds. We also discuss a set of
transcription protocols which make use of the proposed stochastic model.
</summary>
    <author>
      <name>Avigdor Gal</name>
    </author>
    <author>
      <name>Jonathan Eckstein</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0007044v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0007044v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0011041v1</id>
    <updated>2000-11-27T09:15:08Z</updated>
    <published>2000-11-27T09:15:08Z</published>
    <title>EquiX---A Search and Query Language for XML</title>
    <summary>  EquiX is a search language for XML that combines the power of querying with
the simplicity of searching. Requirements for such languages are discussed and
it is shown that EquiX meets the necessary criteria. Both a graphical abstract
syntax and a formal concrete syntax are presented for EquiX queries. In
addition, the semantics is defined and an evaluation algorithm is presented.
The evaluation algorithm is polynomial under combined complexity.
  EquiX combines pattern matching, quantification and logical expressions to
query both the data and meta-data of XML documents. The result of a query in
EquiX is a set of XML documents. A DTD describing the result documents is
derived automatically from the query.
</summary>
    <author>
      <name>Sara Cohen</name>
    </author>
    <author>
      <name>Yaron Kanza</name>
    </author>
    <author>
      <name>Yakov Kogan</name>
    </author>
    <author>
      <name>Werner Nutt</name>
    </author>
    <author>
      <name>Yehoshua Sagiv</name>
    </author>
    <author>
      <name>Alexander Serebrenik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">technical report of Hebrew University Jerusalem Israel</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0011041v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0011041v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0103004v1</id>
    <updated>2001-03-02T20:00:16Z</updated>
    <published>2001-03-02T20:00:16Z</published>
    <title>Rapid Application Evolution and Integration Through Document
  Metamorphosis</title>
    <summary>  The Harland document management system implements a data model in which
document (object) structure can be altered by mixin-style multiple inheritance
at any time. This kind of structural fluidity has long been supported by
knowledge-base management systems, but its use has primarily been in support of
reasoning and inference. In this paper, we report our experiences building and
supporting several non-trivial applications on top of this data model. Based on
these experiences, we argue that structural fluidity is convenient for
data-intensive applications other than knowledge-base management. Specifically,
we suggest that this flexible data model is a natural fit for the decoupled
programming methodology that arises naturally when using enterprise component
frameworks.
</summary>
    <author>
      <name>Paul M. Aoki</name>
    </author>
    <author>
      <name>Ian E. Smith</name>
    </author>
    <author>
      <name>James D. Thornton</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0103004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0103004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.3; D.2.11; K.6.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0104008v1</id>
    <updated>2001-04-03T16:27:48Z</updated>
    <published>2001-04-03T16:27:48Z</published>
    <title>Event Indexing Systems for Efficient Selection and Analysis of HERA Data</title>
    <summary>  The design and implementation of two software systems introduced to improve
the efficiency of offline analysis of event data taken with the ZEUS Detector
at the HERA electron-proton collider at DESY are presented. Two different
approaches were made, one using a set of event directories and the other using
a tag database based on a commercial object-oriented database management
system. These are described and compared. Both systems provide quick direct
access to individual collision events in a sequential data store of several
terabytes, and they both considerably improve the event analysis efficiency. In
particular the tag database provides a very flexible selection mechanism and
can dramatically reduce the computing time needed to extract small subsamples
from the total event sample. Gains as large as a factor 20 have been obtained.
</summary>
    <author>
      <name>L. A. T. Bauerdick</name>
    </author>
    <author>
      <name>Adrian Fox-Murphy</name>
    </author>
    <author>
      <name>Tobias Haas</name>
    </author>
    <author>
      <name>Stefan Stonjek</name>
    </author>
    <author>
      <name>Enrico Tassi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/S0010-4655(01)00162-X</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/S0010-4655(01)00162-X" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in Computer Physics Communications</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Comput.Phys.Commun. 137 (2001) 236-246</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0104008v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0104008v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4; H.3.1; H.3.3; H.3.4; J.2; H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0106012v1</id>
    <updated>2001-06-07T17:29:03Z</updated>
    <published>2001-06-07T17:29:03Z</published>
    <title>Computational Properties of Metaquerying Problems</title>
    <summary>  Metaquerying is a datamining technology by which hidden dependencies among
several database relations can be discovered. This tool has already been
successfully applied to several real-world applications. Recent papers provide
only preliminary results about the complexity of metaquerying. In this paper we
define several variants of metaquerying that encompass, as far as we know, all
variants defined in the literature. We study both the combined complexity and
the data complexity of these variants. We show that, under the combined
complexity measure, metaquerying is generally intractable (unless P=NP), lying
sometimes quite high in the complexity hierarchies (as high as NP^PP),
depending on the characteristics of the plausibility index. However, we are
able to single out some tractable and interesting metaquerying cases (whose
combined complexity is LOGCFL-complete). As for the data complexity of
metaquerying, we prove that, in general, this is in TC0, but lies within AC0 in
some simpler cases. Finally, we discuss implementation of metaqueries, by
providing algorithms to answer them.
</summary>
    <author>
      <name>F. Angiulli</name>
    </author>
    <author>
      <name>R. Ben-Eliyahu-Zohary</name>
    </author>
    <author>
      <name>G. Ianni</name>
    </author>
    <author>
      <name>L. Palopoli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages. Partial and preliminary version appeared in Proc. of 19th
  Symposium on Principles of Database Systems, 2000, Dallas, pp. 237-244</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0106012v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0106012v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.3;F.2.0;H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0106055v1</id>
    <updated>2001-06-28T06:02:46Z</updated>
    <published>2001-06-28T06:02:46Z</published>
    <title>A Seamless Integration of Association Rule Mining with Database Systems</title>
    <summary>  The need for Knowledge and Data Discovery Management Systems (KDDMS) that
support ad hoc data mining queries has been long recognized. A significant
amount of research has gone into building tightly coupled systems that
integrate association rule mining with database systems. In this paper, we
describe a seamless integration scheme for database queries and association
rule discovery using a common query optimizer for both. Query trees of
expressions in an extended algebra are used for internal representation in the
optimizer. The algebraic representation is flexible enough to deal with
constrained association rule queries and other variations of association rule
specifications. We propose modularization to simplify the query tree for
complex tasks in data mining. It paves the way for making use of existing
algorithms for constructing query plans in the optimization process. How the
integration scheme we present will facilitate greater user control over the
data mining process is also discussed. The work described in this paper forms
part of a larger project for fully integrating data mining with database
management.
</summary>
    <author>
      <name>Raj P. Gopalan</name>
    </author>
    <author>
      <name>Tariq Nuruddin</name>
    </author>
    <author>
      <name>Yudho Giri Sucahyo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0106055v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0106055v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0109040v1</id>
    <updated>2001-09-20T12:27:45Z</updated>
    <published>2001-09-20T12:27:45Z</published>
    <title>The Building of BODHI, a Bio-diversity Database System</title>
    <summary>  We have recently built a database system called BODHI, intended to store
plant bio-diversity information. It is based on an object-oriented modeling
approach and is developed completely around public-domain software. The unique
feature of BODHI is that it seamlessly integrates diverse types of data,
including taxonomic characteristics, spatial distributions, and genetic
sequences, thereby spanning the entire range from molecular to organism-level
information. A variety of sophisticated indexing strategies are incorporated to
efficiently access the various types of data, and a rule-based query processor
is employed for optimizing query execution. In this paper, we report on our
experiences in building BODHI and on its performance characteristics for a
representative set of queries.
</summary>
    <author>
      <name>B. J. Srikanta</name>
    </author>
    <author>
      <name>Jayant Haritsa</name>
    </author>
    <author>
      <name>Udaysankar Sen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0109040v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0109040v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0109084v1</id>
    <updated>2001-09-24T20:42:05Z</updated>
    <published>2001-09-24T20:42:05Z</published>
    <title>The Internet and Community Networks: Case Studies of Five U.S. Cities</title>
    <summary>  This paper looks at five U.S. cities (Austin, Cleveland, Nashville, Portland,
and Washington, DC) and explores strategies being employed by community
activists and local governments to create and sustain community networking
projects. In some cities, community networking initiatives are relatively
mature, while in others they are in early or intermediate stages. The paper
looks at several factors that help explain the evolution of community networks
in cities:
  1) Local government support; 2) Federal support 3) Degree of community
activism, often reflected by public-private partnerships that help support
community networks.
  In addition to these (more or less) measurable elements of local support, the
case studies enable description of the different objectives of community
networks in different cities. Several community networking projects aim to
improve the delivery of government services (e.g., Portland and Cleveland),
some have a job-training focus (e.g., Austin, Washington, DC), others are
oriented very explicitly toward community building (Nashville, DC), and others
toward neighborhood entrepreneurship (Portland and Cleveland).
  The paper ties the case studies together by asking whether community
technology initiatives contribute to social capital in the cities studied.
</summary>
    <author>
      <name>John B. Horrigan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29th TPRC Conference</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0109084v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0109084v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.4.m Miscellaneous" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0110044v1</id>
    <updated>2001-10-22T08:03:50Z</updated>
    <published>2001-10-22T08:03:50Z</published>
    <title>EquiX--A Search and Query Language for XML</title>
    <summary>  EquiX is a search language for XML that combines the power of querying with
the simplicity of searching. Requirements for such languages are discussed and
it is shown that EquiX meets the necessary criteria. Both a graph-based
abstract syntax and a formal concrete syntax are presented for EquiX queries.
In addition, the semantics is defined and an evaluation algorithm is presented.
The evaluation algorithm is polynomial under combined complexity.
  EquiX combines pattern matching, quantification and logical expressions to
query both the data and meta-data of XML documents. The result of a query in
EquiX is a set of XML documents. A DTD describing the result documents is
derived automatically from the query.
</summary>
    <author>
      <name>Sara Cohen</name>
    </author>
    <author>
      <name>Yaron Kanza</name>
    </author>
    <author>
      <name>Yakov Kogan</name>
    </author>
    <author>
      <name>Werner Nutt</name>
    </author>
    <author>
      <name>Yehoshua Sagiv</name>
    </author>
    <author>
      <name>Alexander Serebrenik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is a preprint of an article accepted for publication in Journal
  of the American Society for Information Science and Technology @ copyright
  2001 John Wiley &amp; Sons, Inc</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0110044v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0110044v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.5; H.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0110052v1</id>
    <updated>2001-10-25T08:55:57Z</updated>
    <published>2001-10-25T08:55:57Z</published>
    <title>Mragyati : A System for Keyword-based Searching in Databases</title>
    <summary>  The web, through many search engine sites, has popularized the keyword-based
search paradigm, where a user can specify a string of keywords and expect to
retrieve relevant documents, possibly ranked by their relevance to the query.
Since a lot of information is stored in databases (and not as HTML documents),
it is important to provide a similar search paradigm for databases, where users
can query a database without knowing the database schema and database query
languages such as SQL. In this paper, we propose such a database search system,
which accepts a free-form query as a collection of keywords, translates it into
queries on the database using the database metadata, and presents query results
in a well-structured and browsable form. Th eysytem maps keywords onto the
database schema and uses inter-relationships (i.e., data semantics) among the
referred tables to generate meaningful query results. We also describe our
prototype for database search, called Mragyati. Th eapproach proposed here is
scalable, as it does not build an in-memory graph of the entire database for
searching for relationships among the objects selected by the user's query.
</summary>
    <author>
      <name>N. L. Sarda</name>
    </author>
    <author>
      <name>Ankur Jain</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0110052v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0110052v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4; H.3.3; H.3.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0111004v1</id>
    <updated>2001-11-01T22:07:01Z</updated>
    <published>2001-11-01T22:07:01Z</published>
    <title>The Relational Database Aspects of Argonne's ATLAS Control System</title>
    <summary>  The Relational Database Aspects of Argonnes ATLAS Control System Argonnes
ATLAS (Argonne Tandem Linac Accelerator System) control system comprises two
separate database concepts. The first is the distributed real-time database
structure provided by the commercial product Vsystem [1]. The second is a more
static relational database archiving system designed by ATLAS personnel using
Oracle Rdb [2] and Paradox [3] software. The configuration of the ATLAS
facility has presented a unique opportunity to construct a control system
relational database that is capable of storing and retrieving complete archived
tune-up configurations for the entire accelerator. This capability has been a
major factor in allowing the facility to adhere to a rigorous operating
schedule. Most recently, a Web-based operator interface to the control systems
Oracle Rdb database has been installed. This paper explains the history of the
ATLAS database systems, how they interact with each other, the design of the
new Web-based operator interface, and future plans.
</summary>
    <author>
      <name>D. E. R. Quock</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ANL</arxiv:affiliation>
    </author>
    <author>
      <name>F. H. Munson</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ANL</arxiv:affiliation>
    </author>
    <author>
      <name>K. J. Eder</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ANL</arxiv:affiliation>
    </author>
    <author>
      <name>S. L. Dean</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ANL</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICALEPCS 2001 Conference, PSN WEAP066, 3 pages, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">eConf C011127 (2001) WEAP066</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0111004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0111004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0111006v1</id>
    <updated>2001-11-05T18:14:16Z</updated>
    <published>2001-11-05T18:14:16Z</published>
    <title>Proliferation of SDDS Support for Various Platforms and Languages</title>
    <summary>  Since Self-Describing Data Sets (SDDS) were first introduced, the source code
has been ported to many different operating systems and various languages. SDDS
is now available in C, Tcl, Java, Fortran, and Python. All of these versions
are supported on Solaris, Linux, and Windows. The C version of SDDS is also
supported on VxWorks. With the recent addition of the Java port, SDDS can now
be deployed on virtually any operating system. Due to this proliferation, SDDS
files serve to link not only a collection of C programs, but programs and
scripts in many languages on different operating systems. The platform
independent binary feature of SDDS also facilitates portability among operating
systems. This paper presents an overview of various benefits of SDDS platform
interoperability.
</summary>
    <author>
      <name>Robert Soliday</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">APS/ANL</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 2 figures, submitted to ICALEPCS 2001</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">eConfC011127:THAP031,2001</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0111006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0111006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0111018v1</id>
    <updated>2001-11-08T18:28:46Z</updated>
    <published>2001-11-08T18:28:46Z</published>
    <title>Data Acquisition and Database Management System for Samsung
  Superconductor Test Facility</title>
    <summary>  In order to fulfill the test requirement of KSTAR (Korea Superconducting
Tokamak Advanced Research) superconducting magnet system, a large scale
superconducting magnet and conductor test facility, SSTF (Samsung
Superconductor Test Facility), has been constructed at Samsung Advanced
Institute of Technology. The computer system for SSTF DAC (Data Acquisition and
Control) is based on UNIX system and VxWorks is used for the real-time OS of
the VME system. EPICS (Experimental Physics and Industrial Control System) is
used for the communication between IOC server and client. A database program
has been developed for the efficient management of measured data and a Linux
workstation with PENTIUM-4 CPU is used for the database server. In this paper,
the current status of SSTF DAC system, the database management system and
recent test results are presented.
</summary>
    <author>
      <name>Y. Chu</name>
    </author>
    <author>
      <name>S. Baek</name>
    </author>
    <author>
      <name>H. Yonekawa</name>
    </author>
    <author>
      <name>A. Chertovskikh</name>
    </author>
    <author>
      <name>M. Kim</name>
    </author>
    <author>
      <name>J. S. Kim</name>
    </author>
    <author>
      <name>K. Park</name>
    </author>
    <author>
      <name>S. Baang</name>
    </author>
    <author>
      <name>Y. Chang</name>
    </author>
    <author>
      <name>J. H. Kim</name>
    </author>
    <author>
      <name>S. Lee</name>
    </author>
    <author>
      <name>B. Lim</name>
    </author>
    <author>
      <name>W. Chung</name>
    </author>
    <author>
      <name>H. Park</name>
    </author>
    <author>
      <name>K. Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 3 figures, ICALEPCS 2001</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">eConf C011127 (2001) TUAP018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0111018v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0111018v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.1.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0112013v1</id>
    <updated>2001-12-11T19:23:11Z</updated>
    <published>2001-12-11T19:23:11Z</published>
    <title>A Data Mining Framework for Optimal Product Selection in Retail
  Supermarket Data: The Generalized PROFSET Model</title>
    <summary>  In recent years, data mining researchers have developed efficient association
rule algorithms for retail market basket analysis. Still, retailers often
complain about how to adopt association rules to optimize concrete retail
marketing-mix decisions. It is in this context that, in a previous paper, the
authors have introduced a product selection model called PROFSET. This model
selects the most interesting products from a product assortment based on their
cross-selling potential given some retailer defined constraints. However this
model suffered from an important deficiency: it could not deal effectively with
supermarket data, and no provisions were taken to include retail category
management principles. Therefore, in this paper, the authors present an
important generalization of the existing model in order to make it suitable for
supermarket data as well, and to enable retailers to add category restrictions
to the model. Experiments on real world data obtained from a Belgian
supermarket chain produce very promising results and demonstrate the
effectiveness of the generalized PROFSET model.
</summary>
    <author>
      <name>Tom Brijs</name>
    </author>
    <author>
      <name>Bart Goethals</name>
    </author>
    <author>
      <name>Gilbert Swinnen</name>
    </author>
    <author>
      <name>Koen Vanhoof</name>
    </author>
    <author>
      <name>Geert Wets</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0112013v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0112013v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0202014v1</id>
    <updated>2002-02-12T23:47:20Z</updated>
    <published>2002-02-12T23:47:20Z</published>
    <title>Data Mining the SDSS SkyServer Database</title>
    <summary>  An earlier paper (Szalay et. al. "Designing and Mining MultiTerabyte
Astronomy Archives: The Sloan Digital Sky Survey," ACM SIGMOD 2000) described
the Sloan Digital Sky Survey's (SDSS) data management needs by defining twenty
database queries and twelve data visualization tasks that a good data
management system should support. We built a database and interfaces to support
both the query load and also a website for ad-hoc access. This paper reports on
the database design, describes the data loading pipeline, and reports on the
query implementation and performance. The queries typically translated to a
single SQL statement. Most queries run in less than 20 seconds, allowing
scientists to interactively explore the database. This paper is an in-depth
tour of those queries. Readers should first have studied the companion overview
paper Szalay et. al. "The SDSS SkyServer, Public Access to the Sloan Digital
Sky Server Data" ACM SIGMOND 2002.
</summary>
    <author>
      <name>Jim Gray</name>
    </author>
    <author>
      <name>Alex S. Szalay</name>
    </author>
    <author>
      <name>Ani R. Thakar</name>
    </author>
    <author>
      <name>Peter Z. Kunszt</name>
    </author>
    <author>
      <name>Christopher Stoughton</name>
    </author>
    <author>
      <name>Don Slutz</name>
    </author>
    <author>
      <name>Jan vandenBerg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">40 pages, Original source is at
  http://research.microsoft.com/~gray/Papers/MSR_TR_O2_01_20_queries.doc</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0202014v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0202014v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8;H.3.3; H.3.5;h.3.7;H.4.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0202035v1</id>
    <updated>2002-02-21T05:23:51Z</updated>
    <published>2002-02-21T05:23:51Z</published>
    <title>Sprinkling Selections over Join DAGs for Efficient Query Optimization</title>
    <summary>  In optimizing queries, solutions based on AND/OR DAG can generate all
possible join orderings and select placements before searching for optimal
query execution strategy. But as the number of joins and selection conditions
increase, the space and time complexity to generate optimal query plan
increases exponentially. In this paper, we use join graph for a relational
database schema to either pre-compute all possible join orderings that can be
executed and store it as a join DAG or, extract joins in the queries to
incrementally build a history join DAG as and when the queries are executed.
The select conditions in the queries are appropriately placed in the retrieved
join DAG (or, history join DAG) to generate optimal query execution strategy.
We experimentally evaluate our query optimization technique on TPC-D/H query
sets to show their effectiveness over AND/OR DAG query optimization strategy.
Finally, we illustrate how our technique can be used for efficient multiple
query optimization and selection of materialized views in data warehousing
environments.
</summary>
    <author>
      <name>Satyanarayana R Valluri</name>
    </author>
    <author>
      <name>Soujanya Vadapalli</name>
    </author>
    <author>
      <name>Kamalakar Karlapalem</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 Pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0202035v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0202035v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0203028v3</id>
    <updated>2003-01-26T07:30:18Z</updated>
    <published>2002-03-27T06:31:58Z</published>
    <title>When to Update the sequential patterns of stream data?</title>
    <summary>  In this paper, we first define a difference measure between the old and new
sequential patterns of stream data, which is proved to be a distance. Then we
propose an experimental method, called TPD (Tradeoff between Performance and
Difference), to decide when to update the sequential patterns of stream data by
making a tradeoff between the performance of increasingly updating algorithms
and the difference of sequential patterns. The experiments for the incremental
updating algorithm IUS on two data sets show that generally, as the size of
incremental windows grows, the values of the speedup and the values of the
difference will decrease and increase respectively. It is also shown
experimentally that the incremental ratio determined by the TPD method does not
monotonically increase or decrease but changes in a range between 20 and 30
percentage for the IUS algorithm.
</summary>
    <author>
      <name>Qingguo Zheng</name>
    </author>
    <author>
      <name>Ke Xu</name>
    </author>
    <author>
      <name>Shilong Ma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0203028v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0203028v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0204046v1</id>
    <updated>2002-04-22T18:32:44Z</updated>
    <published>2002-04-22T18:32:44Z</published>
    <title>Optimal Aggregation Algorithms for Middleware</title>
    <summary>  Let D be a database of N objects where each object has m fields. The objects
are given in m sorted lists (where the ith list is sorted according to the ith
field). Our goal is to find the top k objects according to a monotone
aggregation function t, while minimizing access to the lists. The problem
arises in several contexts. In particular Fagin (JCSS 1999) considered it for
the purpose of aggregating information in a multimedia database system.
  We are interested in instance optimality, i.e. that our algorithm will be as
good as any other (correct) algorithm on any instance. We provide and analyze
several instance optimal algorithms for the task, with various access costs and
models.
</summary>
    <author>
      <name>Ron Fagin</name>
    </author>
    <author>
      <name>Amnon Lotem</name>
    </author>
    <author>
      <name>Moni Naor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">41 pages. Preliminary version appeared in ACM PODS 2001, pp. 102-113</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0204046v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0204046v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4; F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0205060v1</id>
    <updated>2002-05-23T14:15:55Z</updated>
    <published>2002-05-23T14:15:55Z</published>
    <title>Optimizing Queries Using a Meta-level Database</title>
    <summary>  Graph simulation (using graph schemata or data guides) has been successfully
proposed as a technique for adding structure to semistructured data. Design
patterns for description (such as meta-classes and homomorphisms between schema
layers), which are prominent in the object-oriented programming community,
constitute a generalization of this graph simulation approach.
  In this paper, we show description applicable to a wide range of data models
that have some notion of object (-identity), and propose to turn it into a data
model primitive much like, say, inheritance. We argue that such an extension
fills a practical need in contemporary data management. Then, we present
algebraic techniques for query optimization (using the notions of described and
description queries). Finally, in the semistructured setting, we discuss the
pruning of regular path queries (with nested conditions) using description
meta-data. In this context, our notion of meta-data extends graph schemata and
data guides by meta-level values, allowing to boost query performance and to
reduce the redundancy of data.
</summary>
    <author>
      <name>Christoph Koch</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0205060v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0205060v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.1, H.2.3, D.1.5, D.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0206004v1</id>
    <updated>2002-06-03T14:13:51Z</updated>
    <published>2002-06-03T14:13:51Z</published>
    <title>Mining All Non-Derivable Frequent Itemsets</title>
    <summary>  Recent studies on frequent itemset mining algorithms resulted in significant
performance improvements. However, if the minimal support threshold is set too
low, or the data is highly correlated, the number of frequent itemsets itself
can be prohibitively large. To overcome this problem, recently several
proposals have been made to construct a concise representation of the frequent
itemsets, instead of mining all frequent itemsets. The main goal of this paper
is to identify redundancies in the set of all frequent itemsets and to exploit
these redundancies in order to reduce the result of a mining operation. We
present deduction rules to derive tight bounds on the support of candidate
itemsets. We show how the deduction rules allow for constructing a minimal
representation for all frequent itemsets. We also present connections between
our proposal and recent proposals for concise representations and we give the
results of experiments on real-life datasets that show the effectiveness of the
deduction rules. In fact, the experiments even show that in many cases, first
mining the concise representation, and then creating the frequent itemsets from
this representation outperforms existing frequent set mining algorithms.
</summary>
    <author>
      <name>Toon Calders</name>
    </author>
    <author>
      <name>Bart Goethals</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0206004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0206004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0207093v1</id>
    <updated>2002-07-27T00:46:50Z</updated>
    <published>2002-07-27T00:46:50Z</published>
    <title>Preference Queries</title>
    <summary>  The handling of user preferences is becoming an increasingly important issue
in present-day information systems. Among others, preferences are used for
information filtering and extraction to reduce the volume of data presented to
the user. They are also used to keep track of user profiles and formulate
policies to improve and automate decision making.
  We propose here a simple, logical framework for formulating preferences as
preference formulas. The framework does not impose any restrictions on the
preference relations and allows arbitrary operation and predicate signatures in
preference formulas. It also makes the composition of preference relations
straightforward. We propose a simple, natural embedding of preference formulas
into relational algebra (and SQL) through a single winnow operator
parameterized by a preference formula. The embedding makes possible the
formulation of complex preference queries, e.g., involving aggregation, by
piggybacking on existing SQL constructs. It also leads in a natural way to the
definition of further, preference-related concepts like ranking. Finally, we
present general algebraic laws governing the winnow operator and its
interaction with other relational algebra operators. The preconditions on the
applicability of the laws are captured by logical formulas. The laws provide a
formal foundation for the algebraic optimization of preference queries. We
demonstrate the usefulness of our approach through numerous examples.
</summary>
    <author>
      <name>Jan Chomicki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">34 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0207093v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0207093v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.3; F.4.1; I.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0207094v1</id>
    <updated>2002-07-26T21:18:50Z</updated>
    <published>2002-07-26T21:18:50Z</published>
    <title>Answer Sets for Consistent Query Answering in Inconsistent Databases</title>
    <summary>  A relational database is inconsistent if it does not satisfy a given set of
integrity constraints. Nevertheless, it is likely that most of the data in it
is consistent with the constraints. In this paper we apply logic programming
based on answer sets to the problem of retrieving consistent information from a
possibly inconsistent database. Since consistent information persists from the
original database to every of its minimal repairs, the approach is based on a
specification of database repairs using disjunctive logic programs with
exceptions, whose answer set semantics can be represented and computed by
systems that implement stable model semantics. These programs allow us to
declare persistence by defaults and repairing changes by exceptions. We
concentrate mainly on logic programs for binary integrity constraints, among
which we find most of the integrity constraints found in practice.
</summary>
    <author>
      <name>Marcelo Arenas</name>
    </author>
    <author>
      <name>Leopoldo Bertossi</name>
    </author>
    <author>
      <name>Jan Chomicki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">34 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0207094v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0207094v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.3; F.4.1; I.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0208013v1</id>
    <updated>2002-08-07T22:49:56Z</updated>
    <published>2002-08-07T22:49:56Z</published>
    <title>Petabyte Scale Data Mining: Dream or Reality?</title>
    <summary>  Science is becoming very data intensive1. Today's astronomy datasets with
tens of millions of galaxies already present substantial challenges for data
mining. In less than 10 years the catalogs are expected to grow to billions of
objects, and image archives will reach Petabytes. Imagine having a 100GB
database in 1996, when disk scanning speeds were 30MB/s, and database tools
were immature. Such a task today is trivial, almost manageable with a laptop.
We think that the issue of a PB database will be very similar in six years. In
this paper we scale our current experiments in data archiving and analysis on
the Sloan Digital Sky Survey2,3 data six years into the future. We analyze
these projections and look at the requirements of performing data mining on
such data sets. We conclude that the task scales rather well: we could do the
job today, although it would be expensive. There do not seem to be any
show-stoppers that would prevent us from storing and using a Petabyte dataset
six years from today.
</summary>
    <author>
      <name>Alexander S. Szalay</name>
    </author>
    <author>
      <name>Jim Gray</name>
    </author>
    <author>
      <name>Jan vandenBerg</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1117/12.461427</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1117/12.461427" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">originals at
  http://research.microsoft.com/scripts/pubs/view.asp?TR_ID=MSR-TR-2002-84</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SIPE Astronmy Telescopes and Instruments, 22-28 August 2002,
  Waikoloa, Hawaii</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0208013v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0208013v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8;J.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0208015v1</id>
    <updated>2002-08-07T23:06:40Z</updated>
    <published>2002-08-07T23:06:40Z</published>
    <title>Spatial Clustering of Galaxies in Large Datasets</title>
    <summary>  Datasets with tens of millions of galaxies present new challenges for the
analysis of spatial clustering. We have built a framework that integrates a
database of object catalogs, tools for creating masks of bad regions, and a
fast (NlogN) correlation code. This system has enabled unprecedented efficiency
in carrying out the analysis of galaxy clustering in the SDSS catalog. A
similar approach is used to compute the three-dimensional spatial clustering of
galaxies on very large scales. We describe our strategy to estimate the effect
of photometric errors using a database. We discuss our efforts as an early
example of data-intensive science. While it would have been possible to get
these results without the framework we describe, it will be infeasible to
perform these computations on the future huge datasets without using this
framework.
</summary>
    <author>
      <name>Alexander S. Szalay</name>
    </author>
    <author>
      <name>Tamas Budavari</name>
    </author>
    <author>
      <name>Andrew Connolly</name>
    </author>
    <author>
      <name>Jim Gray</name>
    </author>
    <author>
      <name>Takahiko Matsubara</name>
    </author>
    <author>
      <name>Adrian Pope</name>
    </author>
    <author>
      <name>Istvan Szapudi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1117/12.476761</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1117/12.476761" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">original documents at
  http://research.microsoft.com/scripts/pubs/view.asp?TR_ID=MSR-TR-2002-86</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SIPE Astronomy Telescopes and Instruments, 22-28 August 2002,
  Waikoloa, Hawaii</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0208015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0208015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3;H.2.8; J.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0210028v2</id>
    <updated>2002-10-30T18:27:58Z</updated>
    <published>2002-10-29T19:19:12Z</published>
    <title>Equivalences Among Aggregate Queries with Negation</title>
    <summary>  Query equivalence is investigated for disjunctive aggregate queries with
negated subgoals, constants and comparisons. A full characterization of
equivalence is given for the aggregation functions count, max, sum, prod,
toptwo and parity. A related problem is that of determining, for a given
natural number N, whether two given queries are equivalent over all databases
with at most N constants. We call this problem bounded equivalence. A complete
characterization of decidability of bounded equivalence is given. In
particular, it is shown that this problem is decidable for all the above
aggregation functions as well as for count distinct and average. For
quasilinear queries (i.e., queries where predicates that occur positively are
not repeated) it is shown that equivalence can be decided in polynomial time
for the aggregation functions count, max, sum, parity, prod, toptwo and
average. A similar result holds for count distinct provided that a few
additional conditions hold. The results are couched in terms of abstract
characteristics of aggregation functions, and new proof techniques are used.
Finally, the results above also imply that equivalence, under bag-set
semantics, is decidable for non-aggregate queries with negation.
</summary>
    <author>
      <name>Sara Cohen</name>
    </author>
    <author>
      <name>Werner Nutt</name>
    </author>
    <author>
      <name>Yehoshua Sagiv</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0210028v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0210028v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.4.1;H.2.3;H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0211020v2</id>
    <updated>2003-10-08T01:17:47Z</updated>
    <published>2002-11-15T20:01:54Z</published>
    <title>Monadic Datalog and the Expressive Power of Languages for Web
  Information Extraction</title>
    <summary>  Research on information extraction from Web pages (wrapping) has seen much
activity recently (particularly systems implementations), but little work has
been done on formally studying the expressiveness of the formalisms proposed or
on the theoretical foundations of wrapping. In this paper, we first study
monadic datalog over trees as a wrapping language. We show that this simple
language is equivalent to monadic second order logic (MSO) in its ability to
specify wrappers. We believe that MSO has the right expressiveness required for
Web information extraction and propose MSO as a yardstick for evaluating and
comparing wrappers. Along the way, several other results on the complexity of
query evaluation and query containment for monadic datalog over trees are
established, and a simple normal form for this language is presented. Using the
above results, we subsequently study the kernel fragment Elog$^-$ of the Elog
wrapping language used in the Lixto system (a visual wrapper generator).
Curiously, Elog$^-$ exactly captures MSO, yet is easier to use. Indeed,
programs in this language can be entirely visually specified.
</summary>
    <author>
      <name>Georg Gottlob</name>
    </author>
    <author>
      <name>Christoph Koch</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">40 pages, 3 figures, journal version of PODS 2002 paper, to appear in
  JACM</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0211020v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0211020v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.1; F.4.1; F.4.3; H.2.3; I.7.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0211023v1</id>
    <updated>2002-11-20T04:54:19Z</updated>
    <published>2002-11-20T04:54:19Z</published>
    <title>SkyQuery: A WebService Approach to Federate Databases</title>
    <summary>  Traditional science searched for new objects and phenomena that led to
discoveries. Tomorrow's science will combine together the large pool of
information in scientific archives and make discoveries. Scienthists are
currently keen to federate together the existing scientific databases. The
major challenge in building a federation of these autonomous and heterogeneous
databases is system integration. Ineffective integration will result in defunct
federations and under utilized scientific data.
  Astronomy, in particular, has many autonomous archives spread over the
Internet. It is now seeking to federate these, with minimal effort, into a
Virtual Observatory that will solve complex distributed computing tasks such as
answering federated spatial join queries.
  In this paper, we present SkyQuery, a successful prototype of an evolving
federation of astronomy archives. It interoperates using the emerging Web
services standard. We describe the SkyQuery architecture and show how it
efficiently evaluates a probabilistic federated spatial join query.
</summary>
    <author>
      <name>Tanu Malik</name>
    </author>
    <author>
      <name>Alex S. Szalay</name>
    </author>
    <author>
      <name>Tamas Budavari</name>
    </author>
    <author>
      <name>Ani R. Thakar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 Figures, To Appear in CIDR'03, Also at
  http://www.skyquery.net</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0211023v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0211023v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.2 ;H.3.5;H.2.8 ;H.2.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0212004v1</id>
    <updated>2002-12-05T16:23:35Z</updated>
    <published>2002-12-05T16:23:35Z</published>
    <title>Minimal-Change Integrity Maintenance Using Tuple Deletions</title>
    <summary>  We address the problem of minimal-change integrity maintenance in the context
of integrity constraints in relational databases. We assume that
integrity-restoration actions are limited to tuple deletions. We identify two
basic computational issues: repair checking (is a database instance a repair of
a given database?) and consistent query answers (is a tuple an answer to a
given query in every repair of a given database?). We study the computational
complexity of both problems, delineating the boundary between the tractable and
the intractable. We consider denial constraints, general functional and
inclusion dependencies, as well as key and foreign key constraints. Our results
shed light on the computational feasibility of minimal-change integrity
maintenance. The tractable cases should lead to practical implementations. The
intractability results highlight the inherent limitations of any integrity
enforcement mechanism, e.g., triggers or referential constraint actions, as a
way of performing minimal-change integrity maintenance.
</summary>
    <author>
      <name>Jan Chomicki</name>
    </author>
    <author>
      <name>Jerzy Marcinkowski</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0212004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0212004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.3; F.4.1; I.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0212051v1</id>
    <updated>2002-12-23T10:26:36Z</updated>
    <published>2002-12-23T10:26:36Z</published>
    <title>ExploitingWeb Service Semantics: Taxonomies vs. Ontologies</title>
    <summary>  Comprehensive semantic descriptions of Web services are essential to exploit
them in their full potential, that is, discovering them dynamically, and
enabling automated service negotiation, composition and monitoring. The
semantic mechanisms currently available in service registries which are based
on taxonomies fail to provide the means to achieve this. Although the terms
taxonomy and ontology are sometimes used interchangably there is a critical
difference. A taxonomy indicates only class/subclass relationship whereas an
ontology describes a domain completely. The essential mechanisms that ontology
languages provide include their formal specification (which allows them to be
queried) and their ability to define properties of classes. Through properties
very accurate descriptions of services can be defined and services can be
related to other services or resources. In this paper, we discuss the
advantages of describing service semantics through ontology languages and
describe how to relate the semantics defined with the services advertised in
service registries like UDDI and ebXML.
</summary>
    <author>
      <name>Asuman Dogac</name>
    </author>
    <author>
      <name>Gokce Laleci</name>
    </author>
    <author>
      <name>Yildiray Kabak</name>
    </author>
    <author>
      <name>Ibrahim Cingil</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Data Engineering Bulletin, Vol. 25, No. 4, December 2002</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0212051v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0212051v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0212052v1</id>
    <updated>2002-12-25T15:15:16Z</updated>
    <published>2002-12-25T15:15:16Z</published>
    <title>Improving the Functionality of UDDI Registries through Web Service
  Semantics</title>
    <summary>  In this paper we describe a framework for exploiting the semantics of Web
services through UDDI registries. As a part of this framework, we extend the
DAML-S upper ontology to describe the functionality we find essential for
e-businesses. This functionality includes relating the services with electronic
catalogs, describing the complementary services and finding services according
to the properties of products or services. Once the semantics is defined, there
is a need for a mechanism in the service registry to relate it with the service
advertised. The ontology model developed is general enough to be used with any
service registry. However when it comes to relating the semantics with services
advertised, the capabilities provided by the registry effects how this is
achieved. We demonstrate how to integrate the described service semantics to
UDDI registries.
</summary>
    <author>
      <name>Asuman Dogac</name>
    </author>
    <author>
      <name>Ibrahim Cingil</name>
    </author>
    <author>
      <name>Gokce Laleci</name>
    </author>
    <author>
      <name>Yildiray Kabak</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">3rd VLDB Workshop on Technologies for E-Services (TES-02), Hong
  Kong, China, August 23-24, 2002</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0212052v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0212052v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0301017v1</id>
    <updated>2003-01-20T00:13:18Z</updated>
    <published>2003-01-20T00:13:18Z</published>
    <title>Completeness and Decidability Properties for Functional Dependencies in
  XML</title>
    <summary>  XML is of great importance in information storage and retrieval because of
its recent emergence as a standard for data representation and interchange on
the Internet. However XML provides little semantic content and as a result
several papers have addressed the topic of how to improve the semantic
expressiveness of XML. Among the most important of these approaches has been
that of defining integrity constraints in XML. In a companion paper we defined
strong functional dependencies in XML(XFDs). We also presented a set of axioms
for reasoning about the implication of XFDs and showed that the axiom system is
sound for arbitrary XFDs. In this paper we prove that the axioms are also
complete for unary XFDs (XFDs with a single path on the l.h.s.). The second
contribution of the paper is to prove that the implication problem for unary
XFDs is decidable and to provide a linear time algorithm for it.
</summary>
    <author>
      <name>Millist W. Vincent</name>
    </author>
    <author>
      <name>Jixue Liu</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0301017v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0301017v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0305056v1</id>
    <updated>2003-05-29T21:37:47Z</updated>
    <published>2003-05-29T21:37:47Z</published>
    <title>Configuration Database for BaBar On-line</title>
    <summary>  The configuration database is one of the vital systems in the BaBar on-line
system. It provides services for the different parts of the data acquisition
system and control system, which require run-time parameters. The original
design and implementation of the configuration database played a significant
role in the successful BaBar operations since the beginning of experiment.
Recent additions to the design of the configuration database provide better
means for the management of data and add new tools to simplify main
configuration tasks. We describe the design of the configuration database, its
implementation with the Objectivity/DB object-oriented database, and our
experience collected during the years of operation.
</summary>
    <author>
      <name>R. Bartoldus</name>
    </author>
    <author>
      <name>G. Dubois-Felsmann</name>
    </author>
    <author>
      <name>Y. Kolomensky</name>
    </author>
    <author>
      <name>A. Salnikov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 5 pages, 4 figures, PDF. PSN MOKT004</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0305056v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0305056v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4; H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0306019v1</id>
    <updated>2003-06-04T17:38:23Z</updated>
    <published>2003-06-04T17:38:23Z</published>
    <title>Relational databases for data management in PHENIX</title>
    <summary>  PHENIX is one of the two large experiments at the Relativistic Heavy Ion
Collider (RHIC) at Brookhaven National Laboratory (BNL) and archives roughly
100TB of experimental data per year. In addition, large volumes of simulated
data are produced at multiple off-site computing centers. For any file catalog
to play a central role in data management it has to face problems associated
with the need for distributed access and updates. To be used effectively by the
hundreds of PHENIX collaborators in 12 countries the catalog must satisfy the
following requirements: 1) contain up-to-date data, 2) provide fast and
reliable access to the data, 3) have write permissions for the sites that store
portions of data. We present an analysis of several available Relational
Database Management Systems (RDBMS) to support a catalog meeting the above
requirements and discuss the PHENIX experience with building and using the
distributed file catalog.
</summary>
    <author>
      <name>I. Sourikova</name>
    </author>
    <author>
      <name>D. Morrison</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 4 pages, LaTeX, 4 eps figures. PSN
  TUKT003</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0306019v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0306019v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0306020v2</id>
    <updated>2003-06-04T20:12:08Z</updated>
    <published>2003-06-04T19:22:52Z</published>
    <title>On the Verge of One Petabyte - the Story Behind the BaBar Database
  System</title>
    <summary>  The BaBar database has pioneered the use of a commercial ODBMS within the HEP
community. The unique object-oriented architecture of Objectivity/DB has made
it possible to manage over 700 terabytes of production data generated since
May'99, making the BaBar database the world's largest known database. The
ongoing development includes new features, addressing the ever-increasing
luminosity of the detector as well as other changing physics requirements.
Significant efforts are focused on reducing space requirements and operational
costs. The paper discusses our experience with developing a large scale
database system, emphasizing universal aspects which may be applied to any
large scale system, independently of underlying technology used.
</summary>
    <author>
      <name>Adeyemi Adesanya</name>
    </author>
    <author>
      <name>Tofigh Azemoon</name>
    </author>
    <author>
      <name>Jacek Becla</name>
    </author>
    <author>
      <name>Andrew Hanushevsky</name>
    </author>
    <author>
      <name>Adil Hasan</name>
    </author>
    <author>
      <name>Wilko Kroeger</name>
    </author>
    <author>
      <name>Artem Trunov</name>
    </author>
    <author>
      <name>Daniel Wang</name>
    </author>
    <author>
      <name>Igor Gaponenko</name>
    </author>
    <author>
      <name>Simon Patton</name>
    </author>
    <author>
      <name>David Quarrie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 6 pages. PSN MOKT010</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0306020v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0306020v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2.4; H.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0306023v1</id>
    <updated>2003-06-04T23:51:52Z</updated>
    <published>2003-06-04T23:51:52Z</published>
    <title>The Redesigned BaBar Event Store: Believe the Hype</title>
    <summary>  As the BaBar experiment progresses, it produces new and unforeseen
requirements and increasing demands on capacity and feature base. The current
system is being utilized well beyond its original design specifications, and
has scaled appropriately, maintaining data consistency and durability. The
persistent event storage system has remained largely unchanged since the
initial implementation, and thus includes many design features which have
become performance bottlenecks. Programming interfaces were designed before
sufficient usage information became available. Performance and efficiency were
traded off for added flexibility to cope with future demands. With significant
experience in managing actual production data under our belt, we are now in a
position to recraft the system to better suit current needs. The Event Store
redesign is intended to eliminate redundant features while adding new ones,
increase overall performance, and contain the physical storage cost of the
world's largest database.
</summary>
    <author>
      <name>Adeyemi Adesanya</name>
    </author>
    <author>
      <name>Jacek Becla</name>
    </author>
    <author>
      <name>Daniel Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), 5 pages, 2 ps figures, PSN TUKT008</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0306023v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0306023v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.1; H.2.4; E.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0306056v1</id>
    <updated>2003-06-12T17:24:16Z</updated>
    <published>2003-06-12T17:24:16Z</published>
    <title>Twelve Ways to Build CMS Crossings from ROOT Files</title>
    <summary>  The simulation of CMS raw data requires the random selection of one hundred
and fifty pileup events from a very large set of files, to be superimposed in
memory to the signal event. The use of ROOT I/O for that purpose is quite
unusual: the events are not read sequentially but pseudo-randomly, they are not
processed one by one in memory but by bunches, and they do not contain orthodox
ROOT objects but many foreign objects and templates. In this context, we have
compared the performance of ROOT containers versus the STL vectors, and the use
of trees versus a direct storage of containers. The strategy with best
performances is by far the one using clones within trees, but it stays hard to
tune and very dependant on the exact use-case. The use of STL vectors could
bring more easily similar performances in a future ROOT release.
</summary>
    <author>
      <name>D. Chamont</name>
    </author>
    <author>
      <name>C. Charlot</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 8 pages, LaTeX, 1 eps figures. PSN
  TUKT004</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0306056v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0306056v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0306061v1</id>
    <updated>2003-06-13T00:40:18Z</updated>
    <published>2003-06-13T00:40:18Z</published>
    <title>Operational Aspects of Dealing with the Large BaBar Data Set</title>
    <summary>  To date, the BaBar experiment has stored over 0.7PB of data in an
Objectivity/DB database. Approximately half this data-set comprises simulated
data of which more than 70% has been produced at more than 20 collaborating
institutes outside of SLAC. The operational aspects of managing such a large
data set and providing access to the physicists in a timely manner is a
challenging and complex problem. We describe the operational aspects of
managing such a large distributed data-set as well as importing and exporting
data from geographically spread BaBar collaborators. We also describe problems
common to dealing with such large datasets.
</summary>
    <author>
      <name>Tofigh Azemoon</name>
    </author>
    <author>
      <name>Adil Hasan</name>
    </author>
    <author>
      <name>Wilko Kroeger</name>
    </author>
    <author>
      <name>Artem Trunov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented for Computing in High Energy Physics, San Diego, March 2003</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0306061v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0306061v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.7; E.5; J.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0306065v1</id>
    <updated>2003-06-13T16:21:53Z</updated>
    <published>2003-06-13T16:21:53Z</published>
    <title>POOL File Catalog, Collection and Metadata Components</title>
    <summary>  The POOL project is the common persistency framework for the LHC experiments
to store petabytes of experiment data and metadata in a distributed and grid
enabled way. POOL is a hybrid event store consisting of a data streaming layer
and a relational layer. This paper describes the design of file catalog,
collection and metadata components which are not part of the data streaming
layer of POOL and outlines how POOL aims to provide transparent and efficient
data access for a wide range of environments and use cases - ranging from a
large production site down to a single disconnected laptops. The file catalog
is the central POOL component translating logical data references to physical
data files in a grid environment. POOL collections with their associated
metadata provide an abstract way of accessing experiment data via their logical
grouping into sets of related data objects.
</summary>
    <author>
      <name>C. Cioffi</name>
    </author>
    <author>
      <name>S. Eckmann</name>
    </author>
    <author>
      <name>M. Girone</name>
    </author>
    <author>
      <name>J. Hrivnac</name>
    </author>
    <author>
      <name>D. Malon</name>
    </author>
    <author>
      <name>H. Schmuecker</name>
    </author>
    <author>
      <name>A. Vaniachine</name>
    </author>
    <author>
      <name>J. Wojcieszuk</name>
    </author>
    <author>
      <name>Z. Xie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 4 pages, 1 eps figure, PSN MOKT009</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0306065v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0306065v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0306077v1</id>
    <updated>2003-06-13T22:10:36Z</updated>
    <published>2003-06-13T22:10:36Z</published>
    <title>The TESLA Requirements Database</title>
    <summary>  In preparation for the planned linear collider TESLA, DESY is designing the
required buildings and facilities. The accelerator and infrastructure
components have to be allocated to buildings, and their required areas for
installation, operation and maintenance have to be determined.
Interdisciplinary working groups specify the project from different viewpoints
and need to develop a common vision as a precondition for an optimal solution.
A commercial requirements database is used as a collaborative tool, enabling
concurrent requirements specification by independent working groups. The
requirements database ensures long term storage and availability of the
emerging knowledge, and it offers a central platform for communication which is
available for all project members. It is successfully operating since summer
2002 and has since then become an important tool for the design team.
</summary>
    <author>
      <name>Lars Hagge</name>
    </author>
    <author>
      <name>Jens Kreutzkamp</name>
    </author>
    <author>
      <name>Kathrin Lappe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Contribution to CHEP2003 conference</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0306077v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0306077v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.1;H.4.0;K.6.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0306079v1</id>
    <updated>2003-06-13T23:04:10Z</updated>
    <published>2003-06-13T23:04:10Z</published>
    <title>Integrated Information Management for TESLA</title>
    <summary>  Next-generation projects in High Energy Physics will reach again a new
dimension of complexity. Information management has to ensure an efficient and
economic information flow within the collaborations, offering world-wide
up-to-date information access to the collaborators as one condition for
successful projects. DESY introduces several information systems in preparation
for the planned linear collider TESLA: a Requirements Management System (RMS)
is in production for the TESLA planning group, a Product Data Management System
(PDMS) is in production since the beginning of 2002 and is supporting the
cavity preparation and the general engineering of accelerator components. A
pilot Asset Management System (AMS) is in production for supporting the
management and maintenance of the technical infrastructure, and a Facility
Management System (FMS) with a Geographic Information System (GIS) is currently
being introduced to support civil engineering. Efforts have been started to
integrate the systems with the goal that users can retrieve information through
a single point of access. The paper gives an introduction to information
management and the activities at DESY.
</summary>
    <author>
      <name>Jochen Buerger</name>
    </author>
    <author>
      <name>Lars Hagge</name>
    </author>
    <author>
      <name>Jens Kreutzkamp</name>
    </author>
    <author>
      <name>Kathrin Lappe</name>
    </author>
    <author>
      <name>Andrea Robben</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Contribution to CHEP2003 conference</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0306079v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0306079v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.4.0;K.6.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0307015v2</id>
    <updated>2006-06-10T15:18:44Z</updated>
    <published>2003-07-07T14:07:59Z</published>
    <title>Architecture of an Open-Sourced, Extensible Data Warehouse Builder:
  InterBase 6 Data Warehouse Builder (IB-DWB)</title>
    <summary>  We report the development of an open-sourced data warehouse builder,
InterBase Data Warehouse Builder (IB-DWB), based on Borland InterBase 6 Open
Edition Database Server. InterBase 6 is used for its low maintenance and small
footprint. IB-DWB is designed modularly and consists of 5 main components, Data
Plug Platform, Discoverer Platform, Multi-Dimensional Cube Builder, and Query
Supporter, bounded together by a Kernel. It is also an extensible system, made
possible by the Data Plug Platform and the Discoverer Platform. Currently,
extensions are only possible via dynamic linked-libraries (DLLs).
Multi-Dimensional Cube Builder represents a basal mean of data aggregation. The
architectural philosophy of IB-DWB centers around providing a base platform
that is extensible, which is functionally supported by expansion modules.
IB-DWB is currently being hosted by sourceforge.net (Project Unix Name:
ib-dwb), licensed under GNU General Public License, Version 2.
</summary>
    <author>
      <name>Maurice HT Ling</name>
    </author>
    <author>
      <name>Chi Wai So</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Ling, Maurice HT and So, Chi Wai. 2003. Proceedings of the First
  Australian Undergraduate Students' Computing Conference. (pp. 40-45)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0307015v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0307015v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8; H.4.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0307032v2</id>
    <updated>2003-07-16T12:49:46Z</updated>
    <published>2003-07-12T12:35:37Z</published>
    <title>Data Management and Mining in Astrophysical Databases</title>
    <summary>  We analyse the issues involved in the management and mining of astrophysical
data. The traditional approach to data management in the astrophysical field is
not able to keep up with the increasing size of the data gathered by modern
detectors. An essential role in the astrophysical research will be assumed by
automatic tools for information extraction from large datasets, i.e. data
mining techniques, such as clustering and classification algorithms. This asks
for an approach to data management based on data warehousing, emphasizing the
efficiency and simplicity of data access; efficiency is obtained using
multidimensional access methods and simplicity is achieved by properly handling
metadata. Clustering and classification techniques, on large datasets, pose
additional requirements: computational and memory scalability with respect to
the data size, interpretability and objectivity of clustering or classification
results. In this study we address some possible solutions.
</summary>
    <author>
      <name>M. Frailis</name>
    </author>
    <author>
      <name>A. De Angelis</name>
    </author>
    <author>
      <name>V. Roberto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, Latex</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">S. Ciprini, A. De Angelis, P. Lubrano and O. Mansutti (eds.):
  Proc. of ``Science with the New Generation of High Energy Gamma-ray
  Experiments'' (Perugia, Italy, May 2003). Forum, Udine 2003, p. 157</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0307032v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0307032v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4; H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0308004v1</id>
    <updated>2003-08-02T08:13:06Z</updated>
    <published>2003-08-02T08:13:06Z</published>
    <title>DPG: A Cache-Efficient Accelerator for Sorting and for Join Operators</title>
    <summary>  We present a new algorithm for fast record retrieval,
distribute-probe-gather, or DPG. DPG has important applications both in sorting
and in joins. Current main memory sorting algorithms split their work into
three phases: extraction of key-pointer pairs; sorting of the key-pointer
pairs; and copying of the original records into the destination array according
the sorted key-pointer pairs. The copying in the last phase dominates today's
sorting time. Hence, the use of DPG in the third phase provides an accelerator
for existing sorting algorithms.
  DPG also provides two new join methods for foreign key joins: DPG-move join
and DPG-sort join. The resulting join methods with DPG are faster because DPG
join is cache-efficient and at the same time DPG join avoids the need for
sorting or for hashing. The ideas presented for foreign key join can also be
extended to faster record pair retrieval for spatial and temporal databases.
</summary>
    <author>
      <name>Gene Cooperman</name>
    </author>
    <author>
      <name>Xiaoqin Ma</name>
    </author>
    <author>
      <name>Viet Ha Nguyen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0308004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0308004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; E.2; F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0309011v1</id>
    <updated>2003-09-08T19:57:46Z</updated>
    <published>2003-09-08T19:57:46Z</published>
    <title>Indexing of Tables Referencing Complex Structures</title>
    <summary>  We introduce indexing of tables referencing complex structures such as
digraphs and spatial objects, appearing in genetics and other data intensive
analysis. The indexing is achieved by extracting dimension schemas from the
referenced structures. The schemas and their dimensionality are determined by
proper coloring algorithms and the duality between all such schemas and all
such possible proper colorings is established. This duality, in turn, provides
us with an extensive library of solutions when addressing indexing questions.
It is illustrated how to use the schemas, in connection with additional
relational database technologies, to optimize queries conditioned on the
structural information being referenced. Comparisons using bitmap indexing in
the Oracle 9.2i database, on the one hand, and multidimensional clustering in
DB2 8.1.2, on the other hand, are used to illustrate the applicability of the
indexing to different technology settings. Finally, we illustrate how the
indexing can be used to extract low dimensional schemas from a binary interval
tree in order to resolve efficiently interval and stabbing queries.
</summary>
    <author>
      <name>Agust S. Egilsson</name>
    </author>
    <author>
      <name>Hakon Gudbjartsson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0309011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0309011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.1;H.2.8;J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0310028v1</id>
    <updated>2003-10-15T16:07:56Z</updated>
    <published>2003-10-15T16:07:56Z</published>
    <title>Providing Diversity in K-Nearest Neighbor Query Results</title>
    <summary>  Given a point query Q in multi-dimensional space, K-Nearest Neighbor (KNN)
queries return the K closest answers according to given distance metric in the
database with respect to Q. In this scenario, it is possible that a majority of
the answers may be very similar to some other, especially when the data has
clusters. For a variety of applications, such homogeneous result sets may not
add value to the user. In this paper, we consider the problem of providing
diversity in the results of KNN queries, that is, to produce the closest result
set such that each answer is sufficiently different from the rest. We first
propose a user-tunable definition of diversity, and then present an algorithm,
called MOTLEY, for producing a diverse result set as per this definition.
Through a detailed experimental evaluation on real and synthetic data, we show
that MOTLEY can produce diverse result sets by reading only a small fraction of
the tuples in the database. Further, it imposes no additional overhead on the
evaluation of traditional KNN queries, thereby providing a seamless interface
between diversity and distance.
</summary>
    <author>
      <name>Anoop Jain</name>
    </author>
    <author>
      <name>Parag Sarda</name>
    </author>
    <author>
      <name>Jayant R. Haritsa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0310028v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0310028v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0310038v1</id>
    <updated>2003-10-17T16:55:08Z</updated>
    <published>2003-10-17T16:55:08Z</published>
    <title>On Addressing Efficiency Concerns in Privacy Preserving Data Mining</title>
    <summary>  Data mining services require accurate input data for their results to be
meaningful, but privacy concerns may influence users to provide spurious
information. To encourage users to provide correct inputs, we recently proposed
a data distortion scheme for association rule mining that simultaneously
provides both privacy to the user and accuracy in the mining results. However,
mining the distorted database can be orders of magnitude more time-consuming as
compared to mining the original database. In this paper, we address this issue
and demonstrate that by (a) generalizing the distortion process to perform
symbol-specific distortion, (b) appropriately choosing the distortion
parameters, and (c) applying a variety of optimizations in the reconstruction
process, runtime efficiencies that are well within an order of magnitude of
undistorted mining can be achieved.
</summary>
    <author>
      <name>Shipra Agrawal</name>
    </author>
    <author>
      <name>Vijay Krishnan</name>
    </author>
    <author>
      <name>Jayant Haritsa</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0310038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0310038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0311031v1</id>
    <updated>2003-11-21T14:44:22Z</updated>
    <published>2003-11-21T14:44:22Z</published>
    <title>Towards an Intelligent Database System Founded on the SP Theory of
  Computing and Cognition</title>
    <summary>  The SP theory of computing and cognition, described in previous publications,
is an attractive model for intelligent databases because it provides a simple
but versatile format for different kinds of knowledge, it has capabilities in
artificial intelligence, and it can also function like established database
models when that is required.
  This paper describes how the SP model can emulate other models used in
database applications and compares the SP model with those other models. The
artificial intelligence capabilities of the SP model are reviewed and its
relationship with other artificial intelligence systems is described. Also
considered are ways in which current prototypes may be translated into an
'industrial strength' working system.
</summary>
    <author>
      <name>J. Gerard Wolff</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.datak.2006.04.003</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.datak.2006.04.003" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">J G Wolff, Data &amp; Knowledge Engineering 60, 596-624, 2007</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0311031v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0311031v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0311038v1</id>
    <updated>2003-11-25T09:42:59Z</updated>
    <published>2003-11-25T09:42:59Z</published>
    <title>XPath-Logic and XPathLog: A Logic-Programming Style XML Data
  Manipulation Language</title>
    <summary>  We define XPathLog as a Datalog-style extension of XPath. XPathLog provides a
clear, declarative language for querying and manipulating XML whose
perspectives are especially in XML data integration. In our characterization,
the formal semantics is defined wrt. an edge-labeled graph-based model which
covers the XML data model. We give a complete, logic-based characterization of
XML data and the main language concept for XML, XPath. XPath-Logic extends the
XPath language with variable bindings and embeds it into first-order logic.
XPathLog is then the Horn fragment of XPath-Logic, providing a Datalog-style,
rule-based language for querying and manipulating XML data. The model-theoretic
semantics of XPath-Logic serves as the base of XPathLog as a logic-programming
language, whereas also an equivalent answer-set semantics for evaluating
XPathLog queries is given. In contrast to other approaches, the XPath syntax
and semantics is also used for a declarative specification how the database
should be updated: when used in rule heads, XPath filters are interpreted as
specifications of elements and properties which should be added to the
database.
</summary>
    <author>
      <name>Wolfgang May</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0311038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0311038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2; D.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0312041v1</id>
    <updated>2003-12-18T17:29:05Z</updated>
    <published>2003-12-18T17:29:05Z</published>
    <title>Greedy Algorithms in Datalog</title>
    <summary>  In the design of algorithms, the greedy paradigm provides a powerful tool for
solving efficiently classical computational problems, within the framework of
procedural languages. However, expressing these algorithms within the
declarative framework of logic-based languages has proven a difficult research
challenge. In this paper, we extend the framework of Datalog-like languages to
obtain simple declarative formulations for such problems, and propose effective
implementation techniques to ensure computational complexities comparable to
those of procedural formulations. These advances are achieved through the use
of the "choice" construct, extended with preference annotations to effect the
selection of alternative stable-models and nondeterministic fixpoints. We show
that, with suitable storage structures, the differential fixpoint computation
of our programs matches the complexity of procedural algorithms in classical
search and optimization problems.
</summary>
    <author>
      <name>Sergio Greco</name>
    </author>
    <author>
      <name>Carlo Zaniolo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Theory and Practice of Logic Programming, 1(4): 381-407, 2001</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0312041v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0312041v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.1.6; F.3.1; F.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0312046v1</id>
    <updated>2003-12-19T15:25:53Z</updated>
    <published>2003-12-19T15:25:53Z</published>
    <title>On the Abductive or Deductive Nature of Database Schema Validation and
  Update Processing Problems</title>
    <summary>  We show that database schema validation and update processing problems such
as view updating, materialized view maintenance, integrity constraint checking,
integrity constraint maintenance or condition monitoring can be classified as
problems of either abductive or deductive nature, according to the reasoning
paradigm that inherently suites them. This is done by performing abductive and
deductive reasoning on the event rules [Oli91], a set of rules that define the
difference between consecutive database states In this way, we show that it is
possible to provide methods able to deal with all these problems as a whole. We
also show how some existing general deductive and abductive procedures may be
used to reason on the event rules. In this way, we show that these procedures
can deal with all database schema validation and update processing problems
considered in this paper.
</summary>
    <author>
      <name>Ernest Teniente</name>
    </author>
    <author>
      <name>Toni Urpi</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Theory and Practice of Logic Programming 3(3):287-327, may 2003</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0312046v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0312046v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.1;H.2.4; H.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0402003v1</id>
    <updated>2004-02-02T01:42:35Z</updated>
    <published>2004-02-02T01:42:35Z</published>
    <title>Semantic Optimization of Preference Queries</title>
    <summary>  The notion of preference is becoming more and more ubiquitous in present-day
information systems. Preferences are primarily used to filter and personalize
the information reaching the users of such systems. In database systems,
preferences are usually captured as preference relations that are used to build
preference queries. In our approach, preference queries are relational algebra
or SQL queries that contain occurrences of the winnow operator ("find the most
preferred tuples in a given relation").
  We present here a number of semantic optimization techniques applicable to
preference queries. The techniques make use of integrity constraints, and make
it possible to remove redundant occurrences of the winnow operator and to apply
a more efficient algorithm for the computation of winnow. We also study the
propagation of integrity constraints in the result of the winnow. We have
identified necessary and sufficient conditions for the applicability of our
techniques, and formulated those conditions as constraint satisfiability
problems.
</summary>
    <author>
      <name>Jan Chomicki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0402003v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0402003v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.3; F.4.1; I.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0402007v1</id>
    <updated>2004-02-02T20:12:53Z</updated>
    <published>2004-02-02T20:12:53Z</published>
    <title>An Integrated Approach for Extraction of Objects from XML and
  Transformation to Heterogeneous Object Oriented Databases</title>
    <summary>  CERN's (European Organization for Nuclear Research) WISDOM project uses XML
for the replication of data between different data repositories in a
heterogeneous operating system environment. For exchanging data from
Web-resident databases, the data needs to be transformed into XML and back to
the database format. Many different approaches are employed to do this
transformation. This paper addresses issues that make this job more efficient
and robust than existing approaches. It incorporates the World Wide Web
Consortium (W3C) XML Schema specification in the database-XML relationship.
Incorporation of the XML Schema exhibits significant improvements in XML
content usage and reduces the limitations of DTD-based database XML services.
Secondly the paper explores the possibility of database independent
transformation of data between XML and different databases. It proposes a
standard XML format that every serialized object should follow. This makes it
possible to use objects of heterogeneous database seamlessly using XML.
</summary>
    <author>
      <name>Uzair Ahmad</name>
    </author>
    <author>
      <name>Mohammad Waseem Hassan</name>
    </author>
    <author>
      <name>Arshad Ali</name>
    </author>
    <author>
      <name>Richard McClatchey</name>
    </author>
    <author>
      <name>Ian Willers</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 5 figures. Presented at the 5th Int Conf on Enterprise
  Information Systems, ICEIS'03. Angers France April 2003</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0402007v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0402007v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0402008v1</id>
    <updated>2004-02-02T20:18:23Z</updated>
    <published>2004-02-02T20:18:23Z</published>
    <title>A Use-Case Driven Approach in Requirements Engineering : The Mammogrid
  Project</title>
    <summary>  We report on the application of the use-case modeling technique to identify
and specify the user requirements of the MammoGrid project in an incremental
and controlled iterative approach. Modeling has been carried out in close
collaboration with clinicians and radiologists with no prior experience of use
cases. The study reveals the advantages and limitations of applying this
technique to requirements specification in the domains of breast cancer
screening and mammography research, with implications for medical imaging more
generally. In addition, this research has shown a return on investment in
use-case modeling in shorter gaps between phases of the requirements
engineering process. The qualitative result of this analysis leads us to
propose that a use-case modeling approach may result in reducing the cycle of
the requirements engineering process for medical imaging.
</summary>
    <author>
      <name>Mohammed Odeh</name>
    </author>
    <author>
      <name>Tamas Hauer</name>
    </author>
    <author>
      <name>Richard McClatchey</name>
    </author>
    <author>
      <name>Tony Solomonides</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures. Presented at the 7th IASTED Int Conf on Software
  Engineering Applications. Marina del Rey, USA November 2003</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0402008v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0402008v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0402009v1</id>
    <updated>2004-02-03T14:32:39Z</updated>
    <published>2004-02-03T14:32:39Z</published>
    <title>Resolving Clinicians Queries Across a Grids Infrastructure</title>
    <summary>  The past decade has witnessed order of magnitude increases in computing
power, data storage capacity and network speed, giving birth to applications
which may handle large data volumes of increased complexity, distributed over
the Internet. Grids computing promises to resolve many of the difficulties in
facilitating medical image analysis to allow radiologists to collaborate
without having to co-locate. The EU-funded MammoGrid project aims to
investigate the feasibility of developing a Grid-enabled European database of
mammograms and provide an information infrastructure which federates multiple
mammogram databases. This will enable clinicians to develop new common,
collaborative and co-operative approaches to the analysis of mammographic data.
This paper focuses on one of the key requirements for large-scale distributed
mammogram analysis: resolving queries across a grid-connected federation of
images.
</summary>
    <author>
      <name>F Estrella</name>
    </author>
    <author>
      <name>C del Frate</name>
    </author>
    <author>
      <name>T Hauer</name>
    </author>
    <author>
      <name>R McClatchey</name>
    </author>
    <author>
      <name>M Odeh</name>
    </author>
    <author>
      <name>D Rogulin</name>
    </author>
    <author>
      <name>S R Amendolia</name>
    </author>
    <author>
      <name>D Schottlander</name>
    </author>
    <author>
      <name>T Solomonides</name>
    </author>
    <author>
      <name>R Warren</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures. Presented at the 2nd Int Conf on HealthGrids
  Clermont-Ferrand, France January 2004 and accepted by Methods of Information
  in Medicine</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0402009v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0402009v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H2.4; J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0402023v1</id>
    <updated>2004-02-12T15:00:18Z</updated>
    <published>2004-02-12T15:00:18Z</published>
    <title>A Service-Based Approach for Managing Mammography Data</title>
    <summary>  Grid-based technologies are emerging as a potential open-source
standards-based solution for managing and collabo-rating distributed resources.
In view of these new computing solutions, the Mammogrid project is developing a
service-based and Grid-aware application which manages a Euro-pean-wide
database of mammograms. Medical conditions such as breast cancer, and
mammograms as images, are ex-tremely complex with many dimensions of
variability across the population. An effective solution for the management of
disparate mammogram data sources is a federation of autonomous multi-centre
sites which transcends national boundaries. The Mammogrid solution utilizes the
Grid tech-nologies to integrate geographically distributed data sets. The
Mammogrid application will explore the potential of the Grid to support
effective co-working among radiologists through-out the EU. This paper outlines
the Mammogrid service-based approach in managing a federation of grid-connected
mam-mography databases.
</summary>
    <author>
      <name>Florida Estrella</name>
    </author>
    <author>
      <name>Richard McClatchey</name>
    </author>
    <author>
      <name>Dmitry Rogulina</name>
    </author>
    <author>
      <name>Roberto Amendolia</name>
    </author>
    <author>
      <name>Tony Solomonides</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 7 figures. Accepted by the 11th World Congress on Medical
  Informatics (MedInfo'04). San Francisco, USA. September 2004</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0402023v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0402023v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H2.4; J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0402025v1</id>
    <updated>2004-02-12T14:36:05Z</updated>
    <published>2004-02-12T14:36:05Z</published>
    <title>A perspective on the Healthgrid initiative</title>
    <summary>  This paper presents a perspective on the Healthgrid initiative which involves
European projects deploying pioneering applications of grid technology in the
health sector. In the last couple of years, several grid projects have been
funded on health related issues at national and European levels. A crucial
issue is to maximize their cross fertilization in the context of an environment
where data of medical interest can be stored and made easily available to the
different actors in healthcare, physicians, healthcare centres and
administrations, and of course the citizens. The Healthgrid initiative,
represented by the Healthgrid association (http://www.healthgrid.org), was
initiated to bring the necessary long term continuity, to reinforce and promote
awareness of the possibilities and advantages linked to the deployment of GRID
technologies in health. Technologies to address the specific requirements for
medical applications are under development. Results from the DataGrid and other
projects are given as examples of early applications.
</summary>
    <author>
      <name>V. Breton</name>
    </author>
    <author>
      <name>A. E. Solomonides</name>
    </author>
    <author>
      <name>R. H. McClatchey</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CCGrid.2004.1336598</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CCGrid.2004.1336598" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 figure. Accepted by the Second International Workshop on
  Biomedical Computations on the Grid, at the 4th IEEE/ACM International
  Symposium on Cluster Computing and the Grid (CCGrid 2004). Chicago USA, April
  2004</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0402025v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0402025v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H2.4,J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0403018v1</id>
    <updated>2004-03-12T09:57:43Z</updated>
    <published>2004-03-12T09:57:43Z</published>
    <title>The World Wide Telescope: An Archetype for Online Science</title>
    <summary>  Most scientific data will never be directly examined by scientists; rather it
will be put into online databases where it will be analyzed and summarized by
computer programs. Scientists increasingly see their instruments through online
scientific archives and analysis tools, rather than examining the raw data.
Today this analysis is primarily driven by scientists asking queries, but
scientific archives are becoming active databases that self-organize and
recognize interesting and anomalous facts as data arrives. In some fields, data
from many different archives can be cross-correlated to produce new insights.
Astronomy presents an excellent example of these trends; and, federating
Astronomy archives presents interesting challenges for computer scientists.
</summary>
    <author>
      <name>Jim Gray</name>
    </author>
    <author>
      <name>Alexander S. Szalay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, msword posted at
  http://research.microsoft.com/research/pubs/view.aspx?tr_id=590</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">CACM, V. 45.11, pp. 50-54, Nov. 2002</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0403018v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0403018v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0403020v1</id>
    <updated>2004-03-12T10:20:23Z</updated>
    <published>2004-03-12T10:20:23Z</published>
    <title>The Sloan Digital Sky Survey Science Archive: Migrating a Multi-Terabyte
  Astronomical Archive from Object to Relational DBMS</title>
    <summary>  The Sloan Digital Sky Survey Science Archive is the first in a series of
multi-Terabyte digital archives in Astronomy and other data-intensive sciences.
To facilitate data mining in the SDSS archive, we adapted a commercial database
engine and built specialized tools on top of it. Originally we chose an
object-oriented database management system due to its data organization
capabilities, platform independence, query performance and conceptual fit to
the data. However, after using the object database for the first couple of
years of the project, it soon began to fall short in terms of its query support
and data mining performance. This was as much due to the inability of the
database vendor to respond our demands for features and bug fixes as it was due
to their failure to keep up with the rapid improvements in hardware
performance, particularly faster RAID disk systems. In the end, we were forced
to abandon the object database and migrate our data to a relational database.
We describe below the technical issues that we faced with the object database
and how and why we migrated to relational technology.
</summary>
    <author>
      <name>Aniruddha R. Thakar</name>
    </author>
    <author>
      <name>Alexander S. Szalay</name>
    </author>
    <author>
      <name>Peter Z. Kunszt</name>
    </author>
    <author>
      <name>Jim Gray</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Comput.Sci.Eng. 5 (2003) 16-29</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0403020v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0403020v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0403021v1</id>
    <updated>2004-03-12T10:42:25Z</updated>
    <published>2004-03-12T10:42:25Z</published>
    <title>A Quick Look at SATA Disk Performance</title>
    <summary>  We have been investigating the use of low-cost, commodity components for
multi-terabyte SQL Server databases. Dubbed storage bricks, these servers are
white box PCs containing the largest ATA drives, value-priced AMD or Intel
processors, and inexpensive ECC memory. One issue has been the wiring mess, air
flow problems, length restrictions, and connector failures created by seven or
more parallel ATA (PATA) ribbon cables and drives in]a tower or 3U rack-mount
chassis. Large capacity Serial ATA (SATA) drives have recently become widely
available for the PC environment at a reasonable price. In addition to being
faster, the SATA connectors seem more reliable, have a more reasonable length
restriction (1m) and allow better airflow. We tested two drive brands along
with two RAID controllers to evaluate SATA drive performance and reliablility.
This paper documents our results so far.
</summary>
    <author>
      <name>Tom Barclay</name>
    </author>
    <author>
      <name>Wyman Chong</name>
    </author>
    <author>
      <name>Jim Gray</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0403021v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0403021v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0404003v1</id>
    <updated>2004-04-02T02:03:32Z</updated>
    <published>2004-04-02T02:03:32Z</published>
    <title>Enhancing the expressive power of the U-Datalog language</title>
    <summary>  U-Datalog has been developed with the aim of providing a set-oriented logical
update language, guaranteeing update parallelism in the context of a
Datalog-like language. In U-Datalog, updates are expressed by introducing
constraints (+p(X), to denote insertion, and [minus sign]p(X), to denote
deletion) inside Datalog rules. A U-Datalog program can be interpreted as a CLP
program. In this framework, a set of updates (constraints) is satisfiable if it
does not represent an inconsistent theory, that is, it does not require the
insertion and the deletion of the same fact. This approach resembles a very
simple form of negation. However, on the other hand, U-Datalog does not provide
any mechanism to explicitly deal with negative information, resulting in a
language with limited expressive power. In this paper, we provide a semantics,
based on stratification, handling the use of negated atoms in U-Datalog
programs, and we show which problems arise in defining a compositional
semantics.
</summary>
    <author>
      <name>Elisa Bertino</name>
    </author>
    <author>
      <name>Barbara Catania</name>
    </author>
    <author>
      <name>Roberta Gori</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appeared in Theory and Practice of Logic Programming, vol. 1, no. 1,
  2001</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Theory and Practice of Logic Programming, vol. 1, no. 1, 2001</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0404003v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0404003v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.1.6; D.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
