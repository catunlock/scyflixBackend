<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acs.NE%26id_list%3D%26start%3D0%26max_results%3D500" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:cs.NE&amp;id_list=&amp;start=0&amp;max_results=500</title>
  <id>http://arxiv.org/api/RpwnGq+QQZCbV698iWyxgH72kjM</id>
  <updated>2017-10-08T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">4256</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">500</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/cs/0702055v1</id>
    <updated>2007-02-09T13:16:14Z</updated>
    <published>2007-02-09T13:16:14Z</published>
    <title>On the possibility of making the complete computer model of a human
  brain</title>
    <summary>  The development of the algorithm of a neural network building by the
corresponding parts of a DNA code is discussed.
</summary>
    <author>
      <name>A. V. Paraskevov</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0702055v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0702055v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.4495v1</id>
    <updated>2010-09-22T22:32:37Z</updated>
    <published>2010-09-22T22:32:37Z</published>
    <title>Unary Coding for Neural Network Learning</title>
    <summary>  This paper presents some properties of unary coding of significance for
biological learning and instantaneously trained neural networks.
</summary>
    <author>
      <name>Subhash Kak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1009.4495v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.4495v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.7806v1</id>
    <updated>2014-11-28T10:39:24Z</updated>
    <published>2014-11-28T10:39:24Z</published>
    <title>Two Gaussian Approaches to Black-Box Optomization</title>
    <summary>  Outline of several strategies for using Gaussian processes as surrogate
models for the covariance matrix adaptation evolution strategy (CMA-ES).
</summary>
    <author>
      <name>Lukáš Bajer</name>
    </author>
    <author>
      <name>Martin Holeňa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.7806v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.7806v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.05549v1</id>
    <updated>2017-01-19T18:43:56Z</updated>
    <published>2017-01-19T18:43:56Z</published>
    <title>Deep Neural Networks - A Brief History</title>
    <summary>  Introduction to deep neural networks and their history.
</summary>
    <author>
      <name>Krzysztof J. Cios</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.05549v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.05549v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.02746v2</id>
    <updated>2017-07-12T20:08:12Z</updated>
    <published>2017-07-10T08:44:46Z</published>
    <title>Backpropagation in matrix notation</title>
    <summary>  In this note we calculate the gradient of the network function in matrix
notation.
</summary>
    <author>
      <name>N. M. Mishachev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, Remark 6 added</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.02746v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.02746v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0504056v1</id>
    <updated>2005-04-13T13:59:55Z</updated>
    <published>2005-04-13T13:59:55Z</published>
    <title>Self-Organizing Multilayered Neural Networks of Optimal Complexity</title>
    <summary>  The principles of self-organizing the neural networks of optimal complexity
is considered under the unrepresentative learning set. The method of
self-organizing the multi-layered neural networks is offered and used to train
the logical neural networks which were applied to the medical diagnostics.
</summary>
    <author>
      <name>V. Schetinin</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0504056v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0504056v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.0798v1</id>
    <updated>2009-06-03T23:10:25Z</updated>
    <published>2009-06-03T23:10:25Z</published>
    <title>Single Neuron Memories and the Network's Proximity Matrix</title>
    <summary>  This paper extends the treatment of single-neuron memories obtained by the
B-matrix approach. The spreading of the activity within the network is
determined by the network's proximity matrix which represents the separations
amongst the neurons through the neural pathways.
</summary>
    <author>
      <name>Subhash Kak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0906.0798v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.0798v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.3390v1</id>
    <updated>2010-05-19T09:29:39Z</updated>
    <published>2010-05-19T09:29:39Z</published>
    <title>Critical control of a genetic algorithm</title>
    <summary>  Based on speculations coming from statistical mechanics and the conjectured
existence of critical states, I propose a simple heuristic in order to control
the mutation probability and the population size of a genetic algorithm.
</summary>
    <author>
      <name>Raphaël Cerf</name>
    </author>
    <link href="http://arxiv.org/abs/1005.3390v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.3390v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.5081v2</id>
    <updated>2011-06-28T21:32:26Z</updated>
    <published>2011-03-25T20:59:13Z</published>
    <title>Using Variable Threshold to Increase Capacity in a Feedback Neural
  Network</title>
    <summary>  The article presents new results on the use of variable thresholds to
increase the capacity of a feedback neural network. Non-binary networks are
also considered in this analysis.
</summary>
    <author>
      <name>Praveen Kuruvada</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1103.5081v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.5081v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1104.5538v1</id>
    <updated>2011-04-29T02:28:50Z</updated>
    <published>2011-04-29T02:28:50Z</published>
    <title>Complex Networks</title>
    <summary>  Introduction to the Special Issue on Complex Networks, Artificial Life
journal.
</summary>
    <author>
      <name>Carlos Gershenson</name>
    </author>
    <author>
      <name>Mikhail Prokopenko</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1162/artl_e_00037</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1162/artl_e_00037" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, in press</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Artificial Life 17(4):259--261. 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1104.5538v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.5538v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.1534v1</id>
    <updated>2011-05-08T16:47:35Z</updated>
    <published>2011-05-08T16:47:35Z</published>
    <title>Taking the redpill: Artificial Evolution in native x86 systems</title>
    <summary>  In analogon to successful artificial evolution simulations as Tierra or
avida, this text presents a way to perform artificial evolution in a native x86
system. The implementation of the artificial chemistry and first results of
statistical experiments are presented.
</summary>
    <author>
      <name>Thomas Sperl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1105.1534v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.1534v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.6223v1</id>
    <updated>2011-06-30T13:38:35Z</updated>
    <published>2011-06-30T13:38:35Z</published>
    <title>Why 'GSA: A Gravitational Search Algorithm' Is Not Genuinely Based on
  the Law of Gravity</title>
    <summary>  Why 'GSA: A Gravitational Search Algorithm' Is Not Genuinely Based on the Law
of Gravity
</summary>
    <author>
      <name>Melvin Gauci</name>
    </author>
    <author>
      <name>Tony J. Dodd</name>
    </author>
    <author>
      <name>Roderich Gross</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11047-012-9322-0</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11047-012-9322-0" rel="related"/>
    <link href="http://arxiv.org/abs/1106.6223v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.6223v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.5674v1</id>
    <updated>2013-07-22T12:16:08Z</updated>
    <published>2013-07-22T12:16:08Z</published>
    <title>Solving Traveling Salesman Problem by Marker Method</title>
    <summary>  In this paper we use marker method and propose a new mutation operator that
selects the nearest neighbor among all near neighbors solving Traveling
Salesman Problem.
</summary>
    <author>
      <name>Masoumeh Vali</name>
    </author>
    <link href="http://arxiv.org/abs/1307.5674v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.5674v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.4675v2</id>
    <updated>2017-06-30T03:01:50Z</updated>
    <published>2013-08-16T04:36:03Z</published>
    <title>Genetic Algorithm for Solving Simple Mathematical Equality Problem</title>
    <summary>  This paper explains genetic algorithm for novice in this field. Basic
philosophy of genetic algorithm and its flowchart are described. Step by step
numerical computation of genetic algorithm for solving simple mathematical
equality problem will be briefly explained
</summary>
    <author>
      <name>Denny Hermawanto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Tutorial paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1308.4675v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.4675v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.5997v2</id>
    <updated>2014-04-26T23:10:51Z</updated>
    <published>2014-04-23T22:37:56Z</published>
    <title>One weird trick for parallelizing convolutional neural networks</title>
    <summary>  I present a new way to parallelize the training of convolutional neural
networks across multiple GPUs. The method scales significantly better than all
alternatives when applied to modern convolutional neural networks.
</summary>
    <author>
      <name>Alex Krizhevsky</name>
    </author>
    <link href="http://arxiv.org/abs/1404.5997v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.5997v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.02128v1</id>
    <updated>2015-01-09T13:21:11Z</updated>
    <published>2015-01-09T13:21:11Z</published>
    <title>Introduction and Ranking Results of the ICSI 2014 Competition on Single
  Objective Optimization</title>
    <summary>  This technical report includes the introduction and ranking results of the
ICSI 2014 Competition on Single Objective Optimization.
</summary>
    <author>
      <name>Ying Tan</name>
    </author>
    <author>
      <name>Junzhi Li</name>
    </author>
    <author>
      <name>Zhongyang Zheng</name>
    </author>
    <link href="http://arxiv.org/abs/1501.02128v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.02128v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.01524v1</id>
    <updated>2015-03-05T03:29:16Z</updated>
    <published>2015-03-05T03:29:16Z</published>
    <title>Genetic optimization of the Hyperloop route through the Grapevine</title>
    <summary>  We demonstrate a genetic algorithm that employs a versatile fitness function
to optimize route selection for the Hyperloop, a proposed high speed passenger
transportation system.
</summary>
    <author>
      <name>Casey J. Handmer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 4 figures, 1 Mathematica notebook attachment</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.01524v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.01524v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.04306v1</id>
    <updated>2016-06-14T10:55:53Z</updated>
    <published>2016-06-14T10:55:53Z</published>
    <title>Viral Search algorithm</title>
    <summary>  The article, after a brief introduction on genetic algorithms and their
functioning, presents a kind of genetic algorithm called Viral Search. We
present the key concepts, we formally derive the algorithm and we perform
numerical tests designed to illustrate the potential and limits.
</summary>
    <author>
      <name>Matteo Gardini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, in Italian, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.04306v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04306v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0408006v1</id>
    <updated>2004-08-01T15:04:53Z</updated>
    <published>2004-08-01T15:04:53Z</published>
    <title>Why Two Sexes?</title>
    <summary>  Evolutionary role of the separation into two sexes from a cyberneticist's
point of view. [I translated this 1965 article from Russian "Nauka i Zhizn"
(Science and Life) in 1988. In a popular form, the article puts forward several
useful ideas not all of which even today are necessarily well known or widely
accepted. Boris Lubachevsky, bdl@bell-labs.com ]
</summary>
    <author>
      <name>Vigen A. Geodakian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Nauka i zhizn (Science and Life), 1965</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0408006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0408006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.1; J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0504068v1</id>
    <updated>2005-04-14T10:45:06Z</updated>
    <published>2005-04-14T10:45:06Z</published>
    <title>Self-Organization of the Neuron Collective of Optimal Complexity</title>
    <summary>  The optimal complexity of neural networks is achieved when the
self-organization principles is used to eliminate the contradictions existing
in accordance with the K. Godel theorem about incompleteness of the systems
based on axiomatics. The principle of S. Beer exterior addition the Heuristic
Group Method of Data Handling by A. Ivakhnenko realized is used.
</summary>
    <author>
      <name>V. Schetinin</name>
    </author>
    <author>
      <name>A. Kostunin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NOLTA-1996, Japan</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0504068v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0504068v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0505016v1</id>
    <updated>2005-05-07T20:56:58Z</updated>
    <published>2005-05-07T20:56:58Z</published>
    <title>Visual Character Recognition using Artificial Neural Networks</title>
    <summary>  The recognition of optical characters is known to be one of the earliest
applications of Artificial Neural Networks, which partially emulate human
thinking in the domain of artificial intelligence. In this paper, a simplified
neural approach to recognition of optical or visual characters is portrayed and
discussed. The document is expected to serve as a resource for learners and
amateur investigators in pattern recognition, neural networking and related
disciplines.
</summary>
    <author>
      <name>Shashank Araokar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, tutorial resource</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0505016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0505016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0505019v1</id>
    <updated>2005-05-10T06:37:31Z</updated>
    <published>2005-05-10T06:37:31Z</published>
    <title>Artificial Neural Networks and their Applications</title>
    <summary>  The Artificial Neural network is a functional imitation of simplified model
of the biological neurons and their goal is to construct useful computers for
real world problems. The ANN applications have increased dramatically in the
last few years fired by both theoretical and practical applications in a wide
variety of applications. A brief theory of ANN is presented and potential areas
are identified and future trends are discussed.
</summary>
    <author>
      <name>Nitin Malik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0505019v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0505019v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0601129v1</id>
    <updated>2006-01-30T22:02:47Z</updated>
    <published>2006-01-30T22:02:47Z</published>
    <title>Instantaneously Trained Neural Networks</title>
    <summary>  This paper presents a review of instantaneously trained neural networks
(ITNNs). These networks trade learning time for size and, in the basic model, a
new hidden node is created for each training sample. Various versions of the
corner-classification family of ITNNs, which have found applications in
artificial intelligence (AI), are described. Implementation issues are also
considered.
</summary>
    <author>
      <name>Abhilash Ponnath</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0601129v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0601129v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0607019v1</id>
    <updated>2006-07-06T18:49:20Z</updated>
    <published>2006-07-06T18:49:20Z</published>
    <title>Modelling the Probability Density of Markov Sources</title>
    <summary>  This paper introduces an objective function that seeks to minimise the
average total number of bits required to encode the joint state of all of the
layers of a Markov source. This type of encoder may be applied to the problem
of optimising the bottom-up (recognition model) and top-down (generative model)
connections in a multilayer neural network, and it unifies several previous
results on the optimisation of multilayer neural networks.
</summary>
    <author>
      <name>Stephen Luttrell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0607019v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0607019v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0703134v5</id>
    <updated>2008-01-07T13:04:20Z</updated>
    <published>2007-03-27T16:57:39Z</published>
    <title>Automatic Generation of Benchmarks for Plagiarism Detection Tools using
  Grammatical Evolution</title>
    <summary>  This paper has been withdrawn by the authors due to a major rewriting.
</summary>
    <author>
      <name>Manuel Cebrian</name>
    </author>
    <author>
      <name>Manuel Alfonseca</name>
    </author>
    <author>
      <name>Alfonso Ortega</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0703134v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0703134v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.1; I.2.2; D.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0708.2686v1</id>
    <updated>2007-07-24T18:50:22Z</updated>
    <published>2007-07-24T18:50:22Z</published>
    <title>The universal evolutionary computer based on super-recursive algorithms
  of evolvability</title>
    <summary>  This work exposes which mechanisms and procesess in the Nature of evolution
compute a function not computable by Turing machine. The computer with
intelligence that is not higher than one bacteria population could have, but
with efficency to solve the problems that are non-computable by Turing machine
is represented. This theoretical construction is called Universal Evolutinary
Computer and it is based on the superecursive algorithms of evolvability.
</summary>
    <author>
      <name>D. Roglic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 1 table, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/0708.2686v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0708.2686v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.1; H.1.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0802.3875v1</id>
    <updated>2008-02-26T19:07:53Z</updated>
    <published>2008-02-26T19:07:53Z</published>
    <title>Are complex systems hard to evolve?</title>
    <summary>  Evolutionary complexity is here measured by the number of trials/evaluations
needed for evolving a logical gate in a non-linear medium. Behavioural
complexity of the gates evolved is characterised in terms of cellular automata
behaviour. We speculate that hierarchies of behavioural and evolutionary
complexities are isomorphic up to some degree, subject to substrate specificity
of evolution and the spectrum of evolution parameters.
</summary>
    <author>
      <name>Andy Adamatzky</name>
    </author>
    <author>
      <name>Larry Bull</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1002/cplx.20269</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1002/cplx.20269" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Volume 14, Issue 6, pages 15-20, July/August 2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0802.3875v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0802.3875v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0904.1888v1</id>
    <updated>2009-04-13T00:59:10Z</updated>
    <published>2009-04-13T00:59:10Z</published>
    <title>On Fodor on Darwin on Evolution</title>
    <summary>  Jerry Fodor argues that Darwin was wrong about "natural selection" because
(1) it is only a tautology rather than a scientific law that can support
counterfactuals ("If X had happened, Y would have happened") and because (2)
only minds can select. Hence Darwin's analogy with "artificial selection" by
animal breeders was misleading and evolutionary explanation is nothing but
post-hoc historical narrative. I argue that Darwin was right on all counts.
</summary>
    <author>
      <name>Stevan Harnad</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21</arxiv:comment>
    <link href="http://arxiv.org/abs/0904.1888v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0904.1888v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0905.0677v1</id>
    <updated>2009-05-05T19:45:13Z</updated>
    <published>2009-05-05T19:45:13Z</published>
    <title>Feasibility of random basis function approximators for modeling and
  control</title>
    <summary>  We discuss the role of random basis function approximators in modeling and
control. We analyze the published work on random basis function approximators
and demonstrate that their favorable error rate of convergence O(1/n) is
guaranteed only with very substantial computational resources. We also discuss
implications of our analysis for applications of neural networks in modeling
and control.
</summary>
    <author>
      <name>Ivan Tyukin</name>
    </author>
    <author>
      <name>Danil Prokhorov</name>
    </author>
    <link href="http://arxiv.org/abs/0905.0677v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0905.0677v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0905.2649v1</id>
    <updated>2009-05-16T02:34:32Z</updated>
    <published>2009-05-16T02:34:32Z</published>
    <title>An Immune System Inspired Approach to Automated Program Verification</title>
    <summary>  An immune system inspired Artificial Immune System (AIS) algorithm is
presented, and is used for the purposes of automated program verification.
Relevant immunological concepts are discussed and the field of AIS is briefly
reviewed. It is proposed to use this AIS algorithm for a specific automated
program verification task: that of predicting shape of program invariants. It
is shown that the algorithm correctly predicts program invariant shape for a
variety of benchmarked programs.
</summary>
    <author>
      <name>Soumya Banerjee</name>
    </author>
    <link href="http://arxiv.org/abs/0905.2649v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0905.2649v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.0516v1</id>
    <updated>2009-07-03T02:21:36Z</updated>
    <published>2009-07-03T02:21:36Z</published>
    <title>Adaptation and Self-Organization in Evolutionary Algorithms</title>
    <summary>  Abbreviated Abstract: The objective of Evolutionary Computation is to solve
practical problems (e.g. optimization, data mining) by simulating the
mechanisms of natural evolution. This thesis addresses several topics related
to adaptation and self-organization in evolving systems with the overall aims
of improving the performance of Evolutionary Algorithms (EA), understanding its
relation to natural evolution, and incorporating new mechanisms for mimicking
complex biological systems.
</summary>
    <author>
      <name>James M Whitacre</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PhD Thesis</arxiv:comment>
    <link href="http://arxiv.org/abs/0907.0516v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.0516v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0911.2324v1</id>
    <updated>2009-11-12T08:48:38Z</updated>
    <published>2009-11-12T08:48:38Z</published>
    <title>Deterministic Autopoietic Automata</title>
    <summary>  This paper studies two issues related to the paper on Computing by
Self-reproduction: Autopoietic Automata by Jiri Wiedermann. It is shown that
all results presented there extend to deterministic computations. In
particular, nondeterminism is not needed for a lineage to generate all
autopoietic automata.
</summary>
    <author>
      <name>Martin Fürer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.9.6</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.9.6" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 9, 2009, pp. 49-53</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0911.2324v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.2324v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.2280v2</id>
    <updated>2010-09-23T16:27:39Z</updated>
    <published>2010-04-13T22:12:22Z</published>
    <title>XOR at a Single Vertex -- Artificial Dendrites</title>
    <summary>  New to neuroscience with implications for AI, the exclusive OR, or any other
Boolean gate may be biologically accomplished within a single region where
active dendrites merge. This is demonstrated below using dynamic circuit
analysis. Medical knowledge aside, this observation points to the possibility
of specially coated conductors to accomplish artificial dendrites.
</summary>
    <author>
      <name>John Robert Burger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Edited for clarity; added Kandel reference</arxiv:comment>
    <link href="http://arxiv.org/abs/1004.2280v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.2280v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.6.1; C.1.3; I.2.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.1434v1</id>
    <updated>2010-06-08T01:17:00Z</updated>
    <published>2010-06-08T01:17:00Z</published>
    <title>Computing by Means of Physics-Based Optical Neural Networks</title>
    <summary>  We report recent research on computing with biology-based neural network
models by means of physics-based opto-electronic hardware. New technology
provides opportunities for very-high-speed computation and uncovers problems
obstructing the wide-spread use of this new capability. The Computation
Modeling community may be able to offer solutions to these cross-boundary
research problems.
</summary>
    <author>
      <name>A. Steven Younger</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Missouri State University</arxiv:affiliation>
    </author>
    <author>
      <name>Emmett Redd</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Missouri State University</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.26.15</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.26.15" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 26, 2010, pp. 159-167</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1006.1434v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.1434v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1007.0417v1</id>
    <updated>2010-07-02T18:05:29Z</updated>
    <published>2010-07-02T18:05:29Z</published>
    <title>Delta Learning Rule for the Active Sites Model</title>
    <summary>  This paper reports the results on methods of comparing the memory retrieval
capacity of the Hebbian neural network which implements the B-Matrix approach,
by using the Widrow-Hoff rule of learning. We then, extend the recently
proposed Active Sites model by developing a delta rule to increase memory
capacity. Also, this paper extends the binary neural network to a multi-level
(non-binary) neural network.
</summary>
    <author>
      <name>Krishna Chaithanya Lingashetty</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 Pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1007.0417v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1007.0417v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.0915v1</id>
    <updated>2010-09-05T13:17:01Z</updated>
    <published>2010-09-05T13:17:01Z</published>
    <title>Results of Evolution Supervised by Genetic Algorithms</title>
    <summary>  A series of results of evolution supervised by genetic algorithms with
interest to agricultural and horticultural fields are reviewed. New obtained
original results from the use of genetic algorithms on structure-activity
relationships are reported.
</summary>
    <author>
      <name>Lorentz Jäntschi</name>
    </author>
    <author>
      <name>Sorana D. Bolboac{\ba}</name>
    </author>
    <author>
      <name>Mugur C. Bălan</name>
    </author>
    <author>
      <name>Radu E. Sestraş</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 Table, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1009.0915v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.0915v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="78M32" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.6.4; J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1011.5866v1</id>
    <updated>2010-11-26T19:46:56Z</updated>
    <published>2010-11-26T19:46:56Z</published>
    <title>Evolving difficult SAT instances thanks to local search</title>
    <summary>  We propose to use local search algorithms to produce SAT instances which are
harder to solve than randomly generated k-CNF formulae. The first results,
obtained with rudimentary search algorithms, show that the approach deserves
further study. It could be used as a test of robustness for SAT solvers, and
could help to investigate how branching heuristics, learning strategies, and
other aspects of solvers impact there robustness.
</summary>
    <author>
      <name>Olivier Bailleux</name>
    </author>
    <link href="http://arxiv.org/abs/1011.5866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.5866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.1589v1</id>
    <updated>2011-08-08T00:00:13Z</updated>
    <published>2011-08-08T00:00:13Z</published>
    <title>Imitation of Life: Advanced system for native Artificial Evolution</title>
    <summary>  A model for artificial evolution in native x86 Windows systems has been
developed at the end of 2010. In this text, further improvements and additional
analogies to natural microbiologic processes are presented. Several experiments
indicate the capability of the system - and raise the question of possible
countermeasures.
</summary>
    <author>
      <name>Thomas Sperl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1108.1589v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.1589v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.4080v1</id>
    <updated>2011-08-20T02:11:57Z</updated>
    <published>2011-08-20T02:11:57Z</published>
    <title>Convergence Properties of Two (μ + λ) Evolutionary
  Algorithms On OneMax and Royal Roads Test Functions</title>
    <summary>  We present a number of bounds on convergence time for two elitist
population-based Evolutionary Algorithms using a recombination operator
k-Bit-Swap and a mainstream Randomized Local Search algorithm. We study the
effect of distribution of elite species and population size.
</summary>
    <author>
      <name>Aram Ter-Sarkisov</name>
    </author>
    <author>
      <name>Stephen Marsland</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted for ECTA 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1108.4080v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.4080v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.6441v1</id>
    <updated>2011-09-29T08:53:36Z</updated>
    <published>2011-09-29T08:53:36Z</published>
    <title>Memetic Algorithms: Parametrization and Balancing Local and Global
  Search</title>
    <summary>  This is a preprint of a book chapter from the Handbook of Memetic Algorithms,
Studies in Computational Intelligence, Vol. 379, ISBN 978-3-642-23246-6,
Springer, edited by F. Neri, C. Cotta, and P. Moscato. It is devoted to the
parametrization of memetic algorithms and how to find a good balance between
global and local search.
</summary>
    <author>
      <name>Dirk Sudholt</name>
    </author>
    <link href="http://arxiv.org/abs/1109.6441v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.6441v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.1353v1</id>
    <updated>2011-11-05T21:43:56Z</updated>
    <published>2011-11-05T21:43:56Z</published>
    <title>An efficient implementation of the simulated annealing heuristic for the
  quadratic assignment problem</title>
    <summary>  The quadratic assignment problem (QAP) is one of the most difficult
combinatorial optimization problems. One of the most powerful and commonly used
heuristics to obtain approximations to the optimal solution of the QAP is
simulated annealing (SA). We present an efficient implementation of the SA
heuristic which performs more than 100 times faster then existing
implementations for large problem sizes and a large number of SA iterations.
</summary>
    <author>
      <name>Gerald Paul</name>
    </author>
    <link href="http://arxiv.org/abs/1111.1353v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.1353v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.6722v1</id>
    <updated>2012-06-28T15:13:37Z</updated>
    <published>2012-06-28T15:13:37Z</published>
    <title>Piecewise Linear Topology, Evolutionary Algorithms, and Optimization
  Problems</title>
    <summary>  Schemata theory, Markov chains, and statistical mechanics have been used to
explain how evolutionary algorithms (EAs) work. Incremental success has been
achieved with all of these methods, but each has been stymied by limitations
related to its less-than-global view. We show that moving the investigation
into topological space improves our understanding of why EAs work.
</summary>
    <author>
      <name>Andrew Clark</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PDF from Word docx, 11 pages, no figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.6722v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.6722v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.6.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.4009v1</id>
    <updated>2012-08-20T13:54:26Z</updated>
    <published>2012-08-20T13:54:26Z</published>
    <title>Learning sparse messages in networks of neural cliques</title>
    <summary>  An extension to a recently introduced binary neural network is proposed in
order to allow the learning of sparse messages, in large numbers and with high
memory efficiency. This new network is justified both in biological and
informational terms. The learning and retrieval rules are detailed and
illustrated by various simulation results.
</summary>
    <author>
      <name>Behrooz Kamary Aliabadi</name>
    </author>
    <author>
      <name>Claude Berrou</name>
    </author>
    <author>
      <name>Vincent Gripon</name>
    </author>
    <author>
      <name>Xiaoran Jiang</name>
    </author>
    <link href="http://arxiv.org/abs/1208.4009v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.4009v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.3909v2</id>
    <updated>2015-08-17T00:38:32Z</updated>
    <published>2012-09-18T11:12:23Z</published>
    <title>Network Routing Optimization Using Swarm Intelligence</title>
    <summary>  The aim of this paper is to highlight and explore a traditional problem,
which is the minimum spanning tree, and finding the shortest-path in network
routing, by using Swarm Intelligence. This work to be considered as an
investigation topic with combination between operations research, discrete
mathematics, and evolutionary computing aiming to solve one of networking
problems.
</summary>
    <author>
      <name>Mohamed A. El Galil</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 Pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.3909v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.3909v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.6082v1</id>
    <updated>2012-10-22T22:59:58Z</updated>
    <published>2012-10-22T22:59:58Z</published>
    <title>Interplay: Dispersed Activation in Neural Networks</title>
    <summary>  This paper presents a multi-point stimulation of a Hebbian neural network
with investigation of the interplay between the stimulus waves through the
neurons of the network. Equilibrium of the resulting memory is achieved for
recall of specific memory data at a rate faster than single point stimulus. The
interplay of the intersecting stimuli appears to parallel the clarification
process of recall in biological systems.
</summary>
    <author>
      <name>Richard L. Churchill</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1210.6082v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.6082v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.0639v2</id>
    <updated>2017-03-17T20:06:29Z</updated>
    <published>2012-12-04T08:48:29Z</published>
    <title>Evaluation of Particle Swarm Optimization Algorithms for Weighted
  Max-Sat Problem: Technical Report</title>
    <summary>  An experimental evaluation is conducted to asses the performance of 4
different Particle Swarm Optimization neighborhood structures in solving
Max-Sat problem. The experiment has shown that none of the algorithms achieves
statistically significant performance over the others under confidence level of
0.05.
</summary>
    <author>
      <name>Osama Khalil</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Not useful</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.0639v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.0639v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.0254v3</id>
    <updated>2013-05-03T16:05:32Z</updated>
    <published>2012-12-27T23:04:01Z</published>
    <title>Group theory, group actions, evolutionary algorithms, and global
  optimization</title>
    <summary>  In this paper we use group, action and orbit to understand how evolutionary
solve nonconvex optimization problems.
</summary>
    <author>
      <name>Andrew Clark</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.0254v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.0254v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.RA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="90-02, 90-08, 90B99, 37D05" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.1; G.1.6; G.2.0; I.1.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.7056v1</id>
    <updated>2013-05-30T10:28:06Z</updated>
    <published>2013-05-30T10:28:06Z</published>
    <title>Dienstplanerstellung in Krankenhaeusern mittels genetischer Algorithmen</title>
    <summary>  This thesis investigates the use of problem-specific knowledge to enhance a
genetic algorithm approach to multiple-choice optimisation problems. It shows
that such information can significantly enhance performance, but that the
choice of information and the way it is included are important factors for
success.
</summary>
    <author>
      <name>Uwe Aickelin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Diplomarbeit, in German, Universitaet Mannheim, 1996</arxiv:comment>
    <link href="http://arxiv.org/abs/1305.7056v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.7056v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.5667v1</id>
    <updated>2013-06-24T16:35:37Z</updated>
    <published>2013-06-24T16:35:37Z</published>
    <title>Using Genetic Programming to Model Software</title>
    <summary>  We study a generic program to investigate the scope for automatically
customising it for a vital current task, which was not considered when it was
first written. In detail, we show genetic programming (GP) can evolve models of
aspects of BLAST's output when it is used to map Solexa Next-Gen DNA sequences
to the human genome.
</summary>
    <author>
      <name>W. B. Langdon</name>
    </author>
    <author>
      <name>M. Harman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">As UCL computer science Technical Report RN/13/12</arxiv:comment>
    <link href="http://arxiv.org/abs/1306.5667v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.5667v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.5519v1</id>
    <updated>2013-07-21T11:29:16Z</updated>
    <published>2013-07-21T11:29:16Z</published>
    <title>Optimal Recombination in Genetic Algorithms</title>
    <summary>  This paper surveys results on complexity of the optimal recombination problem
(ORP), which consists in finding the best possible offspring as a result of a
recombination operator in a genetic algorithm, given two parent solutions. We
consider efficient reductions of the ORPs, allowing to establish polynomial
solvability or NP-hardness of the ORPs, as well as direct proofs of hardness
results.
</summary>
    <author>
      <name>Anton V. Eremeev</name>
    </author>
    <author>
      <name>Julia V. Kovalenko</name>
    </author>
    <link href="http://arxiv.org/abs/1307.5519v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.5519v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.6995v2</id>
    <updated>2013-08-02T21:04:24Z</updated>
    <published>2013-07-26T11:11:47Z</published>
    <title>Finite State Machine Synthesis for Evolutionary Hardware</title>
    <summary>  This article considers application of genetic algorithms for finite machine
synthesis. The resulting genetic finite state machines synthesis algorithm
allows for creation of machines with less number of states and within shorter
time. This makes it possible to use hardware-oriented genetic finite machines
synthesis algorithm in autonomous systems on reconfigurable platforms.
</summary>
    <author>
      <name>Andrey Bereza</name>
    </author>
    <author>
      <name>Maksim Lyashov</name>
    </author>
    <author>
      <name>Luis Blanco</name>
    </author>
    <link href="http://arxiv.org/abs/1307.6995v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.6995v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.4044v1</id>
    <updated>2013-12-14T13:33:48Z</updated>
    <published>2013-12-14T13:33:48Z</published>
    <title>CACO : Competitive Ant Colony Optimization, A Nature-Inspired
  Metaheuristic For Large-Scale Global Optimization</title>
    <summary>  Large-scale problems are nonlinear problems that need metaheuristics, or
global optimization algorithms. This paper reviews nature-inspired
metaheuristics, then it introduces a framework named Competitive Ant Colony
Optimization inspired by the chemical communications among insects. Then a case
study is presented to investigate the proposed framework for large-scale global
optimization.
</summary>
    <author>
      <name>M. A. El-Dosuky</name>
    </author>
    <link href="http://arxiv.org/abs/1312.4044v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.4044v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.4696v1</id>
    <updated>2014-01-19T16:42:57Z</updated>
    <published>2014-01-19T16:42:57Z</published>
    <title>Evolutionary Optimization for Decision Making under Uncertainty</title>
    <summary>  Optimizing decision problems under uncertainty can be done using a variety of
solution methods. Soft computing and heuristic approaches tend to be powerful
for solving such problems. In this overview article, we survey Evolutionary
Optimization techniques to solve Stochastic Programming problems - both for the
single-stage and multi-stage case.
</summary>
    <author>
      <name>Ronald Hochreiter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Keynote talk at the MENDEL 2011</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of MENDEL 2011: 107-113. 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1401.4696v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.4696v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.4699v1</id>
    <updated>2014-02-19T15:33:35Z</updated>
    <published>2014-02-19T15:33:35Z</published>
    <title>A Powerful Genetic Algorithm for Traveling Salesman Problem</title>
    <summary>  This paper presents a powerful genetic algorithm(GA) to solve the traveling
salesman problem (TSP). To construct a powerful GA, I use edge swapping(ES)
with a local search procedure to determine good combinations of building blocks
of parent solutions for generating even better offspring solutions.
Experimental results on well studied TSP benchmarks demonstrate that the
proposed GA is competitive in finding very high quality solutions on instances
with up to 16,862 cities.
</summary>
    <author>
      <name>Shujia Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1402.4699v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.4699v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.3115v1</id>
    <updated>2014-03-12T21:19:26Z</updated>
    <published>2014-03-12T21:19:26Z</published>
    <title>Memory Capacity of Neural Networks using a Circulant Weight Matrix</title>
    <summary>  This paper presents results on the memory capacity of a generalized feedback
neural network using a circulant matrix. Children are capable of learning soon
after birth which indicates that the neural networks of the brain have prior
learnt capacity that is a consequence of the regular structures in the brain's
organization. Motivated by this idea, we consider the capacity of circulant
matrices as weight matrices in a feedback network.
</summary>
    <author>
      <name>Vamsi Sashank Kotagiri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1403.3115v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.3115v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.1958v2</id>
    <updated>2015-08-22T00:51:00Z</updated>
    <published>2014-05-08T15:05:28Z</published>
    <title>A Self-Adaptive Network Protection System</title>
    <summary>  In this treatise we aim to build a hybrid network automated (self-adaptive)
security threats discovery and prevention system; by using unconventional
techniques and methods, including fuzzy logic and biological inspired
algorithms under the context of soft computing.
</summary>
    <author>
      <name>Mohamed Hassan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">91. arXiv admin note: text overlap with arXiv:1204.1336 by other
  authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.1958v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.1958v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.2507v4</id>
    <updated>2014-08-07T05:17:20Z</updated>
    <published>2014-06-10T11:00:54Z</published>
    <title>WebAL-1: Workshop on Artificial Life and the Web 2014 Proceedings</title>
    <summary>  Proceedings of WebAL-1: Workshop on Artificial Life and the Web 2014, held at
the 14th International Conference on the Synthesis and Simulation of Living
Systems (ALIFE 14), New York, NY, 31 July 2014.
</summary>
    <author>
      <name>Tim Taylor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Editors: Tim Taylor, Josh Auerbach, Josh Bongard, Jeff Clune, Simon
  Hickinbotham, Greg Hornby</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.2507v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.2507v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.2671v1</id>
    <updated>2014-06-10T19:21:33Z</updated>
    <published>2014-06-10T19:21:33Z</published>
    <title>Conceptors: an easy introduction</title>
    <summary>  Conceptors provide an elementary neuro-computational mechanism which sheds a
fresh and unifying light on a diversity of cognitive phenomena. A number of
demanding learning and processing tasks can be solved with unprecedented ease,
robustness and accuracy. Some of these tasks were impossible to solve before.
This entirely informal paper introduces the basic principles of conceptors and
highlights some of their usages.
</summary>
    <author>
      <name>Herbert Jaeger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.2671v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.2671v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.3100v1</id>
    <updated>2014-06-12T02:08:31Z</updated>
    <published>2014-06-12T02:08:31Z</published>
    <title>Learning ELM network weights using linear discriminant analysis</title>
    <summary>  We present an alternative to the pseudo-inverse method for determining the
hidden to output weight values for Extreme Learning Machines performing
classification tasks. The method is based on linear discriminant analysis and
provides Bayes optimal single point estimates for the weight values.
</summary>
    <author>
      <name>Philip de Chazal</name>
    </author>
    <author>
      <name>Jonathan Tapson</name>
    </author>
    <author>
      <name>André van Schaik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In submission to the ELM 2014 conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.3100v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.3100v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.5719v1</id>
    <updated>2014-07-22T03:11:28Z</updated>
    <published>2014-07-22T03:11:28Z</published>
    <title>Artificial Life and the Web: WebAL Comes of Age</title>
    <summary>  A brief survey is presented of the first 18 years of web-based Artificial
Life ("WebAL") research and applications, covering the period 1995-2013. The
survey is followed by a short discussion of common methodologies employed and
current technologies relevant to WebAL research. The paper concludes with a
quick look at what the future may hold for work in this exciting area.
</summary>
    <author>
      <name>Tim Taylor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at WebAL-1: Workshop on Artificial Life and the Web 2014
  (arXiv:1406.2507)</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.5719v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.5719v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.7737v1</id>
    <updated>2014-07-29T14:26:57Z</updated>
    <published>2014-07-29T14:26:57Z</published>
    <title>A CUDA-Based Real Parameter Optimization Benchmark</title>
    <summary>  Benchmarking is key for developing and comparing optimization algorithms. In
this paper, a CUDA-based real parameter optimization benchmark (cuROB) is
introduced. Test functions of diverse properties are included within cuROB and
implemented efficiently with CUDA. Speedup of one order of magnitude can be
achieved in comparison with CPU-based benchmark of CEC'14.
</summary>
    <author>
      <name>Ke Ding</name>
    </author>
    <author>
      <name>Ying Tan</name>
    </author>
    <link href="http://arxiv.org/abs/1407.7737v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.7737v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.3277v1</id>
    <updated>2014-11-12T18:56:45Z</updated>
    <published>2014-11-12T18:56:45Z</published>
    <title>Using Ants as a Genetic Crossover Operator in GLS to Solve STSP</title>
    <summary>  Ant Colony Algorithm (ACA) and Genetic Local Search (GLS) are two
optimization algorithms that have been successfully applied to the Traveling
Salesman Problem (TSP). In this paper we define new crossover operator then
redefine ACAs ants as operate according to defined crossover operator then put
forward our GLS that uses these ants to solve Symmetric TSP (STSP) instances.
</summary>
    <author>
      <name>Hassan Ismkhan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2010 International Conference of Soft Computing and Pattern
  Recognition</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.3277v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.3277v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.5067v1</id>
    <updated>2014-12-16T16:29:34Z</updated>
    <published>2014-12-16T16:29:34Z</published>
    <title>Analysis of Optimal Recombination in Genetic Algorithm for a Scheduling
  Problem with Setups</title>
    <summary>  In this paper, we perform an experimental study of optimal recombination
operator for makespan minimization problem on single machine with
sequence-dependent setup times ($1|s_{vu}|C_{\max}$). The computational
experiment on benchmark problems from TSPLIB library indicates practical
applicability of optimal recombination in crossover operator of genetic
algorithm for $1|s_{vu}|C_{\max}$.
</summary>
    <author>
      <name>A. V. Eremeev</name>
    </author>
    <author>
      <name>Ju. V. Kovalenko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, in Russian</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.5067v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.5067v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.00299v1</id>
    <updated>2015-01-01T18:37:36Z</updated>
    <published>2015-01-01T18:37:36Z</published>
    <title>Sequence Modeling using Gated Recurrent Neural Networks</title>
    <summary>  In this paper, we have used Recurrent Neural Networks to capture and model
human motion data and generate motions by prediction of the next immediate data
point at each time-step. Our RNN is armed with recently proposed Gated
Recurrent Units which has shown promising results in some sequence modeling
problems such as Machine Translation and Speech Synthesis. We demonstrate that
this model is able to capture long-term dependencies in data and generate
realistic motions.
</summary>
    <author>
      <name>Mohammad Pezeshki</name>
    </author>
    <link href="http://arxiv.org/abs/1501.00299v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.00299v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.00130v1</id>
    <updated>2015-01-31T16:10:29Z</updated>
    <published>2015-01-31T16:10:29Z</published>
    <title>The Search for Computational Intelligence</title>
    <summary>  We define and explore in simulation several rules for the local evolution of
generative rules for 1D and 2D cellular automata. Our implementation uses
strategies from conceptual blending. We discuss potential applications to
modelling social dynamics.
</summary>
    <author>
      <name>Joseph Corneli</name>
    </author>
    <author>
      <name>Ewen Maclean</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages. Submitted to Social Aspects of Cognition and Computing
  symposium at AISB 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.00130v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.00130v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.1; I.6.3; J.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.02444v1</id>
    <updated>2015-02-09T11:45:02Z</updated>
    <published>2015-02-09T11:45:02Z</published>
    <title>On the Dynamics of a Recurrent Hopfield Network</title>
    <summary>  In this research paper novel real/complex valued recurrent Hopfield Neural
Network (RHNN) is proposed. The method of synthesizing the energy landscape of
such a network and the experimental investigation of dynamics of Recurrent
Hopfield Network is discussed. Parallel modes of operation (other than fully
parallel mode) in layered RHNN is proposed. Also, certain potential
applications are proposed.
</summary>
    <author>
      <name>Rama Garimella</name>
    </author>
    <author>
      <name>Berkay Kicanaoglu</name>
    </author>
    <author>
      <name>Moncef Gabbouj</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 6 figures, 1 table, submitted to IJCNN-2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.02444v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.02444v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.00444v1</id>
    <updated>2015-05-03T16:11:26Z</updated>
    <published>2015-05-03T16:11:26Z</published>
    <title>Some Theoretical Properties of a Network of Discretely Firing Neurons</title>
    <summary>  The problem of optimising a network of discretely firing neurons is
addressed. An objective function is introduced which measures the average
number of bits that are needed for the network to encode its state. When this
is minimised, it is shown that this leads to a number of results, such as
topographic mappings, piecewise linear dependence on the input of the
probability of a neuron firing, and factorial encoder networks.
</summary>
    <author>
      <name>Stephen Luttrell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.00444v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.00444v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.01573v1</id>
    <updated>2015-06-04T13:08:04Z</updated>
    <published>2015-06-04T13:08:04Z</published>
    <title>Programs as Polypeptides</title>
    <summary>  We describe a visual programming language for defining behaviors manifested
by reified actors in a 2D virtual world that can be compiled into programs
comprised of sequences of combinators that are themselves reified as actors.
This makes it possible to build programs that build programs from components of
a few fixed types delivered by diffusion using processes that resemble
chemistry as much as computation.
</summary>
    <author>
      <name>Lance R. Williams</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in European Conference on Artificial Life (ECAL '15), York, UK, 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.01573v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.01573v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.02788v1</id>
    <updated>2015-08-12T01:01:11Z</updated>
    <published>2015-08-12T01:01:11Z</published>
    <title>The Effects of Hyperparameters on SGD Training of Neural Networks</title>
    <summary>  The performance of neural network classifiers is determined by a number of
hyperparameters, including learning rate, batch size, and depth. A number of
attempts have been made to explore these parameters in the literature, and at
times, to develop methods for optimizing them. However, exploration of
parameter spaces has often been limited. In this note, I report the results of
large scale experiments exploring these different parameters and their
interactions.
</summary>
    <author>
      <name>Thomas M. Breuel</name>
    </author>
    <link href="http://arxiv.org/abs/1508.02788v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.02788v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.06987v1</id>
    <updated>2015-11-22T10:05:33Z</updated>
    <published>2015-11-22T10:05:33Z</published>
    <title>Evolutionary algorithms</title>
    <summary>  This manuscript contains an outline of lectures course "Evolutionary
Algorithms" read by the author in Omsk State University n.a. F.M.Dostoevsky.
The course covers Canonic Genetic Algorithm and various other genetic
algorithms as well as evolutioanry algorithms in general. Some facts, such as
the Rotation Property of crossover, the Schemata Theorem, GA performance as a
local search and "almost surely" convergence of evolutionary algorithms are
given with complete proofs. The text is in Russian.
</summary>
    <author>
      <name>Anton V. Eremeev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Outline of lectures course "Evolutionary Algorithms" (in Russian)</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.06987v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.06987v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.05925v1</id>
    <updated>2016-02-18T19:56:39Z</updated>
    <published>2016-02-18T19:56:39Z</published>
    <title>Encoding Data for HTM Systems</title>
    <summary>  Hierarchical Temporal Memory (HTM) is a biologically inspired machine
intelligence technology that mimics the architecture and processes of the
neocortex. In this white paper we describe how to encode data as Sparse
Distributed Representations (SDRs) for use in HTM systems. We explain several
existing encoders, which are available through the open source project called
NuPIC, and we discuss requirements for creating encoders for new types of data.
</summary>
    <author>
      <name>Scott Purdy</name>
    </author>
    <link href="http://arxiv.org/abs/1602.05925v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.05925v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.07893v3</id>
    <updated>2016-08-28T09:56:23Z</updated>
    <published>2016-03-25T12:28:02Z</published>
    <title>Investigation Into The Effectiveness Of Long Short Term Memory Networks
  For Stock Price Prediction</title>
    <summary>  The effectiveness of long short term memory networks trained by
backpropagation through time for stock price prediction is explored in this
paper. A range of different architecture LSTM networks are constructed trained
and tested.
</summary>
    <author>
      <name>Hengjian Jia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.07893v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.07893v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.09002v1</id>
    <updated>2016-03-29T23:48:27Z</updated>
    <published>2016-03-29T23:48:27Z</published>
    <title>Dataflow Matrix Machines as a Generalization of Recurrent Neural
  Networks</title>
    <summary>  Dataflow matrix machines are a powerful generalization of recurrent neural
networks. They work with multiple types of arbitrary linear streams, multiple
types of powerful neurons, and allow to incorporate higher-order constructions.
We expect them to be useful in machine learning and probabilistic programming,
and in the synthesis of dynamic systems and of deterministic and probabilistic
programs.
</summary>
    <author>
      <name>Michael Bukatin</name>
    </author>
    <author>
      <name>Steve Matthews</name>
    </author>
    <author>
      <name>Andrey Radul</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages position paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.09002v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.09002v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.01818v1</id>
    <updated>2016-08-05T09:55:59Z</updated>
    <published>2016-08-05T09:55:59Z</published>
    <title>The BioDynaMo Project: a platform for computer simulations of biological
  dynamics</title>
    <summary>  This paper is a brief update on developments in the BioDynaMo project, a new
platform for computer simulations for biological research. We will discuss the
new capabilities of the simulator, important new concepts simulation
methodology as well as its numerous applications to the computational biology
and nanoscience communities.
</summary>
    <author>
      <name>Leonard Johard</name>
    </author>
    <author>
      <name>Lukas Breitwieser</name>
    </author>
    <author>
      <name>Alberto Di Meglio</name>
    </author>
    <author>
      <name>Marco Manca</name>
    </author>
    <author>
      <name>Manuel Mazzara</name>
    </author>
    <author>
      <name>Max Talanov</name>
    </author>
    <link href="http://arxiv.org/abs/1608.01818v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.01818v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.01982v1</id>
    <updated>2016-08-16T02:18:33Z</updated>
    <published>2016-08-16T02:18:33Z</published>
    <title>Uniform Transformation of Non-Separable Probability Distributions</title>
    <summary>  A theoretical framework is developed to describe the transformation that
distributes probability density functions uniformly over space. In one
dimension, the cumulative distribution can be used, but does not generalize to
higher dimensions, or non-separable distributions. A potential function is
shown to link probability density functions to their transformation, and to
generalize the cumulative. A numerical method is developed to compute the
potential, and examples are shown in two dimensions.
</summary>
    <author>
      <name>Eric Kee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.01982v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.01982v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.01430v2</id>
    <updated>2016-10-06T13:28:41Z</updated>
    <published>2016-10-05T14:14:51Z</published>
    <title>LAYERS: Yet another Neural Network toolkit</title>
    <summary>  Layers is an open source neural network toolkit aim at providing an easy way
to implement modern neural networks. The main user target are students and to
this end layers provides an easy scriptting language that can be early adopted.
The user has to focus only on design details as network totpology and parameter
tunning.
</summary>
    <author>
      <name>Roberto Paredes</name>
    </author>
    <author>
      <name>José-Miguel Benedí</name>
    </author>
    <link href="http://arxiv.org/abs/1610.01430v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.01430v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.06212v1</id>
    <updated>2016-12-19T14:59:14Z</updated>
    <published>2016-12-19T14:59:14Z</published>
    <title>A recurrent neural network without chaos</title>
    <summary>  We introduce an exceptionally simple gated recurrent neural network (RNN)
that achieves performance comparable to well-known gated architectures, such as
LSTMs and GRUs, on the word-level language modeling task. We prove that our
model has simple, predicable and non-chaotic dynamics. This stands in stark
contrast to more standard gated architectures, whose underlying dynamical
systems exhibit chaotic behavior.
</summary>
    <author>
      <name>Thomas Laurent</name>
    </author>
    <author>
      <name>James von Brecht</name>
    </author>
    <link href="http://arxiv.org/abs/1612.06212v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.06212v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.05923v1</id>
    <updated>2017-01-20T20:53:51Z</updated>
    <published>2017-01-20T20:53:51Z</published>
    <title>Gate-Variants of Gated Recurrent Unit (GRU) Neural Networks</title>
    <summary>  The paper evaluates three variants of the Gated Recurrent Unit (GRU) in
recurrent neural networks (RNN) by reducing parameters in the update and reset
gates. We evaluate the three variant GRU models on MNIST and IMDB datasets and
show that these GRU-RNN variant models perform as well as the original GRU RNN
model while reducing the computational expense.
</summary>
    <author>
      <name>Rahul Dey</name>
    </author>
    <author>
      <name>Fathi M. Salem</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 8 Figures, 4 Tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.05923v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.05923v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.02217v2</id>
    <updated>2017-04-24T16:33:34Z</updated>
    <published>2017-02-07T22:20:43Z</published>
    <title>Multitask Evolution with Cartesian Genetic Programming</title>
    <summary>  We introduce a genetic programming method for solving multiple Boolean
circuit synthesis tasks simultaneously. This allows us to solve a set of
elementary logic functions twice as easily as with a direct, single-task
approach.
</summary>
    <author>
      <name>Eric O. Scott</name>
    </author>
    <author>
      <name>Kenneth A. De Jong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.02217v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.02217v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.00207v2</id>
    <updated>2017-04-04T23:48:24Z</updated>
    <published>2017-04-01T18:22:33Z</published>
    <title>A Brownian Motion Model and Extreme Belief Machine for Modeling Sensor
  Data Measurements</title>
    <summary>  As the title suggests, we will describe (and justify through the presentation
of some of the relevant mathematics) prediction methodologies for sensor
measurements. This exposition will mainly be concerned with the mathematics
related to modeling the sensor measurements.
</summary>
    <author>
      <name>Robert A. Murphy</name>
    </author>
    <link href="http://arxiv.org/abs/1704.00207v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.00207v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60J70" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.01365v1</id>
    <updated>2017-05-03T11:29:28Z</updated>
    <published>2017-05-03T11:29:28Z</published>
    <title>Quantified advantage of discontinuous weight selection in approximations
  with deep neural networks</title>
    <summary>  We consider approximations of 1D Lipschitz functions by deep ReLU networks of
a fixed width. We prove that without the assumption of continuous weight
selection the uniform approximation error is lower than with this assumption at
least by a factor logarithmic in the size of the network.
</summary>
    <author>
      <name>Dmitry Yarotsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, submitted to J. Approx. Theory</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.01365v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.01365v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.08675v1</id>
    <updated>2017-06-27T05:28:06Z</updated>
    <published>2017-06-27T05:28:06Z</published>
    <title>Proceedings of the First International Workshop on Deep Learning and
  Music</title>
    <summary>  Proceedings of the First International Workshop on Deep Learning and Music,
joint with IJCNN, Anchorage, US, May 17-18, 2017
</summary>
    <author>
      <name>Dorien Herremans</name>
    </author>
    <author>
      <name>Ching-Hua Chuan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.13140/RG.2.2.22227.99364/1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.13140/RG.2.2.22227.99364/1" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the First International Workshop on Deep Learning
  and Music, joint with IJCNN, Anchorage, US, May 17-18, 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1706.08675v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.08675v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3; H.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.02617v1</id>
    <updated>2017-07-09T18:34:45Z</updated>
    <published>2017-07-09T18:34:45Z</published>
    <title>Deepest Neural Networks</title>
    <summary>  This paper shows that a long chain of perceptrons (that is, a multilayer
perceptron, or MLP, with many hidden layers of width one) can be a universal
classifier. The classification procedure is not necessarily computationally
efficient, but the technique throws some light on the kind of computations
possible with narrow and deep MLPs.
</summary>
    <author>
      <name>Raul Rojas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.02617v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.02617v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T10" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3; I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.04498v1</id>
    <updated>2017-06-16T12:29:41Z</updated>
    <published>2017-06-16T12:29:41Z</published>
    <title>Self-adaptive node-based PCA encodings</title>
    <summary>  In this paper we propose an algorithm, Simple Hebbian PCA, and prove that it
is able to calculate the principal component analysis (PCA) in a distributed
fashion across nodes. It simplifies existing network structures by removing
intralayer weights, essentially cutting the number of weights that need to be
trained in half.
</summary>
    <author>
      <name>Leonard Johard</name>
    </author>
    <author>
      <name>Victor Rivera</name>
    </author>
    <author>
      <name>Manuel Mazzara</name>
    </author>
    <author>
      <name>JooYoung Lee</name>
    </author>
    <link href="http://arxiv.org/abs/1708.04498v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.04498v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.02797v1</id>
    <updated>2017-09-06T07:26:59Z</updated>
    <published>2017-09-06T07:26:59Z</published>
    <title>On the exact relationship between the denoising function and the data
  distribution</title>
    <summary>  We prove an exact relationship between the optimal denoising function and the
data distribution in the case of additive Gaussian noise, showing that
denoising implicitly models the structure of data allowing it to be exploited
in the unsupervised learning of representations. This result generalizes a
known relationship [2], which is valid only in the limit of small corruption
noise.
</summary>
    <author>
      <name>Heikki Arponen</name>
    </author>
    <author>
      <name>Matti Herranen</name>
    </author>
    <author>
      <name>Harri Valpola</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.02797v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.02797v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0905.4570v2</id>
    <updated>2010-02-17T04:26:06Z</updated>
    <published>2009-05-28T08:09:46Z</published>
    <title>Weak Evolvability Equals Strong Evolvability</title>
    <summary>  An updated version will be uploaded later.
</summary>
    <author>
      <name>Yang Yu</name>
    </author>
    <author>
      <name>Zhi-Hua Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by the authors</arxiv:comment>
    <link href="http://arxiv.org/abs/0905.4570v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0905.4570v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9809111v1</id>
    <updated>1998-09-28T03:48:22Z</updated>
    <published>1998-09-28T03:48:22Z</published>
    <title>Evolution of Neural Networks to Play the Game of Dots-and-Boxes</title>
    <summary>  Dots-and-Boxes is a child's game which remains analytically unsolved. We
implement and evolve artificial neural networks to play this game, evaluating
them against simple heuristic players. Our networks do not evaluate or predict
the final outcome of the game, but rather recommend moves at each stage.
Superior generalisation of play by co-evolved populations is found, and a
comparison made with networks trained by back-propagation using simple
heuristics as an oracle.
</summary>
    <author>
      <name>Lex Weaver</name>
    </author>
    <author>
      <name>Terry Bossomaier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures, LaTeX 2.09 (works with LaTeX2e)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Alife V: Poster Presentations, May 16-18 1996, pages 43-50</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9809111v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9809111v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9811030v1</id>
    <updated>1998-11-24T22:51:20Z</updated>
    <published>1998-11-24T22:51:20Z</published>
    <title>Generating Segment Durations in a Text-To-Speech System: A Hybrid
  Rule-Based/Neural Network Approach</title>
    <summary>  A combination of a neural network with rule firing information from a
rule-based system is used to generate segment durations for a text-to-speech
system. The system shows a slight improvement in performance over a neural
network system without the rule firing information. Synthesized speech using
segment durations was accepted by listeners as having about the same quality as
speech generated using segment durations extracted from natural speech.
</summary>
    <author>
      <name>Gerald Corrigan</name>
    </author>
    <author>
      <name>Noel Massey</name>
    </author>
    <author>
      <name>Orhan Karaali</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, PostScript</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of Eurospeech (1997) 2675-2678. Rhodes, Greece</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9811030v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9811030v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; K.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9811031v1</id>
    <updated>1998-11-24T23:33:12Z</updated>
    <published>1998-11-24T23:33:12Z</published>
    <title>Speech Synthesis with Neural Networks</title>
    <summary>  Text-to-speech conversion has traditionally been performed either by
concatenating short samples of speech or by using rule-based systems to convert
a phonetic representation of speech into an acoustic representation, which is
then converted into speech. This paper describes a system that uses a
time-delay neural network (TDNN) to perform this phonetic-to-acoustic mapping,
with another neural network to control the timing of the generated speech. The
neural network system requires less memory than a concatenation system, and
performed well in tests comparing it to commercial systems using other
technologies.
</summary>
    <author>
      <name>Orhan Karaali</name>
    </author>
    <author>
      <name>Gerald Corrigan</name>
    </author>
    <author>
      <name>Ira Gerson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, PostScript</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">World Congress on Neural Networks (1996) 45-50. San Diego</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9811031v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9811031v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; K.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9811032v1</id>
    <updated>1998-11-24T23:51:56Z</updated>
    <published>1998-11-24T23:51:56Z</published>
    <title>Text-To-Speech Conversion with Neural Networks: A Recurrent TDNN
  Approach</title>
    <summary>  This paper describes the design of a neural network that performs the
phonetic-to-acoustic mapping in a speech synthesis system. The use of a
time-domain neural network architecture limits discontinuities that occur at
phone boundaries. Recurrent data input also helps smooth the output parameter
tracks. Independent testing has demonstrated that the voice quality produced by
this system compares favorably with speech from existing commercial
text-to-speech systems.
</summary>
    <author>
      <name>Orhan Karaali</name>
    </author>
    <author>
      <name>Gerald Corrigan</name>
    </author>
    <author>
      <name>Ira Gerson</name>
    </author>
    <author>
      <name>Noel Massey</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, PostScript</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of Eurospeech (1997) 561-564. Rhodes, Greece</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9811032v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9811032v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; K.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9812002v1</id>
    <updated>1998-12-03T09:08:03Z</updated>
    <published>1998-12-03T09:08:03Z</published>
    <title>Training Reinforcement Neurocontrollers Using the Polytope Algorithm</title>
    <summary>  A new training algorithm is presented for delayed reinforcement learning
problems that does not assume the existence of a critic model and employs the
polytope optimization algorithm to adjust the weights of the action network so
that a simple direct measure of the training performance is maximized.
Experimental results from the application of the method to the pole balancing
problem indicate improved training performance compared with critic-based and
genetic reinforcement approaches.
</summary>
    <author>
      <name>A. Likas</name>
    </author>
    <author>
      <name>I. E. Lagaris</name>
    </author>
    <link href="http://arxiv.org/abs/cs/9812002v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9812002v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0102015v1</id>
    <updated>2001-02-20T13:08:15Z</updated>
    <published>2001-02-20T13:08:15Z</published>
    <title>Non-convex cost functionals in boosting algorithms and methods for panel
  selection</title>
    <summary>  In this document we propose a new improvement for boosting techniques as
proposed in Friedman '99 by the use of non-convex cost functional. The idea is
to introduce a correlation term to better deal with forecasting of additive
time series. The problem is discussed in a theoretical way to prove the
existence of minimizing sequence, and in a numerical way to propose a new
"ArgMin" algorithm. The model has been used to perform the touristic presence
forecast for the winter season 1999/2000 in Trentino (italian Alps).
</summary>
    <author>
      <name>Marco Visentin</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0102015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0102015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6;G.1.2;G.3;I.6.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0108011v1</id>
    <updated>2001-08-21T15:00:47Z</updated>
    <published>2001-08-21T15:00:47Z</published>
    <title>On Classes of Functions for which No Free Lunch Results Hold</title>
    <summary>  In a recent paper it was shown that No Free Lunch results hold for any subset
F of the set of all possible functions from a finite set X to a finite set Y
iff F is closed under permutation of X. In this article, we prove that the
number of those subsets can be neglected compared to the overall number of
possible subsets. Further, we present some arguments why problem classes
relevant in practice are not likely to be closed under permutation.
</summary>
    <author>
      <name>Christian Igel</name>
    </author>
    <author>
      <name>Marc Toussaint</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 1 figure, see http://www.neuroinformatik.ruhr-uni-bochum.de/</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0108011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0108011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0202009v1</id>
    <updated>2002-02-11T11:04:08Z</updated>
    <published>2002-02-11T11:04:08Z</published>
    <title>Non-negative sparse coding</title>
    <summary>  Non-negative sparse coding is a method for decomposing multivariate data into
non-negative sparse components. In this paper we briefly describe the
motivation behind this type of data representation and its relation to standard
sparse coding and non-negative matrix factorization. We then give a simple yet
efficient multiplicative algorithm for finding the optimal values of the hidden
components. In addition, we show how the basis vectors can be learned from the
observed data. Simulations demonstrate the effectiveness of the proposed
method.
</summary>
    <author>
      <name>Patrik O. Hoyer</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0202009v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0202009v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0202034v1</id>
    <updated>2002-02-20T17:12:03Z</updated>
    <published>2002-02-20T17:12:03Z</published>
    <title>Covariance Plasticity and Regulated Criticality</title>
    <summary>  We propose that a regulation mechanism based on Hebbian covariance plasticity
may cause the brain to operate near criticality. We analyze the effect of such
a regulation on the dynamics of a network with excitatory and inhibitory
neurons and uniform connectivity within and across the two populations. We show
that, under broad conditions, the system converges to a critical state lying at
the common boundary of three regions in parameter space; these correspond to
three modes of behavior: high activity, low activity, oscillation.
</summary>
    <author>
      <name>Elie Bienenstock</name>
    </author>
    <author>
      <name>Daniel Lehmann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages, 8 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Advances in Complex Systems, 1(4) (1998) pp. 361-384</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0202034v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0202034v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0212042v1</id>
    <updated>2002-12-12T22:39:39Z</updated>
    <published>2002-12-12T22:39:39Z</published>
    <title>Increasing Evolvability Considered as a Large-Scale Trend in Evolution</title>
    <summary>  Evolvability is the capacity to evolve. This paper introduces a simple
computational model of evolvability and demonstrates that, under certain
conditions, evolvability can increase indefinitely, even when there is no
direct selection for evolvability. The model shows that increasing evolvability
implies an accelerating evolutionary pace. It is suggested that the conditions
for indefinitely increasing evolvability are satisfied in biological and
cultural evolution. We claim that increasing evolvability is a large-scale
trend in evolution. This hypothesis leads to testable predictions about
biological and cultural evolution.
</summary>
    <author>
      <name>Peter D. Turney</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">National Research Council of Canada</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 1999 Genetic and Evolutionary Computation
  Conference Workshop Program, (1999), 43-46</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0212042v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0212042v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.6.3; I.6.8; J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0402014v1</id>
    <updated>2004-02-09T19:44:33Z</updated>
    <published>2004-02-09T19:44:33Z</published>
    <title>Self-Organising Networks for Classification: developing Applications to
  Science Analysis for Astroparticle Physics</title>
    <summary>  Physics analysis in astroparticle experiments requires the capability of
recognizing new phenomena; in order to establish what is new, it is important
to develop tools for automatic classification, able to compare the final result
with data from different detectors. A typical example is the problem of Gamma
Ray Burst detection, classification, and possible association to known sources:
for this task physicists will need in the next years tools to associate data
from optical databases, from satellite experiments (EGRET, GLAST), and from
Cherenkov telescopes (MAGIC, HESS, CANGAROO, VERITAS).
</summary>
    <author>
      <name>A. De Angelis</name>
    </author>
    <author>
      <name>P. Boinee</name>
    </author>
    <author>
      <name>M. Frailis</name>
    </author>
    <author>
      <name>E. Milotti</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.physa.2004.02.023</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.physa.2004.02.023" rel="related"/>
    <link href="http://arxiv.org/abs/cs/0402014v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0402014v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.1; I.5.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0402047v1</id>
    <updated>2004-02-19T19:37:32Z</updated>
    <published>2004-02-19T19:37:32Z</published>
    <title>Parameter-less Optimization with the Extended Compact Genetic Algorithm
  and Iterated Local Search</title>
    <summary>  This paper presents a parameter-less optimization framework that uses the
extended compact genetic algorithm (ECGA) and iterated local search (ILS), but
is not restricted to these algorithms. The presented optimization algorithm
(ILS+ECGA) comes as an extension of the parameter-less genetic algorithm (GA),
where the parameters of a selecto-recombinative GA are eliminated. The approach
that we propose is tested on several well known problems. In the absence of
domain knowledge, it is shown that ILS+ECGA is a robust and easy-to-use
optimization method.
</summary>
    <author>
      <name>Claudio F. Lima</name>
    </author>
    <author>
      <name>Fernando G. Lobo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, submitted to gecco 2004</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0402047v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0402047v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.6; I.2.6; I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0402049v1</id>
    <updated>2004-02-20T16:36:20Z</updated>
    <published>2004-02-20T16:36:20Z</published>
    <title>An architecture for massive parallelization of the compact genetic
  algorithm</title>
    <summary>  This paper presents an architecture which is suitable for a massive
parallelization of the compact genetic algorithm. The resulting scheme has
three major advantages. First, it has low synchronization costs. Second, it is
fault tolerant, and third, it is scalable.
  The paper argues that the benefits that can be obtained with the proposed
approach is potentially higher than those obtained with traditional parallel
genetic algorithms. In addition, the ideas suggested in the paper may also be
relevant towards parallelizing more complex probabilistic model building
genetic algorithms.
</summary>
    <author>
      <name>Fernando G. Lobo</name>
    </author>
    <author>
      <name>Claudio F. Lima</name>
    </author>
    <author>
      <name>Hugo Martires</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, submitted to gecco 2004</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0402049v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0402049v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.4; G.1.6; I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0402050v1</id>
    <updated>2004-02-20T16:52:11Z</updated>
    <published>2004-02-20T16:52:11Z</published>
    <title>A philosophical essay on life and its connections with genetic
  algorithms</title>
    <summary>  This paper makes a number of connections between life and various facets of
genetic and evolutionary algorithms research. Specifically, it addresses the
topics of adaptation, multiobjective optimization, decision making, deception,
and search operators, among others. It argues that human life, from birth to
death, is an adaptive or dynamic optimization problem where people are
continuously searching for happiness. More important, the paper speculates that
genetic algorithms can be used as a source of inspiration for helping people
make decisions in their everyday life.
</summary>
    <author>
      <name>Fernando G. Lobo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, submitted to gecco 2004</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0402050v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0402050v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.8; J.4; K.4.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0404019v1</id>
    <updated>2004-04-07T07:08:09Z</updated>
    <published>2004-04-07T07:08:09Z</published>
    <title>Optimizing genetic algorithm strategies for evolving networks</title>
    <summary>  This paper explores the use of genetic algorithms for the design of networks,
where the demands on the network fluctuate in time. For varying network
constraints, we find the best network using the standard genetic algorithm
operators such as inversion, mutation and crossover. We also examine how the
choice of genetic algorithm operators affects the quality of the best network
found. Such networks typically contain redundancy in servers, where several
servers perform the same task and pleiotropy, where servers perform multiple
tasks. We explore this trade-off between pleiotropy versus redundancy on the
cost versus reliability as a measure of the quality of the network.
</summary>
    <author>
      <name>Matthew J. Berryman</name>
    </author>
    <author>
      <name>Andrew Allison</name>
    </author>
    <author>
      <name>Derek Abbott</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1117/12.548122</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1117/12.548122" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0404019v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0404019v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.6; C.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0404042v2</id>
    <updated>2004-04-22T09:00:52Z</updated>
    <published>2004-04-21T16:05:27Z</published>
    <title>Extraction of topological features from communication network
  topological patterns using self-organizing feature maps</title>
    <summary>  Different classes of communication network topologies and their
representation in the form of adjacency matrix and its eigenvalues are
presented. A self-organizing feature map neural network is used to map
different classes of communication network topological patterns. The neural
network simulation results are reported.
</summary>
    <author>
      <name>W. Ali</name>
    </author>
    <author>
      <name>R. J. Mondragon</name>
    </author>
    <author>
      <name>F. Alavi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 Pages, 5 figures, To be appeared in IEE Electronics Letter Journal</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0404042v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0404042v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2; I.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0412023v1</id>
    <updated>2004-12-06T20:23:15Z</updated>
    <published>2004-12-06T20:23:15Z</published>
    <title>Multidimensional data classification with artificial neural networks</title>
    <summary>  Multi-dimensional data classification is an important and challenging problem
in many astro-particle experiments. Neural networks have proved to be versatile
and robust in multi-dimensional data classification. In this article we shall
study the classification of gamma from the hadrons for the MAGIC Experiment.
Two neural networks have been used for the classification task. One is
Multi-Layer Perceptron based on supervised learning and other is
Self-Organising Map (SOM), which is based on unsupervised learning technique.
The results have been shown and the possible ways of combining these networks
have been proposed to yield better and faster classification results.
</summary>
    <author>
      <name>P. Boinee</name>
    </author>
    <author>
      <name>F. Barbarino</name>
    </author>
    <author>
      <name>A. De Angelis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 4 figures, Submitted to EURASIP Journal on Applied Signal
  Processing, 2004</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0412023v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0412023v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.1; K.3.2; I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0412109v1</id>
    <updated>2004-12-24T13:41:10Z</updated>
    <published>2004-12-24T13:41:10Z</published>
    <title>Global minimization of a quadratic functional: neural network approach</title>
    <summary>  The problem of finding out the global minimum of a multiextremal functional
is discussed. One frequently faces with such a functional in various
applications. We propose a procedure, which depends on the dimensionality of
the problem polynomially. In our approach we use the eigenvalues and
eigenvectors of the connection matrix.
</summary>
    <author>
      <name>L. B. Litinskii</name>
    </author>
    <author>
      <name>B. M. Magomedov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, Lecture on 7th International Conference on Pattern
  Recognition and Image Analysis PRIA-07-2004, St. Petersburg, Russia</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0412109v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0412109v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0412110v1</id>
    <updated>2004-12-24T13:48:44Z</updated>
    <published>2004-12-24T13:48:44Z</published>
    <title>Q-valued neural network as a system of fast identification and pattern
  recognition</title>
    <summary>  An effective neural network algorithm of the perceptron type is proposed. The
algorithm allows us to identify strongly distorted input vector reliably. It is
shown that its reliability and processing speed are orders of magnitude higher
than that of full connected neural networks. The processing speed of our
algorithm exceeds the one of the stack fast-access retrieval algorithm that is
modified for working when there are noises in the input channel.
</summary>
    <author>
      <name>D. I. Alieva</name>
    </author>
    <author>
      <name>B. V. Kryzhanovsky</name>
    </author>
    <author>
      <name>V. M. Kryzhanovsky</name>
    </author>
    <author>
      <name>A. B. Fonarev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, Presentation on the 7th International Conference on Pattern
  Recognition and Image Analysis PRIA-07-2004, St. Petersburg, Russia</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0412110v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0412110v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0501005v1</id>
    <updated>2005-01-03T18:55:47Z</updated>
    <published>2005-01-03T18:55:47Z</published>
    <title>Portfolio selection using neural networks</title>
    <summary>  In this paper we apply a heuristic method based on artificial neural networks
in order to trace out the efficient frontier associated to the portfolio
selection problem. We consider a generalization of the standard Markowitz
mean-variance model which includes cardinality and bounding constraints. These
constraints ensure the investment in a given number of different assets and
limit the amount of capital to be invested in each asset. We present some
experimental results obtained with the neural network heuristic and we compare
them to those obtained with three previous heuristic methods.
</summary>
    <author>
      <name>Alberto Fernandez</name>
    </author>
    <author>
      <name>Sergio Gomez</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.cor.2005.06.017</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.cor.2005.06.017" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages; submitted to "Computers &amp; Operations Research"</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computers &amp; Operations Research 34 (2007) 1177-1191</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0501005v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0501005v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0502057v1</id>
    <updated>2005-02-12T22:29:45Z</updated>
    <published>2005-02-12T22:29:45Z</published>
    <title>Decomposable Problems, Niching, and Scalability of Multiobjective
  Estimation of Distribution Algorithms</title>
    <summary>  The paper analyzes the scalability of multiobjective estimation of
distribution algorithms (MOEDAs) on a class of boundedly-difficult
additively-separable multiobjective optimization problems. The paper
illustrates that even if the linkage is correctly identified, massive
multimodality of the search problems can easily overwhelm the nicher and lead
to exponential scale-up. Facetwise models are subsequently used to propose a
growth rate of the number of differing substructures between the two objectives
to avoid the niching method from being overwhelmed and lead to polynomial
scalability of MOEDAs.
</summary>
    <author>
      <name>Kumara Sastry</name>
    </author>
    <author>
      <name>Martin Pelikan</name>
    </author>
    <author>
      <name>David E. Goldberg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Genetic and Evolutionary Computation Conference,
  GECCO-2005</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0502057v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0502057v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0502096v1</id>
    <updated>2005-02-28T16:40:34Z</updated>
    <published>2005-02-28T16:40:34Z</published>
    <title>Property analysis of symmetric travelling salesman problem instances
  acquired through evolution</title>
    <summary>  We show how an evolutionary algorithm can successfully be used to evolve a
set of difficult to solve symmetric travelling salesman problem instances for
two variants of the Lin-Kernighan algorithm. Then we analyse the instances in
those sets to guide us towards deferring general knowledge about the efficiency
of the two variants in relation to structural properties of the symmetric
travelling sale sman problem.
</summary>
    <author>
      <name>J. I. van Hemert</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published in G. Raidl and J. Gottlieb, editors, Evolutionary
  Computation in Combinatorial Optimization, Springer Lecture Notes on Computer
  Science, pages 122-131. Springer-Verlag, Berlin, 2005</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0502096v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0502096v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G1.6;I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0504052v1</id>
    <updated>2005-04-13T13:22:49Z</updated>
    <published>2005-04-13T13:22:49Z</published>
    <title>Learning Multi-Class Neural-Network Models from Electroencephalograms</title>
    <summary>  We describe a new algorithm for learning multi-class neural-network models
from large-scale clinical electroencephalograms (EEGs). This algorithm trains
hidden neurons separately to classify all the pairs of classes. To find best
pairwise classifiers, our algorithm searches for input variables which are
relevant to the classification problem. Despite patient variability and heavily
overlapping classes, a 16-class model learnt from EEGs of 65 sleeping newborns
correctly classified 80.8% of the training and 80.1% of the testing examples.
Additionally, the neural-network model provides a probabilistic interpretation
of decisions.
</summary>
    <author>
      <name>Vitaly Schetinin</name>
    </author>
    <author>
      <name>Joachim Schult</name>
    </author>
    <author>
      <name>Burkhart Scheidt</name>
    </author>
    <author>
      <name>Valery Kuriakin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">KES-2003</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0504052v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0504052v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0504053v1</id>
    <updated>2005-04-13T13:28:15Z</updated>
    <published>2005-04-13T13:28:15Z</published>
    <title>A Neural-Network Technique for Recognition of Filaments in Solar Images</title>
    <summary>  We describe a new neural-network technique developed for an automated
recognition of solar filaments visible in the hydrogen H-alpha line full disk
spectroheliograms. This technique allows neural networks learn from a few image
fragments labelled manually to recognize the single filaments depicted on a
local background. The trained network is able to recognize filaments depicted
on the backgrounds with variations in brightness caused by atmospherics
distortions. Despite the difference in backgrounds in our experiments the
neural network has properly recognized filaments in the testing image
fragments. Using a parabolic activation function we extend this technique to
recognize multiple solar filaments which may appear in one fragment.
</summary>
    <author>
      <name>V. V. Zharkova</name>
    </author>
    <author>
      <name>V. Schetinin</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0504053v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0504053v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0504055v1</id>
    <updated>2005-04-13T13:57:56Z</updated>
    <published>2005-04-13T13:57:56Z</published>
    <title>A Learning Algorithm for Evolving Cascade Neural Networks</title>
    <summary>  A new learning algorithm for Evolving Cascade Neural Networks (ECNNs) is
described. An ECNN starts to learn with one input node and then adding new
inputs as well as new hidden neurons evolves it. The trained ECNN has a nearly
minimal number of input and hidden neurons as well as connections. The
algorithm was successfully applied to classify artifacts and normal segments in
clinical electroencephalograms (EEGs). The EEG segments were visually labeled
by EEG-viewer. The trained ECNN has correctly classified 96.69% of the testing
segments. It is slightly better than a standard fully connected neural network.
</summary>
    <author>
      <name>Vitaly Schetinin</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Processing Letter 17:21-31, 2003. Kluwer</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0504055v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0504055v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0504058v1</id>
    <updated>2005-04-13T14:06:32Z</updated>
    <published>2005-04-13T14:06:32Z</published>
    <title>Polynomial Neural Networks Learnt to Classify EEG Signals</title>
    <summary>  A neural network based technique is presented, which is able to successfully
extract polynomial classification rules from labeled electroencephalogram (EEG)
signals. To represent the classification rules in an analytical form, we use
the polynomial neural networks trained by a modified Group Method of Data
Handling (GMDH). The classification rules were extracted from clinical EEG data
that were recorded from an Alzheimer patient and the sudden death risk
patients. The third data is EEG recordings that include the normal and artifact
segments. These EEG data were visually identified by medical experts. The
extracted polynomial rules verified on the testing EEG data allow to correctly
classify 72% of the risk group patients and 96.5% of the segments. These rules
performs slightly better than standard feedforward neural networks.
</summary>
    <author>
      <name>Vitaly Schetinin</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0504058v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0504058v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0504067v1</id>
    <updated>2005-04-14T10:36:54Z</updated>
    <published>2005-04-14T10:36:54Z</published>
    <title>An Evolving Cascade Neural Network Technique for Cleaning Sleep
  Electroencephalograms</title>
    <summary>  Evolving Cascade Neural Networks (ECNNs) and a new training algorithm capable
of selecting informative features are described. The ECNN initially learns with
one input node and then evolves by adding new inputs as well as new hidden
neurons. The resultant ECNN has a near minimal number of hidden neurons and
inputs. The algorithm is successfully used for training ECNN to recognise
artefacts in sleep electroencephalograms (EEGs) which were visually labelled by
EEG-viewers. In our experiments, the ECNN outperforms the standard
neural-network as well as evolutionary techniques.
</summary>
    <author>
      <name>Vitaly Schetinin</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Natural Computing Application, 2005</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0504067v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0504067v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0504070v1</id>
    <updated>2005-04-14T10:49:55Z</updated>
    <published>2005-04-14T10:49:55Z</published>
    <title>The Combined Technique for Detection of Artifacts in Clinical
  Electroencephalograms of Sleeping Newborns</title>
    <summary>  In this paper we describe a new method combining the polynomial neural
network and decision tree techniques in order to derive comprehensible
classification rules from clinical electroencephalograms (EEGs) recorded from
sleeping newborns. These EEGs are heavily corrupted by cardiac, eye movement,
muscle and noise artifacts and as a consequence some EEG features are
irrelevant to classification problems. Combining the polynomial network and
decision tree techniques, we discover comprehensible classification rules
whilst also attempting to keep their classification error down. This technique
is shown to outperform a number of commonly used machine learning technique
applied to automatically recognize artifacts in the sleep EEGs.
</summary>
    <author>
      <name>Vitaly Schetinin</name>
    </author>
    <author>
      <name>Joachim Schult</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0504070v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0504070v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0505003v1</id>
    <updated>2005-04-30T16:13:32Z</updated>
    <published>2005-04-30T16:13:32Z</published>
    <title>A New Kind of Hopfield Networks for Finding Global Optimum</title>
    <summary>  The Hopfield network has been applied to solve optimization problems over
decades. However, it still has many limitations in accomplishing this task.
Most of them are inherited from the optimization algorithms it implements. The
computation of a Hopfield network, defined by a set of difference equations,
can easily be trapped into one local optimum or another, sensitive to initial
conditions, perturbations, and neuron update orders. It doesn't know how long
it will take to converge, as well as if the final solution is a global optimum,
or not. In this paper, we present a Hopfield network with a new set of
difference equations to fix those problems. The difference equations directly
implement a new powerful optimization algorithm.
</summary>
    <author>
      <name>Xiaofei Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, accepted by International Joint Conference on Neural
  Networks 2005</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0505003v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0505003v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0505065v2</id>
    <updated>2005-05-27T04:41:11Z</updated>
    <published>2005-05-24T14:54:06Z</published>
    <title>A dissipative particle swarm optimization</title>
    <summary>  A dissipative particle swarm optimization is developed according to the
self-organization of dissipative structure. The negative entropy is introduced
to construct an opening dissipative system that is far-from-equilibrium so as
to driving the irreversible evolution process with better fitness. The testing
of two multimodal functions indicates it improves the performance effectively
</summary>
    <author>
      <name>Xiao-Feng Xie</name>
    </author>
    <author>
      <name>Wen-Jun Zhang</name>
    </author>
    <author>
      <name>Zhi-Lian Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 2002 Congress on Evolutionary Computation, 2002.
  Volume: 2, On page(s): 1456-1461</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0505065v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0505065v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0505067v1</id>
    <updated>2005-05-25T01:28:18Z</updated>
    <published>2005-05-25T01:28:18Z</published>
    <title>Optimizing semiconductor devices by self-organizing particle swarm</title>
    <summary>  A self-organizing particle swarm is presented. It works in dissipative state
by employing the small inertia weight, according to experimental analysis on a
simplified model, which with fast convergence. Then by recognizing and
replacing inactive particles according to the process deviation information of
device parameters, the fluctuation is introduced so as to driving the
irreversible evolution process with better fitness. The testing on benchmark
functions and an application example for device optimization with designed
fitness function indicates it improves the performance effectively.
</summary>
    <author>
      <name>Xiao-Feng Xie</name>
    </author>
    <author>
      <name>Wen-Jun Zhang</name>
    </author>
    <author>
      <name>De-Chun Bi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Congress on Evolutionary Computation, 2004. CEC2004. Volume: 2, On
  page(s): 2017- 2022 Vol.2</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0505067v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0505067v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0505068v1</id>
    <updated>2005-05-25T01:32:30Z</updated>
    <published>2005-05-25T01:32:30Z</published>
    <title>Handling equality constraints by adaptive relaxing rule for swarm
  algorithms</title>
    <summary>  The adaptive constraints relaxing rule for swarm algorithms to handle with
the problems with equality constraints is presented. The feasible space of such
problems may be similiar to ridge function class, which is hard for applying
swarm algorithms. To enter the solution space more easily, the relaxed quasi
feasible space is introduced and shrinked adaptively. The experimental results
on benchmark functions are compared with the performance of other algorithms,
which show its efficiency.
</summary>
    <author>
      <name>Xiao-Feng Xie</name>
    </author>
    <author>
      <name>Wen-Jun Zhang</name>
    </author>
    <author>
      <name>De-Chun Bi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Congress on Evolutionary Computation, 2004. CEC2004. Volume: 2, On
  page(s): 2012- 2016 Vol.2</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0505068v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0505068v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0505069v1</id>
    <updated>2005-05-25T01:36:07Z</updated>
    <published>2005-05-25T01:36:07Z</published>
    <title>Handling boundary constraints for numerical optimization by particle
  swarm flying in periodic search space</title>
    <summary>  The periodic mode is analyzed together with two conventional boundary
handling modes for particle swarm. By providing an infinite space that
comprises periodic copies of original search space, it avoids possible
disorganizing of particle swarm that is induced by the undesired mutations at
the boundary. The results on benchmark functions show that particle swarm with
periodic mode is capable of improving the search performance significantly, by
compared with that of conventional modes and other algorithms.
</summary>
    <author>
      <name>Wen-Jun Zhang</name>
    </author>
    <author>
      <name>Xiao-Feng Xie</name>
    </author>
    <author>
      <name>De-Chun Bi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Congress on Evolutionary Computation, 2004. CEC2004. Volume: 2, On
  page(s): 2307- 2311 Vol.2</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0505069v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0505069v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0505070v1</id>
    <updated>2005-05-25T01:39:55Z</updated>
    <published>2005-05-25T01:39:55Z</published>
    <title>SWAF: Swarm Algorithm Framework for Numerical Optimization</title>
    <summary>  A swarm algorithm framework (SWAF), realized by agent-based modeling, is
presented to solve numerical optimization problems. Each agent is a bare bones
cognitive architecture, which learns knowledge by appropriately deploying a set
of simple rules in fast and frugal heuristics. Two essential categories of
rules, the generate-and-test and the problem-formulation rules, are
implemented, and both of the macro rules by simple combination and subsymbolic
deploying of multiple rules among them are also studied. Experimental results
on benchmark problems are presented, and performance comparison between SWAF
and other existing algorithms indicates that it is efficiently.
</summary>
    <author>
      <name>Xiao-Feng Xie</name>
    </author>
    <author>
      <name>Wen-Jun Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Genetic and Evolutionary Computation Conference (GECCO), Part I,
  2004: 238-250 (LNCS 3102)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0505070v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0505070v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0508117v1</id>
    <updated>2005-08-26T10:38:36Z</updated>
    <published>2005-08-26T10:38:36Z</published>
    <title>Long-term neuronal behavior caused by two synaptic modification
  mechanisms</title>
    <summary>  We report the first results of simulating the coupling of neuronal,
astrocyte, and cerebrovascular activity. It is suggested that the dynamics of
the system is different from systems that only include neurons. In the
neuron-vascular coupling, distribution of synapse strengths affects neuronal
behavior and thus balance of the blood flow; oscillations are induced in the
neuron-to-astrocyte coupling.
</summary>
    <author>
      <name>Xi Shen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Imperial College London, United Kingdom</arxiv:affiliation>
    </author>
    <author>
      <name>Philippe De Wilde</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Heriot-Watt University, United Kingdom</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0508117v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0508117v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0511027v1</id>
    <updated>2005-11-07T19:09:01Z</updated>
    <published>2005-11-07T19:09:01Z</published>
    <title>Discrete Network Dynamics. Part 1: Operator Theory</title>
    <summary>  An operator algebra implementation of Markov chain Monte Carlo algorithms for
simulating Markov random fields is proposed. It allows the dynamics of networks
whose nodes have discrete state spaces to be specified by the action of an
update operator that is composed of creation and annihilation operators. This
formulation of discrete network dynamics has properties that are similar to
those of a quantum field theory of bosons, which allows reuse of many
conceptual and theoretical structures from QFT. The equilibrium behaviour of
one of these generalised MRFs and of the adaptive cluster expansion network
(ACEnet) are shown to be equivalent, which provides a way of unifying these two
theories.
</summary>
    <author>
      <name>Stephen Luttrell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">39 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0511027v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0511027v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.2; G.2.1; G.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0512019v1</id>
    <updated>2005-12-05T11:12:42Z</updated>
    <published>2005-12-05T11:12:42Z</published>
    <title>Amazing geometry of genetic space or are genetic algorithms convergent?</title>
    <summary>  There is no proof yet of convergence of Genetic Algorithms. We do not supply
it too. Instead, we present some thoughts and arguments to convince the Reader,
that Genetic Algorithms are essentially bound for success. For this purpose, we
consider only the crossover operators, single- or multiple-point, together with
selection procedure. We also give a proof that the soft selection is superior
to other selection schemes.
</summary>
    <author>
      <name>Marek W. Gutowski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, presented on VII KAEiOG (VII Domestic Conference on
  Evolutionary Algorithms and Global Optimization), May 24-26, 2004, Kazimierz
  Dolny, Poland</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0512019v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0512019v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; G.1.6; G.4; I.1.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0602036v1</id>
    <updated>2006-02-10T06:32:29Z</updated>
    <published>2006-02-10T06:32:29Z</published>
    <title>Réseaux d'Automates de Caianiello Revisité</title>
    <summary>  We exhibit a family of neural networks of McCulloch and Pitts of size $2nk+2$
which can be simulated by a neural networks of Caianiello of size $2n+2$ and
memory length $k$. This simulation allows us to find again one of the result of
the following article: [Cycles exponentiels des r\'{e}seaux de Caianiello et
compteurs en arithm\'{e}tique redondante, Technique et Science Informatiques
Vol. 19, pages 985-1008] on the existence of neural networks of Caianiello of
size $2n+2$ and memory length $k$ which describes a cycle of length $k \times
2^{nk}$.
</summary>
    <author>
      <name>René Ndoundam</name>
    </author>
    <author>
      <name>Maurice Tchuente</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">39 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0602036v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0602036v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0603015v1</id>
    <updated>2006-03-02T23:59:19Z</updated>
    <published>2006-03-02T23:59:19Z</published>
    <title>The Basic Kak Neural Network with Complex Inputs</title>
    <summary>  The Kak family of neural networks is able to learn patterns quickly, and this
speed of learning can be a decisive advantage over other competing models in
many applications. Amongst the implementations of these networks are those
using reconfigurable networks, FPGAs and optical networks. In some
applications, it is useful to use complex data, and it is with that in mind
that this introduction to the basic Kak network with complex inputs is being
presented. The training algorithm is prescriptive and the network weights are
assigned simply upon examining the inputs. The input is mapped using quaternary
encoding for purpose of efficienty. This network family is part of a larger
hierarchy of learning schemes that include quantum models.
</summary>
    <author>
      <name>Pritam Rajagopal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 9 figures, 7 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0603015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0603015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0607007v4</id>
    <updated>2009-10-04T15:23:16Z</updated>
    <published>2006-07-03T04:03:23Z</published>
    <title>Theory of sexes by Geodakian as it is advanced by Iskrin</title>
    <summary>  In 1960s V.Geodakian proposed a theory that explains sexes as a mechanism for
evolutionary adaptation of the species to changing environmental conditions. In
2001 V.Iskrin refined and augmented the concepts of Geodakian and gave a new
and interesting explanation to several phenomena which involve sex, and sex
ratio, including the war-years phenomena. He also introduced a new concept of
the "catastrophic sex ratio." This note is an attempt to digest technical
aspects of the new ideas by Iskrin.
</summary>
    <author>
      <name>Boris D. Lubachevsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0607007v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0607007v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.1; J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0607090v1</id>
    <updated>2006-07-18T21:01:43Z</updated>
    <published>2006-07-18T21:01:43Z</published>
    <title>Neural Networks with Complex and Quaternion Inputs</title>
    <summary>  This article investigates Kak neural networks, which can be instantaneously
trained, for complex and quaternion inputs. The performance of the basic
algorithm has been analyzed and shown how it provides a plausible model of
human perception and understanding of images. The motivation for studying
quaternion inputs is their use in representing spatial rotations that find
applications in computer graphics, robotics, global navigation, computer vision
and the spatial orientation of instruments. The problem of efficient mapping of
data in quaternion neural networks is examined. Some problems that need to be
addressed before quaternion neural networks find applications are identified.
</summary>
    <author>
      <name>Adityan Rishiyur</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0607090v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0607090v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0610041v1</id>
    <updated>2006-10-09T11:42:27Z</updated>
    <published>2006-10-09T11:42:27Z</published>
    <title>A Computational Model of Spatial Memory Anticipation during Visual
  Search</title>
    <summary>  Some visual search tasks require to memorize the location of stimuli that
have been previously scanned. Considerations about the eye movements raise the
question of how we are able to maintain a coherent memory, despite the frequent
drastically changes in the perception. In this article, we present a
computational model that is able to anticipate the consequences of the eye
movements on the visual perception in order to update a spatial memory
</summary>
    <author>
      <name>Jérémy Fix</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Julien Vitay</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Nicolas Rougier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Anticipatory Behavior in Adaptive Learning Systems 2006
  (2006)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0610041v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0610041v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0611006v1</id>
    <updated>2006-11-02T00:47:57Z</updated>
    <published>2006-11-02T00:47:57Z</published>
    <title>Evolving controllers for simulated car racing</title>
    <summary>  This paper describes the evolution of controllers for racing a simulated
radio-controlled car around a track, modelled on a real physical track. Five
different controller architectures were compared, based on neural networks,
force fields and action sequences. The controllers use either egocentric (first
person), Newtonian (third person) or no information about the state of the car
(open-loop controller). The only controller that was able to evolve good racing
behaviour was based on a neural network acting on egocentric inputs.
</summary>
    <author>
      <name>Julian Togelius</name>
    </author>
    <author>
      <name>Simon M. Lucas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Won the CEC 2005 Best Student Paper Award</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 2005 Congress on Evolutionary Computation,
  pages 1906-1913</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0611006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0611006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0611032v2</id>
    <updated>2007-04-08T23:01:48Z</updated>
    <published>2006-11-07T20:25:28Z</published>
    <title>V-like formations in flocks of artificial birds</title>
    <summary>  We consider flocks of artificial birds and study the emergence of V-like
formations during flight. We introduce a small set of fully distributed
positioning rules to guide the birds' movements and demonstrate, by means of
simulations, that they tend to lead to stabilization into several of the
well-known V-like formations that have been observed in nature. We also provide
quantitative indicators that we believe are closely related to achieving V-like
formations, and study their behavior over a large set of independent
simulations.
</summary>
    <author>
      <name>Andre Nathan</name>
    </author>
    <author>
      <name>Valmir C. Barbosa</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1162/artl.2008.14.2.179</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1162/artl.2008.14.2.179" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Artificial Life 14 (2008), 179-188</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0611032v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0611032v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0611077v1</id>
    <updated>2006-11-16T03:27:16Z</updated>
    <published>2006-11-16T03:27:16Z</published>
    <title>Evolutionary Optimization in an Algorithmic Setting</title>
    <summary>  Evolutionary processes proved very useful for solving optimization problems.
In this work, we build a formalization of the notion of cooperation and
competition of multiple systems working toward a common optimization goal of
the population using evolutionary computation techniques. It is justified that
evolutionary algorithms are more expressive than conventional recursive
algorithms. Three subclasses of evolutionary algorithms are proposed here:
bounded finite, unbounded finite and infinite types. Some results on
completeness, optimality and search decidability for the above classes are
presented. A natural extension of Evolutionary Turing Machine model developed
in this paper allows one to mathematically represent and study properties of
cooperation and competition in a population of optimized species.
</summary>
    <author>
      <name>Mark Burgin</name>
    </author>
    <author>
      <name>Eugene Eberbach</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0611077v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0611077v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0611104v1</id>
    <updated>2006-11-21T12:54:29Z</updated>
    <published>2006-11-21T12:54:29Z</published>
    <title>Learning and discrimination through STDP in a top-down modulated
  associative memory</title>
    <summary>  This article underlines the learning and discrimination capabilities of a
model of associative memory based on artificial networks of spiking neurons.
Inspired from neuropsychology and neurobiology, the model implements top-down
modulations, as in neocortical layer V pyramidal neurons, with a learning rule
based on synaptic plasticity (STDP), for performing a multimodal association
learning task. A temporal correlation method of analysis proves the ability of
the model to associate specific activity patterns to different samples of
stimulation. Even in the absence of initial learning and with continuously
varying weights, the activity patterns become stable enough for discrimination.
</summary>
    <author>
      <name>Anthony Mouraud</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ISC, GRIMAAG</arxiv:affiliation>
    </author>
    <author>
      <name>Hélène Paugam-Moisy</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ISC</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of 14 European Symposium on Artificial Neural Networks
  (ESANN 2006) (03/2006) 611-616</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0611104v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0611104v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0705.0199v2</id>
    <updated>2007-05-08T01:06:10Z</updated>
    <published>2007-05-02T04:04:51Z</published>
    <title>The Parameter-Less Self-Organizing Map algorithm</title>
    <summary>  The Parameter-Less Self-Organizing Map (PLSOM) is a new neural network
algorithm based on the Self-Organizing Map (SOM). It eliminates the need for a
learning rate and annealing schemes for learning rate and neighbourhood size.
We discuss the relative performance of the PLSOM and the SOM and demonstrate
some tasks in which the SOM fails but the PLSOM performs satisfactory. Finally
we discuss some example applications of the PLSOM and present a proof of
ordering under certain limited conditions.
</summary>
    <author>
      <name>Erik Berglund</name>
    </author>
    <author>
      <name>Joaquin Sitte</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 27 figures. Based on publication in IEEE Trans. on Neural
  Networks</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Neural Networks, 2006 v.17, n.2, pp.305-316</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0705.0199v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0705.0199v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0709.3587v1</id>
    <updated>2007-09-22T15:54:37Z</updated>
    <published>2007-09-22T15:54:37Z</published>
    <title>Self-organizing maps and symbolic data</title>
    <summary>  In data analysis new forms of complex data have to be considered like for
example (symbolic data, functional data, web data, trees, SQL query and
multimedia data, ...). In this context classical data analysis for knowledge
discovery based on calculating the center of gravity can not be used because
input are not $\mathbb{R}^p$ vectors. In this paper, we present an application
on real world symbolic data using the self-organizing map. To this end, we
propose an extension of the self-organizing map that can handle symbolic data.
</summary>
    <author>
      <name>Aïcha El Golli</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt / INRIA Sophia Antipolis</arxiv:affiliation>
    </author>
    <author>
      <name>Brieuc Conan-Guez</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt / INRIA Sophia Antipolis</arxiv:affiliation>
    </author>
    <author>
      <name>Fabrice Rossi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt / INRIA Sophia Antipolis</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Symbolic Data Analysis 2, 1 (2004)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0709.3587v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0709.3587v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0709.3642v1</id>
    <updated>2007-09-23T14:10:48Z</updated>
    <published>2007-09-23T14:10:48Z</published>
    <title>Functional Multi-Layer Perceptron: a Nonlinear Tool for Functional Data
  Analysis</title>
    <summary>  In this paper, we study a natural extension of Multi-Layer Perceptrons (MLP)
to functional inputs. We show that fundamental results for classical MLP can be
extended to functional MLP. We obtain universal approximation results that show
the expressive power of functional MLP is comparable to that of numerical MLP.
We obtain consistency results which imply that the estimation of optimal
parameters for functional MLP is statistically well defined. We finally show on
simulated and real world data that the proposed model performs in a very
satisfactory way.
</summary>
    <author>
      <name>Fabrice Rossi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt / INRIA Sophia Antipolis, CEREMADE</arxiv:affiliation>
    </author>
    <author>
      <name>Brieuc Conan-Guez</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt / INRIA Sophia Antipolis, CEREMADE</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.neunet.2004.07.001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.neunet.2004.07.001" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">http://www.sciencedirect.com/science/journal/08936080</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Networks 18, 1 (2005) 45--60</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0709.3642v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0709.3642v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.0213v1</id>
    <updated>2007-10-01T06:51:42Z</updated>
    <published>2007-10-01T06:51:42Z</published>
    <title>Optimising the topology of complex neural networks</title>
    <summary>  In this paper, we study instances of complex neural networks, i.e. neural
netwo rks with complex topologies. We use Self-Organizing Map neural networks
whose n eighbourhood relationships are defined by a complex network, to
classify handwr itten digits. We show that topology has a small impact on
performance and robus tness to neuron failures, at least at long learning
times. Performance may howe ver be increased (by almost 10%) by artificial
evolution of the network topo logy. In our experimental conditions, the evolved
networks are more random than their parents, but display a more heterogeneous
degree distribution.
</summary>
    <author>
      <name>Fei Jiang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Futurs, INRIA Futurs</arxiv:affiliation>
    </author>
    <author>
      <name>Hugues Berry</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Futurs</arxiv:affiliation>
    </author>
    <author>
      <name>Marc Schoenauer</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Futurs</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans ECCS'07 (2007)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.0213v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.0213v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.2227v1</id>
    <updated>2007-10-11T12:09:33Z</updated>
    <published>2007-10-11T12:09:33Z</published>
    <title>A System for Predicting Subcellular Localization of Yeast Genome Using
  Neural Network</title>
    <summary>  The subcellular location of a protein can provide valuable information about
its function. With the rapid increase of sequenced genomic data, the need for
an automated and accurate tool to predict subcellular localization becomes
increasingly important. Many efforts have been made to predict protein
subcellular localization. This paper aims to merge the artificial neural
networks and bioinformatics to predict the location of protein in yeast genome.
We introduce a new subcellular prediction method based on a backpropagation
neural network. The results show that the prediction within an error limit of 5
to 10 percentage can be achieved with the system.
</summary>
    <author>
      <name>Sabu M. Thampi</name>
    </author>
    <author>
      <name>K. Chandra Sekaran</name>
    </author>
    <link href="http://arxiv.org/abs/0710.2227v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.2227v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4725v1</id>
    <updated>2007-10-25T09:37:48Z</updated>
    <published>2007-10-25T09:37:48Z</published>
    <title>Fault-Trajectory Approach for Fault Diagnosis on Analog Circuits</title>
    <summary>  This issue discusses the fault-trajectory approach suitability for fault
diagnosis on analog networks. Recent works have shown promising results
concerning a method based on this concept for ATPG for diagnosing faults on
analog networks. Such method relies on evolutionary techniques, where a generic
algorithm (GA) is coded to generate a set of optimum frequencies capable to
disclose faults.
</summary>
    <author>
      <name>Carlos Eduardo Savioli</name>
    </author>
    <author>
      <name>Claudio C. Czendrodi</name>
    </author>
    <author>
      <name>Jose Vicente Calvano</name>
    </author>
    <author>
      <name>Antonio Carneiro De Mesquita Filho</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4725v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4725v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0712.2630v1</id>
    <updated>2007-12-17T19:59:42Z</updated>
    <published>2007-12-17T19:59:42Z</published>
    <title>Evolving XSLT stylesheets</title>
    <summary>  This paper introduces a procedure based on genetic programming to evolve XSLT
programs (usually called stylesheets or logicsheets). XSLT is a general
purpose, document-oriented functional language, generally used to transform XML
documents (or, in general, solve any problem that can be coded as an XML
document). The proposed solution uses a tree representation for the stylesheets
as well as diverse specific operators in order to obtain, in the studied cases
and a reasonable time, a XSLT stylesheet that performs the transformation.
Several types of representation have been compared, resulting in different
performance and degree of success.
</summary>
    <author>
      <name>Nestor Zorzano</name>
    </author>
    <author>
      <name>Daniel Merino</name>
    </author>
    <author>
      <name>J. L. J. Laredo</name>
    </author>
    <author>
      <name>J. P. Sevilla</name>
    </author>
    <author>
      <name>Pablo Garcia</name>
    </author>
    <author>
      <name>J. J. Merelo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">First draft, preparing for WCCI 2008</arxiv:comment>
    <link href="http://arxiv.org/abs/0712.2630v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0712.2630v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0801.1883v1</id>
    <updated>2008-01-12T08:02:12Z</updated>
    <published>2008-01-12T08:02:12Z</published>
    <title>D-optimal Bayesian Interrogation for Parameter and Noise Identification
  of Recurrent Neural Networks</title>
    <summary>  We introduce a novel online Bayesian method for the identification of a
family of noisy recurrent neural networks (RNNs). We develop Bayesian active
learning technique in order to optimize the interrogating stimuli given past
experiences. In particular, we consider the unknown parameters as stochastic
variables and use the D-optimality principle, also known as `\emph{infomax
method}', to choose optimal stimuli. We apply a greedy technique to maximize
the information gain concerning network parameters at each time step. We also
derive the D-optimal estimation of the additive noise that perturbs the
dynamical system of the RNN. Our analytical results are approximation-free. The
analytic derivation gives rise to attractive quadratic update rules.
</summary>
    <author>
      <name>Barnabas Poczos</name>
    </author>
    <author>
      <name>Andras Lorincz</name>
    </author>
    <link href="http://arxiv.org/abs/0801.1883v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.1883v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0801.3113v1</id>
    <updated>2008-01-21T00:34:55Z</updated>
    <published>2008-01-21T00:34:55Z</published>
    <title>iBOA: The Incremental Bayesian Optimization Algorithm</title>
    <summary>  This paper proposes the incremental Bayesian optimization algorithm (iBOA),
which modifies standard BOA by removing the population of solutions and using
incremental updates of the Bayesian network. iBOA is shown to be able to learn
and exploit unrestricted Bayesian networks using incremental techniques for
updating both the structure as well as the parameters of the probabilistic
model. This represents an important step toward the design of competent
incremental estimation of distribution algorithms that can solve difficult
nearly decomposable problems scalably and reliably.
</summary>
    <author>
      <name>Martin Pelikan</name>
    </author>
    <author>
      <name>Kumara Sastry</name>
    </author>
    <author>
      <name>David E. Goldberg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Also available at the MEDAL web site, http://medal.cs.umsl.edu/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Genetic and Evolutionary Computation Conference
  (GECCO-2008), ACM Press, 455-462</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0801.3113v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.3113v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.2.8; G.1.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0801.4287v2</id>
    <updated>2008-03-03T17:05:58Z</updated>
    <published>2008-01-28T14:19:12Z</published>
    <title>Movie Recommendation Systems Using An Artificial Immune System</title>
    <summary>  We apply the Artificial Immune System (AIS) technology to the Collaborative
Filtering (CF) technology when we build the movie recommendation system. Two
different affinity measure algorithms of AIS, Kendall tau and Weighted Kappa,
are used to calculate the correlation coefficients for this movie
recommendation system. From the testing we think that Weighted Kappa is more
suitable than Kendall tau for movie problems.
</summary>
    <author>
      <name>Qi Chen</name>
    </author>
    <author>
      <name>Uwe Aickelin</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">6th International Conference in Adaptive Computing in Design and
  Manufacture (ACDM 2004), Bristol, UK, 2004</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0801.4287v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.4287v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0802.0252v1</id>
    <updated>2008-02-02T15:10:35Z</updated>
    <published>2008-02-02T15:10:35Z</published>
    <title>Accélération des cartes auto-organisatrices sur tableau de
  dissimilarités par séparation et évaluation</title>
    <summary>  In this paper, a new implementation of the adaptation of Kohonen
self-organising maps (SOM) to dissimilarity matrices is proposed. This
implementation relies on the branch and bound principle to reduce the algorithm
running time. An important property of this new approach is that the obtained
algorithm produces exactly the same results as the standard algorithm.
</summary>
    <author>
      <name>Brieuc Conan-Guez</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LITA</arxiv:affiliation>
    </author>
    <author>
      <name>Fabrice Rossi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt / INRIA Sophia Antipolis</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A para\^itre</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">REVUE DES NOUVELLES TECHNOLOGIES DE L'INFORMATION (2008)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0802.0252v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0802.0252v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0802.0861v1</id>
    <updated>2008-02-06T18:50:16Z</updated>
    <published>2008-02-06T18:50:16Z</published>
    <title>Using Bayesian Blocks to Partition Self-Organizing Maps</title>
    <summary>  Self organizing maps (SOMs) are widely-used for unsupervised classification.
For this application, they must be combined with some partitioning scheme that
can identify boundaries between distinct regions in the maps they produce. We
discuss a novel partitioning scheme for SOMs based on the Bayesian Blocks
segmentation algorithm of Scargle [1998]. This algorithm minimizes a cost
function to identify contiguous regions over which the values of the attributes
can be represented as approximately constant. Because this cost function is
well-defined and largely independent of assumptions regarding the number and
structure of clusters in the original sample space, this partitioning scheme
offers significant advantages over many conventional methods. Sample code is
available.
</summary>
    <author>
      <name>Paul R. Gazis</name>
    </author>
    <author>
      <name>Jeffrey D. Scargle</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0802.0861v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0802.0861v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0802.2138v1</id>
    <updated>2008-02-15T04:53:33Z</updated>
    <published>2008-02-15T04:53:33Z</published>
    <title>Support Vector classifiers for Land Cover Classification</title>
    <summary>  Support vector machines represent a promising development in machine learning
research that is not widely used within the remote sensing community. This
paper reports the results of Multispectral(Landsat-7 ETM+) and Hyperspectral
DAIS)data in which multi-class SVMs are compared with maximum likelihood and
artificial neural network methods in terms of classification accuracy. Our
results show that the SVM achieves a higher level of classification accuracy
than either the maximum likelihood or the neural classifier, and that the
support vector machine can be used with small training datasets and
high-dimensional data.
</summary>
    <author>
      <name>Mahesh Pal</name>
    </author>
    <author>
      <name>Paul M. Mather</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1080/01431160802007624</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1080/01431160802007624" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 1 figure, Published in MapIndia Conference 2003</arxiv:comment>
    <link href="http://arxiv.org/abs/0802.2138v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0802.2138v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0802.3235v3</id>
    <updated>2009-07-02T17:24:41Z</updated>
    <published>2008-02-21T23:41:09Z</published>
    <title>Characterization of the convergence of stationary Fokker-Planck learning</title>
    <summary>  The convergence properties of the stationary Fokker-Planck algorithm for the
estimation of the asymptotic density of stochastic search processes is studied.
Theoretical and empirical arguments for the characterization of convergence of
the estimation in the case of separable and nonseparable nonlinear optimization
problems are given. Some implications of the convergence of stationary
Fokker-Planck learning for the inference of parameters in artificial neural
network models are outlined.
</summary>
    <author>
      <name>Arturo Berrones</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.neucom.2008.12.042</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.neucom.2008.12.042" rel="related"/>
    <link href="http://arxiv.org/abs/0802.3235v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0802.3235v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0803.1568v1</id>
    <updated>2008-03-11T12:39:01Z</updated>
    <published>2008-03-11T12:39:01Z</published>
    <title>Dempster-Shafer for Anomaly Detection</title>
    <summary>  In this paper, we implement an anomaly detection system using the
Dempster-Shafer method. Using two standard benchmark problems we show that by
combining multiple signals it is possible to achieve better results than by
using a single signal. We further show that by applying this approach to a
real-world email dataset the algorithm works for email worm detection.
Dempster-Shafer can be a promising method for anomaly detection problems with
multiple features (data sources), and two or more classes.
</summary>
    <author>
      <name>Qi Chen</name>
    </author>
    <author>
      <name>Uwe Aickelin</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the International Conference on Data Mining (DMIN
  2006), pp 232-238, Las Vegas, USA 2006</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0803.1568v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0803.1568v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0803.1596v1</id>
    <updated>2008-03-11T14:19:33Z</updated>
    <published>2008-03-11T14:19:33Z</published>
    <title>Using Intelligent Agents to understand organisational behaviour</title>
    <summary>  This paper introduces two ongoing research projects which seek to apply
computer modelling techniques in order to simulate human behaviour within
organisations. Previous research in other disciplines has suggested that
complex social behaviours are governed by relatively simple rules which, when
identified, can be used to accurately model such processes using computer
technology. The broad objective of our research is to develop a similar
capability within organisational psychology.
</summary>
    <author>
      <name>Helen Celia</name>
    </author>
    <author>
      <name>Christopher Clegg</name>
    </author>
    <author>
      <name>Mark Robinson</name>
    </author>
    <author>
      <name>Peer-Olaf Siebers</name>
    </author>
    <author>
      <name>Uwe Aickelin</name>
    </author>
    <author>
      <name>Christine Sprigg</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the British Psychology Society Annual Conference,
  Occupational Psychology Division (BPS 2007), Bristol, UK 2007</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0803.1596v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0803.1596v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0803.1926v1</id>
    <updated>2008-03-13T11:43:25Z</updated>
    <published>2008-03-13T11:43:25Z</published>
    <title>Improved evolutionary generation of XSLT stylesheets</title>
    <summary>  This paper introduces a procedure based on genetic programming to evolve XSLT
programs (usually called stylesheets or logicsheets). XSLT is a general
purpose, document-oriented functional language, generally used to transform XML
documents (or, in general, solve any problem that can be coded as an XML
document). The proposed solution uses a tree representation for the stylesheets
as well as diverse specific operators in order to obtain, in the studied cases
and a reasonable time, a XSLT stylesheet that performs the transformation.
Several types of representation have been compared, resulting in different
performance and degree of success.
</summary>
    <author>
      <name>Pablo Garcia-Sanchez</name>
    </author>
    <author>
      <name>J. L. J. Laredo</name>
    </author>
    <author>
      <name>J. P. Sevilla</name>
    </author>
    <author>
      <name>Pedro Castillo</name>
    </author>
    <author>
      <name>J. J. Merelo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted as poster at the GECCO-2008 conference</arxiv:comment>
    <link href="http://arxiv.org/abs/0803.1926v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0803.1926v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0803.2695v1</id>
    <updated>2008-03-18T18:27:14Z</updated>
    <published>2008-03-18T18:27:14Z</published>
    <title>KohonAnts: A Self-Organizing Ant Algorithm for Clustering and Pattern
  Classification</title>
    <summary>  In this paper we introduce a new ant-based method that takes advantage of the
cooperative self-organization of Ant Colony Systems to create a naturally
inspired clustering and pattern recognition method. The approach considers each
data item as an ant, which moves inside a grid changing the cells it goes
through, in a fashion similar to Kohonen's Self-Organizing Maps. The resulting
algorithm is conceptually more simple, takes less free parameters than other
ant-based clustering algorithms, and, after some parameter tuning, yields very
good results on some benchmark problems.
</summary>
    <author>
      <name>C. Fernandes</name>
    </author>
    <author>
      <name>A. M. Mora</name>
    </author>
    <author>
      <name>J. J. Merelo</name>
    </author>
    <author>
      <name>V. Ramos</name>
    </author>
    <author>
      <name>J. L. J. Laredo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ALIFE XI</arxiv:comment>
    <link href="http://arxiv.org/abs/0803.2695v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0803.2695v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0803.2925v1</id>
    <updated>2008-03-20T03:50:53Z</updated>
    <published>2008-03-20T03:50:53Z</published>
    <title>Equivalence of Probabilistic Tournament and Polynomial Ranking Selection</title>
    <summary>  Crucial to an Evolutionary Algorithm's performance is its selection scheme.
We mathematically investigate the relation between polynomial rank and
probabilistic tournament methods which are (respectively) generalisations of
the popular linear ranking and tournament selection schemes. We show that every
probabilistic tournament is equivalent to a unique polynomial rank scheme. In
fact, we derived explicit operators for translating between these two types of
selection. Of particular importance is that most linear and most practical
quadratic rank schemes are probabilistic tournaments.
</summary>
    <author>
      <name>Kassel Hingee</name>
    </author>
    <author>
      <name>Marcus Hutter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 double-column pages, 5 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. 2008 Congress on Evolutionary Computation (CEC 2008) pages
  564-571</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0803.2925v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0803.2925v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0803.2957v1</id>
    <updated>2008-03-20T10:19:01Z</updated>
    <published>2008-03-20T10:19:01Z</published>
    <title>Enhanced Direct and Indirect Genetic Algorithm Approaches for a Mall
  Layout and Tenant Selection Problem</title>
    <summary>  During our earlier research, it was recognised that in order to be successful
with an indirect genetic algorithm approach using a decoder, the decoder has to
strike a balance between being an optimiser in its own right and finding
feasible solutions. Previously this balance was achieved manually. Here we
extend this by presenting an automated approach where the genetic algorithm
itself, simultaneously to solving the problem, sets weights to balance the
components out. Subsequently we were able to solve a complex and non-linear
scheduling problem better than with a standard direct genetic algorithm
implementation.
</summary>
    <author>
      <name>Uwe Aickelin</name>
    </author>
    <author>
      <name>Kathryn Dowsland</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1023/A:1016536623961,</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1023/A:1016536623961," rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Heuristics, 8(5), pp 503-514, 2002</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0803.2957v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0803.2957v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0803.3746v1</id>
    <updated>2008-03-26T15:14:33Z</updated>
    <published>2008-03-26T15:14:33Z</published>
    <title>Cluster Approach to the Domains Formation</title>
    <summary>  As a rule, a quadratic functional depending on a great number of binary
variables has a lot of local minima. One of approaches allowing one to find in
averaged deeper local minima is aggregation of binary variables into larger
blocks/domains. To minimize the functional one has to change the states of
aggregated variables (domains). In the present publication we discuss methods
of domains formation. It is shown that the best results are obtained when
domains are formed by variables that are strongly connected with each other.
</summary>
    <author>
      <name>Leonid B. Litinskii</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 5 figures, PDF-file</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Optical Memory &amp; Neural Networks (Information Optics), 2007,
  v.16(3) pp.144-153</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0803.3746v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0803.3746v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0803.4240v1</id>
    <updated>2008-03-29T07:50:24Z</updated>
    <published>2008-03-29T07:50:24Z</published>
    <title>Neutral Fitness Landscape in the Cellular Automata Majority Problem</title>
    <summary>  We study in detail the fitness landscape of a difficult cellular automata
computational task: the majority problem. Our results show why this problem
landscape is so hard to search, and we quantify the large degree of neutrality
found in various ways. We show that a particular subspace of the solution
space, called the "Olympus", is where good solutions concentrate, and give
measures to quantitatively characterize this subspace.
</summary>
    <author>
      <name>Sébastien Verel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">I3S</arxiv:affiliation>
    </author>
    <author>
      <name>Philippe Collard</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">I3S</arxiv:affiliation>
    </author>
    <author>
      <name>Marco Tomassini</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ISI</arxiv:affiliation>
    </author>
    <author>
      <name>Leonardo Vanneschi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ISI</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans ACRI 2006 - 7th International Conference on Cellular Automata
  For Research and Industry - ACRI 2006, France (2006)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0803.4240v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0803.4240v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0804.0353v1</id>
    <updated>2008-04-02T13:56:10Z</updated>
    <published>2008-04-02T13:56:10Z</published>
    <title>Graphical Estimation of Permeability Using RST&amp;NFIS</title>
    <summary>  This paper pursues some applications of Rough Set Theory (RST) and
neural-fuzzy model to analysis of "lugeon data". In the manner, using Self
Organizing Map (SOM) as a pre-processing the data are scaled and then the
dominant rules by RST, are elicited. Based on these rules variations of
permeability in the different levels of Shivashan dam, Iran has been
highlighted. Then, via using a combining of SOM and an adaptive Neuro-Fuzzy
Inference System (NFIS) another analysis on the data was carried out. Finally,
a brief comparison between the obtained results of RST and SOM-NFIS (briefly
SONFIS) has been rendered.
</summary>
    <author>
      <name>H. Owladeghaffari</name>
    </author>
    <author>
      <name>K. Shahriar W. Pedrycz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages;NAFIPS08</arxiv:comment>
    <link href="http://arxiv.org/abs/0804.0353v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0804.0353v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0804.1133v1</id>
    <updated>2008-04-07T20:11:24Z</updated>
    <published>2008-04-07T20:11:24Z</published>
    <title>Prospective Algorithms for Quantum Evolutionary Computation</title>
    <summary>  This effort examines the intersection of the emerging field of quantum
computing and the more established field of evolutionary computation. The goal
is to understand what benefits quantum computing might offer to computational
intelligence and how computational intelligence paradigms might be implemented
as quantum programs to be run on a future quantum computer. We critically
examine proposed algorithms and methods for implementing computational
intelligence paradigms, primarily focused on heuristic optimization methods
including and related to evolutionary computation, with particular regard for
their potential for eventual implementation on quantum computing hardware.
</summary>
    <author>
      <name>Donald A. Sofge</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0804.1133v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0804.1133v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0804.4237v1</id>
    <updated>2008-04-26T16:55:59Z</updated>
    <published>2008-04-26T16:55:59Z</published>
    <title>Explaining the Logical Nature of Electrical Solitons in Neural Circuits</title>
    <summary>  Neurons are modeled electrically based on ferroelectric membranes thin enough
to permit charge transfer, conjectured to be the tunneling result of thermally
energetic ions and random electrons. These membranes can be triggered to
produce electrical solitons, the main signals for brain associative memory and
logical processing. Dendritic circuits are modeled, and electrical solitons are
simulated to demonstrate the nature of soliton propagation, soliton reflection,
the collision of solitons, as well as soliton OR gates, AND gates, XOR gates
and NOT gates.
</summary>
    <author>
      <name>John Robert Burger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 16 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0804.4237v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0804.4237v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0804.4808v1</id>
    <updated>2008-04-30T12:23:05Z</updated>
    <published>2008-04-30T12:23:05Z</published>
    <title>Solving Time of Least Square Systems in Sigma-Pi Unit Networks</title>
    <summary>  The solving of least square systems is a useful operation in
neurocomputational modeling of learning, pattern matching, and pattern
recognition. In these last two cases, the solution must be obtained on-line,
thus the time required to solve a system in a plausible neural architecture is
critical. This paper presents a recurrent network of Sigma-Pi neurons, whose
solving time increases at most like the logarithm of the system size, and of
its condition number, which provides plausible computation times for biological
systems.
</summary>
    <author>
      <name>Pierre Courrieu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LPC</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Nombre de pages: 7</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Information Processing - Letters and Reviews 4, 3 (2004)
  39-45</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0804.4808v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0804.4808v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0805.0231v4</id>
    <updated>2008-05-18T06:38:04Z</updated>
    <published>2008-05-02T13:55:37Z</published>
    <title>CMA-ES with Two-Point Step-Size Adaptation</title>
    <summary>  We combine a refined version of two-point step-size adaptation with the
covariance matrix adaptation evolution strategy (CMA-ES). Additionally, we
suggest polished formulae for the learning rate of the covariance matrix and
the recombination weights. In contrast to cumulative step-size adaptation or to
the 1/5-th success rule, the refined two-point adaptation (TPA) does not rely
on any internal model of optimality. In contrast to conventional
self-adaptation, the TPA will achieve a better target step-size in particular
with large populations. The disadvantage of TPA is that it relies on two
additional objective function
</summary>
    <author>
      <name>Nikolaus Hansen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Futurs</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/0805.0231v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0805.0231v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0805.0697v1</id>
    <updated>2008-05-06T11:06:49Z</updated>
    <published>2008-05-06T11:06:49Z</published>
    <title>Stochastic Optimization Approaches for Solving Sudoku</title>
    <summary>  In this paper the Sudoku problem is solved using stochastic search techniques
and these are: Cultural Genetic Algorithm (CGA), Repulsive Particle Swarm
Optimization (RPSO), Quantum Simulated Annealing (QSA) and the Hybrid method
that combines Genetic Algorithm with Simulated Annealing (HGASA). The results
obtained show that the CGA, QSA and HGASA are able to solve the Sudoku puzzle
with CGA finding a solution in 28 seconds, while QSA finding a solution in 65
seconds and HGASA in 1.447 seconds. This is mainly because HGASA combines the
parallel searching of GA with the flexibility of SA. The RPSO was found to be
unable to solve the puzzle.
</summary>
    <author>
      <name>Meir Perez</name>
    </author>
    <author>
      <name>Tshilidzi Marwala</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.eswa.2012.04.019</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.eswa.2012.04.019" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0805.0697v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0805.0697v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0805.1153v1</id>
    <updated>2008-05-08T12:10:21Z</updated>
    <published>2008-05-08T12:10:21Z</published>
    <title>Contact state analysis using NFIS and SOM</title>
    <summary>  This paper reports application of neuro- fuzzy inference system (NFIS) and
self organizing feature map neural networks (SOM) on detection of contact state
in a block system. In this manner, on a simple system, the evolution of contact
states, by parallelization of DDA, has been investigated. So, a comparison
between NFIS and SOM results has been presented. The results show applicability
of the proposed methods, by different accuracy, on detection of contact's
distribution.
</summary>
    <author>
      <name>H. Owladeghaffari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. International Symposium on Computational Mechanics (ISCM2007),
  Yao ZH &amp; Yuan MW (eds.), Beijing: Tsinghua University Press &amp; Springer, July
  30-August 1, 2007, Beijing, China,</arxiv:comment>
    <link href="http://arxiv.org/abs/0805.1153v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0805.1153v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0806.3646v2</id>
    <updated>2008-08-09T21:45:53Z</updated>
    <published>2008-06-23T10:04:14Z</published>
    <title>Round Trip Time Prediction Using the Symbolic Function Network Approach</title>
    <summary>  In this paper, we develop a novel approach to model the Internet round trip
time using a recently proposed symbolic type neural network model called
symbolic function network. The developed predictor is shown to have good
generalization performance and simple representation compared to the multilayer
perceptron based predictors.
</summary>
    <author>
      <name>George S. Eskander</name>
    </author>
    <author>
      <name>Amir Atiya</name>
    </author>
    <author>
      <name>Kil To Chong</name>
    </author>
    <author>
      <name>Hyongsuk Kim</name>
    </author>
    <author>
      <name>Sung Goo Yoo</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ISITC, pp. 3-7, 2007 International Symposium on Information
  Technology Convergence (ISITC 2007), 2007</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0806.3646v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0806.3646v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0808.1378v1</id>
    <updated>2008-08-09T22:05:48Z</updated>
    <published>2008-08-09T22:05:48Z</published>
    <title>A Novel Symbolic Type Neural Network Model- Application to River Flow
  Forecasting</title>
    <summary>  In this paper we introduce a new symbolic type neural tree network called
symbolic function network (SFN) that is based on using elementary functions to
model systems in a symbolic form. The proposed formulation permits feature
selection, functional selection, and flexible structure. We applied this model
on the River Flow forecasting problem. The results found to be superior in both
fitness and sparsity.
</summary>
    <author>
      <name>George S. Eskander</name>
    </author>
    <author>
      <name>Amir F. Atiya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in ICENCO2007, Cairo, December 2007</arxiv:comment>
    <link href="http://arxiv.org/abs/0808.1378v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0808.1378v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.4622v1</id>
    <updated>2008-09-26T13:12:36Z</updated>
    <published>2008-09-26T13:12:36Z</published>
    <title>A computational approach to the covert and overt deployment of spatial
  attention</title>
    <summary>  Popular computational models of visual attention tend to neglect the
influence of saccadic eye movements whereas it has been shown that the primates
perform on average three of them per seconds and that the neural substrate for
the deployment of attention and the execution of an eye movement might
considerably overlap. Here we propose a computational model in which the
deployment of attention with or without a subsequent eye movement emerges from
local, distributed and numerical computations.
</summary>
    <author>
      <name>Jérémy Fix</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - Loria</arxiv:affiliation>
    </author>
    <author>
      <name>Nicolas P. Rougier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - Loria, University of Colorado, Boulder</arxiv:affiliation>
    </author>
    <author>
      <name>Frédéric Alexandre</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - Loria</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans NeuroComp 2008 : 2i\`eme Conf\'erence Fran\c{c}aise de
  Neurosciences Computationnelles (2008)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0809.4622v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.4622v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0810.3356v1</id>
    <updated>2008-10-19T00:38:06Z</updated>
    <published>2008-10-19T00:38:06Z</published>
    <title>The Fundamental Problem with the Building Block Hypothesis</title>
    <summary>  Skepticism of the building block hypothesis (BBH) has previously been
expressed on account of the weak theoretical foundations of this hypothesis and
the anomalies in the empirical record of the simple genetic algorithm. In this
paper we hone in on a more fundamental cause for skepticism--the extraordinary
strength of some of the assumptions that undergird the BBH. Specifically, we
focus on assumptions made about the distribution of fitness over the genome
set, and argue that these assumptions are unacceptably strong. As most of these
assumptions have been embraced by the designers of so-called "competent"
genetic algorithms, our critique is relevant to an appraisal of such algorithms
as well.
</summary>
    <author>
      <name>Keki Burjorjee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preliminary version. 26 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/0810.3356v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0810.3356v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0810.3492v1</id>
    <updated>2008-10-20T08:36:22Z</updated>
    <published>2008-10-20T08:36:22Z</published>
    <title>The Connectivity of NK Landscapes' Basins: A Network Analysis</title>
    <summary>  We propose a network characterization of combinatorial fitness landscapes by
adapting the notion of inherent networks proposed for energy surfaces. We use
the well-known family of NK landscapes as an example. In our case the inherent
network is the graph where the vertices represent the local maxima in the
landscape, and the edges account for the transition probabilities between their
corresponding basins of attraction. We exhaustively extracted such networks on
representative small NK landscape instances, and performed a statistical
characterization of their properties. We found that most of these network
properties can be related to the search difficulty on the underlying NK
landscapes with varying values of K.
</summary>
    <author>
      <name>Sébastien Verel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">I3S</arxiv:affiliation>
    </author>
    <author>
      <name>Gabriela Ochoa</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ISI</arxiv:affiliation>
    </author>
    <author>
      <name>Marco Tomassini</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ISI</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Artificial Life XI, Winchester : France (2008)</arxiv:comment>
    <link href="http://arxiv.org/abs/0810.3492v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0810.3492v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0812.0882v1</id>
    <updated>2008-12-04T09:12:14Z</updated>
    <published>2008-12-04T09:12:14Z</published>
    <title>Elagage d'un perceptron multicouches : utilisation de l'analyse de la
  variance de la sensibilité des paramètres</title>
    <summary>  The stucture determination of a neural network for the modelisation of a
system remain the core of the problem. Within this framework, we propose a
pruning algorithm of the network based on the use of the analysis of the
sensitivity of the variance of all the parameters of the network. This
algorithm will be tested on two examples of simulation and its performances
will be compared with three other algorithms of pruning of the literature
</summary>
    <author>
      <name>Philippe Thomas</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CRAN</arxiv:affiliation>
    </author>
    <author>
      <name>André Thomas</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CRAN</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Conf\'erence Internationale Francophone d'Automatique CIFA'08,
  Bucarest : Roumanie (2008)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0812.0882v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0812.0882v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0812.1094v1</id>
    <updated>2008-12-05T08:49:53Z</updated>
    <published>2008-12-05T08:49:53Z</published>
    <title>Sélection de la structure d'un perceptron multicouches pour la
  réduction dun modèle de simulation d'une scierie</title>
    <summary>  Simulation is often used to evaluate the relevance of a Directing Program of
Production (PDP) or to evaluate its impact on detailed sc\'enarii of
scheduling. Within this framework, we propose to reduce the complexity of a
model of simulation by exploiting a multilayer perceptron. A main phase of the
modeling of one system using a multilayer perceptron remains the determination
of the structure of the network. We propose to compare and use various pruning
algorithms in order to determine the optimal structure of the network used to
reduce the complexity of the model of simulation of our case of application: a
sawmill.
</summary>
    <author>
      <name>Philippe Thomas</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CRAN</arxiv:affiliation>
    </author>
    <author>
      <name>André Thomas</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CRAN</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Conf\'erence Internationale Francophone d'Automatique CIFA'08,
  Bucarest : Roumanie (2008)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0812.1094v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0812.1094v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0901.0317v1</id>
    <updated>2009-01-03T17:35:49Z</updated>
    <published>2009-01-03T17:35:49Z</published>
    <title>Design of a P System based Artificial Graph Chemistry</title>
    <summary>  Artificial Chemistries (ACs) are symbolic chemical metaphors for the
exploration of Artificial Life, with specific focus on the origin of life. In
this work we define a P system based artificial graph chemistry to understand
the principles leading to the evolution of life-like structures in an AC set up
and to develop a unified framework to characterize and classify symbolic
artificial chemistries by devising appropriate formalism to capture semantic
and organizational information. An extension of P system is considered by
associating probabilities with the rules providing the topological framework
for the evolution of a labeled undirected graph based molecular reaction
semantics.
</summary>
    <author>
      <name>Janardan Misra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0901.0317v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0901.0317v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0901.0597v4</id>
    <updated>2010-09-13T12:40:44Z</updated>
    <published>2009-01-06T06:36:54Z</published>
    <title>On the Optimal Convergence Probability of Univariate Estimation of
  Distribution Algorithms</title>
    <summary>  In this paper, we obtain bounds on the probability of convergence to the
optimal solution for the compact Genetic Algorithm (cGA) and the Population
Based Incremental Learning (PBIL). We also give a sufficient condition for
convergence of these algorithms to the optimal solution and compute a range of
possible values of the parameters of these algorithms for which they converge
to the optimal solution with a confidence level.
</summary>
    <author>
      <name>Reza Rastegar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">evolutionary computation</arxiv:comment>
    <link href="http://arxiv.org/abs/0901.0597v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0901.0597v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0902.1690v1</id>
    <updated>2009-02-10T16:43:06Z</updated>
    <published>2009-02-10T16:43:06Z</published>
    <title>Back analysis of microplane model parameters using soft computing
  methods</title>
    <summary>  A new procedure based on layered feed-forward neural networks for the
microplane material model parameters identification is proposed in the present
paper. Novelties are usage of the Latin Hypercube Sampling method for the
generation of training sets, a systematic employment of stochastic sensitivity
analysis and a genetic algorithm-based training of a neural network by an
evolutionary algorithm. Advantages and disadvantages of this approach together
with possible extensions are thoroughly discussed and analyzed.
</summary>
    <author>
      <name>A. Kucerova</name>
    </author>
    <author>
      <name>M. Leps</name>
    </author>
    <author>
      <name>J. Zeman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 27 figures, 7 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">CAMES: Computer Assisted Mechanics and Engineering Sciences, 14
  (2), 219-242, 2007</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0902.1690v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0902.1690v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.2516v1</id>
    <updated>2009-03-14T00:18:08Z</updated>
    <published>2009-03-14T00:18:08Z</published>
    <title>Effect of Degree Distribution on Evolutionary Search</title>
    <summary>  This paper introduces a method to generate hierarchically modular networks
with prescribed node degree list and proposes a metric to measure network
modularity based on the notion of edge distance. The generated networks are
used as test problems to explore the effect of modularity and degree
distribution on evolutionary algorithm performance. Results from the
experiments (i) confirm a previous finding that modularity increases the
performance advantage of genetic algorithms over hill climbers, and (ii)
support a new conjecture that test problems with modularized constraint
networks having heavy-tailed right-skewed degree distributions are more easily
solved than test problems with modularized constraint networks having
bell-shaped normal degree distributions.
</summary>
    <author>
      <name>Susan Khor</name>
    </author>
    <link href="http://arxiv.org/abs/0903.2516v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.2516v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0904.3650v1</id>
    <updated>2009-04-23T10:44:21Z</updated>
    <published>2009-04-23T10:44:21Z</published>
    <title>The use of invariant moments in hand-written character recognition</title>
    <summary>  The goal of this paper is to present the implementation of a Radial Basis
Function neural network with built-in knowledge to recognize hand-written
characters. The neural network includes in its architecture gates controlled by
an attraction/repulsion system of coefficients. These coefficients are derived
from a preprocessing stage which groups the characters according to their
ascendant, central, or descendent components. The neural network is trained
using data from invariant moment functions. Results are compared with those
obtained using a K nearest neighbor method on the same moment data.
</summary>
    <author>
      <name>Dan L. Lacrama</name>
    </author>
    <author>
      <name>Ioan Snep</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages,exposed on 1st "European Conference on Computer Sciences &amp;
  Applications" - XA2006, Timisoara, Romania</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Ann. Univ. Tibiscus Comp. Sci. Series IV (2006), 91-102</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0904.3650v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0904.3650v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0905.3771v1</id>
    <updated>2009-05-22T22:09:59Z</updated>
    <published>2009-05-22T22:09:59Z</published>
    <title>Memory Retrieved from Single Neurons</title>
    <summary>  The paper examines the problem of accessing a vector memory from a single
neuron in a Hebbian neural network. It begins with the review of the author's
earlier method, which is different from the Hopfield model in that it recruits
neighboring neurons by spreading activity, making it possible for single or
group of neurons to become associated with vector memories. Some open issues
associated with this approach are identified. It is suggested that fragments
that generate stored memories could be associated with single neurons through
local spreading activity.
</summary>
    <author>
      <name>Subhash Kak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0905.3771v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0905.3771v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.1900v1</id>
    <updated>2009-06-10T09:56:29Z</updated>
    <published>2009-06-10T09:56:29Z</published>
    <title>How deals with discrete data for the reduction of simulation models
  using neural network</title>
    <summary>  Simulation is useful for the evaluation of a Master Production/distribution
Schedule (MPS). Also, the goal of this paper is the study of the design of a
simulation model by reducing its complexity. According to theory of
constraints, we want to build reduced models composed exclusively by
bottlenecks and a neural network. Particularly a multilayer perceptron, is
used. The structure of the network is determined by using a pruning procedure.
This work focuses on the impact of discrete data on the results and compares
different approaches to deal with these data. This approach is applied to
sawmill internal supply chain
</summary>
    <author>
      <name>Philippe Thomas</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CRAN</arxiv:affiliation>
    </author>
    <author>
      <name>André Thomas</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CRAN</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">13th IFAC Symp. On Information Control Problems in Manufacturing
  INCOM'09, Moscou : Russie (2009)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0906.1900v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.1900v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.4846v1</id>
    <updated>2009-06-26T06:25:07Z</updated>
    <published>2009-06-26T06:25:07Z</published>
    <title>A genetic algorithm for structure-activity relationships: software
  implementation</title>
    <summary>  The design and the implementation of a genetic algorithm are described. The
applicability domain is on structure-activity relationships expressed as
multiple linear regressions and predictor variables are from families of
structure-based molecular descriptors. An experiment to compare different
selection and survival strategies was designed and realized. The genetic
algorithm was run using the designed experiment on a set of 206 polychlorinated
biphenyls searching on structure-activity relationships having known the
measured octanol-water partition coefficients and a family of molecular
descriptors. The experiment shows that different selection and survival
strategies create different partitions on the entire population of all possible
genotypes.
</summary>
    <author>
      <name>Lorentz Jantschi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages; 10 figures; 14 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/0906.4846v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.4846v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3; I.6.4; J.2; J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.0229v1</id>
    <updated>2009-07-01T19:54:39Z</updated>
    <published>2009-07-01T19:54:39Z</published>
    <title>A new model of artificial neuron: cyberneuron and its use</title>
    <summary>  This article describes a new type of artificial neuron, called the authors
"cyberneuron". Unlike classical models of artificial neurons, this type of
neuron used table substitution instead of the operation of multiplication of
input values for the weights. This allowed to significantly increase the
information capacity of a single neuron, but also greatly simplify the process
of learning. Considered an example of the use of "cyberneuron" with the task of
detecting computer viruses.
</summary>
    <author>
      <name>S. V. Polikarpov</name>
    </author>
    <author>
      <name>V. S. Dergachev</name>
    </author>
    <author>
      <name>K. E. Rumyantsev</name>
    </author>
    <author>
      <name>D. M. Golubchikov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 23 figures, in Russian</arxiv:comment>
    <link href="http://arxiv.org/abs/0907.0229v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.0229v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3; I.2.6; I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.0520v1</id>
    <updated>2009-07-03T02:32:34Z</updated>
    <published>2009-07-03T02:32:34Z</published>
    <title>Computational Scenario-based Capability Planning</title>
    <summary>  Scenarios are pen-pictures of plausible futures, used for strategic planning.
The aim of this investigation is to expand the horizon of scenario-based
planning through computational models that are able to aid the analyst in the
planning process. The investigation builds upon the advances of Information and
Communication Technology (ICT) to create a novel, flexible and customizable
computational capability-based planning methodology that is practical and
theoretically sound. We will show how evolutionary computation, in particular
evolutionary multi-objective optimization, can play a central role - both as an
optimizer and as a source for innovation.
</summary>
    <author>
      <name>Hussein Abbass</name>
    </author>
    <author>
      <name>Axel Bender</name>
    </author>
    <author>
      <name>Helen Dam</name>
    </author>
    <author>
      <name>Stephen Baker</name>
    </author>
    <author>
      <name>James M Whitacre</name>
    </author>
    <author>
      <name>Ruhul Sarker</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/1389095.1389378</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/1389095.1389378" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">GECCO-2008, Atlanta, GA, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/0907.0520v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.0520v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.0592v1</id>
    <updated>2009-07-03T11:02:25Z</updated>
    <published>2009-07-03T11:02:25Z</published>
    <title>Credit Assignment in Adaptive Evolutionary Algorithms</title>
    <summary>  In this paper, a new method for assigning credit to search operators is
presented. Starting with the principle of optimizing search bias, search
operators are selected based on an ability to create solutions that are
historically linked to future generations. Using a novel framework for defining
performance measurements, distributing credit for performance, and the
statistical interpretation of this credit, a new adaptive method is developed
and shown to outperform a variety of adaptive and non-adaptive competitors.
</summary>
    <author>
      <name>James M. Whitacre</name>
    </author>
    <author>
      <name>Tuan Q. Pham</name>
    </author>
    <author>
      <name>Ruhul A. Sarker</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/1143997.1144206</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/1143997.1144206" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Genetic And Evolutionary Computation Conference, 2006</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0907.0592v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.0592v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0908.1597v1</id>
    <updated>2009-08-11T23:45:08Z</updated>
    <published>2009-08-11T23:45:08Z</published>
    <title>A quantum diffusion network</title>
    <summary>  Wong's diffusion network is a stochastic, zero-input Hopfield network with a
Gibbs stationary distribution over a bounded, connected continuum. Previously,
logarithmic thermal annealing was demonstrated for the diffusion network and
digital versions of it were studied and applied to imaging. Recently, "quantum"
annealed Markov chains have garnered significant attention because of their
improved performance over "pure" thermal annealing. In this note, a joint
quantum and thermal version of Wong's diffusion network is described and its
convergence properties are studied. Different choices for "auxiliary" functions
are discussed, including those of the kinetic type previously associated with
quantum annealing.
</summary>
    <author>
      <name>George Kesidis</name>
    </author>
    <link href="http://arxiv.org/abs/0908.1597v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0908.1597v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0908.3184v2</id>
    <updated>2009-11-19T18:31:27Z</updated>
    <published>2009-08-21T19:53:54Z</published>
    <title>Location of Single Neuron Memories in a Hebbian Network</title>
    <summary>  This paper reports the results of an experiment on the use of Kak's B-Matrix
approach to spreading activity in a Hebbian neural network. Specifically, it
concentrates on the memory retrieval from single neurons and compares the
performance of the B-Matrix approach to that of the traditional approach.
</summary>
    <author>
      <name>Krishna Chaithanya Lingashetty</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 11 figures, Presented at the Conference on Theoretical and
  Applied Computer Science 2009(TACS'09), Stillwater, Oklahoma. Corrected
  results and formatting changes</arxiv:comment>
    <link href="http://arxiv.org/abs/0908.3184v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0908.3184v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0910.0646v1</id>
    <updated>2009-10-04T22:09:18Z</updated>
    <published>2009-10-04T22:09:18Z</published>
    <title>Digital Business Ecosystems: Natural Science Paradigms</title>
    <summary>  A primary motivation for research in Digital Ecosystems is the desire to
exploit the self-organising properties of natural ecosystems. Ecosystems arc
thought to be robust, scalable architectures that can automatically solve
complex, dynamic problems. However, the biological processes that contribute to
these properties have not been made explicit in Digital Ecosystem research.
Here, we introduce how biological properties contribute to the self-organising
features of natural ecosystems. These properties include populations of
evolving agents, a complex dynamic environment, and spatial distributions which
generate local interactions. The potential for exploiting these properties in
artificial systems is then considered.
</summary>
    <author>
      <name>Gerard Briscoe</name>
    </author>
    <author>
      <name>Suzanne Sadedin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0910.0646v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0910.0646v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0910.4116v1</id>
    <updated>2009-10-21T15:02:12Z</updated>
    <published>2009-10-21T15:02:12Z</published>
    <title>Swarm Intelligence</title>
    <summary>  Biologically inspired computing is an area of computer science which uses the
advantageous properties of biological systems. It is the amalgamation of
computational intelligence and collective intelligence. Biologically inspired
mechanisms have already proved successful in achieving major advances in a wide
range of problems in computing and communication systems. The consortium of
bio-inspired computing are artificial neural networks, evolutionary algorithms,
swarm intelligence, artificial immune systems, fractal geometry, DNA computing
and quantum computing, etc. This article gives an introduction to swarm
intelligence.
</summary>
    <author>
      <name>Sabu M. Thampi</name>
    </author>
    <link href="http://arxiv.org/abs/0910.4116v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0910.4116v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.0936v1</id>
    <updated>2009-12-04T21:33:10Z</updated>
    <published>2009-12-04T21:33:10Z</published>
    <title>Neural-estimator for the surface emission rate of atmospheric gases</title>
    <summary>  The emission rate of minority atmospheric gases is inferred by a new approach
based on neural networks. The neural network applied is the multi-layer
perceptron with backpropagation algorithm for learning. The identification of
these surface fluxes is an inverse problem. A comparison between the new
neural-inversion and regularized inverse solution id performed. The results
obtained from the neural networks are significantly better. In addition, the
inversion with the neural netwroks is fster than regularized approaches, after
training.
</summary>
    <author>
      <name>F. F. Paes</name>
    </author>
    <author>
      <name>H. F. Campos Velho</name>
    </author>
    <link href="http://arxiv.org/abs/0912.0936v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.0936v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.1534v2</id>
    <updated>2010-01-18T21:20:22Z</updated>
    <published>2009-12-08T16:29:24Z</published>
    <title>Evolutionary multi-stage financial scenario tree generation</title>
    <summary>  Multi-stage financial decision optimization under uncertainty depends on a
careful numerical approximation of the underlying stochastic process, which
describes the future returns of the selected assets or asset categories.
Various approaches towards an optimal generation of discrete-time,
discrete-state approximations (represented as scenario trees) have been
suggested in the literature. In this paper, a new evolutionary algorithm to
create scenario trees for multi-stage financial optimization models will be
presented. Numerical results and implementation details conclude the paper.
</summary>
    <author>
      <name>Ronald Hochreiter</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-642-12242-2_19</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-642-12242-2_19" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Lecture Notes in Computer Science 6025:182-191. 2010.</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0912.1534v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.1534v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.CP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.PM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.4301v1</id>
    <updated>2010-01-25T02:09:42Z</updated>
    <published>2010-01-25T02:09:42Z</published>
    <title>Probabilistic Approach to Neural Networks Computation Based on Quantum
  Probability Model Probabilistic Principal Subspace Analysis Example</title>
    <summary>  In this paper, we introduce elements of probabilistic model that is suitable
for modeling of learning algorithms in biologically plausible artificial neural
networks framework. Model is based on two of the main concepts in quantum
physics - a density matrix and the Born rule. As an example, we will show that
proposed probabilistic interpretation is suitable for modeling of on-line
learning algorithms for PSA, which are preferably realized by a parallel
hardware based on very simple computational units. Proposed concept (model) can
be used in the context of improving algorithm convergence speed, learning
factor choice, or input signal scale robustness. We are going to see how the
Born rule and the Hebbian learning rule are connected
</summary>
    <author>
      <name>Marko V. Jankovic</name>
    </author>
    <link href="http://arxiv.org/abs/1001.4301v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.4301v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.0745v1</id>
    <updated>2010-02-03T14:02:39Z</updated>
    <published>2010-02-03T14:02:39Z</published>
    <title>Using CODEQ to Train Feed-forward Neural Networks</title>
    <summary>  CODEQ is a new, population-based meta-heuristic algorithm that is a hybrid of
concepts from chaotic search, opposition-based learning, differential evolution
and quantum mechanics. CODEQ has successfully been used to solve different
types of problems (e.g. constrained, integer-programming, engineering) with
excellent results. In this paper, CODEQ is used to train feed-forward neural
networks. The proposed method is compared with particle swarm optimization and
differential evolution algorithms on three data sets with encouraging results.
</summary>
    <author>
      <name>Mahamed G. H. Omran</name>
    </author>
    <author>
      <name>Faisal al-Adwani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1002.0745v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.0745v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.2012v1</id>
    <updated>2010-02-10T01:16:21Z</updated>
    <published>2010-02-10T01:16:21Z</published>
    <title>Implementing Genetic Algorithms on Arduino Micro-Controllers</title>
    <summary>  Since their conception in 1975, Genetic Algorithms have been an extremely
popular approach to find exact or approximate solutions to optimization and
search problems. Over the last years there has been an enhanced interest in the
field with related techniques, such as grammatical evolution, being developed.
Unfortunately, work on developing genetic optimizations for low-end embedded
architectures hasn't embraced the same enthusiasm. This short paper tackles
that situation by demonstrating how genetic algorithms can be implemented in
Arduino Duemilanove, a 16 MHz open-source micro-controller, with limited
computation power and storage resources. As part of this short paper, the
libraries used in this implementation are released into the public domain under
a GPL license.
</summary>
    <author>
      <name>Nuno Alves</name>
    </author>
    <link href="http://arxiv.org/abs/1002.2012v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.2012v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.0358v1</id>
    <updated>2010-03-01T14:32:11Z</updated>
    <published>2010-03-01T14:32:11Z</published>
    <title>Deep Big Simple Neural Nets Excel on Handwritten Digit Recognition</title>
    <summary>  Good old on-line back-propagation for plain multi-layer perceptrons yields a
very low 0.35% error rate on the famous MNIST handwritten digits benchmark. All
we need to achieve this best result so far are many hidden layers, many neurons
per layer, numerous deformed training images, and graphics cards to greatly
speed up learning.
</summary>
    <author>
      <name>Dan Claudiu Ciresan</name>
    </author>
    <author>
      <name>Ueli Meier</name>
    </author>
    <author>
      <name>Luca Maria Gambardella</name>
    </author>
    <author>
      <name>Juergen Schmidhuber</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1162/NECO_a_00052</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1162/NECO_a_00052" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 2 figures, 4 listings</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Computation, Volume 22, Number 12, December 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1003.0358v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.0358v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.2724v1</id>
    <updated>2010-03-13T16:36:47Z</updated>
    <published>2010-03-13T16:36:47Z</published>
    <title>Particle Swarm Optimization Based Diophantine Equation Solver</title>
    <summary>  The paper introduces particle swarm optimization as a viable strategy to find
numerical solution of Diophantine equation, for which there exists no general
method of finding solutions. The proposed methodology uses a population of
integer particles. The candidate solutions in the feasible space are optimized
to have better positions through particle best and global best positions. The
methodology, which follows fully connected neighborhood topology, can offer
many solutions of such equations.
</summary>
    <author>
      <name>Siby Abraham</name>
    </author>
    <author>
      <name>Sugata Sanyal</name>
    </author>
    <author>
      <name>Mukund Sanglikar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 Pages, 12 Figures, 5 Tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1003.2724v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.2724v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.3557v1</id>
    <updated>2010-04-20T20:17:41Z</updated>
    <published>2010-04-20T20:17:41Z</published>
    <title>Neuroevolutionary optimization</title>
    <summary>  This paper presents an application of evolutionary search procedures to
artificial neural networks. Here, we can distinguish among three kinds of
evolution in artificial neural networks, i.e. the evolution of connection
weights, of architectures, and of learning rules. We review each kind of
evolution in detail and analyse critical issues related to different
evolutions. This article concentrates on finding the suitable way of using
evolutionary algorithms for optimizing the artificial neural network
parameters.
</summary>
    <author>
      <name>Eva Volna</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science Issues online at
  http://ijcsi.org/articles/Neuroevolutionary-optimization.php</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSI, Volume 7, Issue 2, March 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1004.3557v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.3557v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.4610v1</id>
    <updated>2010-04-26T19:18:48Z</updated>
    <published>2010-04-26T19:18:48Z</published>
    <title>Mobility Prediction in Wireless Ad Hoc Networks using Neural Networks</title>
    <summary>  Mobility prediction allows estimating the stability of paths in a mobile
wireless Ad Hoc networks. Identifying stable paths helps to improve routing by
reducing the overhead and the number of connection interruptions. In this
paper, we introduce a neural network based method for mobility prediction in Ad
Hoc networks. This method consists of a multi-layer and recurrent neural
network using back propagation through time algorithm for training.
</summary>
    <author>
      <name>Heni Kaaniche</name>
    </author>
    <author>
      <name>Farouk Kamoun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Heni Kaaniche and Farouk Kamoun, "Mobility Prediction in Wireless Ad
  Hoc Networks using Neural Networks", Journal of Telecommunications, Volume 2,
  Issue 1, p95-101, April 2010</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Telecommunications, Volume 2, Issue 1, p95-101, April
  2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1004.4610v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.4610v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.5262v1</id>
    <updated>2010-04-29T12:25:16Z</updated>
    <published>2010-04-29T12:25:16Z</published>
    <title>On Application of the Local Search and the Genetic Algorithms Techniques
  to Some Combinatorial Optimization Problems</title>
    <summary>  In this paper the approach to solving several combinatorial optimization
problems using the local search and the genetic algorithm techniques is
proposed. Initially this approach was developed in purpose to overcome some
difficulties inhibiting the application of above mentioned techniques to the
problems of the Questionnaire Theory. But when the algorithms were developed it
became clear that them could be successfully applied also to the Minimum Set
Cover, the 0-1-Knapsack and probably to other combinatorial optimization
problems.
</summary>
    <author>
      <name>Anton Bondarenko</name>
    </author>
    <link href="http://arxiv.org/abs/1004.5262v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.5262v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="90C27, 68P10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.4446v1</id>
    <updated>2010-05-24T22:15:57Z</updated>
    <published>2010-05-24T22:15:57Z</published>
    <title>Genetic algorithms and the art of Zen</title>
    <summary>  In this paper we present a novel genetic algorithm (GA) solution to a simple
yet challenging commercial puzzle game known as the Zen Puzzle Garden (ZPG). We
describe the game in detail, before presenting a suitable encoding scheme and
fitness function for candidate solutions. We then compare the performance of
the genetic algorithm with that of the A* algorithm. Our results show that the
GA is competitive with informed search in terms of solution quality, and
significantly out-performs it in terms of computational resource requirements.
We conclude with a brief discussion of the implications of our findings for
game solving and other "real world" problems.
</summary>
    <author>
      <name>Jack Coldridge</name>
    </author>
    <author>
      <name>Martyn Amos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted</arxiv:comment>
    <link href="http://arxiv.org/abs/1005.4446v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.4446v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1007.4221v2</id>
    <updated>2010-10-01T23:55:16Z</updated>
    <published>2010-07-23T21:30:18Z</published>
    <title>Building Blocks Propagation in Quantum-Inspired Genetic Algorithm</title>
    <summary>  This paper presents an analysis of building blocks propagation in
Quantum-Inspired Genetic Algorithm, which belongs to a new class of
metaheuristics drawing their inspiration from both biological evolution and
unitary evolution of quantum systems. The expected number of quantum
chromosomes matching a schema has been analyzed and a random variable
corresponding to this issue has been introduced. The results have been compared
with Simple Genetic Algorithm. Also, it has been presented how selected binary
quantum chromosomes cover a domain of one-dimensional fitness function.
</summary>
    <author>
      <name>Robert Nowotniak</name>
    </author>
    <author>
      <name>Jacek Kucharski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1007.4221v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1007.4221v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1007.4636v2</id>
    <updated>2010-11-15T08:52:23Z</updated>
    <published>2010-07-27T08:18:52Z</published>
    <title>Computational Complexity Analysis of Simple Genetic Programming On Two
  Problems Modeling Isolated Program Semantics</title>
    <summary>  Analyzing the computational complexity of evolutionary algorithms for binary
search spaces has significantly increased their theoretical understanding. With
this paper, we start the computational complexity analysis of genetic
programming. We set up several simplified genetic programming algorithms and
analyze them on two separable model problems, ORDER and MAJORITY, each of which
captures an important facet of typical genetic programming problems. Both
analyses give first rigorous insights on aspects of genetic programming design,
highlighting in particular the impact of accepting or rejecting neutral moves
and the importance of a local mutation operator.
</summary>
    <author>
      <name>Greg Durrett</name>
    </author>
    <author>
      <name>Frank Neumann</name>
    </author>
    <author>
      <name>Una-May O'Reilly</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1007.4636v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1007.4636v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.5031v1</id>
    <updated>2010-09-25T19:44:51Z</updated>
    <published>2010-09-25T19:44:51Z</published>
    <title>A Genetic Algorithm for the Multi-Pickup and Delivery Problem with time
  windows</title>
    <summary>  In This paper we present a genetic algorithm for the multi-pickup and
delivery problem with time windows (m-PDPTW). The m-PDPTW is an optimization
vehicles routing problem which must meet requests for transport between
suppliers and customers satisfying precedence, capacity and time constraints.
This paper purposes a brief literature review of the PDPTW, present our
approach based on genetic algorithms to minimizing the total travel distance
and thereafter the total travel cost, by showing that an encoding represents
the parameters of each individual.
</summary>
    <author>
      <name>Imen Harbaoui Dridi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAGIS</arxiv:affiliation>
    </author>
    <author>
      <name>Ryan Kammarti</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ACS</arxiv:affiliation>
    </author>
    <author>
      <name>Mekki Ksouri</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ACS</arxiv:affiliation>
    </author>
    <author>
      <name>Pierre Borne</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAGIS</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Studies in Informatics and Control 18, 2 (2009) pages 173-180</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1009.5031v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.5031v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1010.0771v2</id>
    <updated>2013-02-01T07:35:40Z</updated>
    <published>2010-10-05T06:01:24Z</published>
    <title>Genetic Algorithm for Mulicriteria Optimization of a Multi-Pickup and
  Delivery Problem with Time Windows</title>
    <summary>  In This paper we present a genetic algorithm for mulicriteria optimization of
a multipickup and delivery problem with time windows (m-PDPTW). The m-PDPTW is
an optimization vehicles routing problem which must meet requests for transport
between suppliers and customers satisfying precedence, capacity and time
constraints. This paper purposes a brief literature review of the PDPTW,
present an approach based on genetic algorithms and Pareto dominance method to
give a set of satisfying solutions to the m-PDPTW minimizing total travel cost,
total tardiness time and the vehicles number.
</summary>
    <author>
      <name>Imen Harbaoui Dridi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ACS</arxiv:affiliation>
    </author>
    <author>
      <name>Ryan Kammarti</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ACS</arxiv:affiliation>
    </author>
    <author>
      <name>Mekki Ksouri</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ACS</arxiv:affiliation>
    </author>
    <author>
      <name>Pierre Borne</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAGIS</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">INCOM'09 IFAC, Russie, F\'ed\'eration De (2009)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1010.0771v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1010.0771v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1010.0979v1</id>
    <updated>2010-10-05T18:54:43Z</updated>
    <published>2010-10-05T18:54:43Z</published>
    <title>Un Algorithme génétique pour le problème de ramassage et de
  livraison avec fenêtres de temps à plusieurs véhicules</title>
    <summary>  The PDPTW is an optimization vehicles routing problem which must meet
requests for transport between suppliers and customers satisfying precedence,
capacity and time constraints. We present, in this paper, a genetic algorithm
for optimization of a multi pickup and delivery problem with time windows
(m-PDPTW). We purposes a brief literature review of the PDPTW, present an
approach based on genetic algorithms to give a satisfying solution to the
m-PDPTW minimizing the total travel cost.
</summary>
    <author>
      <name>Imen Harbaoui Dridi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAGIS, ACS</arxiv:affiliation>
    </author>
    <author>
      <name>Ryan Kammarti</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ACS</arxiv:affiliation>
    </author>
    <author>
      <name>Pierre Borne</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAGIS</arxiv:affiliation>
    </author>
    <author>
      <name>Mekki Ksouri</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ACS</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">CIFA, Roumanie (2008)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1010.0979v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1010.0979v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1011.4682v1</id>
    <updated>2010-11-21T17:39:27Z</updated>
    <published>2010-11-21T17:39:27Z</published>
    <title>Analysis of attractor distances in Random Boolean Networks</title>
    <summary>  We study the properties of the distance between attractors in Random Boolean
Networks, a prominent model of genetic regulatory networks. We define three
distance measures, upon which attractor distance matrices are constructed and
their main statistic parameters are computed. The experimental analysis shows
that ordered networks have a very clustered set of attractors, while chaotic
networks' attractors are scattered; critical networks show, instead, a pattern
with characteristics of both ordered and chaotic networks.
</summary>
    <author>
      <name>Andrea Roli</name>
    </author>
    <author>
      <name>Stefano Benedettini</name>
    </author>
    <author>
      <name>Roberto Serra</name>
    </author>
    <author>
      <name>Marco Villani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 6 figures. Presented at WIRN 2010 - Italian workshop on
  neural networks, May 2010. To appear in a volume published by IOS Press</arxiv:comment>
    <link href="http://arxiv.org/abs/1011.4682v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.4682v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.0952v1</id>
    <updated>2010-12-04T22:11:48Z</updated>
    <published>2010-12-04T22:11:48Z</published>
    <title>Faster Black-Box Algorithms Through Higher Arity Operators</title>
    <summary>  We extend the work of Lehre and Witt (GECCO 2010) on the unbiased black-box
model by considering higher arity variation operators. In particular, we show
that already for binary operators the black-box complexity of \leadingones
drops from $\Theta(n^2)$ for unary operators to $O(n \log n)$. For \onemax, the
$\Omega(n \log n)$ unary black-box complexity drops to O(n) in the binary case.
For $k$-ary operators, $k \leq n$, the \onemax-complexity further decreases to
$O(n/\log k)$.
</summary>
    <author>
      <name>Benjamin Doerr</name>
    </author>
    <author>
      <name>Daniel Johannsen</name>
    </author>
    <author>
      <name>Timo Kötzing</name>
    </author>
    <author>
      <name>Per Kristian Lehre</name>
    </author>
    <author>
      <name>Markus Wagner</name>
    </author>
    <author>
      <name>Carola Winzen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at FOGA 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1012.0952v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.0952v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.3656v1</id>
    <updated>2010-12-16T16:21:42Z</updated>
    <published>2010-12-16T16:21:42Z</published>
    <title>Adaptive Cluster Expansion (ACE): A Multilayer Network for Estimating
  Probability Density Functions</title>
    <summary>  We derive an adaptive hierarchical method of estimating high dimensional
probability density functions. We call this method of density estimation the
"adaptive cluster expansion" or ACE for short. We present an application of
this approach, based on a multilayer topographic mapping network, that
adaptively estimates the joint probability density function of the pixel values
of an image, and presents this result as a "probability image". We apply this
to the problem of identifying statistically anomalous regions in otherwise
statistically homogeneous images.
</summary>
    <author>
      <name>Stephen Luttrell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1012.3656v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.3656v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.3724v1</id>
    <updated>2010-12-16T19:30:20Z</updated>
    <published>2010-12-16T19:30:20Z</published>
    <title>The Development of Dominance Stripes and Orientation Maps in a
  Self-Organising Visual Cortex Network (VICON)</title>
    <summary>  A self-organising neural network is presented that is based on a rigorous
Bayesian analysis of the information contained in individual neural firing
events. This leads to a visual cortex network (VICON) that has many of the
properties emerge when a mammalian visual cortex is exposed to data arriving
from two imaging sensors (i.e. the two retinae), such as dominance stripes and
orientation maps.
</summary>
    <author>
      <name>Stephen Luttrell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">33 pages, 19 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1012.3724v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.3724v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.4173v1</id>
    <updated>2010-12-19T14:48:55Z</updated>
    <published>2010-12-19T14:48:55Z</published>
    <title>A Self-Organising Neural Network for Processing Data from Multiple
  Sensors</title>
    <summary>  This paper shows how a folded Markov chain network can be applied to the
problem of processing data from multiple sensors, with an emphasis on the
special case of 2 sensors. It is necessary to design the network so that it can
transform a high dimensional input vector into a posterior probability, for
which purpose the partitioned mixture distribution network is ideally suited.
The underlying theory is presented in detail, and a simple numerical simulation
is given that shows the emergence of ocular dominance stripes.
</summary>
    <author>
      <name>S P Luttrell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1012.4173v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.4173v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1101.5997v1</id>
    <updated>2011-01-31T15:47:05Z</updated>
    <published>2011-01-31T15:47:05Z</published>
    <title>New Model for Multi-Objective Evolutionary Algorithms</title>
    <summary>  Multi-Objective Evolutionary Algorithms (MOEAs) have been proved efficient to
deal with Multi-objective Optimization Problems (MOPs). Until now tens of MOEAs
have been proposed. The unified mode would provide a more systematic approach
to build new MOEAs. Here a new model is proposed which includes two sub-models
based on two classes of different schemas of MOEAs. According to the new model,
some representatives algorithms are decomposed and some interesting issues are
discussed.
</summary>
    <author>
      <name>Bojin Zheng</name>
    </author>
    <author>
      <name>Yuanxiang Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages,ICCS 2007</arxiv:comment>
    <link href="http://arxiv.org/abs/1101.5997v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1101.5997v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1102.2559v1</id>
    <updated>2011-02-13T05:09:32Z</updated>
    <published>2011-02-13T05:09:32Z</published>
    <title>Toward Measuring the Scaling of Genetic Programming</title>
    <summary>  Several genetic programming systems are created, each solving a different
problem. In these systems, the median number of generations G needed to evolve
a working program is measured. The behavior of G is observed as the difficulty
of the problem is increased. In these systems, the density D of working
programs in the universe of all possible programs is measured. The relationship
G ~ 1/sqrt(D) is observed to approximately hold for two program-like systems.
For parallel systems (systems that look like several independent programs
evolving in parallel), the relationship G ~ 1/(n ln n) is observed to
approximately hold. Finally, systems that are anti-parallel are considered.
</summary>
    <author>
      <name>Mike Stimpson</name>
    </author>
    <link href="http://arxiv.org/abs/1102.2559v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1102.2559v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.2741v1</id>
    <updated>2011-03-14T18:58:59Z</updated>
    <published>2011-03-14T18:58:59Z</published>
    <title>Memory Retrieval in the B-Matrix Neural Network</title>
    <summary>  This paper is an extension to the memory retrieval procedure of the B-Matrix
approach [6],[17] to neural network learning. The B-Matrix is a part of the
interconnection matrix generated from the Hebbian neural network, and in memory
retrieval, the B-matrix is clamped with a small fragment of the memory. The
fragment gradually enlarges by means of feedback, until the entire vector is
obtained. In this paper, we propose the use of delta learning to enhance the
retrieval rate of the stored memories.
</summary>
    <author>
      <name>Prerana Laddha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 Pages, 4 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1103.2741v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.2741v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.4820v1</id>
    <updated>2011-03-24T17:59:10Z</updated>
    <published>2011-03-24T17:59:10Z</published>
    <title>Design and classification of dynamic multi-objective optimization
  problems</title>
    <summary>  In this work we provide a formal model for the different time-dependent
components that can appear in dynamic multi-objective optimization problems,
along with a classification of these components. Four main classes are
identified, corresponding to the influence of the parameters, objective
functions, previous states of the dynamic system and, last, environment
changes, which in turn lead to online optimization problems. For illustration
purposes, examples are provided for each class identified - by no means
standing as the most representative ones or exhaustive in scope.
</summary>
    <author>
      <name>Alexandru-Adrian Tantar</name>
    </author>
    <author>
      <name>Emilia Tantar</name>
    </author>
    <author>
      <name>Pascal Bouvry</name>
    </author>
    <link href="http://arxiv.org/abs/1103.4820v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.4820v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.5797v2</id>
    <updated>2011-05-06T06:07:01Z</updated>
    <published>2011-03-29T23:52:30Z</published>
    <title>Computational Complexity Results for Genetic Programming and the Sorting
  Problem</title>
    <summary>  Genetic Programming (GP) has found various applications. Understanding this
type of algorithm from a theoretical point of view is a challenging task. The
first results on the computational complexity of GP have been obtained for
problems with isolated program semantics. With this paper, we push forward the
computational complexity analysis of GP on a problem with dependent program
semantics. We study the well-known sorting problem in this context and analyze
rigorously how GP can deal with different measures of sortedness.
</summary>
    <author>
      <name>Markus Wagner</name>
    </author>
    <author>
      <name>Frank Neumann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1103.5797v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.5797v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1104.0775v2</id>
    <updated>2011-06-16T05:42:09Z</updated>
    <published>2011-04-05T08:46:33Z</published>
    <title>Evolving Pacing Strategies for Team Pursuit Track Cycling</title>
    <summary>  Team pursuit track cycling is a bicycle racing sport held on velodromes and
is part of the Summer Olympics. It involves the use of strategies to minimize
the overall time that a team of cyclists needs to complete a race. We present
an optimisation framework for team pursuit track cycling and show how to evolve
strategies using metaheuristics for this interesting real-world problem. Our
experimental results show that these heuristics lead to significantly better
strategies than state-of-art strategies that are currently used by teams of
cyclists.
</summary>
    <author>
      <name>Markus Wagner</name>
    </author>
    <author>
      <name>Jareth Day</name>
    </author>
    <author>
      <name>Diora Jordan</name>
    </author>
    <author>
      <name>Trent Kroeger</name>
    </author>
    <author>
      <name>Frank Neumann</name>
    </author>
    <link href="http://arxiv.org/abs/1104.0775v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.0775v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1104.2644v1</id>
    <updated>2011-04-13T23:24:49Z</updated>
    <published>2011-04-13T23:24:49Z</published>
    <title>Idealized Dynamic Population Sizing for Uniformly Scaled Problems</title>
    <summary>  This paper explores an idealized dynamic population sizing strategy for
solving additive decomposable problems of uniform scale. The method is designed
on top of the foundations of existing population sizing theory for this class
of problems, and is carefully compared with an optimal fixed population sized
genetic algorithm. The resulting strategy should be close to a lower bound in
terms of what can be achieved, performance-wise, by self-adjusting population
sizing algorithms for this class of problems.
</summary>
    <author>
      <name>Fernando G. Lobo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, submitted to ACM GECCO-2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1104.2644v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.2644v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.3351v2</id>
    <updated>2017-02-23T10:03:27Z</updated>
    <published>2011-05-17T12:24:23Z</published>
    <title>Splitting method for spatio-temporal search efforts planning</title>
    <summary>  This article deals with the spatio-temporal sensors deployment in order to
maximize detection probability of an intelligent and randomly moving target in
an area under surveillance. Our work is based on the rare events simulation
framework. More precisely, we derive a novel stochastic optimization algorithm
based on the generalized splitting method. This new approach offers promising
results without any state-space discretization and can handle various types of
constraints.
</summary>
    <author>
      <name>Chouchane Mathieu</name>
    </author>
    <author>
      <name>Paris Sébastien</name>
    </author>
    <author>
      <name>Le Gland François</name>
    </author>
    <author>
      <name>Ouladsine Mustapha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Article rejected</arxiv:comment>
    <link href="http://arxiv.org/abs/1105.3351v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.3351v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.0118v2</id>
    <updated>2011-06-02T08:23:54Z</updated>
    <published>2011-06-01T08:29:28Z</published>
    <title>1st International Workshop on Distributed Evolutionary Computation in
  Informal Environments</title>
    <summary>  Online conference proceedings for the IWDECIE workshop, taking place in New
Orleans on June 5th, 2011. The workshop focuses on non-conventional
implementations of bioinspired algorithms and its conceptual implications.
</summary>
    <author>
      <name>Juan-J. Merelo</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">editors</arxiv:affiliation>
    </author>
    <author>
      <name>Maribel García-Arenas</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">editors</arxiv:affiliation>
    </author>
    <author>
      <name>Juan-Luis J. Laredo</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">editors</arxiv:affiliation>
    </author>
    <author>
      <name>Francisco Fernández de la Vega</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">editors</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Five papers, workshop took place together with CEC 2011 in New
  Orleans (LA, USA). http://geneura.ugr.es/~iwdecie</arxiv:comment>
    <link href="http://arxiv.org/abs/1106.0118v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.0118v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.6185v1</id>
    <updated>2011-06-30T11:08:05Z</updated>
    <published>2011-06-30T11:08:05Z</published>
    <title>Effects of Compensation, Connectivity and Tau in a Computational Model
  of Alzheimer's Disease</title>
    <summary>  This work updates an existing, simplistic computational model of Alzheimer's
Disease (AD) to investigate the behaviour of synaptic compensatory mechanisms
in neural networks with small-world connectivity, and varying methods of
calculating compensation. It additionally introduces a method for simulating
tau neurofibrillary pathology, resulting in a more dramatic damage profile.
Small-world connectivity is shown to have contrasting effects on capacity,
retrieval time, and robustness to damage, whilst the use of more
easily-obtained remote memories rather than recent memories for synaptic
compensation is found to lead to rapid network damage.
</summary>
    <author>
      <name>Mark Rowan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, submitted to International Joint Conference on Neural
  Networks 2011</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The 2011 International Joint Conference on Neural Networks
  (IJCNN), (2011) 543--550</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1106.6185v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.6185v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.4083v2</id>
    <updated>2011-08-24T07:43:10Z</updated>
    <published>2011-08-20T02:40:33Z</published>
    <title>Convergence of a Recombination-Based Elitist Evolutionary Algorithm on
  the Royal Roads Test Function</title>
    <summary>  We present an analysis of the performance of an elitist Evolutionary
algorithm using a recombination operator known as 1-Bit-Swap on the Royal Roads
test function based on a population. We derive complete, approximate and
asymptotic convergence rates for the algorithm. The complete model shows the
benefit of the size of the population and re- combination pool.
</summary>
    <author>
      <name>Aram Ter-Sarkisov</name>
    </author>
    <author>
      <name>Stephen Marsland</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted for AI 2011: 24th Australasian Joint Conference on
  Artificial Intelligence</arxiv:comment>
    <link href="http://arxiv.org/abs/1108.4083v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.4083v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.4548v1</id>
    <updated>2011-08-23T10:47:23Z</updated>
    <published>2011-08-23T10:47:23Z</published>
    <title>Ant Colony Optimization of Rough Set for HV Bushings Fault Detection</title>
    <summary>  Most transformer failures are attributed to bushings failures. Hence it is
necessary to monitor the condition of bushings. In this paper three methods are
developed to monitor the condition of oil filled bushing. Multi-layer
perceptron (MLP), Radial basis function (RBF) and Rough Set (RS) models are
developed and combined through majority voting to form a committee. The MLP
performs better that the RBF and the RS is terms of classification accuracy.
The RBF is the fasted to train. The committee performs better than the
individual models. The diversity of models is measured to evaluate their
similarity when used in the committee.
</summary>
    <author>
      <name>J. L. Mpanza</name>
    </author>
    <author>
      <name>T. Marwala</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Fourth International Workshop on Advanced Computational Intelligence
  (IWACI 2011)</arxiv:comment>
    <link href="http://arxiv.org/abs/1108.4548v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.4548v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.4618v1</id>
    <updated>2011-08-23T14:44:44Z</updated>
    <published>2011-08-23T14:44:44Z</published>
    <title>Artificial Neural Network and Rough Set for HV Bushings Condition
  Monitoring</title>
    <summary>  Most transformer failures are attributed to bushings failures. Hence it is
necessary to monitor the condition of bushings. In this paper three methods are
developed to monitor the condition of oil filled bushing. Multi-layer
perceptron (MLP), Radial basis function (RBF) and Rough Set (RS) models are
developed and combined through majority voting to form a committee. The MLP
performs better that the RBF and the RS is terms of classification accuracy.
The RBF is the fasted to train. The committee performs better than the
individual models. The diversity of models is measured to evaluate their
similarity when used in the committee.
</summary>
    <author>
      <name>LJ Mpanza</name>
    </author>
    <author>
      <name>T. Marwala</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE INES 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1108.4618v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.4618v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.1211v1</id>
    <updated>2011-09-06T15:19:48Z</updated>
    <published>2011-09-06T15:19:48Z</published>
    <title>An Efficient Preprocessing Methodology for Discovering Patterns and
  Clustering of Web Users using a Dynamic ART1 Neural Network</title>
    <summary>  In this paper, a complete preprocessing methodology for discovering patterns
in web usage mining process to improve the quality of data by reducing the
quantity of data has been proposed. A dynamic ART1 neural network clustering
algorithm to group users according to their Web access patterns with its neat
architecture is also proposed. Several experiments are conducted and the
results show the proposed methodology reduces the size of Web log files down to
73-82% of the initial size and the proposed ART1 algorithm is dynamic and
learns relatively stable quality clusters.
</summary>
    <author>
      <name>C. Ramya</name>
    </author>
    <author>
      <name>G. Kavitha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages; International Conference on Information Processing,
  august-2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1109.1211v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.1211v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.2034v2</id>
    <updated>2013-08-22T12:21:24Z</updated>
    <published>2011-09-09T14:59:59Z</published>
    <title>Learning Sequence Neighbourhood Metrics</title>
    <summary>  Recurrent neural networks (RNNs) in combination with a pooling operator and
the neighbourhood components analysis (NCA) objective function are able to
detect the characterizing dynamics of sequences and embed them into a
fixed-length vector space of arbitrary dimensionality. Subsequently, the
resulting features are meaningful and can be used for visualization or nearest
neighbour classification in linear time. This kind of metric learning for
sequential data enables the use of algorithms tailored towards fixed length
vector spaces such as R^n.
</summary>
    <author>
      <name>Justin Bayer</name>
    </author>
    <author>
      <name>Christian Osendorfer</name>
    </author>
    <author>
      <name>Patrick van der Smagt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Artificial Neural Networks and Machine Learning ICANN 2012 Springer
  Berlin Heidelberg 2012. 531-538</arxiv:comment>
    <link href="http://arxiv.org/abs/1109.2034v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.2034v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.0477v1</id>
    <updated>2011-10-03T20:03:46Z</updated>
    <published>2011-10-03T20:03:46Z</published>
    <title>Distributed Evolutionary Graph Partitioning</title>
    <summary>  We present a novel distributed evolutionary algorithm, KaFFPaE, to solve the
Graph Partitioning Problem, which makes use of KaFFPa (Karlsruhe Fast Flow
Partitioner). The use of our multilevel graph partitioner KaFFPa provides new
effective crossover and mutation operators. By combining these with a scalable
communication protocol we obtain a system that is able to improve the best
known partitioning results for many inputs in a very short amount of time. For
example, in Walshaw's well known benchmark tables we are able to improve or
recompute 76% of entries for the tables with 1%, 3% and 5% imbalance.
</summary>
    <author>
      <name>Peter Sanders</name>
    </author>
    <author>
      <name>Christian Schulz</name>
    </author>
    <link href="http://arxiv.org/abs/1110.0477v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.0477v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.1038v1</id>
    <updated>2011-10-05T16:52:26Z</updated>
    <published>2011-10-05T16:52:26Z</published>
    <title>Using Genetic Algorithm in the Evolutionary Design of Sequential Logic
  Circuits</title>
    <summary>  Evolvable hardware (EHW) is a set of techniques that are based on the idea of
combining reconfiguration hardware systems with evolutionary algorithms. In
other word, EHW has two sections; the reconfigurable hardware and evolutionary
algorithm where the configurations are under the control of an evolutionary
algorithm. This paper, suggests a method to design and optimize the synchronous
sequential circuits. Genetic algorithm (GA) was applied as evolutionary
algorithm. In this approach, for building input combinational logic circuit of
each DFF, and also output combinational logic circuit, the cell arrays have
been used. The obtained results show that our method can reduce the average
number of generations by limitation the search space.
</summary>
    <author>
      <name>Parisa Soleimani</name>
    </author>
    <author>
      <name>Reza Sabbaghi-Nadooshan</name>
    </author>
    <author>
      <name>Sattar Mirzakuchaki</name>
    </author>
    <author>
      <name>Mahdi Bagheri</name>
    </author>
    <link href="http://arxiv.org/abs/1110.1038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.1038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.2988v1</id>
    <updated>2011-11-13T04:46:28Z</updated>
    <published>2011-11-13T04:46:28Z</published>
    <title>Application of PSO, Artificial Bee Colony and Bacterial Foraging
  Optimization algorithms to economic load dispatch: An analysis</title>
    <summary>  This paper illustrates successful implementation of three evolutionary
algorithms, namely- Particle Swarm Optimization(PSO), Artificial Bee Colony
(ABC) and Bacterial Foraging Optimization (BFO) algorithms to economic load
dispatch problem (ELD). Power output of each generating unit and optimum fuel
cost obtained using all three algorithms have been compared. The results
obtained show that ABC and BFO algorithms converge to optimal fuel cost with
reduced computational time when compared to PSO for the two example problems
considered.
</summary>
    <author>
      <name>Anant Baijal</name>
    </author>
    <author>
      <name>Vikram Singh Chauhan</name>
    </author>
    <author>
      <name>T. Jayabarathi</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 4, No 1, 2011, 467-470</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1111.2988v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.2988v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.4737v1</id>
    <updated>2012-01-20T20:30:59Z</updated>
    <published>2012-01-20T20:30:59Z</published>
    <title>Production System Rules as Protein Complexes from Genetic Regulatory
  Networks</title>
    <summary>  This short paper introduces a new way by which to design production system
rules. An indirect encoding scheme is presented which views such rules as
protein complexes produced by the temporal behaviour of an artificial genetic
regulatory network. This initial study begins by using a simple Boolean
regulatory network to produce traditional ternary-encoded rules before moving
to a fuzzy variant to produce real-valued rules. Competitive performance is
shown with related genetic regulatory networks and rule-based systems on
benchmark problems.
</summary>
    <author>
      <name>Larry Bull</name>
    </author>
    <link href="http://arxiv.org/abs/1201.4737v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.4737v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.4908v1</id>
    <updated>2012-01-24T03:16:43Z</updated>
    <published>2012-01-24T03:16:43Z</published>
    <title>Self-Organisation of Evolving Agent Populations in Digital Ecosystems</title>
    <summary>  We investigate the self-organising behaviour of Digital Ecosystems, because a
primary motivation for our research is to exploit the self-organising
properties of biological ecosystems. We extended a definition for the
complexity, grounded in the biological sciences, providing a measure of the
information in an organism's genome. Next, we extended a definition for the
stability, originating from the computer sciences, based upon convergence to an
equilibrium distribution. Finally, we investigated a definition for the
diversity, relative to the selection pressures provided by the user requests.
We conclude with a summary and discussion of the achievements, including the
experimental results.
</summary>
    <author>
      <name>Gerard Briscoe</name>
    </author>
    <author>
      <name>Philippe De Wilde</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">50 pages, 25 figures, journal</arxiv:comment>
    <link href="http://arxiv.org/abs/1201.4908v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.4908v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.2249v1</id>
    <updated>2012-02-10T12:57:34Z</updated>
    <published>2012-02-10T12:57:34Z</published>
    <title>Supervised Learning in Multilayer Spiking Neural Networks</title>
    <summary>  The current article introduces a supervised learning algorithm for multilayer
spiking neural networks. The algorithm presented here overcomes some
limitations of existing learning algorithms as it can be applied to neurons
firing multiple spikes and it can in principle be applied to any linearisable
neuron model. The algorithm is applied successfully to various benchmarks, such
as the XOR problem and the Iris data set, as well as complex classifications
problems. The simulations also show the flexibility of this supervised learning
algorithm which permits different encodings of the spike timing patterns,
including precise spike trains encoding.
</summary>
    <author>
      <name>Ioana Sporea</name>
    </author>
    <author>
      <name>André Grüning</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1162/NECO_a_00396</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1162/NECO_a_00396" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">38 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Compuation February 2013, Vol. 25, No. 2, Pages 473-509</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1202.2249v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.2249v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.4170v1</id>
    <updated>2012-02-19T16:56:45Z</updated>
    <published>2012-02-19T16:56:45Z</published>
    <title>Classification by Ensembles of Neural Networks</title>
    <summary>  We introduce a new procedure for training of artificial neural networks by
using the approximation of an objective function by arithmetic mean of an
ensemble of selected randomly generated neural networks, and apply this
procedure to the classification (or pattern recognition) problem. This approach
differs from the standard one based on the optimization theory. In particular,
any neural network from the mentioned ensemble may not be an approximation of
the objective function.
</summary>
    <author>
      <name>S. V. Kozyrev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, LaTeX</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">p-Adic Numbers, Ultrametric Analysis and Applications, 2012, Vol.
  4, No. 1, pp. 27-33</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1202.4170v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.4170v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.5284v3</id>
    <updated>2012-04-02T04:27:11Z</updated>
    <published>2012-02-23T20:21:57Z</published>
    <title>Elitism Levels Traverse Mechanism For The Derivation of Upper Bounds on
  Unimodal Functions</title>
    <summary>  In this article we present an Elitism Levels Traverse Mechanism that we
designed to find bounds on population-based Evolutionary algorithms solving
unimodal functions. We prove its efficiency theoretically and test it on OneMax
function deriving bounds c{\mu}n log n - O({\mu} n). This analysis can be
generalized to any similar algorithm using variants of tournament selection and
genetic operators that flip or swap only 1 bit in each string.
</summary>
    <author>
      <name>Aram Ter-Sarkisov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted to Congress on Evolutionary Computation (WCCI/CEC) 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1202.5284v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.5284v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.5953v1</id>
    <updated>2012-02-27T14:36:43Z</updated>
    <published>2012-02-27T14:36:43Z</published>
    <title>On an Ethical Use of Neural Networks: A Case Study on a North Indian
  Raga</title>
    <summary>  The paper gives an artificial neural network (ANN) approach to time series
modeling, the data being instance versus notes (characterized by pitch)
depicting the structure of a North Indian raga, namely, Bageshree. Respecting
the sentiments of the artists' community, the paper argues why it is more
ethical to model a structure than try and "manufacture" an artist by training
the neural network to copy performances of artists. Indian Classical Music
centers on the ragas, where emotion and devotion are both important and neither
can be substituted by such "calculated artistry" which the ANN generated copies
are ultimately up to.
</summary>
    <author>
      <name>Ripunjai Kumar Shukla</name>
    </author>
    <author>
      <name>Soubhik Chakraborty</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Ann. Univ. Tibiscus Comp. Sci. Series VII/2 (2009), 41-56</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1202.5953v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.5953v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.0197v2</id>
    <updated>2012-03-06T09:02:14Z</updated>
    <published>2012-03-01T14:25:50Z</published>
    <title>Statistical Approach for Selecting Elite Ants</title>
    <summary>  Applications of ACO algorithms to obtain better solutions for combinatorial
optimization problems have become very popular in recent years. In ACO
algorithms, group of agents repeatedly perform well defined actions and
collaborate with other ants in order to accomplish the defined task. In this
paper, we introduce new mechanisms for selecting the Elite ants dynamically
based on simple statistical tools. We also investigate the performance of newly
proposed mechanisms.
</summary>
    <author>
      <name>G. S. Raghavendra</name>
    </author>
    <author>
      <name>N. Prasanna Kumar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Ann. Univ. Tibiscus Comp. Sci. Series IX/2 (2011), 69-90</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1203.0197v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.0197v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.3097v1</id>
    <updated>2012-03-14T14:32:45Z</updated>
    <published>2012-03-14T14:32:45Z</published>
    <title>A Comparative Study of Adaptive Crossover Operators for Genetic
  Algorithms to Resolve the Traveling Salesman Problem</title>
    <summary>  Genetic algorithm includes some parameters that should be adjusting so that
the algorithm can provide positive results. Crossover operators play very
important role by constructing competitive Genetic Algorithms (GAs). In this
paper, the basic conceptual features and specific characteristics of various
crossover operators in the context of the Traveling Salesman Problem (TSP) are
discussed. The results of experimental comparison of more than six different
crossover operators for the TSP are presented. The experiment results show that
OX operator enables to achieve a better solutions than other operators tested.
</summary>
    <author>
      <name>Otman Abdoun</name>
    </author>
    <author>
      <name>Jaafar Abouchabaka</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Applications (0975 - 8887)
  Volume 31 - No.11, October 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1203.3097v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.3097v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.4416v1</id>
    <updated>2012-03-20T12:59:15Z</updated>
    <published>2012-03-20T12:59:15Z</published>
    <title>On Training Deep Boltzmann Machines</title>
    <summary>  The deep Boltzmann machine (DBM) has been an important development in the
quest for powerful "deep" probabilistic models. To date, simultaneous or joint
training of all layers of the DBM has been largely unsuccessful with existing
training methods. We introduce a simple regularization scheme that encourages
the weight vectors associated with each hidden unit to have similar norms. We
demonstrate that this regularization can be easily combined with standard
stochastic maximum likelihood to yield an effective training strategy for the
simultaneous training of all layers of the deep Boltzmann machine.
</summary>
    <author>
      <name>Guillaume Desjardins</name>
    </author>
    <author>
      <name>Aaron Courville</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <link href="http://arxiv.org/abs/1203.4416v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.4416v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.4881v1</id>
    <updated>2012-03-22T04:22:36Z</updated>
    <published>2012-03-22T04:22:36Z</published>
    <title>Computational Complexity Analysis of Multi-Objective Genetic Programming</title>
    <summary>  The computational complexity analysis of genetic programming (GP) has been
started recently by analyzing simple (1+1) GP algorithms for the problems ORDER
and MAJORITY. In this paper, we study how taking the complexity as an
additional criteria influences the runtime behavior. We consider
generalizations of ORDER and MAJORITY and present a computational complexity
analysis of (1+1) GP using multi-criteria fitness functions that take into
account the original objective and the complexity of a syntax tree as a
secondary measure. Furthermore, we study the expected time until
population-based multi-objective genetic programming algorithms have computed
the Pareto front when taking the complexity of a syntax tree as an equally
important objective.
</summary>
    <author>
      <name>Frank Neumann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A conference version has been accepted for GECCO 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1203.4881v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.4881v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.5028v1</id>
    <updated>2012-03-22T16:09:51Z</updated>
    <published>2012-03-22T16:09:51Z</published>
    <title>Hybridizing PSM and RSM Operator for Solving NP-Complete Problems:
  Application to Travelling Salesman Problem</title>
    <summary>  In this paper, we present a new mutation operator, Hybrid Mutation (HPRM),
for a genetic algorithm that generates high quality solutions to the Traveling
Salesman Problem (TSP). The Hybrid Mutation operator constructs an offspring
from a pair of parents by hybridizing two mutation operators, PSM and RSM. The
efficiency of the HPRM is compared as against some existing mutation operators;
namely, Reverse Sequence Mutation (RSM) and Partial Shuffle Mutation (PSM) for
BERLIN52 as instance of TSPLIB. Experimental results show that the new mutation
operator is better than the RSM and PSM.
</summary>
    <author>
      <name>Otman Abdoun</name>
    </author>
    <author>
      <name>Chakir Tajani</name>
    </author>
    <author>
      <name>Jaafar Abouchabka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ISSN (Online): 1694-0814</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 1, No 1, 2012, 374-378</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1203.5028v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.5028v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.0262v2</id>
    <updated>2012-04-26T23:17:34Z</updated>
    <published>2012-04-01T20:17:32Z</published>
    <title>Managing contextual artificial neural networks with a service-based
  mediator</title>
    <summary>  Today, a wide variety of probabilistic and expert AI systems used to analyze
real world inputs such as unstructured text, sounds, images, and statistical
data. However, all these systems exist on different platforms, with different
implementations, and with very different, often very specific goals in mind.
This paper introduces a concept for a mediator framework for such systems and
seeks to show several architectures which would support it, potential benefits
in combining the signals of disparate networks for formalized, high level logic
and signal processing, and its possible academic and industrial uses.
</summary>
    <author>
      <name>Greg Fish</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.0262v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.0262v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="97R40" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.6.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.2321v6</id>
    <updated>2013-08-30T22:11:40Z</updated>
    <published>2012-04-11T02:25:34Z</published>
    <title>Derivation of Upper Bounds on Optimization Time of Population-Based
  Evolutionary Algorithm on a Function with Fitness Plateaus Using Elitism
  Levels Traverse Mechanism</title>
    <summary>  In this article a tool for the analysis of population-based EAs is used to
derive asymptotic upper bounds on the optimization time of the algorithm
solving Royal Roads problem, a test function with plateaus of fitness. In
addition to this, limiting distribution of a certain subset of the population
is approximated.
</summary>
    <author>
      <name>Aram Ter-Sarkisov</name>
    </author>
    <author>
      <name>Stephen Marsland</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper will be replaced by a new version with a different title</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.2321v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.2321v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.2601v1</id>
    <updated>2012-04-12T01:45:21Z</updated>
    <published>2012-04-12T01:45:21Z</published>
    <title>Detecting lateral genetic material transfer</title>
    <summary>  The bioinformatical methods to detect lateral gene transfer events are mainly
based on functional coding DNA characteristics. In this paper, we propose the
use of DNA traits not depending on protein coding requirements. We introduce
several semilocal variables that depend on DNA primary sequence and that
reflect thermodynamic as well as physico-chemical magnitudes that are able to
tell apart the genome of different organisms. After combining these variables
in a neural classificator, we obtain results whose power of resolution go as
far as to detect the exchange of genomic material between bacteria that are
phylogenetically close.
</summary>
    <author>
      <name>C. Calderón</name>
    </author>
    <author>
      <name>L. Delaye</name>
    </author>
    <author>
      <name>V. Mireles</name>
    </author>
    <author>
      <name>P. Miramontes</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Applied Computational Intelligence and Soft Computing</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.2601v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.2601v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.4632v1</id>
    <updated>2012-07-19T12:08:43Z</updated>
    <published>2012-07-19T12:08:43Z</published>
    <title>Clustering of Local Optima in Combinatorial Fitness Landscapes</title>
    <summary>  Using the recently proposed model of combinatorial landscapes: local optima
networks, we study the distribution of local optima in two classes of instances
of the quadratic assignment problem. Our results indicate that the two problem
instance classes give rise to very different configuration spaces. For the
so-called real-like class, the optima networks possess a clear modular
structure, while the networks belonging to the class of random uniform
instances are less well partitionable into clusters. We briefly discuss the
consequences of the findings for heuristically searching the corresponding
problem spaces.
</summary>
    <author>
      <name>Gabriela Ochoa</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lille - Nord Europe</arxiv:affiliation>
    </author>
    <author>
      <name>Sébastien Verel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lille - Nord Europe</arxiv:affiliation>
    </author>
    <author>
      <name>Fabio Daolio</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ISI</arxiv:affiliation>
    </author>
    <author>
      <name>Marco Tomassini</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ISI</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-642-25566-3_35</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-642-25566-3_35" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Learning and Intelligent OptimizatioN Conference (LION 5), Rome :
  Italy (2011)</arxiv:comment>
    <link href="http://arxiv.org/abs/1207.4632v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.4632v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.6028v1</id>
    <updated>2012-08-13T11:32:03Z</updated>
    <published>2012-08-13T11:32:03Z</published>
    <title>Design of Low Noise Amplifiers Using Particle Swarm Optimization</title>
    <summary>  This short paper presents a work on the design of low noise microwave
amplifiers using particle swarm optimization (PSO) technique. Particle Swarm
Optimization is used as a method that is applied to a single stage amplifier
circuit to meet two criteria: desired gain and desired low noise. The aim is to
get the best optimized design using the predefined constraints for gain and low
noise values. The code is written to apply the algorithm to meet the desired
goals and the obtained results are verified using different simulators. The
results obtained show that PSO can be applied very efficiently for this kind of
design problems with multiple constraints.
</summary>
    <author>
      <name>Sadik Ulker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Artificial Intelligence &amp; Applications
  vol 3, no 4, July 2012, 99-106</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1208.6028v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.6028v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.2717v1</id>
    <updated>2012-09-12T20:17:05Z</updated>
    <published>2012-09-12T20:17:05Z</published>
    <title>Comparison Study for Clonal Selection Algorithm and Genetic Algorithm</title>
    <summary>  Two metaheuristic algorithms namely Artificial Immune Systems (AIS) and
Genetic Algorithms are classified as computational systems inspired by
theoretical immunology and genetics mechanisms. In this work we examine the
comparative performances of two algorithms. A special selection algorithm,
Clonal Selection Algorithm (CLONALG), which is a subset of Artificial Immune
Systems, and Genetic Algorithms are tested with certain benchmark functions. It
is shown that depending on type of a function Clonal Selection Algorithm and
Genetic Algorithm have better performance over each other.
</summary>
    <author>
      <name>Ezgi Deniz Ulker</name>
    </author>
    <author>
      <name>Sadik Ulker</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijcsit.2012.4410</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijcsit.2012.4410" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 12 figures, 2 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science &amp; Information Technology
  (IJCSIT) Vol 4, No 4, August 2012 pp 107-118</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1209.2717v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.2717v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.5339v1</id>
    <updated>2012-09-24T17:26:29Z</updated>
    <published>2012-09-24T17:26:29Z</published>
    <title>Developing Improved Greedy Crossover to Solve Symmetric Traveling
  Salesman Problem</title>
    <summary>  The Traveling Salesman Problem (TSP) is one of the most famous optimization
problems. Greedy crossover designed by Greffenstette et al, can be used while
Symmetric TSP (STSP) is resolved by Genetic Algorithm (GA). Researchers have
proposed several versions of greedy crossover. Here we propose improved version
of it. We compare our greedy crossover with some of recent crossovers, we use
our greedy crossover and some recent crossovers in GA then compare crossovers
on speed and accuracy.
</summary>
    <author>
      <name>Hassan Ismkhan</name>
    </author>
    <author>
      <name>Kamran Zamanifar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science Issues, Volume 9, Issue 4,
  No 3, July 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.5339v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.5339v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.4502v1</id>
    <updated>2012-10-10T11:08:40Z</updated>
    <published>2012-10-10T11:08:40Z</published>
    <title>Comparing several heuristics for a packing problem</title>
    <summary>  Packing problems are in general NP-hard, even for simple cases. Since now
there are no highly efficient algorithms available for solving packing
problems. The two-dimensional bin packing problem is about packing all given
rectangular items, into a minimum size rectangular bin, without overlapping.
The restriction is that the items cannot be rotated. The current paper is
comparing a greedy algorithm with a hybrid genetic algorithm in order to see
which technique is better for the given problem. The algorithms are tested on
different sizes data.
</summary>
    <author>
      <name>Camelia-M. Pintea</name>
    </author>
    <author>
      <name>Cristian Pascan</name>
    </author>
    <author>
      <name>Mara Hajdu-Macelaru</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 figures, 2 tables; accepted: International Journal of Advanced
  Intelligence Paradigms</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Int J Advanced Intelligence Paradigms 4(3/4) (2012) 268-277</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1210.4502v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.4502v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.5500v1</id>
    <updated>2012-10-19T19:03:11Z</updated>
    <published>2012-10-19T19:03:11Z</published>
    <title>Modeling with Copulas and Vines in Estimation of Distribution Algorithms</title>
    <summary>  The aim of this work is studying the use of copulas and vines in the
optimization with Estimation of Distribution Algorithms (EDAs). Two EDAs are
built around the multivariate product and normal copulas, and other two are
based on pair-copula decomposition of vine models. Empirically we study the
effect of both marginal distributions and dependence structure separately, and
show that both aspects play a crucial role in the success of the optimization.
The results show that the use of copulas and vines opens new opportunities to a
more appropriate modeling of search distributions in EDAs.
</summary>
    <author>
      <name>Marta Soto</name>
    </author>
    <author>
      <name>Yasser González-Fernández</name>
    </author>
    <author>
      <name>Alberto Ochoa</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Investigaci\'on Operacional, 36(1), 1-23</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1210.5500v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.5500v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.6511v1</id>
    <updated>2012-10-24T12:37:53Z</updated>
    <published>2012-10-24T12:37:53Z</published>
    <title>Neural Networks for Complex Data</title>
    <summary>  Artificial neural networks are simple and efficient machine learning tools.
Defined originally in the traditional setting of simple vector data, neural
network models have evolved to address more and more difficulties of complex
real world problems, ranging from time evolving data to sophisticated data
structures such as graphs and functions. This paper summarizes advances on
those themes from the last decade, with a focus on results obtained by members
of the SAMM team of Universit\'e Paris 1
</summary>
    <author>
      <name>Marie Cottrell</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">SAMM</arxiv:affiliation>
    </author>
    <author>
      <name>Madalina Olteanu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">SAMM</arxiv:affiliation>
    </author>
    <author>
      <name>Fabrice Rossi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">SAMM</arxiv:affiliation>
    </author>
    <author>
      <name>Joseph Rynkiewicz</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">SAMM</arxiv:affiliation>
    </author>
    <author>
      <name>Nathalie Villa-Vialaneix</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">SAMM</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s13218-012-0207-2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s13218-012-0207-2" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">K\"unstliche Intelligenz 26, 4 (2012) 373-380</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1210.6511v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.6511v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.8124v1</id>
    <updated>2012-10-30T19:11:06Z</updated>
    <published>2012-10-30T19:11:06Z</published>
    <title>Hierarchical Learning Algorithm for the Beta Basis Function Neural
  Network</title>
    <summary>  The paper presents a two-level learning method for the design of the Beta
Basis Function Neural Network BBFNN. A Genetic Algorithm is employed at the
upper level to construct BBFNN, while the key learning parameters :the width,
the centers and the Beta form are optimised using the gradient algorithm at the
lower level. In order to demonstrate the effectiveness of this hierarchical
learning algorithm HLABBFNN, we need to validate our algorithm for the
approximation of non-linear function.
</summary>
    <author>
      <name>Habib Dhahri</name>
    </author>
    <author>
      <name>Mohamed Adel Alimi</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Third International Conference on Systems, Signals &amp; Device, March
  21-24, 2005 , Sousse, Tunisia</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1210.8124v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.8124v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.0660v1</id>
    <updated>2012-11-04T04:49:24Z</updated>
    <published>2012-11-04T04:49:24Z</published>
    <title>Generation of Two-Layer Monotonic Functions</title>
    <summary>  The problem of implementing a class of functions with particular conditions
by using monotonic multilayer functions is considered. A genetic algorithm is
used to create monotonic functions of a certain class, and these are
implemented with two-layer monotonic functions. The existence of a solution to
the given problem suggests that from two monotone functions, a monotonic
function with the same dimensions can be created. A new algorithm based on the
genetic algorithm is proposed, which easily implemented two-layer monotonic
functions of a specific class for up to six variables.
</summary>
    <author>
      <name>Yukihiro Kamada</name>
    </author>
    <author>
      <name>Kiyonori Miyasaki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1211.0660v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.0660v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.2361v2</id>
    <updated>2013-03-22T06:09:29Z</updated>
    <published>2012-11-11T00:26:22Z</published>
    <title>Genetic Algorithm for Designing a Convenient Facility Layout for a
  Circular Flow Path</title>
    <summary>  In this paper, we present a heuristic for designing facility layouts that are
convenient for designing a unidirectional loop for material handling. We use
genetic algorithm where the objective function and crossover and mutation
operators have all been designed specifically for this purpose. Our design is
made under flexible bay structure and comparisons are made with other layouts
from the literature that were designed under flexible bay structure.
</summary>
    <author>
      <name>Hossein Jahandideh</name>
    </author>
    <author>
      <name>Ardavan Asef-Vaziri</name>
    </author>
    <author>
      <name>Mohammad Modarres</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to the 2013 IEEE Symposium Series on Computational
  Intelligence: Swarm Intelligence Symposium. This paper has been withdrawn by
  the author, by the request of the supervisor, to be updated, fixed, and
  combined with other papers</arxiv:comment>
    <link href="http://arxiv.org/abs/1211.2361v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.2361v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.3451v1</id>
    <updated>2012-11-14T22:28:56Z</updated>
    <published>2012-11-14T22:28:56Z</published>
    <title>Memory Capacity of a Random Neural Network</title>
    <summary>  This paper considers the problem of information capacity of a random neural
network. The network is represented by matrices that are square and
symmetrical. The matrices have a weight which determines the highest and lowest
possible value found in the matrix. The examined matrices are randomly
generated and analyzed by a computer program. We find the surprising result
that the capacity of the network is a maximum for the binary random neural
network and it does not change as the number of quantization levels associated
with the weights increases.
</summary>
    <author>
      <name>Matt Stowe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1211.3451v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.3451v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.5098v1</id>
    <updated>2012-11-21T17:32:52Z</updated>
    <published>2012-11-21T17:32:52Z</published>
    <title>Scaling Genetic Programming for Source Code Modification</title>
    <summary>  In Search Based Software Engineering, Genetic Programming has been used for
bug fixing, performance improvement and parallelisation of programs through the
modification of source code. Where an evolutionary computation algorithm, such
as Genetic Programming, is to be applied to similar code manipulation tasks,
the complexity and size of source code for real-world software poses a
scalability problem. To address this, we intend to inspect how the Software
Engineering concepts of modularity, granularity and localisation of change can
be reformulated as additional mechanisms within a Genetic Programming
algorithm.
</summary>
    <author>
      <name>Brendan Cody-Kenny</name>
    </author>
    <author>
      <name>Stephen Barrett</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, Accepted for Graduate Student Workshop, GECCO 2012,
  Retracted by Authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1211.5098v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.5098v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.6410v1</id>
    <updated>2012-11-24T01:30:36Z</updated>
    <published>2012-11-24T01:30:36Z</published>
    <title>New Hoopoe Heuristic Optimization</title>
    <summary>  Most optimization problems in real life applications are often highly
nonlinear. Local optimization algorithms do not give the desired performance.
So, only global optimization algorithms should be used to obtain optimal
solutions. This paper introduces a new nature-inspired metaheuristic
optimization algorithm, called Hoopoe Heuristic (HH). In this paper, we will
study HH and validate it against some test functions. Investigations show that
it is very promising and could be seen as an optimization of the powerful
algorithm of cuckoo search. Finally, we discuss the features of Hoopoe
Heuristic and propose topics for further studies.
</summary>
    <author>
      <name>Mohammed El-Dosuky</name>
    </author>
    <author>
      <name>Ahmed EL-Bassiouny</name>
    </author>
    <author>
      <name>Taher Hamza</name>
    </author>
    <author>
      <name>Magdy Rashad</name>
    </author>
    <link href="http://arxiv.org/abs/1211.6410v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.6410v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.7184v1</id>
    <updated>2012-11-30T08:49:46Z</updated>
    <published>2012-11-30T08:49:46Z</published>
    <title>Erratum: Simplified Drift Analysis for Proving Lower Bounds in
  Evolutionary Computation</title>
    <summary>  This erratum points out an error in the simplified drift theorem (SDT)
[Algorithmica 59(3), 369-386, 2011]. It is also shown that a minor modification
of one of its conditions is sufficient to establish a valid result. In many
respects, the new theorem is more general than before. We no longer assume a
Markov process nor a finite search space. Furthermore, the proof of the theorem
is more compact than the previous ones. Finally, previous applications of the
SDT are revisited. It turns out that all of these either meet the modified
condition directly or by means of few additional arguments.
</summary>
    <author>
      <name>Pietro S. Oliveto</name>
    </author>
    <author>
      <name>Carsten Witt</name>
    </author>
    <link href="http://arxiv.org/abs/1211.7184v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.7184v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.2529v1</id>
    <updated>2012-12-11T16:40:45Z</updated>
    <published>2012-12-11T16:40:45Z</published>
    <title>On The Delays In Spiking Neural P Systems</title>
    <summary>  In this work we extend and improve the results done in a previous work on
simulating Spiking Neural P systems (SNP systems in short) with delays using
SNP systems without delays. We simulate the former with the latter over
sequential, iteration, join, and split routing. Our results provide
constructions so that both systems halt at exactly the same time, start with
only one spike, and produce the same number of spikes to the environment after
halting.
</summary>
    <author>
      <name>Francis George C. Cabarle</name>
    </author>
    <author>
      <name>Kelvin C. Buño</name>
    </author>
    <author>
      <name>Henry N. Adorna</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the 6th Symposium on the Mathematical Aspects of
  Computer Science (SMACS2012), Boracay, Philippines. 6 figures, 6 pages, 2
  columns</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.2529v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.2529v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="97P20" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.5374v4</id>
    <updated>2015-04-01T05:59:58Z</updated>
    <published>2013-02-21T18:57:09Z</published>
    <title>A Weight-coded Evolutionary Algorithm for the Multidimensional Knapsack
  Problem</title>
    <summary>  A revised weight-coded evolutionary algorithm (RWCEA) is proposed for solving
multidimensional knapsack problems. This RWCEA uses a new decoding method and
incorporates a heuristic method in initialization. Computational results show
that the RWCEA performs better than a weight-coded evolutionary algorithm
proposed by Raidl (1999) and to some existing benchmarks, it can yield better
results than the ones reported in the OR-library.
</summary>
    <author>
      <name>Quan Yuan</name>
    </author>
    <author>
      <name>Zhixin Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Applied Mathematics and Computation on April 8, 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1302.5374v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.5374v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="90B50" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.2096v1</id>
    <updated>2013-03-08T20:16:02Z</updated>
    <published>2013-03-08T20:16:02Z</published>
    <title>Gene-Machine, a new search heuristic algorithm</title>
    <summary>  This paper introduces Gene-Machine, an efficient and new search heuristic
algorithm, based in the building-block hypothesis. It is inspired by natural
evolution, but does not use some of the concepts present in genetic algorithms
like population, mutation and generation. This heuristic exhibits good
performance in comparison with genetic algorithms, and can be used to generate
useful solutions to optimization and search problems.
</summary>
    <author>
      <name>Alfredo Garcia Woods</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">GeneMachine uses the chromosome notion, genes and evolution, but it
  differs from genetic algorithms, in that it does not use mutation, nor
  population of individuals, neither the notion of generation</arxiv:comment>
    <link href="http://arxiv.org/abs/1303.2096v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.2096v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.6310v3</id>
    <updated>2013-06-05T15:46:53Z</updated>
    <published>2013-03-25T20:53:09Z</published>
    <title>A hybrid bat algorithm</title>
    <summary>  Swarm intelligence is a very powerful technique to be used for optimization
purposes. In this paper we present a new swarm intelligence algorithm, based on
the bat algorithm. The Bat algorithm is hybridized with differential evolution
strategies. Besides showing very promising results of the standard benchmark
functions, this hybridization also significantly improves the original bat
algorithm.
</summary>
    <author>
      <name>Iztok Fister Jr.</name>
    </author>
    <author>
      <name>Dušan Fister</name>
    </author>
    <author>
      <name>Xin-She Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Electrotechnical review, 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1303.6310v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.6310v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.3610v1</id>
    <updated>2013-04-12T11:54:35Z</updated>
    <published>2013-04-12T11:54:35Z</published>
    <title>Modified Soft Brood Crossover in Genetic Programming</title>
    <summary>  Premature convergence is one of the important issues while using Genetic
Programming for data modeling. It can be avoided by improving population
diversity. Intelligent genetic operators can help to improve the population
diversity. Crossover is an important operator in Genetic Programming. So, we
have analyzed number of intelligent crossover operators and proposed an
algorithm with the modification of soft brood crossover operator. It will help
to improve the population diversity and reduce the premature convergence. We
have performed experiments on three different symbolic regression problems.
Then we made the performance comparison of our proposed crossover (Modified
Soft Brood Crossover) with the existing soft brood crossover and subtree
crossover operators.
</summary>
    <author>
      <name>Hardik M. Parekh</name>
    </author>
    <author>
      <name>Vipul K. Dabhi</name>
    </author>
    <link href="http://arxiv.org/abs/1304.3610v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.3610v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.3612v1</id>
    <updated>2013-04-12T12:08:07Z</updated>
    <published>2013-04-12T12:08:07Z</published>
    <title>A Novel Metaheuristics To Solve Mixed Shop Scheduling Problems</title>
    <summary>  This paper represents the metaheuristics proposed for solving a class of Shop
Scheduling problem. The Bacterial Foraging Optimization algorithm is featured
with Ant Colony Optimization algorithm and proposed as a natural inspired
computing approach to solve the Mixed Shop Scheduling problem. The Mixed Shop
is the combination of Job Shop, Flow Shop and Open Shop scheduling problems.
The sample instances for all mentioned Shop problems are used as test data and
Mixed Shop survive its computational complexity to minimize the makespan. The
computational results show that the proposed algorithm is gentler to solve and
performs better than the existing algorithms.
</summary>
    <author>
      <name>V. Ravibabu</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal in Foundations of Computer Science &amp;
  Technology, March2013, Volume 3, Number 2, ISSN : 1839-7662</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1304.3612v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.3612v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.3892v1</id>
    <updated>2013-04-14T08:56:10Z</updated>
    <published>2013-04-14T08:56:10Z</published>
    <title>An accelerated CLPSO algorithm</title>
    <summary>  The particle swarm approach provides a low complexity solution to the
optimization problem among various existing heuristic algorithms. Recent
advances in the algorithm resulted in improved performance at the cost of
increased computational complexity, which is undesirable. Literature shows that
the particle swarm optimization algorithm based on comprehensive learning
provides the best complexity-performance trade-off. We show how to reduce the
complexity of this algorithm further, with a slight but acceptable performance
loss. This enhancement allows the application of the algorithm in time critical
applications, such as, real-time tracking, equalization etc.
</summary>
    <author>
      <name>Muhammad Omer Bin Saeed</name>
    </author>
    <author>
      <name>Muhammad Saqib Sohail</name>
    </author>
    <author>
      <name>Syed Zeeshan Rizvi</name>
    </author>
    <author>
      <name>Mobien Shoaib</name>
    </author>
    <author>
      <name>Asrar Ul Haq Sheikh</name>
    </author>
    <link href="http://arxiv.org/abs/1304.3892v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.3892v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.2490v2</id>
    <updated>2014-04-15T09:23:36Z</updated>
    <published>2013-05-11T09:57:15Z</published>
    <title>Combining Drift Analysis and Generalized Schema Theory to Design
  Efficient Hybrid and/or Mixed Strategy EAs</title>
    <summary>  Hybrid and mixed strategy EAs have become rather popular for tackling various
complex and NP-hard optimization problems. While empirical evidence suggests
that such algorithms are successful in practice, rather little theoretical
support for their success is available, not mentioning a solid mathematical
foundation that would provide guidance towards an efficient design of this type
of EAs. In the current paper we develop a rigorous mathematical framework that
suggests such designs based on generalized schema theory, fitness levels and
drift analysis. An example-application for tackling one of the classical
NP-hard problems, the "single-machine scheduling problem" is presented.
</summary>
    <author>
      <name>Boris Mitavskiy</name>
    </author>
    <author>
      <name>Jun He</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CEC.2013.6557808</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CEC.2013.6557808" rel="related"/>
    <link href="http://arxiv.org/abs/1305.2490v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.2490v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.6537v1</id>
    <updated>2013-05-28T15:42:51Z</updated>
    <published>2013-05-28T15:42:51Z</published>
    <title>A Cooperative Coevolutionary Genetic Algorithm for Learning Bayesian
  Network Structures</title>
    <summary>  We propose a cooperative coevolutionary genetic algorithm for learning
Bayesian network structures from fully observable data sets. Since this problem
can be decomposed into two dependent subproblems, that is to find an ordering
of the nodes and an optimal connectivity matrix, our algorithm uses two
subpopulations, each one representing a subtask. We describe the empirical
results obtained with simulations of the Alarm and Insurance networks. We show
that our algorithm outperforms the deterministic algorithm K2.
</summary>
    <author>
      <name>Arthur Carvalho</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2001576.2001729</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2001576.2001729" rel="related"/>
    <link href="http://arxiv.org/abs/1305.6537v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.6537v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.0442v1</id>
    <updated>2013-06-03T14:47:11Z</updated>
    <published>2013-06-03T14:47:11Z</published>
    <title>Evolutionary Approach for the Containers Bin-Packing Problem</title>
    <summary>  This paper deals with the resolution of combinatorial optimization problems,
particularly those concerning the maritime transport scheduling. We are
interested in the management platforms in a river port and more specifically in
container organisation operations with a view to minimizing the number of
container rehandlings. Subsequently, we rmeet customers delivery deadlines and
we reduce ship stoppage time In this paper, we propose a genetic algorithm to
solve this problem and we present some experiments and results.
</summary>
    <author>
      <name>R. Kammarti</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">: LACS, Ecole Nationale des Ingenieurs de Tunis, Tunis - Belvedere. TUNISIE</arxiv:affiliation>
    </author>
    <author>
      <name>I. Ayachi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">: LAGIS, Ecole Centrale de Lille, Villeneuve dAscq, France</arxiv:affiliation>
    </author>
    <author>
      <name>M. Ksouri</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">: LACS, Ecole Nationale des Ingenieurs de Tunis, Tunis - Belvedere. TUNISIE</arxiv:affiliation>
    </author>
    <author>
      <name>P. Borne</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">: LAGIS, Ecole Centrale de Lille, Villeneuve dAscq, France</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Studies in Informatics and Control, Vol. 18, No. 4/2009, pages
  315-324</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1306.0442v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.0442v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.4622v1</id>
    <updated>2013-06-19T17:31:47Z</updated>
    <published>2013-06-19T17:31:47Z</published>
    <title>Solution to Quadratic Equation Using Genetic Algorithm</title>
    <summary>  Solving Quadratic equation is one of the intrinsic interests as it is the
simplest nonlinear equations. A novel approach for solving Quadratic Equation
based on Genetic Algorithms (GAs) is presented. Genetic Algorithms (GAs) are a
technique to solve problems which need optimization. Generation of trial
solutions have been formed by this method. Many examples have been worked out,
and in most cases we find out the exact solution. We have discussed the effect
of different parameters on the performance of the developed algorithm. The
results are concluded after rigorous testing on different equations.
</summary>
    <author>
      <name>Tanistha Nayak</name>
    </author>
    <author>
      <name>Tirtharaj Dash</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">appeared in: Conf. Proceedings of National Conference on Artificial
  Intelligence, Robotics and Embedded Systems (AIRES-2012), Andhra University,
  Vishakhapatnam, India (29-30 June, 2012), pp. 10-13</arxiv:comment>
    <link href="http://arxiv.org/abs/1306.4622v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.4622v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.4793v1</id>
    <updated>2013-06-20T09:00:40Z</updated>
    <published>2013-06-20T09:00:40Z</published>
    <title>Evolving Boolean Regulatory Networks with Epigenetic Control</title>
    <summary>  The significant role of epigenetic mechanisms within natural systems has
become increasingly clear. This paper uses a recently presented abstract,
tunable Boolean genetic regulatory network model to explore aspects of
epigenetics. It is shown how dynamically controlling transcription via a DNA
methylation-inspired mechanism can be selected for by simulated evolution under
various single and multiple cell scenarios. Further, it is shown that the
effects of such control can be inherited without detriment to fitness.
</summary>
    <author>
      <name>Larry Bull</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 10 figures. arXiv admin note: substantial text overlap with
  arXiv:1303.7220</arxiv:comment>
    <link href="http://arxiv.org/abs/1306.4793v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.4793v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.MN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.1372v2</id>
    <updated>2013-08-19T09:11:13Z</updated>
    <published>2013-07-04T15:22:35Z</published>
    <title>Clustering of Complex Networks and Community Detection Using Group
  Search Optimization</title>
    <summary>  Group Search Optimizer(GSO) is one of the best algorithms, is very new in the
field of Evolutionary Computing. It is very robust and efficient algorithm,
which is inspired by animal searching behaviour. The paper describes an
application of GSO to clustering of networks. We have tested GSO against five
standard benchmark datasets, GSO algorithm is proved very competitive in terms
of accuracy and convergence speed.
</summary>
    <author>
      <name>G. Kishore Kumar</name>
    </author>
    <author>
      <name>V. K. Jayaraman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.1372v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.1372v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.3463v4</id>
    <updated>2014-03-28T02:19:00Z</updated>
    <published>2013-07-12T14:07:09Z</published>
    <title>Non-Elitist Genetic Algorithm as a Local Search Method</title>
    <summary>  Sufficient conditions are found under which the iterated non-elitist genetic
algorithm with tournament selection first visits a local optimum in
polynomially bounded time on average. It is shown that these conditions are
satisfied on a class of problems with guaranteed local optima (GLO) if
appropriate parameters of the algorithm are chosen.
</summary>
    <author>
      <name>Anton Eremeev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended abstract of the talk presented at Dagstuhl Seminar "Theory
  of Evolutionary Algorithms" (Dagstuhl, Germany, 30 June - 5 July 2013)</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.3463v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.3463v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.3782v3</id>
    <updated>2016-04-22T18:45:01Z</updated>
    <published>2013-07-14T21:03:39Z</published>
    <title>Handwritten Digits Recognition using Deep Convolutional Neural Network:
  An Experimental Study using EBlearn</title>
    <summary>  In this paper, results of an experimental study of a deep convolution neural
network architecture which can classify different handwritten digits using
EBLearn library are reported. The purpose of this neural network is to classify
input images into 10 different classes or digits (0-9) and to explore new
findings. The input dataset used consists of digits images of size 32X32 in
grayscale (MNIST dataset).
</summary>
    <author>
      <name>Karim M. Mahmoud</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by the author due to some errors and
  incomplete study</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.3782v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.3782v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.4274v1</id>
    <updated>2013-07-16T13:45:24Z</updated>
    <published>2013-07-16T13:45:24Z</published>
    <title>The Fitness Level Method with Tail Bounds</title>
    <summary>  The fitness-level method, also called the method of f-based partitions, is an
intuitive and widely used technique for the running time analysis of randomized
search heuristics. It was originally defined to prove upper and lower bounds on
the expected running time. Recently, upper tail bounds were added to the
technique; however, these tail bounds only apply to running times that are at
least twice as large as the expectation.
  We remove this restriction and supplement the fitness-level method with sharp
tail bounds, including lower tails. As an exemplary application, we prove that
the running time of randomized local search on OneMax is sharply concentrated
around n ln n - 0.1159 n.
</summary>
    <author>
      <name>Carsten Witt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.4274v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.4274v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.5534v1</id>
    <updated>2013-07-21T15:34:24Z</updated>
    <published>2013-07-21T15:34:24Z</published>
    <title>A New Optimization Approach Based on Rotational Mutation and Crossover
  Operator</title>
    <summary>  Evaluating a global optimal point in many global optimization problems in
large space is required to more calculations. In this paper, there is presented
a new approach for the continuous functions optimization with rotational
mutation and crossover operator. This proposed method (RMC) starts from the
point which has best fitness value by elitism mechanism and after that
rotational mutation and crossover operator are used to reach optimal point. RMC
method is implemented by GA (Briefly RMCGA) and is compared with other
wellknown algorithms such as: DE, PGA, Grefensstette and Eshelman[15,16] and
numerical and simulating results show that RMCGA achieve global optimal point
with more decision by smaller generations.
</summary>
    <author>
      <name>Masoumeh Vali</name>
    </author>
    <link href="http://arxiv.org/abs/1307.5534v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.5534v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.5679v1</id>
    <updated>2013-07-22T12:25:21Z</updated>
    <published>2013-07-22T12:25:21Z</published>
    <title>Sub-Dividing Genetic Method for Optimization Problems</title>
    <summary>  Nowadays, optimization problem have more application in all major but they
have problem in computation. Computation global point in continuous functions
have high calculation and this became clearer in large space .In this paper, we
proposed Sub- Dividing Genetic Method(SGM) that have less computation than
other method for achieving global points . This method userotation mutation and
crossover based sub-division method that sub diving method is used for minimize
search space and rotation mutation with crossover is used for finding global
optimal points. In experimental, SGM algorithm is implemented on De Jong
function. The numerical examples show that SGM is performed more optimal than
other methods such as Grefensstette, Random Value, and PNG.
</summary>
    <author>
      <name>Masoumeh Vali</name>
    </author>
    <link href="http://arxiv.org/abs/1307.5679v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.5679v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.8104v1</id>
    <updated>2013-07-30T19:51:12Z</updated>
    <published>2013-07-30T19:51:12Z</published>
    <title>Neural Network Capacity for Multilevel Inputs</title>
    <summary>  This paper examines the memory capacity of generalized neural networks.
Hopfield networks trained with a variety of learning techniques are
investigated for their capacity both for binary and non-binary alphabets. It is
shown that the capacity can be much increased when multilevel inputs are used.
New learning strategies are proposed to increase Hopfield network capacity, and
the scalability of these methods is also examined in respect to size of the
network. The ability to recall entire patterns from stimulation of a single
neuron is examined for the increased capacity networks.
</summary>
    <author>
      <name>Matt Stowe</name>
    </author>
    <author>
      <name>Subhash Kak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages,17 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.8104v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.8104v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.1603v2</id>
    <updated>2013-08-08T06:14:59Z</updated>
    <published>2013-08-07T15:29:09Z</published>
    <title>A Note on Topology Preservation in Classification, and the Construction
  of a Universal Neuron Grid</title>
    <summary>  It will be shown that according to theorems of K. Menger, every neuron grid
if identified with a curve is able to preserve the adopted qualitative
structure of a data space. Furthermore, if this identification is made, the
neuron grid structure can always be mapped to a subset of a universal neuron
grid which is constructable in three space dimensions. Conclusions will be
drawn for established neuron grid types as well as neural fields.
</summary>
    <author>
      <name>Dietmar Volz</name>
    </author>
    <link href="http://arxiv.org/abs/1308.1603v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.1603v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="92F99" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.3400v1</id>
    <updated>2013-08-14T18:05:13Z</updated>
    <published>2013-08-14T18:05:13Z</published>
    <title>Guiding Designs of Self-Organizing Swarms: Interactive and Automated
  Approaches</title>
    <summary>  Self-organization of heterogeneous particle swarms is rich in its dynamics
but hard to design in a traditional top-down manner, especially when many types
of kinetically distinct particles are involved. In this chapter, we discuss how
we have been addressing this problem by (1) utilizing and enhancing interactive
evolutionary design methods and (2) realizing spontaneous evolution of self
organizing swarms within an artificial ecosystem.
</summary>
    <author>
      <name>Hiroki Sayama</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 16 figures, 3 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Guided Self-Organization: Inception, Springer, 2014, pp.365-387</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1308.3400v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.3400v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.2183v1</id>
    <updated>2013-09-09T15:03:59Z</updated>
    <published>2013-09-09T15:03:59Z</published>
    <title>Application of Artificial Neural Networks in Estimating Participation in
  Elections</title>
    <summary>  It is approved that artificial neural networks can be considerable effective
in anticipating and analyzing flows in which traditional methods and statics
are not able to solve. in this article, by using two-layer feedforward network
with tan-sigmoid transmission function in input and output layers, we can
anticipate participation rate of public in kohgiloye and boyerahmad province in
future presidential election of islamic republic of iran with 91% accuracy. the
assessment standards of participation such as confusion matrix and roc diagrams
have been approved our claims.
</summary>
    <author>
      <name>Seyyed Reza Khaze</name>
    </author>
    <author>
      <name>Mohammad Masdari</name>
    </author>
    <author>
      <name>Sohrab Hojjatkhah</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijitmc.2013.1303</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijitmc.2013.1303" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Information Technology, Modeling and
  Computing (IJITMC) Vol.1, No.3,August 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1309.2183v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.2183v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.1090v1</id>
    <updated>2013-11-05T15:33:30Z</updated>
    <published>2013-11-05T15:33:30Z</published>
    <title>Polyhedrons and Perceptrons Are Functionally Equivalent</title>
    <summary>  Mathematical definitions of polyhedrons and perceptron networks are
discussed. The formalization of polyhedrons is done in a rather traditional
way. For networks, previously proposed systems are developed. Perceptron
networks in disjunctive normal form (DNF) and conjunctive normal forms (CNF)
are introduced. The main theme is that single output perceptron neural networks
and characteristic functions of polyhedrons are one and the same class of
functions. A rigorous formulation and proof that three layers suffice is
obtained. The various constructions and results are among several steps
required for algorithms that replace incremental and statistical learning with
more efficient, direct and exact geometric methods for calculation of
perceptron architecture and weights.
</summary>
    <author>
      <name>Daniel Crespin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 0 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.1090v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.1090v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T01" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3; I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.5548v1</id>
    <updated>2013-12-19T13:45:45Z</updated>
    <published>2013-12-19T13:45:45Z</published>
    <title>My First Deep Learning System of 1991 + Deep Learning Timeline 1962-2013</title>
    <summary>  Deep Learning has attracted significant attention in recent years. Here I
present a brief overview of my first Deep Learner of 1991, and its historic
context, with a timeline of Deep Learning highlights.
</summary>
    <author>
      <name>Jürgen Schmidhuber</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages. As a machine learning researcher I am obsessed with proper
  credit assignment. This draft is the result of an experiment in rapid massive
  open online peer review. Since 20 September 2013, subsequent revisions
  published under http://www.deeplearning.me have absorbed many suggestions for
  improvements by experts</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.5548v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.5548v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.0523v1</id>
    <updated>2014-01-02T20:15:00Z</updated>
    <published>2014-01-02T20:15:00Z</published>
    <title>Solving Poisson Equation by Genetic Algorithms</title>
    <summary>  This paper deals with a method for solving Poisson Equation (PE) based on
genetic algorithms and grammatical evolution. The method forms generations of
solutions expressed in an analytical form. Several examples of PE are tested
and in most cases the exact solution is recovered. But, when the solution
cannot be expressed in an analytical form, our method produces a satisfactory
solution with a good level of accuracy
</summary>
    <author>
      <name>Khalid Jebari</name>
    </author>
    <author>
      <name>Mohammed Madiafi</name>
    </author>
    <author>
      <name>Abdelaziz El Moujahid</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Applications Volume 83, No 5,
  December 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1401.0523v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.0523v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.0858v1</id>
    <updated>2014-01-05T01:37:45Z</updated>
    <published>2014-01-05T01:37:45Z</published>
    <title>Multimodal Optimization by Sparkling Squid Populations</title>
    <summary>  The swarm intelligence of animals is a natural paradigm to apply to
optimization problems. Ant colony, bee colony, firefly and bat algorithms are
amongst those that have been demonstrated to efficiently to optimize complex
constraints. This paper proposes the new Sparkling Squid Algorithm (SSA) for
multimodal optimization, inspired by the intelligent swarm behavior of its
namesake. After an introduction, formulation and discussion of its
implementation, it will be compared to other popular metaheuristics. Finally,
applications to well - known problems such as image registration and the
traveling salesperson problem will be discussed.
</summary>
    <author>
      <name>Videh Seksaria</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 4 figues</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.0858v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.0858v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.1333v1</id>
    <updated>2014-01-07T10:29:24Z</updated>
    <published>2014-01-07T10:29:24Z</published>
    <title>Time series forecasting using neural networks</title>
    <summary>  Recent studies have shown the classification and prediction power of the
Neural Networks. It has been demonstrated that a NN can approximate any
continuous function. Neural networks have been successfully used for
forecasting of financial data series. The classical methods used for time
series prediction like Box-Jenkins or ARIMA assumes that there is a linear
relationship between inputs and outputs. Neural Networks have the advantage
that can approximate nonlinear functions. In this paper we compared the
performances of different feed forward and recurrent neural networks and
training algorithms for predicting the exchange rate EUR/RON and USD/RON. We
used data series with daily exchange rates starting from 2005 until 2013.
</summary>
    <author>
      <name>Bogdan Oancea</name>
    </author>
    <author>
      <name>ŞTefan Cristian Ciucu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the CKS 2013 International Conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.1333v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.1333v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.2468v1</id>
    <updated>2014-01-10T21:09:36Z</updated>
    <published>2014-01-10T21:09:36Z</published>
    <title>N2Sky - Neural Networks as Services in the Clouds</title>
    <summary>  We present the N2Sky system, which provides a framework for the exchange of
neural network specific knowledge, as neural network paradigms and objects, by
a virtual organization environment. It follows the sky computing paradigm
delivering ample resources by the usage of federated Clouds. N2Sky is a novel
Cloud-based neural network simulation environment, which follows a pure service
oriented approach. The system implements a transparent environment aiming to
enable both novice and experienced users to do neural network research easily
and comfortably. N2Sky is built using the RAVO reference architecture of
virtual organizations which allows itself naturally integrating into the Cloud
service stack (SaaS, PaaS, and IaaS) of service oriented architectures.
</summary>
    <author>
      <name>Erich Schikuta</name>
    </author>
    <author>
      <name>Erwin Mann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">extended version of paper published at IJCNN 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.2468v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.2468v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.5; I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.2949v1</id>
    <updated>2014-01-10T12:46:56Z</updated>
    <published>2014-01-10T12:46:56Z</published>
    <title>Exploiting generalisation symmetries in accuracy-based learning
  classifier systems: An initial study</title>
    <summary>  Modern learning classifier systems typically exploit a niched genetic
algorithm to facilitate rule discovery. When used for reinforcement learning,
such rules represent generalisations over the state-action-reward space. Whilst
encouraging maximal generality, the niching can potentially hinder the
formation of generalisations in the state space which are symmetrical, or very
similar, over different actions. This paper introduces the use of rules which
contain multiple actions, maintaining accuracy and reward metrics for each
action. It is shown that problem symmetries can be exploited, improving
performance, whilst not degrading performance when symmetries are reduced.
</summary>
    <author>
      <name>Larry Bull</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.2949v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.2949v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.3607v2</id>
    <updated>2014-02-07T11:55:12Z</updated>
    <published>2014-01-15T14:37:48Z</published>
    <title>A Brief History of Learning Classifier Systems: From CS-1 to XCS</title>
    <summary>  Modern Learning Classifier Systems can be characterized by their use of rule
accuracy as the utility metric for the search algorithm(s) discovering useful
rules. Such searching typically takes place within the restricted space of
co-active rules for efficiency. This paper gives an historical overview of the
evolution of such systems up to XCS, and then some of the subsequent
developments of XCS to different types of learning.
</summary>
    <author>
      <name>Larry Bull</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">37 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.3607v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.3607v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.4674v1</id>
    <updated>2014-01-19T14:46:09Z</updated>
    <published>2014-01-19T14:46:09Z</published>
    <title>Evolving Accuracy: A Genetic Algorithm to Improve Election Night
  Forecasts</title>
    <summary>  In this paper, we apply genetic algorithms to the field of electoral studies.
Forecasting election results is one of the most exciting and demanding tasks in
the area of market research, especially due to the fact that decisions have to
be made within seconds on live television. We show that the proposed method
outperforms currently applied approaches and thereby provide an argument to
tighten the intersection between computer science and social science,
especially political science, further. We scrutinize the performance of our
algorithm's runtime behavior to evaluate its applicability in the field.
Numerical results with real data from a local election in the Austrian province
of Styria from 2010 substantiate the applicability of the proposed approach.
</summary>
    <author>
      <name>Ronald Hochreiter</name>
    </author>
    <author>
      <name>Christoph Waldhauser</name>
    </author>
    <link href="http://arxiv.org/abs/1401.4674v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.4674v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.5246v1</id>
    <updated>2014-01-21T10:01:27Z</updated>
    <published>2014-01-21T10:01:27Z</published>
    <title>Genetic Algorithms and its use with back-propagation network</title>
    <summary>  Genetic algorithms are considered as one of the most efficient search
techniques. Although they do not offer an optimal solution, their ability to
reach a suitable solution in considerably short time gives them their
respectable role in many AI techniques. This work introduces genetic algorithms
and describes their characteristics. Then a novel method using genetic
algorithm in best training set generation and selection for a back-propagation
network is proposed. This work also offers a new extension to the original
genetic algorithms
</summary>
    <author>
      <name>Ayman M. Bahaa-Eldin</name>
    </author>
    <author>
      <name>A. M. A. Wahdan</name>
    </author>
    <author>
      <name>H. M. K. Mahdi</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">AIN Shams University, Faculty of Engineering Scientific Bulletin,
  Volume 35, Issue 3, pp 337-348 (2000)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1401.5246v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.5246v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.0708v1</id>
    <updated>2014-02-04T12:25:31Z</updated>
    <published>2014-02-04T12:25:31Z</published>
    <title>Microstrip Coupler Design Using Bat Algorithm</title>
    <summary>  Evolutionary and swarm algorithms have found many applications in design
problems since todays computing power enables these algorithms to find
solutions to complicated design problems very fast. Newly proposed hybrid
algorithm, bat algorithm, has been applied for the design of microwave
microstrip couplers for the first time. Simulation results indicate that the
bat algorithm is a very fast algorithm and it produces very reliable results.
</summary>
    <author>
      <name>Ezgi Deniz Ulker</name>
    </author>
    <author>
      <name>Sadik Ulker</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijaia.2014.5110</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijaia.2014.5110" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 4 figures, 1 table</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Artificial Intelligence &amp; Applications
  (IJAIA), vol. 5, no. 1, January 2014, pp. 127-133</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1402.0708v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.0708v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.5428v1</id>
    <updated>2014-02-21T21:18:58Z</updated>
    <published>2014-02-21T21:18:58Z</published>
    <title>An Evolutionary approach for solving Shrödinger Equation</title>
    <summary>  The purpose of this paper is to present a method of solving the Shr\"odinger
Equation (SE) by Genetic Algorithms and Grammatical Evolution. The method forms
generations of trial solutions expressed in an analytical form. We illustrate
the effectiveness of this method providing, for example, the results of its
application to a quantum system minimal energy, and we compare these results
with those produced by traditional analytical methods
</summary>
    <author>
      <name>Khalid jebari</name>
    </author>
    <author>
      <name>Mohammed Madiafi</name>
    </author>
    <author>
      <name>Abdelaziz Elmoujahid</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1401.0523</arxiv:comment>
    <link href="http://arxiv.org/abs/1402.5428v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.5428v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.6556v1</id>
    <updated>2014-02-26T14:39:57Z</updated>
    <published>2014-02-26T14:39:57Z</published>
    <title>Evolutionary solving of the debts' clearing problem</title>
    <summary>  The debts' clearing problem is about clearing all the debts in a group of n
entities (persons, companies etc.) using a minimal number of money transaction
operations. The problem is known to be NP-hard in the strong sense. As for many
intractable problems, techniques from the field of artificial intelligence are
useful in finding solutions close to optimum for large inputs. An evolutionary
algorithm for solving the debts' clearing problem is proposed.
</summary>
    <author>
      <name>Csaba Patcas</name>
    </author>
    <author>
      <name>Attila Bartha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1402.6556v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.6556v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="97R40" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.8; G.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.1073v1</id>
    <updated>2014-03-05T11:05:16Z</updated>
    <published>2014-03-05T11:05:16Z</published>
    <title>Artificial Neuron Modelling Based on Wave Shape</title>
    <summary>  This paper describes a new model for an artificial neural network processing
unit or neuron. It is slightly different to a traditional feedforward network
by the fact that it favours a mechanism of trying to match the wave-like
'shape' of the input with the shape of the output against specific value error
corrections. The expectation is then that a best fit shape can be transposed
into the desired output values more easily. This allows for notions of
reinforcement through resonance and also the construction of synapses.
</summary>
    <author>
      <name>Kieran Greer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">BRAIN. Broad Research in Artificial Intelligence and Neuroscience,
  Volume 4, Issues 1-4, October 2013, pp. 20 - 25, ISSN 2067-3957 (online),
  ISSN 2068 - 0473 (print)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1403.1073v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.1073v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.1727v1</id>
    <updated>2014-03-07T11:49:23Z</updated>
    <published>2014-03-07T11:49:23Z</published>
    <title>On the Sequence of State Configurations in the Garden of Eden</title>
    <summary>  Autonomous threshold element circuit networks are used to investigate the
structure of neural networks. With these circuits, as the transition functions
are threshold functions, it is necessary to consider the existence of sequences
of state configurations that cannot be transitioned. In this study, we focus on
all logical functions of four or fewer variables, and we discuss the periodic
sequences and transient series that transition from all sequences of state
configurations. Furthermore, by using the sequences of state configurations in
the Garden of Eden, we show that it is easy to obtain functions that determine
the operation of circuit networks.
</summary>
    <author>
      <name>Yukihiro Kamada</name>
    </author>
    <author>
      <name>Kiyonori Miyasaki</name>
    </author>
    <link href="http://arxiv.org/abs/1403.1727v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.1727v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.4871v1</id>
    <updated>2014-03-19T16:24:13Z</updated>
    <published>2014-03-19T16:24:13Z</published>
    <title>Evolutionary Algorithm for Drug Discovery Interim Design Report</title>
    <summary>  A software program which aims to provide an exploration capability over the
Search Space of potential drug molecules. The program explores the search space
by generating random molecules, determining their fitness and then breeding a
new generation from the fittest individuals. The search space, in theory any
combination of any elements in any order, is constrained by the use of a subset
of elements and a list of fragments, molecular parts that are known to be
useful in drug development. The resultant molecules from each generation are
stored in a searchable database, so that the user can browse through previous
generations looking for interesting molecules.
</summary>
    <author>
      <name>Mark Shackelford</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 Pages, Interim Design Document</arxiv:comment>
    <link href="http://arxiv.org/abs/1403.4871v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.4871v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.7178v1</id>
    <updated>2014-03-27T19:16:50Z</updated>
    <published>2014-03-27T19:16:50Z</published>
    <title>Offshore Wind Farm Layout Optimization Using Adapted Genetic Algorithm:
  A different perspective</title>
    <summary>  In this paper we study the problem of optimal layout of an offshore wind farm
to minimize the wake effect impacts. Considering the specific requirements of
concerned offshore wind farm, we propose an adaptive genetic algorithm (AGA)
which introduces location swaps to replace random crossovers in conventional
GAs. That way the total number of turbines in the resulting layout will be
effectively kept to the initially specified value. We experiment the proposed
AGA method on three cases with free wind speed of 12 m/s, 20 m/s, and a typical
offshore wind distribution setting respectively. Numerical results verify the
effectiveness of our proposed algorithm which achieves a much faster
convergence compared to conventional GA algorithms.
</summary>
    <author>
      <name>Feng Liu</name>
    </author>
    <author>
      <name>Zhifang Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1403.7178v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.7178v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.7752v2</id>
    <updated>2015-01-23T19:12:05Z</updated>
    <published>2014-03-30T13:11:55Z</published>
    <title>Auto-encoders: reconstruction versus compression</title>
    <summary>  We discuss the similarities and differences between training an auto-encoder
to minimize the reconstruction error, and training the same auto-encoder to
compress the data via a generative model. Minimizing a codelength for the data
using an auto-encoder is equivalent to minimizing the reconstruction error plus
some correcting terms which have an interpretation as either a denoising or
contractive property of the decoding function. These terms are related but not
identical to those used in denoising or contractive auto-encoders [Vincent et
al. 2010, Rifai et al. 2011]. In particular, the codelength viewpoint fully
determines an optimal noise level for the denoising criterion.
</summary>
    <author>
      <name>Yann Ollivier</name>
    </author>
    <link href="http://arxiv.org/abs/1403.7752v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.7752v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.0695v1</id>
    <updated>2014-04-02T20:28:51Z</updated>
    <published>2014-04-02T20:28:51Z</published>
    <title>Multi-objective Flower Algorithm for Optimization</title>
    <summary>  Flower pollination algorithm is a new nature-inspired algorithm, based on the
characteristics of flowering plants. In this paper, we extend this flower
algorithm to solve multi-objective optimization problems in engineering. By
using the weighted sum method with random weights, we show that the proposed
multi-objective flower algorithm can accurately find the Pareto fronts for a
set of test functions. We then solve a bi-objective disc brake design problem,
which indeed converges quickly.
</summary>
    <author>
      <name>Xin-She Yang</name>
    </author>
    <author>
      <name>M. Karamanoglu</name>
    </author>
    <author>
      <name>Xingshi He</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.procs.2013.05.251</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.procs.2013.05.251" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 figures. arXiv admin note: substantial text overlap with
  arXiv:1312.5673</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">X. S. Yang, M. Karamanoglu, X. S. He, Multi-objective Flower
  Algorithm for Optimization, Procedia Computer Science, vol. 18, pp. 861-868
  (2013)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1404.0695v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.0695v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="90C26" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.0868v1</id>
    <updated>2014-04-03T11:46:42Z</updated>
    <published>2014-04-03T11:46:42Z</published>
    <title>A Novel Genetic Algorithm using Helper Objectives for the 0-1 Knapsack
  Problem</title>
    <summary>  The 0-1 knapsack problem is a well-known combinatorial optimisation problem.
Approximation algorithms have been designed for solving it and they return
provably good solutions within polynomial time. On the other hand, genetic
algorithms are well suited for solving the knapsack problem and they find
reasonably good solutions quickly. A naturally arising question is whether
genetic algorithms are able to find solutions as good as approximation
algorithms do. This paper presents a novel multi-objective optimisation genetic
algorithm for solving the 0-1 knapsack problem. Experiment results show that
the new algorithm outperforms its rivals, the greedy algorithm, mixed strategy
genetic algorithm, and greedy algorithm + mixed strategy genetic algorithm.
</summary>
    <author>
      <name>Jun He</name>
    </author>
    <author>
      <name>Feidun He</name>
    </author>
    <author>
      <name>Hongbin Dong</name>
    </author>
    <link href="http://arxiv.org/abs/1404.0868v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.0868v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.1999v1</id>
    <updated>2014-04-08T03:41:50Z</updated>
    <published>2014-04-08T03:41:50Z</published>
    <title>Notes on Generalized Linear Models of Neurons</title>
    <summary>  Experimental neuroscience increasingly requires tractable models for
analyzing and predicting the behavior of neurons and networks. The generalized
linear model (GLM) is an increasingly popular statistical framework for
analyzing neural data that is flexible, exhibits rich dynamic behavior and is
computationally tractable (Paninski, 2004; Pillow et al., 2008; Truccolo et
al., 2005). What follows is a brief summary of the primary equations governing
the application of GLM's to spike trains with a few sentences linking this work
to the larger statistical literature. Latter sections include extensions of a
basic GLM to model spatio-temporal receptive fields as well as network activity
in an arbitrary numbers of neurons.
</summary>
    <author>
      <name>Jonathon Shlens</name>
    </author>
    <link href="http://arxiv.org/abs/1404.1999v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.1999v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.5767v1</id>
    <updated>2014-04-23T09:53:47Z</updated>
    <published>2014-04-23T09:53:47Z</published>
    <title>Codynamic Fitness Landscapes of Coevolutionary Minimal Substrates</title>
    <summary>  Coevolutionary minimal substrates are simple and abstract models that allow
studying the relationships and codynamics between objective and subjective
fitness. Using these models an approach is presented for defining and analyzing
fitness landscapes of coevolutionary problems. We devise similarity measures of
codynamic fitness landscapes and experimentally study minimal substrates of
test--based and compositional problems for both cooperative and competitive
interaction.
</summary>
    <author>
      <name>Hendrik Richter</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CEC.2014.6900272</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CEC.2014.6900272" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In: Proc. IEEE Congress on Evolutionary Computation, IEEE CEC
  2014, (Ed.: C. A. Coello, Coello), IEEE Press, Piscataway, NJ, 2014,
  2692-2699</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1404.5767v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.5767v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.4510v1</id>
    <updated>2014-05-18T14:25:56Z</updated>
    <published>2014-05-18T14:25:56Z</published>
    <title>A Memetic Algorithm for the Linear Ordering Problem with Cumulative
  Costs</title>
    <summary>  This paper introduces an effective memetic algorithm for the linear ordering
problem with cumulative costs. The proposed algorithm combines an order-based
recombination operator with an improved forward-backward local search procedure
and employs a solution quality based replacement criterion for pool updating.
Extensive experiments on 118 well-known benchmark instances show that the
proposed algorithm achieves competitive results by identifying 46 new upper
bounds. Furthermore, some critical ingredients of our algorithm are analyzed to
understand the source of its performance.
</summary>
    <author>
      <name>Tao Ye</name>
    </author>
    <author>
      <name>Kan Zhou</name>
    </author>
    <author>
      <name>Zhipeng Lu</name>
    </author>
    <author>
      <name>Jin-Kao Hao</name>
    </author>
    <link href="http://arxiv.org/abs/1405.4510v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.4510v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.4894v1</id>
    <updated>2014-04-25T12:59:40Z</updated>
    <published>2014-04-25T12:59:40Z</published>
    <title>Optimization of OFDM radar waveforms using genetic algorithms</title>
    <summary>  In this paper, we present our investigations on the use of single objective
and multiobjective genetic algorithms based optimisation algorithms to improve
the design of OFDM pulses for radar. We discuss these optimization procedures
in the scope of a waveform design intended for two different radar processing
solutions. Lastly, we show how the encoding solution is suited to permit the
optimizations of waveform for OFDM radar related challenges such as enhanced
detection.
</summary>
    <author>
      <name>Gabriel Lellouch</name>
    </author>
    <author>
      <name>Amit Kumar Mishra</name>
    </author>
    <link href="http://arxiv.org/abs/1405.4894v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.4894v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.7777v1</id>
    <updated>2014-05-30T05:24:22Z</updated>
    <published>2014-05-30T05:24:22Z</published>
    <title>Online and Adaptive Pseudoinverse Solutions for ELM Weights</title>
    <summary>  The ELM method has become widely used for classification and regressions
problems as a result of its accuracy, simplicity and ease of use. The solution
of the hidden layer weights by means of a matrix pseudoinverse operation is a
significant contributor to the utility of the method; however, the conventional
calculation of the pseudoinverse by means of a singular value decomposition
(SVD) is not always practical for large data sets or for online updates to the
solution. In this paper we discuss incremental methods for solving the
pseudoinverse which are suitable for ELM. We show that careful choice of
methods allows us to optimize for accuracy, ease of computation, or
adaptability of the solution.
</summary>
    <author>
      <name>André van Schaik</name>
    </author>
    <author>
      <name>Jonathan Tapson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in Neurocomputing</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.7777v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.7777v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.2539v1</id>
    <updated>2014-06-10T13:22:13Z</updated>
    <published>2014-06-10T13:22:13Z</published>
    <title>Maximizing Diversity for Multimodal Optimization</title>
    <summary>  Most multimodal optimization algorithms use the so called \textit{niching
methods}~\cite{mahfoud1995niching} in order to promote diversity during
optimization, while others, like \textit{Artificial Immune
Systems}~\cite{de2010conceptual} try to find multiple solutions as its main
objective. One of such algorithms, called
\textit{dopt-aiNet}~\cite{de2005artificial}, introduced the Line Distance that
measures the distance between two solutions regarding their basis of
attraction. In this short abstract I propose the use of the Line Distance
measure as the main objective-function in order to locate multiple optima at
once in a population.
</summary>
    <author>
      <name>Fabricio Olivetti de Franca</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to PPSN'14 Workshop Advances in Multimodal Optimization</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.2539v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.2539v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.2614v4</id>
    <updated>2015-02-26T07:06:17Z</updated>
    <published>2014-06-07T20:20:11Z</published>
    <title>Application and Verification of Algorithm Learning Based Neural Network</title>
    <summary>  This paper has been withdrawn by the author due to a crucial accuracy error
in Fig. 5. For precise performance of ALBNN please refer to Yoon et al.'s work
in the following article. Yoon, H., Park, C. S., Kim, J. S., &amp; Baek, J. G.
(2013). Algorithm learning based neural network integrating feature selection
and classification. Expert Systems with Applications, 40(1), 231-241.
http://www.sciencedirect.com/science/article/pii/S0957417412008731
</summary>
    <author>
      <name>Rizwana Kalsoom</name>
    </author>
    <author>
      <name>Moomal Qureshi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by the author due to a crucial accuracy
  error in Fig. 5</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.2614v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.2614v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.4518v1</id>
    <updated>2014-06-15T15:20:17Z</updated>
    <published>2014-06-15T15:20:17Z</published>
    <title>A Heuristic Method to Generate Better Initial Population for
  Evolutionary Methods</title>
    <summary>  Initial population plays an important role in heuristic algorithms such as GA
as it help to decrease the time those algorithms need to achieve an acceptable
result. Furthermore, it may influence the quality of the final answer given by
evolutionary algorithms. In this paper, we shall introduce a heuristic method
to generate a target based initial population which possess two mentioned
characteristics. The efficiency of the proposed method has been shown by
presenting the results of our tests on the benchmarks.
</summary>
    <author>
      <name>Erfan Khaji</name>
    </author>
    <author>
      <name>Amin Satlikh Mohammadi</name>
    </author>
    <link href="http://arxiv.org/abs/1406.4518v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.4518v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.0265v1</id>
    <updated>2014-07-01T14:50:36Z</updated>
    <published>2014-07-01T14:50:36Z</published>
    <title>Supervised learning in Spiking Neural Networks with Limited Precision:
  SNN/LP</title>
    <summary>  A new supervised learning algorithm, SNN/LP, is proposed for Spiking Neural
Networks. This novel algorithm uses limited precision for both synaptic weights
and synaptic delays; 3 bits in each case. Also a genetic algorithm is used for
the supervised training. The results are comparable or better than previously
published work. The results are applicable to the realization of large scale
hardware neural networks. One of the trained networks is implemented in
programmable hardware.
</summary>
    <author>
      <name>Evangelos Stromatias</name>
    </author>
    <author>
      <name>John Marsland</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, originally submitted to IJCNN 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.0265v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.0265v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.0698v1</id>
    <updated>2014-07-02T16:41:07Z</updated>
    <published>2014-07-02T16:41:07Z</published>
    <title>Continuous On-line Evolution of Agent Behaviours with Cartesian Genetic
  Programming</title>
    <summary>  Evolutionary Computation has been successfully used to synthesise controllers
for embodied agents and multi-agent systems in general. Notwithstanding this,
continuous on-line adaptation by the means of evolutionary algorithms is still
under-explored, especially outside the evolutionary robotics domain. In this
paper, we present an on-line evolutionary programming algorithm that searches
in the agent design space for the appropriate behavioural policies to cope with
the underlying environment. We discuss the current problems of continuous agent
adaptation, present our on-line evolution testbed for evolutionary simulation.
</summary>
    <author>
      <name>Davide Nunes</name>
    </author>
    <author>
      <name>Luis Antunes</name>
    </author>
    <link href="http://arxiv.org/abs/1407.0698v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.0698v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.0977v1</id>
    <updated>2014-07-02T15:22:11Z</updated>
    <published>2014-07-02T15:22:11Z</published>
    <title>Higher-Order Quantum-Inspired Genetic Algorithms</title>
    <summary>  This paper presents a theory and an empirical evaluation of Higher-Order
Quantum-Inspired Genetic Algorithms. Fundamental notions of the theory have
been introduced, and a novel Order-2 Quantum-Inspired Genetic Algorithm (QIGA2)
has been presented. Contrary to all QIGA algorithms which represent quantum
genes as independent qubits, in higher-order QIGAs quantum registers are used
to represent genes strings which allows modelling of genes relations using
quantum phenomena. Performance comparison has been conducted on a benchmark of
20 deceptive combinatorial optimization problems. It has been presented that
using higher quantum orders is beneficial for genetic algorithm efficiency, and
the new QIGA2 algorithm outperforms the old QIGA algorithm which was tuned in
highly compute intensive metaoptimization process.
</summary>
    <author>
      <name>Robert Nowotniak</name>
    </author>
    <author>
      <name>Jacek Kucharski</name>
    </author>
    <link href="http://arxiv.org/abs/1407.0977v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.0977v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.3077v1</id>
    <updated>2014-07-11T09:18:20Z</updated>
    <published>2014-07-11T09:18:20Z</published>
    <title>Charge Scheduling of an Energy Storage System under Time-of-use Pricing
  and a Demand Charge</title>
    <summary>  A real-coded genetic algorithm is used to schedule the charging of an energy
storage system (ESS), operated in tandem with renewable power by an electricity
consumer who is subject to time-of-use pricing and a demand charge. Simulations
based on load and generation profiles of typical residential customers show
that an ESS scheduled by our algorithm can reduce electricity costs by
approximately 17%, compared to a system without an ESS, and by 8% compared to a
scheduling algorithm based on net power.
</summary>
    <author>
      <name>Yourim Yoon</name>
    </author>
    <author>
      <name>Yong-Hyuk Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 2 figures, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.3077v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.3077v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.5739v1</id>
    <updated>2014-07-22T05:48:08Z</updated>
    <published>2014-07-22T05:48:08Z</published>
    <title>Global optimization using Lévy flights</title>
    <summary>  This paper studies a class of enhanced diffusion processes in which random
walkers perform L\'evy flights and apply it for global optimization. L\'evy
flights offer controlled balance between exploitation and exploration. We
develop four optimization algorithms based on such properties. We compare new
algorithms with the well-known Simulated Annealing on hard test functions and
the results are very promising.
</summary>
    <author>
      <name>Truyen Tran</name>
    </author>
    <author>
      <name>Trung Thanh Nguyen</name>
    </author>
    <author>
      <name>Hoang Linh Nguyen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 6 figures, 4 algorithms,Proceedings of Second National
  Symposium on Research, Development and Application of Information and
  Communication Technology (ICT.rda'04), Hanoi, Sept 24-25, 2004</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.5739v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.5739v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.7211v1</id>
    <updated>2014-07-27T11:45:21Z</updated>
    <published>2014-07-27T11:45:21Z</published>
    <title>An evolutionary solver for linear integer programming</title>
    <summary>  In this paper we introduce an evolutionary algorithm for the solution of
linear integer programs. The strategy is based on the separation of the
variables into the integer subset and the continuous subset; the integer
variables are fixed by the evolutionary system, and the continuous ones are
determined in function of them, by a linear program solver.
  We report results obtained for some standard benchmark problems, and compare
them with those obtained by branch-and-bound. The performance of the
evolutionary algorithm is promising. Good feasible solutions were generally
obtained, and in some of the difficult benchmark tests it outperformed
branch-and-bound.
</summary>
    <author>
      <name>João Pedro Pedroso</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.7211v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.7211v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="80M50" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.6, I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.4077v1</id>
    <updated>2014-08-18T17:49:28Z</updated>
    <published>2014-08-18T17:49:28Z</published>
    <title>Brain: Biological noise-based logic</title>
    <summary>  Neural spikes in the brain form stochastic sequences, i.e., belong to the
class of pulse noises. This stochasticity is a counterintuitive feature because
extracting information - such as the commonly supposed neural information of
mean spike frequency - requires long times for reasonably low error
probability. The mystery could be solved by noise-based logic, wherein
randomness has an important function and allows large speed enhancements for
special-purpose tasks, and the same mechanism is at work for the brain logic
version of this concept.
</summary>
    <author>
      <name>Laszlo B. Kish</name>
    </author>
    <author>
      <name>Claes-Goran Granqvist</name>
    </author>
    <author>
      <name>Sergey M. Bezrukov</name>
    </author>
    <author>
      <name>Tamas Horvath</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-94-017-9548-7_45</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-94-017-9548-7_45" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">paper in press</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Advances in Cognitive Neurodynamics 2015, pp 319-322</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1408.4077v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.4077v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.5403v1</id>
    <updated>2014-08-22T12:15:54Z</updated>
    <published>2014-08-22T12:15:54Z</published>
    <title>Neural Mechanism of Language</title>
    <summary>  This paper is based on our previous work on neural coding. It is a
self-organized model supported by existing evidences. Firstly, we briefly
introduce this model in this paper, and then we explain the neural mechanism of
language and reasoning with it. Moreover, we find that the position of an area
determines its importance. Specifically, language relevant areas are in the
capital position of the cortical kingdom. Therefore they are closely related
with autonomous consciousness and working memories. In essence, language is a
miniature of the real world. Briefly, this paper would like to bridge the gap
between molecule mechanism of neurons and advanced functions such as language
and reasoning.
</summary>
    <author>
      <name>Peilei Liu</name>
    </author>
    <author>
      <name>Ting Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1408.5403v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.5403v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.6741v1</id>
    <updated>2014-08-28T14:55:16Z</updated>
    <published>2014-08-28T14:55:16Z</published>
    <title>Memcomputing and Swarm Intelligence</title>
    <summary>  We explore the relation between memcomputing, namely computing with and in
memory, and swarm intelligence algorithms. In particular, we show that one can
design memristive networks to solve short-path optimization problems that can
also be solved by ant-colony algorithms. By employing appropriate memristive
elements one can demonstrate an almost one-to-one correspondence between
memcomputing and ant colony optimization approaches. However, the memristive
network has the capability of finding the solution in one deterministic step,
compared to the stochastic multi-step ant colony optimization. This result
paves the way for nanoscale hardware implementations of several swarm
intelligence algorithms that are presently explored, from scheduling problems
to robotics.
</summary>
    <author>
      <name>Y. V. Pershin</name>
    </author>
    <author>
      <name>M. Di Ventra</name>
    </author>
    <link href="http://arxiv.org/abs/1408.6741v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.6741v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.mes-hall" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.0334v1</id>
    <updated>2014-09-01T08:59:27Z</updated>
    <published>2014-09-01T08:59:27Z</published>
    <title>Storing sequences in binary tournament-based neural networks</title>
    <summary>  An extension to a recently introduced architecture of clique-based neural
networks is presented. This extension makes it possible to store sequences with
high efficiency. To obtain this property, network connections are provided with
orientation and with flexible redundancy carried by both spatial and temporal
redundancy, a mechanism of anticipation being introduced in the model. In
addition to the sequence storage with high efficiency, this new scheme also
offers biological plausibility. In order to achieve accurate sequence
retrieval, a double layered structure combining hetero-association and
auto-association is also proposed.
</summary>
    <author>
      <name>Xiaoran Jiang</name>
    </author>
    <author>
      <name>Vincent Gripon</name>
    </author>
    <author>
      <name>Claude Berrou</name>
    </author>
    <author>
      <name>Michael Rabbat</name>
    </author>
    <link href="http://arxiv.org/abs/1409.0334v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.0334v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.1715v1</id>
    <updated>2014-09-05T10:01:41Z</updated>
    <published>2014-09-05T10:01:41Z</published>
    <title>An Experimental Study of Adaptive Control for Evolutionary Algorithms</title>
    <summary>  The balance of exploration versus exploitation (EvE) is a key issue on
evolutionary computation. In this paper we will investigate how an adaptive
controller aimed to perform Operator Selection can be used to dynamically
manage the EvE balance required by the search, showing that the search
strategies determined by this control paradigm lead to an improvement of
solution quality found by the evolutionary algorithm.
</summary>
    <author>
      <name>Giacomo di Tollo</name>
    </author>
    <author>
      <name>Frédéric Lardeux</name>
    </author>
    <author>
      <name>Jorge Maturana</name>
    </author>
    <author>
      <name>Frédéric Saubion</name>
    </author>
    <link href="http://arxiv.org/abs/1409.1715v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.1715v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.2329v5</id>
    <updated>2015-02-19T14:46:00Z</updated>
    <published>2014-09-08T13:08:00Z</published>
    <title>Recurrent Neural Network Regularization</title>
    <summary>  We present a simple regularization technique for Recurrent Neural Networks
(RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful
technique for regularizing neural networks, does not work well with RNNs and
LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show
that it substantially reduces overfitting on a variety of tasks. These tasks
include language modeling, speech recognition, image caption generation, and
machine translation.
</summary>
    <author>
      <name>Wojciech Zaremba</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <link href="http://arxiv.org/abs/1409.2329v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.2329v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.4244v1</id>
    <updated>2014-09-12T12:07:47Z</updated>
    <published>2014-09-12T12:07:47Z</published>
    <title>An OvS-MultiObjective Algorithm Approach for Lane Reversal Problem</title>
    <summary>  The lane reversal has proven to be a useful method to mitigate traffic
congestion during rush hour or in case of specific events that affect high
traffic volumes. In this work we propose a methodology that is placed within
optimization via Simulation, by means of which a multi-objective genetic
algorithm and simulations of traffic are used to determine the configuration of
ideal lane reversal.
</summary>
    <author>
      <name>Enrique Gabriel Baquela</name>
    </author>
    <author>
      <name>Ana Carolina Olivera</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ALIO/EURO 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.4244v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.4244v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.7478v1</id>
    <updated>2014-09-26T06:32:52Z</updated>
    <published>2014-09-26T06:32:52Z</published>
    <title>An Analysis on Selection for High-Resolution Approximations in
  Many-Objective Optimization</title>
    <summary>  This work studies the behavior of three elitist multi- and many-objective
evolutionary algorithms generating a high-resolution approximation of the
Pareto optimal set. Several search-assessment indicators are defined to trace
the dynamics of survival selection and measure the ability to simultaneously
keep optimal solutions and discover new ones under different population sizes,
set as a fraction of the size of the Pareto optimal set.
</summary>
    <author>
      <name>Hernan Aguirre</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lille - Nord Europe, LIFL</arxiv:affiliation>
    </author>
    <author>
      <name>Arnaud Liefooghe</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lille - Nord Europe, LIFL</arxiv:affiliation>
    </author>
    <author>
      <name>Sébastien Verel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LISIC</arxiv:affiliation>
    </author>
    <author>
      <name>Kiyoshi Tanaka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">apperas in Parallel Problem Solving from Nature - PPSN XIII,
  Ljubljana : Slovenia (2014)</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.7478v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.7478v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.8191v1</id>
    <updated>2014-09-29T17:08:21Z</updated>
    <published>2014-09-29T17:08:21Z</published>
    <title>A Neural Networks Committee for the Contextual Bandit Problem</title>
    <summary>  This paper presents a new contextual bandit algorithm, NeuralBandit, which
does not need hypothesis on stationarity of contexts and rewards. Several
neural networks are trained to modelize the value of rewards knowing the
context. Two variants, based on multi-experts approach, are proposed to choose
online the parameters of multi-layer perceptrons. The proposed algorithms are
successfully tested on a large dataset with and without stationarity of
rewards.
</summary>
    <author>
      <name>Robin Allesiardo</name>
    </author>
    <author>
      <name>Raphael Feraud</name>
    </author>
    <author>
      <name>Djallel Bouneffouf</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21st International Conference on Neural Information Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.8191v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.8191v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.5401v2</id>
    <updated>2014-12-10T16:01:39Z</updated>
    <published>2014-10-20T19:28:26Z</published>
    <title>Neural Turing Machines</title>
    <summary>  We extend the capabilities of neural networks by coupling them to external
memory resources, which they can interact with by attentional processes. The
combined system is analogous to a Turing Machine or Von Neumann architecture
but is differentiable end-to-end, allowing it to be efficiently trained with
gradient descent. Preliminary results demonstrate that Neural Turing Machines
can infer simple algorithms such as copying, sorting, and associative recall
from input and output examples.
</summary>
    <author>
      <name>Alex Graves</name>
    </author>
    <author>
      <name>Greg Wayne</name>
    </author>
    <author>
      <name>Ivo Danihelka</name>
    </author>
    <link href="http://arxiv.org/abs/1410.5401v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.5401v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.6413v1</id>
    <updated>2014-10-23T16:54:39Z</updated>
    <published>2014-10-23T16:54:39Z</published>
    <title>Initialization of multilayer forecasting artifical neural networks</title>
    <summary>  In this paper, a new method was developed for initialising artificial neural
networks predicting dynamics of time series. Initial weighting coefficients
were determined for neurons analogously to the case of a linear prediction
filter. Moreover, to improve the accuracy of the initialization method for a
multilayer neural network, some variants of decomposition of the transformation
matrix corresponding to the linear prediction filter were suggested. The
efficiency of the proposed neural network prediction method by forecasting
solutions of the Lorentz chaotic system is shown in this paper.
</summary>
    <author>
      <name>Vladimir V. Bochkarev</name>
    </author>
    <author>
      <name>Yulia S. Maslennikova</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Uchenye Zapiski Kazanskogo Universiteta. Seriya
  Fiziko-Matematicheskie Nauki, 2010, vol. 152, no. 1, pp. 7-14. (In Russian)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1410.6413v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.6413v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62M45, 62M10, 68T05" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.7326v3</id>
    <updated>2015-11-03T21:21:41Z</updated>
    <published>2014-10-27T17:46:28Z</published>
    <title>Neuroevolution in Games: State of the Art and Open Challenges</title>
    <summary>  This paper surveys research on applying neuroevolution (NE) to games. In
neuroevolution, artificial neural networks are trained through evolutionary
algorithms, taking inspiration from the way biological brains evolved. We
analyse the application of NE in games along five different axes, which are the
role NE is chosen to play in a game, the different types of neural networks
used, the way these networks are evolved, how the fitness is determined and
what type of input the network receives. The article also highlights important
open research challenges in the field.
</summary>
    <author>
      <name>Sebastian Risi</name>
    </author>
    <author>
      <name>Julian Togelius</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">- Added more references - Corrected typos - Added an overview table
  (Table 1)</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.7326v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.7326v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.7883v1</id>
    <updated>2014-10-29T05:56:00Z</updated>
    <published>2014-10-29T05:56:00Z</published>
    <title>Sub-threshold CMOS Spiking Neuron Circuit Design for Navigation Inspired
  by C. elegans Chemotaxis</title>
    <summary>  We demonstrate a spiking neural network for navigation motivated by the
chemotaxis network of Caenorhabditis elegans. Our network uses information
regarding temporal gradients in the tracking variable's concentration to make
navigational decisions. The gradient information is determined by mimicking the
underlying mechanisms of the ASE neurons of C. elegans. Simulations show that
our model is able to forage and track a target set-point in extremely noisy
environments. We develop a VLSI implementation for the main gradient detector
neurons, which could be integrated with standard comparator circuitry to
develop a robust circuit for navigation and contour tracking.
</summary>
    <author>
      <name>Shibani Santurkar</name>
    </author>
    <author>
      <name>Bipin Rajendran</name>
    </author>
    <link href="http://arxiv.org/abs/1410.7883v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.7883v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.2897v1</id>
    <updated>2014-11-11T17:42:26Z</updated>
    <published>2014-11-11T17:42:26Z</published>
    <title>Accelerating the ANT Colony Optimization By Smart ANTs, Using Genetic
  Operator</title>
    <summary>  This paper research review Ant colony optimization (ACO) and Genetic
Algorithm (GA), both are two powerful meta-heuristics. This paper explains some
major defects of these two algorithm at first then proposes a new model for ACO
in which, artificial ants use a quick genetic operator and accelerate their
actions in selecting next state. Experimental results show that proposed hybrid
algorithm is effective and its performance including speed and accuracy beats
other version.
</summary>
    <author>
      <name>Hassan Ismkhan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal on Computational Science &amp; Applications,
  Volume: 4 - volume NO: 2 - Issue: April 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.2897v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.2897v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.4297v1</id>
    <updated>2014-11-05T15:58:47Z</updated>
    <published>2014-11-05T15:58:47Z</published>
    <title>Application of Multi-core Parallel Programming to a Combination of Ant
  Colony Optimization and Genetic Algorithm</title>
    <summary>  This Paper will deal with a combination of Ant Colony and Genetic Programming
Algorithm to optimize Travelling Salesmen problem (NP-Hard). However, the
complexity of the algorithm requires considerable computational time and
resources. Parallel implementation can reduce the computational time. In this
paper, emphasis in the parallelizing section is given to Multi-core
architecture and Multi-Processor Systems which is developed and used almost
everywhere today and hence, multi-core parallelization to the combination of
algorithm is achieved by OpenMP library by Intel Corporation.
</summary>
    <author>
      <name>Rishita Kalyani</name>
    </author>
    <link href="http://arxiv.org/abs/1411.4297v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.4297v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.5737v5</id>
    <updated>2015-10-06T00:03:43Z</updated>
    <published>2014-11-21T01:21:17Z</published>
    <title>Fuzzy Adaptive Resonance Theory, Diffusion Maps and their applications
  to Clustering and Biclustering</title>
    <summary>  In this paper, we describe an algorithm FARDiff (Fuzzy Adaptive Resonance
Dif- fusion) which combines Diffusion Maps and Fuzzy Adaptive Resonance Theory
to do clustering on high dimensional data. We describe some applications of
this method and some problems for future research.
</summary>
    <author>
      <name>S. B. Damelin</name>
    </author>
    <author>
      <name>Y. Gu</name>
    </author>
    <author>
      <name>D. C. Wunsch II</name>
    </author>
    <author>
      <name>R. Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in Math.Model.Nat.Phenom</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Math.Model.Nat.Phenom. Vol. 10, No 3, 2015, pp. 206-211</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1411.5737v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.5737v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="94A15, 62H30, 60J20, 68T05, 68T45, 68T10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.6768v1</id>
    <updated>2014-11-25T08:52:14Z</updated>
    <published>2014-11-25T08:52:14Z</published>
    <title>Hypotheses of neural code and the information model of the
  neuron-detector</title>
    <summary>  This paper deals with the problem of neural code solving. On the basis of the
formulated hypotheses the information model of a neuron-detector is suggested,
the detector being one of the basic elements of an artificial neural network
(ANN). The paper subjects the connectionist paradigm of ANN building to
criticism and suggests a new presentation paradigm for ANN building and
neuroelements (NE) learning. The adequacy of the suggested model is proved by
the fact that is does not contradict the modern propositions of neuropsychology
and neurophysiology.
</summary>
    <author>
      <name>Yuri Parzhin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.6768v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.6768v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0; I.2.6; I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.3555v1</id>
    <updated>2014-12-11T06:46:53Z</updated>
    <published>2014-12-11T06:46:53Z</published>
    <title>Empirical Evaluation of Gated Recurrent Neural Networks on Sequence
  Modeling</title>
    <summary>  In this paper we compare different types of recurrent units in recurrent
neural networks (RNNs). Especially, we focus on more sophisticated units that
implement a gating mechanism, such as a long short-term memory (LSTM) unit and
a recently proposed gated recurrent unit (GRU). We evaluate these recurrent
units on the tasks of polyphonic music modeling and speech signal modeling. Our
experiments revealed that these advanced recurrent units are indeed better than
more traditional recurrent units such as tanh units. Also, we found GRU to be
comparable to LSTM.
</summary>
    <author>
      <name>Junyoung Chung</name>
    </author>
    <author>
      <name>Caglar Gulcehre</name>
    </author>
    <author>
      <name>KyungHyun Cho</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented in NIPS 2014 Deep Learning and Representation Learning
  Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.3555v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.3555v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.3714v2</id>
    <updated>2014-12-13T00:57:57Z</updated>
    <published>2014-12-11T16:35:27Z</published>
    <title>Feature Weight Tuning for Recursive Neural Networks</title>
    <summary>  This paper addresses how a recursive neural network model can automatically
leave out useless information and emphasize important evidence, in other words,
to perform "weight tuning" for higher-level representation acquisition. We
propose two models, Weighted Neural Network (WNN) and Binary-Expectation Neural
Network (BENN), which automatically control how much one specific unit
contributes to the higher-level representation. The proposed model can be
viewed as incorporating a more powerful compositional function for embedding
acquisition in recursive neural networks. Experimental results demonstrate the
significant improvement over standard neural models.
</summary>
    <author>
      <name>Jiwei Li</name>
    </author>
    <link href="http://arxiv.org/abs/1412.3714v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.3714v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.6122v1</id>
    <updated>2014-12-02T22:58:54Z</updated>
    <published>2014-12-02T22:58:54Z</published>
    <title>Spread Unary Coding</title>
    <summary>  Unary coding is useful but it is redundant in its standard form. Unary coding
can also be seen as spatial coding where the value of the number is determined
by its place in an array. Motivated by biological finding that several neurons
in the vicinity represent the same number, we propose a variant of unary
numeration in its spatial form, where each number is represented by several 1s.
We call this spread unary coding where the number of 1s used is the spread of
the code. Spread unary coding is associated with saturation of the Hamming
distance between code words.
</summary>
    <author>
      <name>Subhash Kak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.6122v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.6122v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.6144v1</id>
    <updated>2014-12-14T23:12:05Z</updated>
    <published>2014-12-14T23:12:05Z</published>
    <title>The Computational Theory of Intelligence: Applications to Genetic
  Programming and Turing Machines</title>
    <summary>  In this paper, we continue the efforts of the Computational Theory of
Intelligence (CTI) by extending concepts to include computational processes in
terms of Genetic Algorithms (GA's) and Turing Machines (TM's). Active, Passive,
and Hybrid Computational Intelligence processes are also introduced and
discussed. We consider the ramifications of the assumptions of CTI with regard
to the qualities of reproduction and virility. Applications to Biology,
Computer Science and Cyber Security are also discussed.
</summary>
    <author>
      <name>Daniel Kovach</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Total of 5 figures. This paper was originally presented at RAMSA 2013
  in Visakhaptnam, India in December 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.6144v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.6144v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="92DXX, 68TXX, 03Dxx" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0; J.3; F.1.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.7009v3</id>
    <updated>2015-04-09T01:54:33Z</updated>
    <published>2014-12-22T14:57:05Z</published>
    <title>Generative Class-conditional Autoencoders</title>
    <summary>  Recent work by Bengio et al. (2013) proposes a sampling procedure for
denoising autoencoders which involves learning the transition operator of a
Markov chain. The transition operator is typically unimodal, which limits its
capacity to model complex data. In order to perform efficient sampling from
conditional distributions, we extend this work, both theoretically and
algorithmically, to gated autoencoders (Memisevic, 2013), The proposed model is
able to generate convincing class-conditional samples when trained on both the
MNIST and TFD datasets.
</summary>
    <author>
      <name>Jan Rudy</name>
    </author>
    <author>
      <name>Graham Taylor</name>
    </author>
    <link href="http://arxiv.org/abs/1412.7009v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.7009v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.7955v1</id>
    <updated>2014-12-26T16:41:04Z</updated>
    <published>2014-12-26T16:41:04Z</published>
    <title>Unsupervised Learning through Prediction in a Model of Cortex</title>
    <summary>  We propose a primitive called PJOIN, for "predictive join," which combines
and extends the operations JOIN and LINK, which Valiant proposed as the basis
of a computational theory of cortex. We show that PJOIN can be implemented in
Valiant's model. We also show that, using PJOIN, certain reasonably complex
learning and pattern matching tasks can be performed, in a way that involves
phenomena which have been observed in cognition and the brain, namely
memory-based prediction and downward traffic in the cortical hierarchy.
</summary>
    <author>
      <name>Christos H. Papadimitriou</name>
    </author>
    <author>
      <name>Santosh S. Vempala</name>
    </author>
    <link href="http://arxiv.org/abs/1412.7955v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.7955v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.8291v1</id>
    <updated>2014-12-29T09:51:20Z</updated>
    <published>2014-12-29T09:51:20Z</published>
    <title>Improving approximate RPCA with a k-sparsity prior</title>
    <summary>  A process centric view of robust PCA (RPCA) allows its fast approximate
implementation based on a special form o a deep neural network with weights
shared across all layers. However, empirically this fast approximation to RPCA
fails to find representations that are parsemonious. We resolve these bad local
minima by relaxing the elementwise L1 and L2 priors and instead utilize a
structure inducing k-sparsity prior. In a discriminative classification task
the newly learned representations outperform these from the original
approximate RPCA formulation significantly.
</summary>
    <author>
      <name>Maximilian Karl</name>
    </author>
    <author>
      <name>Christian Osendorfer</name>
    </author>
    <link href="http://arxiv.org/abs/1412.8291v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.8291v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.04010v1</id>
    <updated>2015-01-16T15:22:19Z</updated>
    <published>2015-01-16T15:22:19Z</published>
    <title>Coevolutionary intransitivity in games: A landscape analysis</title>
    <summary>  Intransitivity is supposed to be a main reason for deficits in coevolutionary
progress and inheritable superiority. Besides, coevolutionary dynamics is
characterized by interactions yielding subjective fitness, but aiming at
solutions that are superior with respect to an objective measurement. Such an
approximation of objective fitness may be, for instance, generalization
performance. In the paper a link between rating-- and ranking--based measures
of intransitivity and fitness landscapes that can address the dichotomy between
subjective and objective fitness is explored. The approach is illustrated by
numerical experiments involving a simple random game with continuously tunable
degree of randomness.
</summary>
    <author>
      <name>Hendrik Richter</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In: Applications of Evolutionary Computation - EvoApplications
  2015, (Eds.: A. M. Mora, G. Squillero), Lecture Notes in Computer Science,
  Vol. 9028, Springer-Verlag, Berlin, 2015, 869-881</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1501.04010v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.04010v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.06633v3</id>
    <updated>2015-01-30T23:50:49Z</updated>
    <published>2015-01-27T01:19:12Z</published>
    <title>maxDNN: An Efficient Convolution Kernel for Deep Learning with Maxwell
  GPUs</title>
    <summary>  This paper describes maxDNN, a computationally efficient convolution kernel
for deep learning with the NVIDIA Maxwell GPU. maxDNN reaches 96.3%
computational efficiency on typical deep learning network architectures. The
design combines ideas from cuda-convnet2 with the Maxas SGEMM assembly code. We
only address forward propagation (FPROP) operation of the network, but we
believe that the same techniques used here will be effective for backward
propagation (BPROP) as well.
</summary>
    <author>
      <name>Andrew Lavin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1501.06633v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.06633v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.00193v1</id>
    <updated>2015-02-01T04:39:30Z</updated>
    <published>2015-02-01T04:39:30Z</published>
    <title>Evolutionary Artificial Neural Network Based on Chemical Reaction
  Optimization</title>
    <summary>  Evolutionary algorithms (EAs) are very popular tools to design and evolve
artificial neural networks (ANNs), especially to train them. These methods have
advantages over the conventional backpropagation (BP) method because of their
low computational requirement when searching in a large solution space. In this
paper, we employ Chemical Reaction Optimization (CRO), a newly developed global
optimization method, to replace BP in training neural networks. CRO is a
population-based metaheuristics mimicking the transition of molecules and their
interactions in a chemical reaction. Simulation results show that CRO
outperforms many EA strategies commonly used to train neural networks.
</summary>
    <author>
      <name>James J. Q. Yu</name>
    </author>
    <author>
      <name>Albert Y. S. Lam</name>
    </author>
    <author>
      <name>Victor O. K. Li</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CEC.2011.5949872</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CEC.2011.5949872" rel="related"/>
    <link href="http://arxiv.org/abs/1502.00193v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.00193v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.00195v1</id>
    <updated>2015-02-01T04:48:18Z</updated>
    <published>2015-02-01T04:48:18Z</published>
    <title>Sensor Deployment for Air Pollution Monitoring Using Public
  Transportation System</title>
    <summary>  Air pollution monitoring is a very popular research topic and many monitoring
systems have been developed. In this paper, we formulate the Bus Sensor
Deployment Problem (BSDP) to select the bus routes on which sensors are
deployed, and we use Chemical Reaction Optimization (CRO) to solve BSDP. CRO is
a recently proposed metaheuristic designed to solve a wide range of
optimization problems. Using the real world data, namely Hong Kong Island bus
route data, we perform a series of simulations and the results show that CRO is
capable of solving this optimization problem efficiently.
</summary>
    <author>
      <name>James J. Q. Yu</name>
    </author>
    <author>
      <name>Victor O. K. Li</name>
    </author>
    <author>
      <name>Albert Y. S. Lam</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CEC.2012.6256495</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CEC.2012.6256495" rel="related"/>
    <link href="http://arxiv.org/abs/1502.00195v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.00195v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.00199v1</id>
    <updated>2015-02-01T04:56:13Z</updated>
    <published>2015-02-01T04:56:13Z</published>
    <title>Chemical Reaction Optimization for the Set Covering Problem</title>
    <summary>  The set covering problem (SCP) is one of the representative combinatorial
optimization problems, having many practical applications. This paper
investigates the development of an algorithm to solve SCP by employing chemical
reaction optimization (CRO), a general-purpose metaheuristic. It is tested on a
wide range of benchmark instances of SCP. The simulation results indicate that
this algorithm gives outstanding performance compared with other heuristics and
metaheuristics in solving SCP.
</summary>
    <author>
      <name>James J. Q. Yu</name>
    </author>
    <author>
      <name>Albert Y. S. Lam</name>
    </author>
    <author>
      <name>Victor O. K. Li</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CEC.2014.6900233</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CEC.2014.6900233" rel="related"/>
    <link href="http://arxiv.org/abs/1502.00199v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.00199v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.02478v1</id>
    <updated>2015-02-09T13:29:48Z</updated>
    <published>2015-02-09T13:29:48Z</published>
    <title>Efficient batchwise dropout training using submatrices</title>
    <summary>  Dropout is a popular technique for regularizing artificial neural networks.
Dropout networks are generally trained by minibatch gradient descent with a
dropout mask turning off some of the units---a different pattern of dropout is
applied to every sample in the minibatch. We explore a very simple alternative
to the dropout mask. Instead of masking dropped out units by setting them to
zero, we perform matrix multiplication using a submatrix of the weight
matrix---unneeded hidden units are never calculated. Performing dropout
batchwise, so that one pattern of dropout is used for each sample in a
minibatch, we can substantially reduce training times. Batchwise dropout can be
used with fully-connected and convolutional neural networks.
</summary>
    <author>
      <name>Ben Graham</name>
    </author>
    <author>
      <name>Jeremy Reizenstein</name>
    </author>
    <author>
      <name>Leigh Robinson</name>
    </author>
    <link href="http://arxiv.org/abs/1502.02478v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.02478v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.03699v1</id>
    <updated>2015-02-12T15:24:19Z</updated>
    <published>2015-02-12T15:24:19Z</published>
    <title>Analysis of Solution Quality of a Multiobjective Optimization-based
  Evolutionary Algorithm for Knapsack Problem</title>
    <summary>  Multi-objective optimisation is regarded as one of the most promising ways
for dealing with constrained optimisation problems in evolutionary
optimisation. This paper presents a theoretical investigation of a
multi-objective optimisation evolutionary algorithm for solving the 0-1
knapsack problem. Two initialisation methods are considered in the algorithm:
local search initialisation and greedy search initialisation. Then the solution
quality of the algorithm is analysed in terms of the approximation ratio.
</summary>
    <author>
      <name>Jun He</name>
    </author>
    <author>
      <name>Yong Wang</name>
    </author>
    <author>
      <name>Yuren Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/1502.03699v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.03699v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.06004v1</id>
    <updated>2015-03-20T06:48:14Z</updated>
    <published>2015-03-20T06:48:14Z</published>
    <title>Feeder Load Balancing using Neural Network</title>
    <summary>  The distribution system problems, such as planning, loss minimization, and
energy restoration, usually involve the phase balancing or network
reconfiguration procedures. The determination of an optimal phase balance is,
in general, a combinatorial optimization problem. This paper proposes optimal
reconfiguration of the phase balancing using the neural network, to switch on
and off the different switches, allowing the three phases supply by the
transformer to the end-users to be balanced. This paper presents the
application examples of the proposed method using the real and simulated test
data.
</summary>
    <author>
      <name>A. Ukil</name>
    </author>
    <author>
      <name>W. Siti</name>
    </author>
    <author>
      <name>J. Jordaan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/11760023_190</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/11760023_190" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages in final published version</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Lecture Notes in Computer Science, Springer, vol. 3972, pp.
  1311-1316, 2006</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1503.06004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.06004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.06866v4</id>
    <updated>2016-02-20T09:57:40Z</updated>
    <published>2015-03-23T22:30:39Z</published>
    <title>Study of all the periods of a Neuronal Recurrence Equation</title>
    <summary>  We characterize the structure of the periods of a neuronal recurrence
equation. Firstly, we give a characterization of k-chains in 0-1 periodic
sequences. Secondly, we characterize the periods of all cycles of some neuronal
recurrence equation. Thirdly, we explain how these results can be used to
deduce the existence of the generalized period-halving bifurcation.
</summary>
    <author>
      <name>Serge Alain Ebélé</name>
    </author>
    <author>
      <name>Renè Ndoundam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Complex Systems, 24, 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1503.06866v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.06866v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="92B20" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.00941v2</id>
    <updated>2015-04-07T22:39:18Z</updated>
    <published>2015-04-03T21:22:52Z</published>
    <title>A Simple Way to Initialize Recurrent Networks of Rectified Linear Units</title>
    <summary>  Learning long term dependencies in recurrent networks is difficult due to
vanishing and exploding gradients. To overcome this difficulty, researchers
have developed sophisticated optimization techniques and network architectures.
In this paper, we propose a simpler solution that use recurrent neural networks
composed of rectified linear units. Key to our solution is the use of the
identity matrix or its scaled version to initialize the recurrent weight
matrix. We find that our solution is comparable to LSTM on our four benchmarks:
two toy problems involving long-range temporal structures, a large language
modeling problem and a benchmark speech recognition problem.
</summary>
    <author>
      <name>Quoc V. Le</name>
    </author>
    <author>
      <name>Navdeep Jaitly</name>
    </author>
    <author>
      <name>Geoffrey E. Hinton</name>
    </author>
    <link href="http://arxiv.org/abs/1504.00941v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.00941v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.02590v1</id>
    <updated>2015-04-10T08:53:03Z</updated>
    <published>2015-04-10T08:53:03Z</published>
    <title>Study of Some Recent Crossovers Effects on Speed and Accuracy of Genetic
  Algorithm, Using Symmetric Travelling Salesman Problem</title>
    <summary>  The Travelling Salesman Problem (TSP) is one of the most famous optimization
problems. The Genetic Algorithm (GA) is one of metaheuristics that have been
applied to TSP. The Crossover and mutation operators are two important elements
of GA. There are many TSP solver crossover operators. In this paper, we state
implementation of some recent TSP solver crossovers at first and then we use
each of them in GA to solve some Symmetric TSP (STSP) instances and finally
compare their effects on speed and accuracy of presented GA.
</summary>
    <author>
      <name>Hassan Ismkhan</name>
    </author>
    <author>
      <name>Kamran Zamanifar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1209.5339</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.02590v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.02590v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.01980v1</id>
    <updated>2015-05-08T10:20:06Z</updated>
    <published>2015-05-08T10:20:06Z</published>
    <title>Evolving Boolean Networks with RNA Editing</title>
    <summary>  The editing of transcribed RNA by other molecules such that the form of the
final product differs from that specified in the corresponding DNA sequence is
ubiquitous. This paper uses an abstract, tunable Boolean genetic regulatory
network model to explore aspects of RNA editing. In particular, it is shown how
dynamically altering expressed sequences via a guide RNA-inspired mechanism can
be selected for by simulated evolution under various single and multicellular
scenarios.
</summary>
    <author>
      <name>Larry Bull</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1306.4793,
  arXiv:1303.7220</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.01980v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.01980v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.MN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.00768v1</id>
    <updated>2015-06-02T06:38:54Z</updated>
    <published>2015-06-02T06:38:54Z</published>
    <title>Soft Computing Techniques for Change Detection in remotely sensed images
  : A Review</title>
    <summary>  With the advent of remote sensing satellites, a huge repository of remotely
sensed images is available. Change detection in remotely sensed images has been
an active research area as it helps us understand the transitions that are
taking place on the Earths surface. This paper discusses the methods and their
classifications proposed by various researchers for change detection. Since use
of soft computing based techniques are now very popular among research
community, this paper also presents a classification based on learning
techniques used in soft-computing methods for change detection.
</summary>
    <author>
      <name>Madhu Khurana</name>
    </author>
    <author>
      <name>Vikas Saxena</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 1 table, 1 figure</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science Issues, Volume 12, Issue
  2, March 2015, pp 245-253</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1506.00768v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.00768v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.02361v1</id>
    <updated>2015-06-08T06:41:37Z</updated>
    <published>2015-06-08T06:41:37Z</published>
    <title>Microscopic approach of a time elapsed neural model</title>
    <summary>  The spike trains are the main components of the information processing in the
brain. To model spike trains several point processes have been investigated in
the literature. And more macroscopic approaches have also been studied, using
partial differential equation models. The main aim of the present article is to
build a bridge between several point processes models (Poisson, Wold, Hawkes)
that have been proved to statistically fit real spike trains data and
age-structured partial differential equations as introduced by Pakdaman,
Perthame and Salort.
</summary>
    <author>
      <name>Julien Chevallier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">JAD</arxiv:affiliation>
    </author>
    <author>
      <name>Maria J. Caceres</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LJLL, MAMBA</arxiv:affiliation>
    </author>
    <author>
      <name>Marie Doumic</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LJLL, MAMBA</arxiv:affiliation>
    </author>
    <author>
      <name>Patricia Reynaud-Bouret</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">JAD</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1506.02361v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.02361v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.05082v2</id>
    <updated>2015-06-29T14:25:18Z</updated>
    <published>2015-06-10T15:38:17Z</published>
    <title>A review of landmark articles in the field of co-evolutionary computing</title>
    <summary>  Coevolution is a powerful tool in evolutionary computing that mitigates some
of its endemic problems, namely stagnation in local optima and lack of
convergence in high dimensionality problems. Since its inception in 1990, there
are multiple articles that have contributed greatly to the development and
improvement of the coevolutionary techniques. In this report we review some of
those landmark articles dwelving in the techniques they propose and how they
fit to conform robust evolutionary algorithms
</summary>
    <author>
      <name>Noe Casas</name>
    </author>
    <link href="http://arxiv.org/abs/1506.05082v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.05082v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.05849v1</id>
    <updated>2015-06-18T23:11:40Z</updated>
    <published>2015-06-18T23:11:40Z</published>
    <title>An Iterative Convolutional Neural Network Algorithm Improves Electron
  Microscopy Image Segmentation</title>
    <summary>  To build the connectomics map of the brain, we developed a new algorithm that
can automatically refine the Membrane Detection Probability Maps (MDPM)
generated to perform automatic segmentation of electron microscopy (EM) images.
To achieve this, we executed supervised training of a convolutional neural
network to recover the removed center pixel label of patches sampled from a
MDPM. MDPM can be generated from other machine learning based algorithms
recognizing whether a pixel in an image corresponds to the cell membrane. By
iteratively applying this network over MDPM for multiple rounds, we were able
to significantly improve membrane segmentation results.
</summary>
    <author>
      <name>Xundong Wu</name>
    </author>
    <link href="http://arxiv.org/abs/1506.05849v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.05849v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.06848v1</id>
    <updated>2015-06-23T03:24:05Z</updated>
    <published>2015-06-23T03:24:05Z</published>
    <title>A Feature-Based Analysis on the Impact of Set of Constraints for
  e-Constrained Differential Evolution</title>
    <summary>  Different types of evolutionary algorithms have been developed for
constrained continuous optimization. We carry out a feature-based analysis of
evolved constrained continuous optimization instances to understand the
characteristics of constraints that make problems hard for evolutionary
algorithm. In our study, we examine how various sets of constraints can
influence the behaviour of e-Constrained Differential Evolution. Investigating
the evolved instances, we obtain knowledge of what type of constraints and
their features make a problem difficult for the examined algorithm.
</summary>
    <author>
      <name>Shayan Poursoltan</name>
    </author>
    <author>
      <name>FranK Neumann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 Pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.06848v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.06848v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.07980v1</id>
    <updated>2015-06-26T07:44:38Z</updated>
    <published>2015-06-26T07:44:38Z</published>
    <title>A Java Implementation of the SGA, UMDA, ECGA, and HBOA</title>
    <summary>  The Simple Genetic Algorithm, the Univariate Marginal Distribution Algorithm,
the Extended Compact Genetic Algorithm, and the Hierarchical Bayesian
Optimization Algorithm are all well known Evolutionary Algorithms.
  In this report we present a Java implementation of these four algorithms with
detailed instructions on how to use each of them to solve a given set of
optimization problems. Additionally, it is explained how to implement and
integrate new problems within the provided set. The source and binary files of
the Java implementations are available for free download at
https://github.com/JoseCPereira/2015EvolutionaryAlgorithmsJava.
</summary>
    <author>
      <name>José C. Pereira</name>
    </author>
    <author>
      <name>Fernando G. Lobo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.07980v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.07980v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.08004v1</id>
    <updated>2015-06-26T09:13:57Z</updated>
    <published>2015-06-26T09:13:57Z</published>
    <title>ASOC: An Adaptive Parameter-free Stochastic Optimization Techinique for
  Continuous Variables</title>
    <summary>  Stochastic optimization is an important task in many optimization problems
where the tasks are not expressible as convex optimization problems. In the
case of non-convex optimization problems, various different stochastic
algorithms like simulated annealing, evolutionary algorithms, and tabu search
are available. Most of these algorithms require user-defined parameters
specific to the problem in order to find out the optimal solution. Moreover, in
many situations, iterative fine-tunings are required for the user-defined
parameters, and therefore these algorithms cannot adapt if the search space and
the optima changes over time. In this paper we propose an \underline{a}daptive
parameter-free \underline{s}tochastic \underline{o}ptimization technique for
\underline{c}ontinuous random variables called ASOC.
</summary>
    <author>
      <name>Jayanta Basak</name>
    </author>
    <link href="http://arxiv.org/abs/1506.08004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.08004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.00088v1</id>
    <updated>2015-07-01T01:54:43Z</updated>
    <published>2015-07-01T01:54:43Z</published>
    <title>Evaluation of Genotypic Diversity Measurements Exploited in Real-Coded
  Representation</title>
    <summary>  Numerous genotypic diversity measures (GDMs) are available in the literature
to assess the convergence status of an evolutionary algorithm (EA) or describe
its search behavior. In a recent study, the authors of this paper drew
attention to the need for a GDM validation framework. In response, this study
proposes three requirements (monotonicity in individual varieties, twinning,
and monotonicity in distance) that can clearly portray any GDMs. These
diversity requirements are analysed by means of controlled population
arrangements. In this paper four GDMs are evaluated with the proposed
validation framework. The results confirm that properly evaluating population
diversity is a rather difficult task, as none of the analysed GDMs complies
with all the diversity requirements.
</summary>
    <author>
      <name>Guillaume Corriveau</name>
    </author>
    <author>
      <name>Raynald Guilbault</name>
    </author>
    <author>
      <name>Antoine Tahan</name>
    </author>
    <author>
      <name>Robert Sabourin</name>
    </author>
    <link href="http://arxiv.org/abs/1507.00088v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.00088v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.01687v1</id>
    <updated>2015-07-07T06:54:47Z</updated>
    <published>2015-07-07T06:54:47Z</published>
    <title>Developing Postfix-GP Framework for Symbolic Regression Problems</title>
    <summary>  This paper describes Postfix-GP system, postfix notation based Genetic
Programming (GP), for solving symbolic regression problems. It presents an
object-oriented architecture of Postfix-GP framework. It assists the user in
understanding of the implementation details of various components of
Postfix-GP. Postfix-GP provides graphical user interface which allows user to
configure the experiment, to visualize evolved solutions, to analyze GP run,
and to perform out-of-sample predictions. The use of Postfix-GP is demonstrated
by solving the benchmark symbolic regression problem. Finally, features of
Postfix-GP framework are compared with that of other GP systems.
</summary>
    <author>
      <name>Vipul K. Dabhi</name>
    </author>
    <author>
      <name>Sanjay Chaudhary</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ACCT.2015.114</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ACCT.2015.114" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.01687v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.01687v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.02491v1</id>
    <updated>2015-07-09T13:10:38Z</updated>
    <published>2015-07-09T13:10:38Z</published>
    <title>Parameter Sensitivity Analysis of Social Spider Algorithm</title>
    <summary>  Social Spider Algorithm (SSA) is a recently proposed general-purpose
real-parameter metaheuristic designed to solve global numerical optimization
problems. This work systematically benchmarks SSA on a suite of 11 functions
with different control parameters. We conduct parameter sensitivity analysis of
SSA using advanced non-parametric statistical tests to generate statistically
significant conclusion on the best performing parameter settings. The
conclusion can be adopted in future work to reduce the effort in parameter
tuning. In addition, we perform a success rate test to reveal the impact of the
control parameters on the convergence speed of the algorithm.
</summary>
    <author>
      <name>James J. Q. Yu</name>
    </author>
    <author>
      <name>Victor O. K. Li</name>
    </author>
    <link href="http://arxiv.org/abs/1507.02491v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.02491v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.02492v1</id>
    <updated>2015-07-09T13:11:13Z</updated>
    <published>2015-07-09T13:11:13Z</published>
    <title>Adaptive Chemical Reaction Optimization for Global Numerical
  Optimization</title>
    <summary>  A newly proposed chemical-reaction-inspired metaheurisic, Chemical Reaction
Optimization (CRO), has been applied to many optimization problems in both
discrete and continuous domains. To alleviate the effort in tuning parameters,
this paper reduces the number of optimization parameters in canonical CRO and
develops an adaptive scheme to evolve them. Our proposed Adaptive CRO (ACRO)
adapts better to different optimization problems. We perform simulations with
ACRO on a widely-used benchmark of continuous problems. The simulation results
show that ACRO has superior performance over canonical CRO.
</summary>
    <author>
      <name>James J. Q. Yu</name>
    </author>
    <author>
      <name>Albert Y. S. Lam</name>
    </author>
    <author>
      <name>Victor O. K. Li</name>
    </author>
    <link href="http://arxiv.org/abs/1507.02492v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.02492v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.02672v2</id>
    <updated>2015-11-24T09:22:23Z</updated>
    <published>2015-07-09T19:52:19Z</published>
    <title>Semi-Supervised Learning with Ladder Networks</title>
    <summary>  We combine supervised learning with unsupervised learning in deep neural
networks. The proposed model is trained to simultaneously minimize the sum of
supervised and unsupervised cost functions by backpropagation, avoiding the
need for layer-wise pre-training. Our work builds on the Ladder network
proposed by Valpola (2015), which we extend by combining the model with
supervision. We show that the resulting model reaches state-of-the-art
performance in semi-supervised MNIST and CIFAR-10 classification, in addition
to permutation-invariant MNIST classification with all labels.
</summary>
    <author>
      <name>Antti Rasmus</name>
    </author>
    <author>
      <name>Harri Valpola</name>
    </author>
    <author>
      <name>Mikko Honkala</name>
    </author>
    <author>
      <name>Mathias Berglund</name>
    </author>
    <author>
      <name>Tapani Raiko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Revised denoising function, updated results, fixed typos</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.02672v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.02672v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.08007v2</id>
    <updated>2016-08-26T06:59:33Z</updated>
    <published>2015-07-29T02:24:55Z</published>
    <title>On Proportions of Fit Individuals in Population of Evolutionary
  Algorithm with Tournament Selection</title>
    <summary>  In this paper, we consider a fitness-level model of a non-elitist
mutation-only evolutionary algorithm (EA) with tournament selection. The model
provides upper and lower bounds for the expected proportion of the individuals
with fitness above given thresholds. In the case of so-called monotone
mutation, the obtained bounds imply that increasing the tournament size
improves the EA performance. As corollaries, we obtain an exponentially
vanishing tail bound for the Randomized Local Search on unimodal functions and
polynomial upper bounds on the runtime of EAs on 2-SAT problem and on a family
of Set Cover problems proposed by E. Balas.
</summary>
    <author>
      <name>Anton Eremeev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submited to Evolutionary Computation journal</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.08007v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.08007v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.08937v1</id>
    <updated>2015-07-31T16:38:25Z</updated>
    <published>2015-07-31T16:38:25Z</published>
    <title>Efficient and robust calibration of the Heston option pricing model for
  American options using an improved Cuckoo Search Algorithm</title>
    <summary>  In this paper an improved Cuckoo Search Algorithm is developed to allow for
an efficient and robust calibration of the Heston option pricing model for
American options. Calibration of stochastic volatility models like the Heston
is significantly harder than classical option pricing models as more parameters
have to be estimated. The difficult task of calibrating one of these models to
American Put options data is the main objective of this paper. Numerical
results are shown to substantiate the suitability of the chosen method to
tackle this problem.
</summary>
    <author>
      <name>Stefan Haring</name>
    </author>
    <author>
      <name>Ronald Hochreiter</name>
    </author>
    <link href="http://arxiv.org/abs/1507.08937v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.08937v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.02774v1</id>
    <updated>2015-08-11T23:31:49Z</updated>
    <published>2015-08-11T23:31:49Z</published>
    <title>Benchmarking of LSTM Networks</title>
    <summary>  LSTM (Long Short-Term Memory) recurrent neural networks have been highly
successful in a number of application areas. This technical report describes
the use of the MNIST and UW3 databases for benchmarking LSTM networks and
explores the effect of different architectural and hyperparameter choices on
performance. Significant findings include: (1) LSTM performance depends
smoothly on learning rates, (2) batching and momentum has no significant effect
on performance, (3) softmax training outperforms least square training, (4)
peephole units are not useful, (5) the standard non-linearities (tanh and
sigmoid) perform best, (6) bidirectional training combined with CTC performs
better than other methods.
</summary>
    <author>
      <name>Thomas M. Breuel</name>
    </author>
    <link href="http://arxiv.org/abs/1508.02774v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.02774v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.02790v1</id>
    <updated>2015-08-12T01:11:47Z</updated>
    <published>2015-08-12T01:11:47Z</published>
    <title>On the Convergence of SGD Training of Neural Networks</title>
    <summary>  Neural networks are usually trained by some form of stochastic gradient
descent (SGD)). A number of strategies are in common use intended to improve
SGD optimization, such as learning rate schedules, momentum, and batching.
These are motivated by ideas about the occurrence of local minima at different
scales, valleys, and other phenomena in the objective function. Empirical
results presented here suggest that these phenomena are not significant factors
in SGD optimization of MLP-related objective functions, and that the behavior
of stochastic gradient descent in these problems is better described as the
simultaneous convergence at different rates of many, largely non-interacting
subproblems
</summary>
    <author>
      <name>Thomas M. Breuel</name>
    </author>
    <link href="http://arxiv.org/abs/1508.02790v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.02790v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.02792v1</id>
    <updated>2015-08-12T01:23:35Z</updated>
    <published>2015-08-12T01:23:35Z</published>
    <title>Possible Mechanisms for Neural Reconfigurability and their Implications</title>
    <summary>  The paper introduces a biologically and evolutionarily plausible neural
architecture that allows a single group of neurons, or an entire cortical
pathway, to be dynamically reconfigured to perform multiple, potentially very
different computations. The paper shows that reconfigurability can account for
the observed stochastic and distributed coding behavior of neurons and provides
a parsimonious explanation for timing phenomena in psychophysical experiments.
It also shows that reconfigurable pathways correspond to classes of statistical
classifiers that include decision lists, decision trees, and hierarchical
Bayesian methods. Implications for the interpretation of neurophysiological and
psychophysical results are discussed, and future experiments for testing the
reconfigurability hypothesis are explored.
</summary>
    <author>
      <name>Thomas M. Breuel</name>
    </author>
    <link href="http://arxiv.org/abs/1508.02792v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.02792v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.05752v1</id>
    <updated>2015-08-24T10:47:47Z</updated>
    <published>2015-08-24T10:47:47Z</published>
    <title>An evolutionary approach to the identification of Cellular Automata
  based on partial observations</title>
    <summary>  In this paper we consider the identification problem of Cellular Automata
(CAs). The problem is defined and solved in the context of partial observations
with time gaps of unknown length, i.e. pre-recorded, partial configurations of
the system at certain, unknown time steps. A solution method based on a
modified variant of a Genetic Algorithm (GA) is proposed and illustrated with
brief experimental results.
</summary>
    <author>
      <name>Witold Bołt</name>
    </author>
    <author>
      <name>Jan M. Baetens</name>
    </author>
    <author>
      <name>Bernard De Baets</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE CEC 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.05752v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.05752v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.07741v1</id>
    <updated>2015-08-31T09:42:33Z</updated>
    <published>2015-08-31T09:42:33Z</published>
    <title>Model Guided Sampling Optimization for Low-dimensional Problems</title>
    <summary>  Optimization of very expensive black-box functions requires utilization of
maximum information gathered by the process of optimization. Model Guided
Sampling Optimization (MGSO) forms a more robust alternative to Jones'
Gaussian-process-based EGO algorithm. Instead of EGO's maximizing expected
improvement, the MGSO uses sampling the probability of improvement which is
shown to be helpful against trapping in local minima. Further, the MGSO can
reach close-to-optimum solutions faster than standard optimization algorithms
on low dimensional or smooth problems.
</summary>
    <author>
      <name>Lukas Bajer</name>
    </author>
    <author>
      <name>Martin Holena</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Bajer, L. &amp; Holena, M. Model Guided Sampling Optimization for
  Low-dimensional Problems. in ICAART 2015 Proceedings of the International
  Conference on Agents and Artificial Intelligence, Volume 2 451-456
  (SCITEPRESS, Lisbon, 2015)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1508.07741v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.07741v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.01126v1</id>
    <updated>2015-09-03T15:28:55Z</updated>
    <published>2015-09-03T15:28:55Z</published>
    <title>Training of CC4 Neural Network with Spread Unary Coding</title>
    <summary>  This paper adapts the corner classification algorithm (CC4) to train the
neural networks using spread unary inputs. This is an important problem as
spread unary appears to be at the basis of data representation in biological
learning. The modified CC4 algorithm is tested using the pattern classification
experiment and the results are found to be good. Specifically, we show that the
number of misclassified points is not particularly sensitive to the chosen
radius of generalization.
</summary>
    <author>
      <name>Pushpa Sree Potluri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.01126v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.01126v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.02512v1</id>
    <updated>2015-09-08T19:59:19Z</updated>
    <published>2015-09-08T19:59:19Z</published>
    <title>DeepCough: A Deep Convolutional Neural Network in A Wearable Cough
  Detection System</title>
    <summary>  In this paper, we present a system that employs a wearable acoustic sensor
and a deep convolutional neural network for detecting coughs. We evaluate the
performance of our system on 14 healthy volunteers and compare it to that of
other cough detection systems that have been reported in the literature.
Experimental results show that our system achieves a classification sensitivity
of 95.1% and a specificity of 99.5%.
</summary>
    <author>
      <name>Justice Amoh</name>
    </author>
    <author>
      <name>Kofi Odame</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">BioCAS-2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.02512v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.02512v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.05982v2</id>
    <updated>2015-09-22T20:51:05Z</updated>
    <published>2015-09-20T09:03:48Z</published>
    <title>Denoising without access to clean data using a partitioned autoencoder</title>
    <summary>  Training a denoising autoencoder neural network requires access to truly
clean data, a requirement which is often impractical. To remedy this, we
introduce a method to train an autoencoder using only noisy data, having
examples with and without the signal class of interest. The autoencoder learns
a partitioned representation of signal and noise, learning to reconstruct each
separately. We illustrate the method by denoising birdsong audio (available
abundantly in uncontrolled noisy datasets) using a convolutional autoencoder.
</summary>
    <author>
      <name>Dan Stowell</name>
    </author>
    <author>
      <name>Richard E. Turner</name>
    </author>
    <link href="http://arxiv.org/abs/1509.05982v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.05982v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.08255v2</id>
    <updated>2015-10-08T16:13:44Z</updated>
    <published>2015-09-28T09:54:08Z</published>
    <title>Encoding Reality: Prediction-Assisted Cortical Learning Algorithm in
  Hierarchical Temporal Memory</title>
    <summary>  In the decade since Jeff Hawkins proposed Hierarchical Temporal Memory (HTM)
as a model of neocortical computation, the theory and the algorithms have
evolved dramatically. This paper presents a detailed description of HTM's
Cortical Learning Algorithm (CLA), including for the first time a rigorous
mathematical formulation of all aspects of the computations. Prediction
Assisted CLA (paCLA), a refinement of the CLA is presented, which is both
closer to the neuroscience and adds significantly to the computational power.
Finally, we summarise the key functions of neocortex which are expressed in
paCLA implementations.
</summary>
    <author>
      <name>Fergal Byrne</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Updated reference to unofficial revision of Hawkins and Ahmad, 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.08255v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.08255v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.00419v1</id>
    <updated>2015-10-01T20:34:37Z</updated>
    <published>2015-10-01T20:34:37Z</published>
    <title>An Asynchronous Implementation of the Limited Memory CMA-ES</title>
    <summary>  We present our asynchronous implementation of the LM-CMA-ES algorithm, which
is a modern evolution strategy for solving complex large-scale continuous
optimization problems. Our implementation brings the best results when the
number of cores is relatively high and the computational complexity of the
fitness function is also high. The experiments with benchmark functions show
that it is able to overcome its origin on the Sphere function, reaches certain
thresholds faster on the Rosenbrock and Ellipsoid function, and surprisingly
performs much better than the original version on the Rastrigin function.
</summary>
    <author>
      <name>Viktor Arkhipov</name>
    </author>
    <author>
      <name>Maxim Buzdalov</name>
    </author>
    <author>
      <name>Anatoly Shalyto</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICMLA.2015.97</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICMLA.2015.97" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 4 figures, 4 tables; this is a full version of a paper which
  has been accepted as a poster to IEEE ICMLA conference 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.00419v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.00419v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="90C56" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.6; I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.02693v1</id>
    <updated>2015-10-09T15:04:11Z</updated>
    <published>2015-10-09T15:04:11Z</published>
    <title>Feedforward Sequential Memory Neural Networks without Recurrent Feedback</title>
    <summary>  We introduce a new structure for memory neural networks, called feedforward
sequential memory networks (FSMN), which can learn long-term dependency without
using recurrent feedback. The proposed FSMN is a standard feedforward neural
networks equipped with learnable sequential memory blocks in the hidden layers.
In this work, we have applied FSMN to several language modeling (LM) tasks.
Experimental results have shown that the memory blocks in FSMN can learn
effective representations of long history. Experiments have shown that FSMN
based language models can significantly outperform not only feedforward neural
network (FNN) based LMs but also the popular recurrent neural network (RNN)
LMs.
</summary>
    <author>
      <name>ShiLiang Zhang</name>
    </author>
    <author>
      <name>Hui Jiang</name>
    </author>
    <author>
      <name>Si Wei</name>
    </author>
    <author>
      <name>LiRong Dai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 1figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.02693v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.02693v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.05711v2</id>
    <updated>2015-10-28T08:42:54Z</updated>
    <published>2015-10-19T22:38:09Z</published>
    <title>Qualitative Projection Using Deep Neural Networks</title>
    <summary>  Deep neural networks (DNN) abstract by demodulating the output of linear
filters. In this article, we refine this definition of abstraction to show that
the inputs of a DNN are abstracted with respect to the filters. Or, to restate,
the abstraction is qualified by the filters. This leads us to introduce the
notion of qualitative projection. We use qualitative projection to abstract
MNIST hand-written digits with respect to the various dogs, horses, planes and
cars of the CIFAR dataset. We then classify the MNIST digits according to the
magnitude of their dogness, horseness, planeness and carness qualities,
illustrating the generality of qualitative projection.
</summary>
    <author>
      <name>Andrew J. R. Simpson</name>
    </author>
    <link href="http://arxiv.org/abs/1510.05711v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.05711v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.06248v1</id>
    <updated>2015-11-19T16:47:01Z</updated>
    <published>2015-11-19T16:47:01Z</published>
    <title>Critical Parameters in Particle Swarm Optimisation</title>
    <summary>  Particle swarm optimisation is a metaheuristic algorithm which finds
reasonable solutions in a wide range of applied problems if suitable parameters
are used. We study the properties of the algorithm in the framework of random
dynamical systems which, due to the quasi-linear swarm dynamics, yields
analytical results for the stability properties of the particles. Such
considerations predict a relationship between the parameters of the algorithm
that marks the edge between convergent and divergent behaviours. Comparison
with simulations indicates that the algorithm performs best near this margin of
instability.
</summary>
    <author>
      <name>J. Michael Herrmann</name>
    </author>
    <author>
      <name>Adam Erskine</name>
    </author>
    <author>
      <name>Thomas Joyce</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.06248v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.06248v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.07889v2</id>
    <updated>2015-12-17T16:30:06Z</updated>
    <published>2015-11-24T21:18:33Z</published>
    <title>rnn : Recurrent Library for Torch</title>
    <summary>  The rnn package provides components for implementing a wide range of
Recurrent Neural Networks. It is built withing the framework of the Torch
distribution for use with the nn package. The components have evolved from 3
iterations, each adding to the flexibility and capability of the package. All
component modules inherit either the AbstractRecurrent or AbstractSequencer
classes. Strong unit testing, continued backwards compatibility and access to
supporting material are the principles followed during its development. The
package is compared against existing implementations of two published papers.
</summary>
    <author>
      <name>Nicholas Léonard</name>
    </author>
    <author>
      <name>Sagar Waghmare</name>
    </author>
    <author>
      <name>Yang Wang</name>
    </author>
    <author>
      <name>Jin-Hwa Kim</name>
    </author>
    <link href="http://arxiv.org/abs/1511.07889v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.07889v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.01332v1</id>
    <updated>2015-12-04T07:51:48Z</updated>
    <published>2015-12-04T07:51:48Z</published>
    <title>Q-Networks for Binary Vector Actions</title>
    <summary>  In this paper reinforcement learning with binary vector actions was
investigated. We suggest an effective architecture of the neural networks for
approximating an action-value function with binary vector actions. The proposed
architecture approximates the action-value function by a linear function with
respect to the action vector, but is still non-linear with respect to the state
input. We show that this approximation method enables the efficient calculation
of greedy action selection and softmax action selection. Using this
architecture, we suggest an online algorithm based on Q-learning. The empirical
results in the grid world and the blocker task suggest that our approximation
architecture would be effective for the RL problems with large discrete action
sets.
</summary>
    <author>
      <name>Naoto Yoshida</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 5 figures, accepted for Deep Reinforcement Learning
  Workshop, NIPS 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.01332v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.01332v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.01596v3</id>
    <updated>2016-04-22T03:20:41Z</updated>
    <published>2015-12-04T23:58:47Z</published>
    <title>Creation of a Deep Convolutional Auto-Encoder in Caffe</title>
    <summary>  The development of a deep (stacked) convolutional auto-encoder in the Caffe
deep learning framework is presented in this paper. We describe simple
principles which we used to create this model in Caffe. The proposed model of
convolutional auto-encoder does not have pooling/unpooling layers yet. The
results of our experimental research show comparable accuracy of dimensionality
reduction in comparison with a classic auto-encoder on the example of MNIST
dataset.
</summary>
    <author>
      <name>Volodymyr Turchenko</name>
    </author>
    <author>
      <name>Artur Luczak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 7 figures, 5 tables, 34 references in the list; Added
  references, corrected Table 3, changed several paragraphs in the text</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.01596v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.01596v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.1; I.2.6; I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.02047v2</id>
    <updated>2015-12-09T03:22:10Z</updated>
    <published>2015-12-07T14:03:43Z</published>
    <title>Level-Based Analysis of Genetic Algorithms for Combinatorial
  Optimization</title>
    <summary>  The paper is devoted to upper bounds on run-time of Non-Elitist Genetic
Algorithms until some target subset of solutions is visited for the first time.
In particular, we consider the sets of optimal solutions and the sets of local
optima as the target subsets. Previously known upper bounds are improved by
means of drift analysis. Finally, we propose conditions ensuring that a
Non-Elitist Genetic Algorithm efficiently finds approximate solutions with
constant approximation ratio on the class of combinatorial optimization
problems with guaranteed local optima (GLO).
</summary>
    <author>
      <name>Duc-Cuong Dang</name>
    </author>
    <author>
      <name>Anton V. Eremeev</name>
    </author>
    <author>
      <name>Per Kristian Lehre</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.02047v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.02047v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.02100v1</id>
    <updated>2015-12-07T15:53:48Z</updated>
    <published>2015-12-07T15:53:48Z</published>
    <title>Digital Genesis: Computers, Evolution and Artificial Life</title>
    <summary>  The application of evolution in the digital realm, with the goal of creating
artificial intelligence and artificial life, has a history as long as that of
the digital computer itself. We illustrate the intertwined history of these
ideas, starting with the early theoretical work of John von Neumann and the
pioneering experimental work of Nils Aall Barricelli. We argue that
evolutionary thinking and artificial life will continue to play an integral
role in the future development of the digital world.
</summary>
    <author>
      <name>Tim Taylor</name>
    </author>
    <author>
      <name>Alan Dorin</name>
    </author>
    <author>
      <name>Kevin Korb</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended abstract of talk presented at the 7th Munich-Sydney-Tilburg
  Philosophy of Science Conference: Evolutionary Thinking, University of
  Sydney, 20-22 March 2014. Presentation slides from talk available at
  http://www.tim-taylor.com/papers/digital-genesis-presentation.pdf</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.02100v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.02100v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.05509v1</id>
    <updated>2015-12-17T09:45:51Z</updated>
    <published>2015-12-17T09:45:51Z</published>
    <title>An Empirical Comparison of Neural Architectures for Reinforcement
  Learning in Partially Observable Environments</title>
    <summary>  This paper explores the performance of fitted neural Q iteration for
reinforcement learning in several partially observable environments, using
three recurrent neural network architectures: Long Short-Term Memory, Gated
Recurrent Unit and MUT1, a recurrent neural architecture evolved from a pool of
several thousands candidate architectures. A variant of fitted Q iteration,
based on Advantage values instead of Q values, is also explored. The results
show that GRU performs significantly better than LSTM and MUT1 for most of the
problems considered, requiring less training episodes and less CPU time before
learning a very good policy. Advantage learning also tends to produce better
results.
</summary>
    <author>
      <name>Denis Steckelmacher</name>
    </author>
    <author>
      <name>Peter Vrancx</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the 27th Benelux Conference on Artificial Intelligence</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.05509v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.05509v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.03809v1</id>
    <updated>2015-11-03T07:21:44Z</updated>
    <published>2015-11-03T07:21:44Z</published>
    <title>Artificial neural network approach for condition-based maintenance</title>
    <summary>  In this research, computerized maintenance management will be investigated.
The rise of maintenance cost forced the research community to look for more
effective ways to schedule maintenance operations. Using computerized models to
come up with optimal maintenance policy has led to better equipment utilization
and lower costs. This research adopts Condition-Based Maintenance model where
the maintenance decision is generated based on equipment conditions. Artificial
Neural Network technique is proposed to capture and analyze equipment condition
signals which lead to higher level of knowledge gathering. This knowledge is
used to accurately estimate equipment failure time. Based on these estimations,
an optimal maintenance management policy can be achieved.
</summary>
    <author>
      <name>Mostafa Sayyed</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">108 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.03809v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.03809v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.06580v1</id>
    <updated>2016-01-25T12:49:28Z</updated>
    <published>2016-01-25T12:49:28Z</published>
    <title>Is swarm intelligence able to create mazes?</title>
    <summary>  In this paper, the idea of applying Computational Intelligence in the process
of creation board games, in particular mazes, is presented. For two different
algorithms the proposed idea has been examined. The results of the experiments
are shown and discussed to present advantages and disadvantages.
</summary>
    <author>
      <name>Dawid Polap</name>
    </author>
    <author>
      <name>Marcin Wozniak</name>
    </author>
    <author>
      <name>Christian Napoli</name>
    </author>
    <author>
      <name>Emiliano Tramontana</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1515/eletel-2015-0039</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1515/eletel-2015-0039" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Electronics and Telecommunications, Vol.
  6, n. 4, pp. 305-310 (2015)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1601.06580v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.06580v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T05, 68T10, 68T45, 68U10, 68W25, 68W99" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.2.10; I.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.07446v1</id>
    <updated>2016-01-27T16:54:51Z</updated>
    <published>2016-01-27T16:54:51Z</published>
    <title>A First Attempt to Cloud-Based User Verification in Distributed System</title>
    <summary>  In this paper, the idea of client verification in distributed systems is
presented. The proposed solution presents a sample system where client
verification through cloud resources using input signature is discussed. For
different signatures the proposed method has been examined. Research results
are presented and discussed to show potential advantages.
</summary>
    <author>
      <name>Marcin Wozniak</name>
    </author>
    <author>
      <name>Dawid Polap</name>
    </author>
    <author>
      <name>Grzegorz Borowik</name>
    </author>
    <author>
      <name>Christian Napoli</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/APCASE.2015.47</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/APCASE.2015.47" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Final version published on: Asia-Pacific Conference on Computer Aided
  System Engineering (APCASE), pp. 226-231 (2015)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Asia-Pacific Conference on Computer Aided System Engineering
  (APCASE), pp. 226-231 (2015)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1601.07446v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.07446v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T05, 68T10, 68T45, 68U10, 68W25, 68W99" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6, I.2.10, I.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.01321v1</id>
    <updated>2016-02-03T14:46:35Z</updated>
    <published>2016-02-03T14:46:35Z</published>
    <title>A continuum among logarithmic, linear, and exponential functions, and
  its potential to improve generalization in neural networks</title>
    <summary>  We present the soft exponential activation function for artificial neural
networks that continuously interpolates between logarithmic, linear, and
exponential functions. This activation function is simple, differentiable, and
parameterized so that it can be trained as the rest of the network is trained.
We hypothesize that soft exponential has the potential to improve neural
network learning, as it can exactly calculate many natural operations that
typical neural networks can only approximate, including addition,
multiplication, inner product, distance, polynomials, and sinusoids.
</summary>
    <author>
      <name>Luke B. Godfrey</name>
    </author>
    <author>
      <name>Michael S. Gashler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 8 figures, conference, In Proceedings of Knowledge Discovery
  and Information Retrieval (KDIR) 2015, Lisbon, Portugal, December 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.01321v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.01321v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.03032v2</id>
    <updated>2016-05-19T14:18:19Z</updated>
    <published>2016-02-09T15:26:26Z</published>
    <title>Associative Long Short-Term Memory</title>
    <summary>  We investigate a new method to augment recurrent neural networks with extra
memory without increasing the number of network parameters. The system has an
associative memory based on complex-valued vectors and is closely related to
Holographic Reduced Representations and Long Short-Term Memory networks.
Holographic Reduced Representations have limited capacity: as they store more
information, each retrieval becomes noisier due to interference. Our system in
contrast creates redundant copies of stored information, which enables
retrieval with reduced noise. Experiments demonstrate faster learning on
multiple memorization tasks.
</summary>
    <author>
      <name>Ivo Danihelka</name>
    </author>
    <author>
      <name>Greg Wayne</name>
    </author>
    <author>
      <name>Benigno Uria</name>
    </author>
    <author>
      <name>Nal Kalchbrenner</name>
    </author>
    <author>
      <name>Alex Graves</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML-2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.03032v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.03032v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.04335v1</id>
    <updated>2016-02-13T14:09:41Z</updated>
    <published>2016-02-13T14:09:41Z</published>
    <title>Learning Over Long Time Lags</title>
    <summary>  The advantage of recurrent neural networks (RNNs) in learning dependencies
between time-series data has distinguished RNNs from other deep learning
models. Recently, many advances are proposed in this emerging field. However,
there is a lack of comprehensive review on memory models in RNNs in the
literature. This paper provides a fundamental review on RNNs and long short
term memory (LSTM) model. Then, provides a surveys of recent advances in
different memory enhancements and learning techniques for capturing long term
dependencies in RNNs.
</summary>
    <author>
      <name>Hojjat Salehinejad</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is a draft article, in preparation to submit for peer-review</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.04335v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.04335v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.04723v1</id>
    <updated>2016-02-15T16:16:56Z</updated>
    <published>2016-02-15T16:16:56Z</published>
    <title>Efficient Representation of Low-Dimensional Manifolds using Deep
  Networks</title>
    <summary>  We consider the ability of deep neural networks to represent data that lies
near a low-dimensional manifold in a high-dimensional space. We show that deep
networks can efficiently extract the intrinsic, low-dimensional coordinates of
such data. We first show that the first two layers of a deep network can
exactly embed points lying on a monotonic chain, a special type of piecewise
linear manifold, mapping them to a low-dimensional Euclidean space. Remarkably,
the network can do this using an almost optimal number of parameters. We also
show that this network projects nearby points onto the manifold and then embeds
them with little error. We then extend these results to more general manifolds.
</summary>
    <author>
      <name>Ronen Basri</name>
    </author>
    <author>
      <name>David Jacobs</name>
    </author>
    <link href="http://arxiv.org/abs/1602.04723v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.04723v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.06057v1</id>
    <updated>2016-02-19T07:09:33Z</updated>
    <published>2016-02-19T07:09:33Z</published>
    <title>Uniresolution representations of white-matter data from CoCoMac</title>
    <summary>  Tracing data as collated by CoCoMac, a seminal neuroinformatics database, is
at multiple resolutions -- white matter tracts were studied for areas and their
subdivisions by different reports. Network theoretic analysis of this
multi-resolution data often assumes that the data at various resolutions is
equivalent, which may not be correct. In this paper we propose three methods to
resolve the multi-resolution issue such that the resultant networks have
connectivity data at only one resolution. The different resultant networks are
compared in terms of their network analysis metrics and degree distributions.
</summary>
    <author>
      <name>Raghavendra Singh</name>
    </author>
    <link href="http://arxiv.org/abs/1602.06057v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.06057v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.07884v1</id>
    <updated>2016-02-25T11:04:09Z</updated>
    <published>2016-02-25T11:04:09Z</published>
    <title>Firefly Algorithm for optimization problems with non-continuous
  variables: A Review and Analysis</title>
    <summary>  Firefly algorithm is a swarm based metaheuristic algorithm inspired by the
flashing behavior of fireflies. It is an effective and an easy to implement
algorithm. It has been tested on different problems from different disciplines
and found to be effective. Even though the algorithm is proposed for
optimization problems with continuous variables, it has been modified and used
for problems with non-continuous variables, including binary and integer valued
problems. In this paper a detailed review of this modifications of firefly
algorithm for problems with non-continuous variables will be discussed. The
strength and weakness of the modifications along with possible future works
will be presented.
</summary>
    <author>
      <name>Surafel Luleseged Tilahun</name>
    </author>
    <author>
      <name>Jean Medard T Ngnotchouye</name>
    </author>
    <link href="http://arxiv.org/abs/1602.07884v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.07884v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.00930v2</id>
    <updated>2016-03-08T21:26:58Z</updated>
    <published>2016-03-02T23:44:03Z</published>
    <title>Super Mario as a String: Platformer Level Generation Via LSTMs</title>
    <summary>  The procedural generation of video game levels has existed for at least 30
years, but only recently have machine learning approaches been used to generate
levels without specifying the rules for generation. A number of these have
looked at platformer levels as a sequence of characters and performed
generation using Markov chains. In this paper we examine the use of Long
Short-Term Memory recurrent neural networks (LSTMs) for the purpose of
generating levels trained from a corpus of Super Mario Brothers levels. We
analyze a number of different data representations and how the generated levels
fit into the space of human authored Super Mario Brothers levels.
</summary>
    <author>
      <name>Adam Summerville</name>
    </author>
    <author>
      <name>Michael Mateas</name>
    </author>
    <link href="http://arxiv.org/abs/1603.00930v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.00930v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.08551v1</id>
    <updated>2016-03-28T20:28:09Z</updated>
    <published>2016-03-28T20:28:09Z</published>
    <title>Genetic cellular neural networks for generating three-dimensional
  geometry</title>
    <summary>  There are a number of ways to procedurally generate interesting
three-dimensional shapes, and a method where a cellular neural network is
combined with a mesh growth algorithm is presented here. The aim is to create a
shape from a genetic code in such a way that a crude search can find
interesting shapes. Identical neural networks are placed at each vertex of a
mesh which can communicate with neural networks on neighboring vertices. The
output of the neural networks determine how the mesh grows, allowing
interesting shapes to be produced emergently, mimicking some of the complexity
of biological organism development. Since the neural networks' parameters can
be freely mutated, the approach is amenable for use in a genetic algorithm.
</summary>
    <author>
      <name>Hugo Martay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.08551v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.08551v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="92B20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.00462v1</id>
    <updated>2016-04-02T05:04:12Z</updated>
    <published>2016-04-02T05:04:12Z</published>
    <title>Centralized and Decentralized Global Outer-synchronization of Asymmetric
  Recurrent Time-varying Neural Network by Data-sampling</title>
    <summary>  In this paper, we discuss the outer-synchronization of the asymmetrically
connected recurrent time-varying neural networks. By both centralized and
decentralized discretization data sampling principles, we derive several
sufficient conditions based on diverse vector norms that guarantee that any two
trajectories from different initial values of the identical neural network
system converge together. The lower bounds of the common time intervals between
data samples in centralized and decentralized principles are proved to be
positive, which guarantees exclusion of Zeno behavior. A numerical example is
provided to illustrate the efficiency of the theoretical results.
</summary>
    <author>
      <name>Wenlian Lu</name>
    </author>
    <author>
      <name>Ren Zheng</name>
    </author>
    <author>
      <name>Tianping Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.00462v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.00462v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.01088v2</id>
    <updated>2016-07-29T00:14:24Z</updated>
    <published>2016-04-04T23:19:00Z</published>
    <title>Optimal Parameter Settings for the $(1+(λ, λ))$ Genetic
  Algorithm</title>
    <summary>  The $(1+(\lambda,\lambda))$ genetic algorithm is one of the few algorithms
for which a super-constant speed-up through the use of crossover could be
proven. So far, this algorithm has been used with parameters based also on
intuitive considerations. In this work, we rigorously regard the whole
parameter space and show that the asymptotic time complexity proven by Doerr
and Doerr (GECCO 2015) for the intuitive choice is best possible among all
settings for population size, mutation probability, and crossover bias.
</summary>
    <author>
      <name>Benjamin Doerr</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of a paper that appeared at GECCO'16</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.01088v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.01088v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.02313v5</id>
    <updated>2017-01-31T21:24:36Z</updated>
    <published>2016-04-08T11:39:31Z</published>
    <title>Norm-preserving Orthogonal Permutation Linear Unit Activation Functions
  (OPLU)</title>
    <summary>  We propose a novel activation function that implements piece-wise orthogonal
non-linear mappings based on permutations. It is straightforward to implement,
and very computationally efficient, also it has little memory requirements. We
tested it on two toy problems for feedforward and recurrent networks, it shows
similar performance to tanh and ReLU. OPLU activation function ensures norm
preservance of the backpropagated gradients, therefore it is potentially good
for the training of deep, extra deep, and recurrent neural networks.
</summary>
    <author>
      <name>Artem Chernodub</name>
    </author>
    <author>
      <name>Dimitri Nowicki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to conference ICANN'2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.02313v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.02313v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.05008v1</id>
    <updated>2016-04-18T06:29:01Z</updated>
    <published>2016-04-18T06:29:01Z</published>
    <title>Forecasting Volatility in Indian Stock Market using Artificial Neural
  Network with Multiple Inputs and Outputs</title>
    <summary>  Volatility in stock markets has been extensively studied in the applied
finance literature. In this paper, Artificial Neural Network models based on
various back propagation algorithms have been constructed to predict volatility
in the Indian stock market through volatility of NIFTY returns and volatility
of gold returns. This model considers India VIX, CBOE VIX, volatility of crude
oil returns (CRUDESDR), volatility of DJIA returns (DJIASDR), volatility of DAX
returns (DAXSDR), volatility of Hang Seng returns (HANGSDR) and volatility of
Nikkei returns (NIKKEISDR) as predictor variables. Three sets of experiments
have been performed over three time periods to judge the effectiveness of the
approach.
</summary>
    <author>
      <name>Tamal Datta Chaudhuri</name>
    </author>
    <author>
      <name>Indranil Ghosh</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5120/21245-4034</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5120/21245-4034" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Applications, 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1604.05008v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.05008v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.06187v1</id>
    <updated>2016-04-21T05:47:05Z</updated>
    <published>2016-04-21T05:47:05Z</published>
    <title>Evolutionary Image Transition Based on Theoretical Insights of Random
  Processes</title>
    <summary>  Evolutionary algorithms have been widely studied from a theoretical
perspective. In particular, the area of runtime analysis has contributed
significantly to a theoretical understanding and provided insights into the
working behaviour of these algorithms. We study how these insights into
evolutionary processes can be used for evolutionary art. We introduce the
notion of evolutionary image transition which transfers a given starting image
into a target image through an evolutionary process. Combining standard
mutation effects known from the optimization of the classical benchmark
function OneMax and different variants of random walks, we present ways of
performing evolutionary image transition with different artistic effects.
</summary>
    <author>
      <name>Aneta Neumann</name>
    </author>
    <author>
      <name>Bradley Alexander</name>
    </author>
    <author>
      <name>Frank Neumann</name>
    </author>
    <link href="http://arxiv.org/abs/1604.06187v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.06187v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07108v1</id>
    <updated>2016-04-25T02:18:45Z</updated>
    <published>2016-04-25T02:18:45Z</published>
    <title>Modeling the Evolution of Gene-Culture Divergence</title>
    <summary>  We present a model for evolving agents using both genetic and cultural
inheritance mechanisms. Within each agent our model maintains two distinct
information stores we call the genome and the memome. Processes of adaptation
are modeled as evolutionary processes at each level of adaptation
(phylogenetic, ontogenetic, sociogenetic). We review relevant competing models
and we show how our model improves on previous attempts to model genetic and
cultural evolutionary processes. In particular we argue our model can achieve
divergent gene-culture co-evolution.
</summary>
    <author>
      <name>Chris Marriott</name>
    </author>
    <author>
      <name>Jobran Chebib</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, ALIFE 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.07108v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07108v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07269v1</id>
    <updated>2016-04-25T14:17:08Z</updated>
    <published>2016-04-25T14:17:08Z</published>
    <title>CMA-ES for Hyperparameter Optimization of Deep Neural Networks</title>
    <summary>  Hyperparameters of deep neural networks are often optimized by grid search,
random search or Bayesian optimization. As an alternative, we propose to use
the Covariance Matrix Adaptation Evolution Strategy (CMA-ES), which is known
for its state-of-the-art performance in derivative-free optimization. CMA-ES
has some useful invariance properties and is friendly to parallel evaluations
of solutions. We provide a toy example comparing CMA-ES and state-of-the-art
Bayesian optimization algorithms for tuning the hyperparameters of a
convolutional neural network for the MNIST dataset on 30 GPUs in parallel.
</summary>
    <author>
      <name>Ilya Loshchilov</name>
    </author>
    <author>
      <name>Frank Hutter</name>
    </author>
    <link href="http://arxiv.org/abs/1604.07269v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07269v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07796v1</id>
    <updated>2016-04-26T19:04:59Z</updated>
    <published>2016-04-26T19:04:59Z</published>
    <title>Scale Normalization</title>
    <summary>  One of the difficulties of training deep neural networks is caused by
improper scaling between layers. Scaling issues introduce exploding / gradient
problems, and have typically been addressed by careful scale-preserving
initialization. We investigate the value of preserving scale, or isometry,
beyond the initial weights. We propose two methods of maintaing isometry, one
exact and one stochastic. Preliminary experiments show that for both
determinant and scale-normalization effectively speeds up learning. Results
suggest that isometry is important in the beginning of learning, and
maintaining it leads to faster learning.
</summary>
    <author>
      <name>Henry Z. Lo</name>
    </author>
    <author>
      <name>Kevin Amaral</name>
    </author>
    <author>
      <name>Wei Ding</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preliminary version submitted to ICLR workshop 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.07796v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07796v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.01514v1</id>
    <updated>2016-05-05T07:30:02Z</updated>
    <published>2016-05-05T07:30:02Z</published>
    <title>Fitness-based Adaptive Control of Parameters in Genetic Programming:
  Adaptive Value Setting of Mutation Rate and Flood Mechanisms</title>
    <summary>  This paper concerns applications of genetic algorithms and genetic
programming to tasks for which it is difficult to find a representation that
does not map to a highly complex and discontinuous fitness landscape. In such
cases the standard algorithm is prone to getting trapped in local extremes. The
paper proposes several adaptive mechanisms that are useful in preventing the
search from getting trapped.
</summary>
    <author>
      <name>Michal Gregor</name>
    </author>
    <author>
      <name>Juraj Spalek</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in 2011 IEEE International Conference on Intelligent Computing and
  Intelligent Systems (ICIS 2011)</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.01514v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.01514v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.01746v1</id>
    <updated>2016-05-05T20:15:47Z</updated>
    <published>2016-05-05T20:15:47Z</published>
    <title>Biobjective Performance Assessment with the COCO Platform</title>
    <summary>  This document details the rationales behind assessing the performance of
numerical black-box optimizers on multi-objective problems within the COCO
platform and in particular on the biobjective test suite bbob-biobj. The
evaluation is based on a hypervolume of all non-dominated solutions in the
archive of candidate solutions and measures the runtime until the hypervolume
value succeeds prescribed target values.
</summary>
    <author>
      <name>Dimo Brockhoff</name>
    </author>
    <author>
      <name>Tea Tušar</name>
    </author>
    <author>
      <name>Dejan Tušar</name>
    </author>
    <author>
      <name>Tobias Wagner</name>
    </author>
    <author>
      <name>Nikolaus Hansen</name>
    </author>
    <author>
      <name>Anne Auger</name>
    </author>
    <link href="http://arxiv.org/abs/1605.01746v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.01746v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.01988v3</id>
    <updated>2017-03-30T18:24:55Z</updated>
    <published>2016-05-06T16:11:45Z</published>
    <title>LSTM with Working Memory</title>
    <summary>  Previous RNN architectures have largely been superseded by LSTM, or "Long
Short-Term Memory". Since its introduction, there have been many variations on
this simple design. However, it is still widely used and we are not aware of a
gated-RNN architecture that outperforms LSTM in a broad sense while still being
as simple and efficient. In this paper we propose a modified LSTM-like
architecture. Our architecture is still simple and achieves better performance
on the tasks that we tested on. We also introduce a new RNN performance
benchmark that uses the handwritten digits and stresses several important
network capabilities.
</summary>
    <author>
      <name>Andrew Pulver</name>
    </author>
    <author>
      <name>Siwei Lyu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at IJCNN 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.01988v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.01988v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.02486v1</id>
    <updated>2016-05-09T09:12:11Z</updated>
    <published>2016-05-09T09:12:11Z</published>
    <title>Efficiency Evaluation of Character-level RNN Training Schedules</title>
    <summary>  We present four training and prediction schedules from the same
character-level recurrent neural network. The efficiency of these schedules is
tested in terms of model effectiveness as a function of training time and
amount of training data seen. We show that the choice of training and
prediction schedule potentially has a considerable impact on the prediction
effectiveness for a given training budget.
</summary>
    <author>
      <name>Cedric De Boom</name>
    </author>
    <author>
      <name>Sam Leroux</name>
    </author>
    <author>
      <name>Steven Bohez</name>
    </author>
    <author>
      <name>Pieter Simoens</name>
    </author>
    <author>
      <name>Thomas Demeester</name>
    </author>
    <author>
      <name>Bart Dhoedt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.02486v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.02486v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.02720v1</id>
    <updated>2016-05-09T19:58:29Z</updated>
    <published>2016-05-09T19:58:29Z</published>
    <title>Anytime Bi-Objective Optimization with a Hybrid Multi-Objective CMA-ES
  (HMO-CMA-ES)</title>
    <summary>  We propose a multi-objective optimization algorithm aimed at achieving good
anytime performance over a wide range of problems. Performance is assessed in
terms of the hypervolume metric. The algorithm called HMO-CMA-ES represents a
hybrid of several old and new variants of CMA-ES, complemented by BOBYQA as a
warm start. We benchmark HMO-CMA-ES on the recently introduced bi-objective
problem suite of the COCO framework (COmparing Continuous Optimizers),
consisting of 55 scalable continuous optimization problems, which is used by
the Black-Box Optimization Benchmarking (BBOB) Workshop 2016.
</summary>
    <author>
      <name>Ilya Loshchilov</name>
    </author>
    <author>
      <name>Tobias Glasmachers</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">BBOB workshop of GECCO'2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.02720v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.02720v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.03560v1</id>
    <updated>2016-05-11T19:49:43Z</updated>
    <published>2016-05-11T19:49:43Z</published>
    <title>COCO: Performance Assessment</title>
    <summary>  We present an any-time performance assessment for benchmarking numerical
optimization algorithms in a black-box scenario, applied within the COCO
benchmarking platform. The performance assessment is based on runtimes measured
in number of objective function evaluations to reach one or several quality
indicator target values. We argue that runtime is the only available measure
with a generic, meaningful, and quantitative interpretation. We discuss the
choice of the target values, runlength-based targets, and the aggregation of
results by using simulated restarts, averages, and empirical distribution
functions.
</summary>
    <author>
      <name>Nikolaus Hansen</name>
    </author>
    <author>
      <name>Anne Auger</name>
    </author>
    <author>
      <name>Dimo Brockhoff</name>
    </author>
    <author>
      <name>Dejan Tušar</name>
    </author>
    <author>
      <name>Tea Tušar</name>
    </author>
    <link href="http://arxiv.org/abs/1605.03560v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.03560v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.03764v1</id>
    <updated>2016-05-12T11:39:14Z</updated>
    <published>2016-05-12T11:39:14Z</published>
    <title>Direct Method for Training Feed-forward Neural Networks using Batch
  Extended Kalman Filter for Multi-Step-Ahead Predictions</title>
    <summary>  This paper is dedicated to the long-term, or multi-step-ahead, time series
prediction problem. We propose a novel method for training feed-forward neural
networks, such as multilayer perceptrons, with tapped delay lines. Special
batch calculation of derivatives called Forecasted Propagation Through Time and
batch modification of the Extended Kalman Filter are introduced. Experiments
were carried out on well-known time series benchmarks, the Mackey-Glass chaotic
process and the Santa Fe Laser Data Series. Recurrent and feed-forward neural
networks were evaluated.
</summary>
    <author>
      <name>Artem Chernodub</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of ICANN'2013-LCNS Series, Volume 8131.
  Springer-Verlag New York, Inc., 2013, P. 138-145</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1605.03764v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.03764v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.05296v1</id>
    <updated>2016-05-17T19:29:37Z</updated>
    <published>2016-05-17T19:29:37Z</published>
    <title>Dataflow matrix machines as programmable, dynamically expandable,
  self-referential generalized recurrent neural networks</title>
    <summary>  Dataflow matrix machines are a powerful generalization of recurrent neural
networks. They work with multiple types of linear streams and multiple types of
neurons, including higher-order neurons which dynamically update the matrix
describing weights and topology of the network in question while the network is
running. It seems that the power of dataflow matrix machines is sufficient for
them to be a convenient general purpose programming platform. This paper
explores a number of useful programming idioms and constructions arising in
this context.
</summary>
    <author>
      <name>Michael Bukatin</name>
    </author>
    <author>
      <name>Steve Matthews</name>
    </author>
    <author>
      <name>Andrey Radul</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.05296v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.05296v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.05448v1</id>
    <updated>2016-05-18T05:53:44Z</updated>
    <published>2016-05-18T05:53:44Z</published>
    <title>The Bees Algorithm for the Vehicle Routing Problem</title>
    <summary>  In this thesis we present a new algorithm for the Vehicle Routing Problem
called the Enhanced Bees Algorithm. It is adapted from a fairly recent
algorithm, the Bees Algorithm, which was developed for continuous optimisation
problems. We show that the results obtained by the Enhanced Bees Algorithm are
competitive with the best meta-heuristics available for the Vehicle Routing
Problem (within 0.5% of the optimal solution for common benchmark problems). We
show that the algorithm has good runtime performance, producing results within
2% of the optimal solution within 60 seconds, making it suitable for use within
real world dispatch scenarios.
</summary>
    <author>
      <name>Aish Fenton</name>
    </author>
    <link href="http://arxiv.org/abs/1605.05448v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.05448v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.06710v1</id>
    <updated>2016-05-21T23:45:38Z</updated>
    <published>2016-05-21T23:45:38Z</published>
    <title>Chess Player by Co-Evolutionary Algorithm</title>
    <summary>  A co-evolutionary algorithm (CA) based chess player is presented.
Implementation details of the algorithms, namely coding, population, variation
operators are described. The alpha-beta or mini-max like behaviour of the
player is achieved through two competitive or cooperative populations. Special
attention is given to the fitness function evaluation (the heart of the
solution). Test results on algorithms vs. algorithms or human player is
provided.
</summary>
    <author>
      <name>Nuno Ramos</name>
    </author>
    <author>
      <name>Sergio Salgado</name>
    </author>
    <author>
      <name>Agostinho C Rosa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 11 figures and 12 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.06710v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.06710v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.00540v1</id>
    <updated>2016-06-02T05:39:54Z</updated>
    <published>2016-06-02T05:39:54Z</published>
    <title>Multi-pretrained Deep Neural Network</title>
    <summary>  Pretraining is widely used in deep neutral network and one of the most famous
pretraining models is Deep Belief Network (DBN). The optimization formulas are
different during the pretraining process for different pretraining models. In
this paper, we pretrained deep neutral network by different pretraining models
and hence investigated the difference between DBN and Stacked Denoising
Autoencoder (SDA) when used as pretraining model. The experimental results show
that DBN get a better initial model. However the model converges to a
relatively worse model after the finetuning process. Yet after pretrained by
SDA for the second time the model converges to a better model if finetuned.
</summary>
    <author>
      <name>Zhen Hu</name>
    </author>
    <author>
      <name>Zhuyin Xue</name>
    </author>
    <author>
      <name>Tong Cui</name>
    </author>
    <author>
      <name>Shiqiang Zong</name>
    </author>
    <author>
      <name>Chenglong He</name>
    </author>
    <link href="http://arxiv.org/abs/1606.00540v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.00540v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02407v1</id>
    <updated>2016-06-08T05:31:43Z</updated>
    <published>2016-06-08T05:31:43Z</published>
    <title>Structured Convolution Matrices for Energy-efficient Deep learning</title>
    <summary>  We derive a relationship between network representation in energy-efficient
neuromorphic architectures and block Toplitz convolutional matrices. Inspired
by this connection, we develop deep convolutional networks using a family of
structured convolutional matrices and achieve state-of-the-art trade-off
between energy efficiency and classification accuracy for well-known image
recognition tasks. We also put forward a novel method to train binary
convolutional networks by utilising an existing connection between
noisy-rectified linear units and binary activations.
</summary>
    <author>
      <name>Rathinakumar Appuswamy</name>
    </author>
    <author>
      <name>Tapan Nayak</name>
    </author>
    <author>
      <name>John Arthur</name>
    </author>
    <author>
      <name>Steven Esser</name>
    </author>
    <author>
      <name>Paul Merolla</name>
    </author>
    <author>
      <name>Jeffrey Mckinstry</name>
    </author>
    <author>
      <name>Timothy Melano</name>
    </author>
    <author>
      <name>Myron Flickner</name>
    </author>
    <author>
      <name>Dharmendra Modha</name>
    </author>
    <link href="http://arxiv.org/abs/1606.02407v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02407v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03674v2</id>
    <updated>2017-03-06T13:34:29Z</updated>
    <published>2016-06-12T07:22:58Z</published>
    <title>Critical Echo State Networks that Anticipate Input using Morphable
  Transfer Functions</title>
    <summary>  The paper investigates a new type of truly critical echo state networks where
individual transfer functions for every neuron can be modified to anticipate
the expected next input. Deviations from expected input are only forgotten
slowly in power law fashion. The paper outlines the theory, numerically
analyzes a one neuron model network and finally discusses technical and also
biological implications of this type of approach.
</summary>
    <author>
      <name>Norbert Michael Mayer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14th International Symposium on Neural Networks (ISNN), Sapporo,
  Hakodate, Japan, June 21st - 26th 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.03674v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03674v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.04217v1</id>
    <updated>2016-06-14T07:04:37Z</updated>
    <published>2016-06-14T07:04:37Z</published>
    <title>Word Representation Models for Morphologically Rich Languages in Neural
  Machine Translation</title>
    <summary>  Dealing with the complex word forms in morphologically rich languages is an
open problem in language processing, and is particularly important in
translation. In contrast to most modern neural systems of translation, which
discard the identity for rare words, in this paper we propose several
architectures for learning word representations from character and morpheme
level word decompositions. We incorporate these representations in a novel
machine translation model which jointly learns word alignments and translations
via a hard attention mechanism. Evaluating on translating from several
morphologically rich languages into English, we show consistent improvements
over strong baseline methods, of between 1 and 1.5 BLEU points.
</summary>
    <author>
      <name>Ekaterina Vylomova</name>
    </author>
    <author>
      <name>Trevor Cohn</name>
    </author>
    <author>
      <name>Xuanli He</name>
    </author>
    <author>
      <name>Gholamreza Haffari</name>
    </author>
    <link href="http://arxiv.org/abs/1606.04217v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04217v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.05784v2</id>
    <updated>2016-06-30T17:29:55Z</updated>
    <published>2016-06-18T17:36:28Z</published>
    <title>Hitting times of local and global optima in genetic algorithms with very
  high selection pressure</title>
    <summary>  The paper is devoted to upper bounds on the expected first hitting times of
the sets of local or global optima for non-elitist genetic algorithms with very
high selection pressure. The results of this paper extend the range of
situations where the upper bounds on the expected runtime are known for genetic
algorithms and apply, in particular, to the Canonical Genetic Algorithm. The
obtained bounds do not require the probability of fitness-decreasing mutation
to be bounded by a constant less than one.
</summary>
    <author>
      <name>Anton Eremeev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Yugoslav Journal of Operations Research. arXiv admin
  note: text overlap with arXiv:1512.02047</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.05784v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.05784v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.05990v1</id>
    <updated>2016-06-20T07:05:14Z</updated>
    <published>2016-06-20T07:05:14Z</published>
    <title>A New Training Method for Feedforward Neural Networks Based on Geometric
  Contraction Property of Activation Functions</title>
    <summary>  We propose a new training method for a feedforward neural network having the
activation functions with the geometric contraction property. The method
consists of constructing a new functional that is less nonlinear in comparison
with the classical functional by removing the nonlinearity of the activation
functions from the output layer. We validate this new method by a series of
experiments that show an improved learning speed and also a better
classification error.
</summary>
    <author>
      <name>Petre Birtea</name>
    </author>
    <author>
      <name>Cosmin Cernazanu-Glavan</name>
    </author>
    <author>
      <name>Alexandru Sisu</name>
    </author>
    <link href="http://arxiv.org/abs/1606.05990v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.05990v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="92B20, 68T05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.02028v1</id>
    <updated>2016-07-06T12:23:47Z</updated>
    <published>2016-07-06T12:23:47Z</published>
    <title>Artificial neural networks and fuzzy logic for recognizing alphabet
  characters and mathematical symbols</title>
    <summary>  Optical Character Recognition software (OCR) are important tools for
obtaining accessible texts. We propose the use of artificial neural networks
(ANN) in order to develop pattern recognition algorithms capable of recognizing
both normal texts and formulae. We present an original improvement of the
backpropagation algorithm. Moreover, we describe a novel image segmentation
algorithm that exploits fuzzy logic for separating touching characters.
</summary>
    <author>
      <name>Giuseppe Airò Farulla</name>
    </author>
    <author>
      <name>Tiziana Armano</name>
    </author>
    <author>
      <name>Anna Capietto</name>
    </author>
    <author>
      <name>Nadir Murru</name>
    </author>
    <author>
      <name>Rosaria Rossini</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-41264-1_1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-41264-1_1" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Lecture Notes in Computer Science, Volume 9759 2016, Computers
  Helping People with Special Needs, p. 7-14</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1607.02028v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.02028v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.04267v1</id>
    <updated>2016-07-11T16:22:47Z</updated>
    <published>2016-07-11T16:22:47Z</published>
    <title>Enhanced Boolean Correlation Matrix Memory</title>
    <summary>  This paper introduces an Enhanced Boolean version of the Correlation Matrix
Memory (CMM), which is useful to work with binary memories. A novel Boolean
Orthonormalization Process (BOP) is presented to convert a non-orthonormal
Boolean basis, i.e., a set of non-orthonormal binary vectors (in a Boolean
sense) to an orthonormal Boolean basis, i.e., a set of orthonormal binary
vectors (in a Boolean sense). This work shows that it is possible to improve
the performance of Boolean CMM thanks BOP algorithm. Besides, the BOP algorithm
has a lot of additional fields of applications, e.g.: Steganography, Hopfield
Networks, Bi-level image processing, etc. Finally, it is important to mention
that the BOP is an extremely stable and fast algorithm.
</summary>
    <author>
      <name>Mario Mastriani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.04267v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.04267v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.05108v1</id>
    <updated>2016-07-18T14:44:26Z</updated>
    <published>2016-07-18T14:44:26Z</published>
    <title>Neural Machine Translation with Recurrent Attention Modeling</title>
    <summary>  Knowing which words have been attended to in previous time steps while
generating a translation is a rich source of information for predicting what
words will be attended to in the future. We improve upon the attention model of
Bahdanau et al. (2014) by explicitly modeling the relationship between previous
and subsequent attention levels for each word using one recurrent network per
input word. This architecture easily captures informative features, such as
fertility and regularities in relative distortion. In experiments, we show our
parameterization of attention improves translation quality.
</summary>
    <author>
      <name>Zichao Yang</name>
    </author>
    <author>
      <name>Zhiting Hu</name>
    </author>
    <author>
      <name>Yuntian Deng</name>
    </author>
    <author>
      <name>Chris Dyer</name>
    </author>
    <author>
      <name>Alex Smola</name>
    </author>
    <link href="http://arxiv.org/abs/1607.05108v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.05108v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.07078v1</id>
    <updated>2016-07-24T18:44:54Z</updated>
    <published>2016-07-24T18:44:54Z</published>
    <title>Effective Connectivity-Based Neural Decoding: A Causal
  Interaction-Driven Approach</title>
    <summary>  We propose a geometric model-free causality measurebased on multivariate
delay embedding that can efficiently detect linear and nonlinear causal
interactions between time series with no prior information. We then exploit the
proposed causal interaction measure in real MEG data analysis. The results are
used to construct effective connectivity maps of brain activity to decode
different categories of visual stimuli. Moreover, we discovered that the
MEG-based effective connectivity maps as a response to structured images
exhibit more geometric patterns, as disclosed by analyzing the evolution of
toplogical structures of the underlying networks using persistent homology.
Extensive simulation and experimental result have been carried out to
substantiate the capabilities of the proposed approach.
</summary>
    <author>
      <name>Saba Emrani</name>
    </author>
    <author>
      <name>Hamid Krim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 13 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.07078v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.07078v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.01783v1</id>
    <updated>2016-08-05T07:15:38Z</updated>
    <published>2016-08-05T07:15:38Z</published>
    <title>The Evolutionary Process of Image Transition in Conjunction with Box and
  Strip Mutation</title>
    <summary>  Evolutionary algorithms have been used in many ways to generate digital art.
We study how evolutionary processes are used for evolutionary art and present a
new approach to the transition of images. Our main idea is to define
evolutionary processes for digital image transition, combining different
variants of mutation and evolutionary mechanisms. We introduce box and strip
mutation operators which are specifically designed for image transition. Our
experimental results show that the process of an evolutionary algorithm in
combination with these mutation operators can be used as a valuable way to
produce unique generative art.
</summary>
    <author>
      <name>Aneta Neumann</name>
    </author>
    <author>
      <name>Bradley Alexander</name>
    </author>
    <author>
      <name>Frank Neumann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Conference version appears at ICONIP 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.01783v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.01783v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.05578v3</id>
    <updated>2017-03-15T15:48:37Z</updated>
    <published>2016-08-19T12:17:59Z</published>
    <title>Haploid-Diploid Evolutionary Algorithms</title>
    <summary>  This paper uses the recent idea that the fundamental haploid-diploid
lifecycle of eukaryotic organisms implements a rudimentary form of learning
within evolution. A general approach for evolutionary computation is here
derived that differs from all previous known work using diploid
representations. The primary role of recombination is also changed from that
previously considered in both natural and artificial evolution under the new
view. Using well-known abstract tuneable models it is shown that varying
fitness landscape ruggedness varies the benefit of the new approach.
</summary>
    <author>
      <name>Larry Bull</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1607.00318</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.05578v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.05578v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.08265v1</id>
    <updated>2016-08-29T22:02:39Z</updated>
    <published>2016-08-29T22:02:39Z</published>
    <title>About Learning in Recurrent Bistable Gradient Networks</title>
    <summary>  Recurrent Bistable Gradient Networks are attractor based neural networks
characterized by bistable dynamics of each single neuron. Coupled together
using linear interaction determined by the interconnection weights, these
networks do not suffer from spurious states or very limited capacity anymore.
Vladimir Chinarov and Michael Menzinger, who invented these networks, trained
them using Hebb's learning rule. We show, that this way of computing the
weights leads to unwanted behaviour and limitations of the networks
capabilities. Furthermore we evince, that using the first order of Hintons
Contrastive Divergence algorithm leads to a quite promising recurrent neural
network. These findings are tested by learning images of the MNIST database for
handwritten numbers.
</summary>
    <author>
      <name>J. Fischer</name>
    </author>
    <author>
      <name>S. Lackner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.08265v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.08265v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.04468v3</id>
    <updated>2016-12-06T14:39:05Z</updated>
    <published>2016-09-14T22:42:23Z</published>
    <title>Sampling Generative Networks</title>
    <summary>  We introduce several techniques for sampling and visualizing the latent
spaces of generative models. Replacing linear interpolation with spherical
linear interpolation prevents diverging from a model's prior distribution and
produces sharper samples. J-Diagrams and MINE grids are introduced as
visualizations of manifolds created by analogies and nearest neighbors. We
demonstrate two new techniques for deriving attribute vectors: bias-corrected
vectors with data replication and synthetic vectors with data augmentation.
Binary classification using attribute vectors is presented as a technique
supporting quantitative analysis of the latent space. Most techniques are
intended to be independent of model type and examples are shown on both
Variational Autoencoders and Generative Adversarial Networks.
</summary>
    <author>
      <name>Tom White</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.04468v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.04468v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.07724v1</id>
    <updated>2016-09-25T10:18:19Z</updated>
    <published>2016-09-25T10:18:19Z</published>
    <title>The RNN-ELM Classifier</title>
    <summary>  In this paper we examine learning methods combining the Random Neural
Network, a biologically inspired neural network and the Extreme Learning
Machine that achieve state of the art classification performance while
requiring much shorter training time. The Random Neural Network is a integrate
and fire computational model of a neural network whose mathematical structure
permits the efficient analysis of large ensembles of neurons. An activation
function is derived from the RNN and used in an Extreme Learning Machine. We
compare the performance of this combination against the ELM with various
activation functions, we reduce the input dimensionality via PCA and compare
its performance vs. autoencoder based versions of the RNN-ELM.
</summary>
    <author>
      <name>Athanasios Vlontzos</name>
    </author>
    <link href="http://arxiv.org/abs/1609.07724v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.07724v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.08663v1</id>
    <updated>2016-09-27T20:53:16Z</updated>
    <published>2016-09-27T20:53:16Z</published>
    <title>Learning Genomic Representations to Predict Clinical Outcomes in Cancer</title>
    <summary>  Genomics are rapidly transforming medical practice and basic biomedical
research, providing insights into disease mechanisms and improving therapeutic
strategies, particularly in cancer. The ability to predict the future course of
a patient's disease from high-dimensional genomic profiling will be essential
in realizing the promise of genomic medicine, but presents significant
challenges for state-of-the-art survival analysis methods. In this abstract we
present an investigation in learning genomic representations with neural
networks to predict patient survival in cancer. We demonstrate the advantages
of this approach over existing survival analysis methods using brain tumor
data.
</summary>
    <author>
      <name>Safoora Yousefi</name>
    </author>
    <author>
      <name>Congzheng Song</name>
    </author>
    <author>
      <name>Nelson Nauata</name>
    </author>
    <author>
      <name>Lee Cooper</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICLR 2016 Workshop Track- May 2nd 2016 International Conference on
  Learning Representations</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.08663v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.08663v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.01439v1</id>
    <updated>2016-10-05T14:26:27Z</updated>
    <published>2016-10-05T14:26:27Z</published>
    <title>Nonlinear Systems Identification Using Deep Dynamic Neural Networks</title>
    <summary>  Neural networks are known to be effective function approximators. Recently,
deep neural networks have proven to be very effective in pattern recognition,
classification tasks and human-level control to model highly nonlinear
realworld systems. This paper investigates the effectiveness of deep neural
networks in the modeling of dynamical systems with complex behavior. Three deep
neural network structures are trained on sequential data, and we investigate
the effectiveness of these networks in modeling associated characteristics of
the underlying dynamical systems. We carry out similar evaluations on select
publicly available system identification datasets. We demonstrate that deep
neural networks are effective model estimators from input-output data
</summary>
    <author>
      <name>Olalekan Ogunmolu</name>
    </author>
    <author>
      <name>Xuejun Gu</name>
    </author>
    <author>
      <name>Steve Jiang</name>
    </author>
    <author>
      <name>Nicholas Gans</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">American Control Conference, 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.01439v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.01439v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.02732v1</id>
    <updated>2016-10-09T22:16:32Z</updated>
    <published>2016-10-09T22:16:32Z</published>
    <title>Investigating the effects Diversity Mechanisms have on Evolutionary
  Algorithms in Dynamic Environments</title>
    <summary>  Evolutionary algorithms have been successfully applied to a variety of
optimisation problems in stationary environments. However, many real world
optimisation problems are set in dynamic environments where the success
criteria shifts regularly. Population diversity affects algorithmic
performance, particularly on multiobjective and dynamic problems. Diversity
mechanisms are methods of altering evolutionary algorithms in a way that
promotes the maintenance of population diversity. This project intends to
measure and compare the performance effect a variety of diversity mechanisms
have on an evolutionary algorithm when facing an assortment of dynamic
problems.
</summary>
    <author>
      <name>Matthew Hughes</name>
    </author>
    <link href="http://arxiv.org/abs/1610.02732v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.02732v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.00577v1</id>
    <updated>2016-01-06T16:32:47Z</updated>
    <published>2016-01-06T16:32:47Z</published>
    <title>The new hybrid COAW method for solving multi-objective problems</title>
    <summary>  In this article using Cuckoo Optimization Algorithm and simple additive
weighting method the hybrid COAW algorithm is presented to solve
multi-objective problems. Cuckoo algorithm is an efficient and structured
method for solving nonlinear continuous problems. The created Pareto frontiers
of the COAW proposed algorithm are exact and have good dispersion. This method
has a high speed in finding the Pareto frontiers and identifies the beginning
and end points of Pareto frontiers properly. In order to validation the
proposed algorithm, several experimental problems were analyzed. The results of
which indicate the proper effectiveness of COAW algorithm for solving
multi-objective problems.
</summary>
    <author>
      <name>Zeinab Borhanifar</name>
    </author>
    <author>
      <name>Elham Shadkam</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijfcst.2015.5602</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijfcst.2015.5602" rel="related"/>
    <link href="http://arxiv.org/abs/1611.00577v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.00577v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.06245v1</id>
    <updated>2016-11-18T21:09:16Z</updated>
    <published>2016-11-18T21:09:16Z</published>
    <title>Spikes as regularizers</title>
    <summary>  We present a confidence-based single-layer feed-forward learning algorithm
SPIRAL (Spike Regularized Adaptive Learning) relying on an encoding of
activation spikes. We adaptively update a weight vector relying on confidence
estimates and activation offsets relative to previous activity. We regularize
updates proportionally to item-level confidence and weight-specific support,
loosely inspired by the observation from neurophysiology that high spike rates
are sometimes accompanied by low temporal precision. Our experiments suggest
that the new learning algorithm SPIRAL is more robust and less prone to
overfitting than both the averaged perceptron and AROW.
</summary>
    <author>
      <name>Anders Søgaard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Computing with Spikes at NIPS 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.06245v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.06245v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.09430v1</id>
    <updated>2016-11-28T23:39:20Z</updated>
    <published>2016-11-28T23:39:20Z</published>
    <title>Emergence of foveal image sampling from learning to attend in visual
  scenes</title>
    <summary>  We describe a neural attention model with a learnable retinal sampling
lattice. The model is trained on a visual search task requiring the
classification of an object embedded in a visual scene amidst background
distractors using the smallest number of fixations. We explore the tiling
properties that emerge in the model's retinal sampling lattice after training.
Specifically, we show that this lattice resembles the eccentricity dependent
sampling lattice of the primate retina, with a high resolution region in the
fovea surrounded by a low resolution periphery. Furthermore, we find conditions
where these emergent properties are amplified or eliminated providing clues to
their function.
</summary>
    <author>
      <name>Brian Cheung</name>
    </author>
    <author>
      <name>Eric Weiss</name>
    </author>
    <author>
      <name>Bruno Olshausen</name>
    </author>
    <link href="http://arxiv.org/abs/1611.09430v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.09430v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.00712v1</id>
    <updated>2016-12-02T15:46:09Z</updated>
    <published>2016-12-02T15:46:09Z</published>
    <title>Probabilistic Neural Programs</title>
    <summary>  We present probabilistic neural programs, a framework for program induction
that permits flexible specification of both a computational model and inference
algorithm while simultaneously enabling the use of deep neural networks.
Probabilistic neural programs combine a computation graph for specifying a
neural network with an operator for weighted nondeterministic choice. Thus, a
program describes both a collection of decisions as well as the neural network
architecture used to make each one. We evaluate our approach on a challenging
diagram question answering task where probabilistic neural programs correctly
execute nearly twice as many programs as a baseline model.
</summary>
    <author>
      <name>Kenton W. Murray</name>
    </author>
    <author>
      <name>Jayant Krishnamurthy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in NAMPI workshop at NIPS 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.00712v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.00712v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.02522v1</id>
    <updated>2016-12-08T03:28:10Z</updated>
    <published>2016-12-08T03:28:10Z</published>
    <title>Geometric Decomposition of Feed Forward Neural Networks</title>
    <summary>  There have been several attempts to mathematically understand neural networks
and many more from biological and computational perspectives. The field has
exploded in the last decade, yet neural networks are still treated much like a
black box. In this work we describe a structure that is inherent to a feed
forward neural network. This will provide a framework for future work on neural
networks to improve training algorithms, compute the homology of the network,
and other applications. Our approach takes a more geometric point of view and
is unlike other attempts to mathematically understand neural networks that rely
on a functional perspective.
</summary>
    <author>
      <name>Sven Cattell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.02522v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.02522v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="92B20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.03707v1</id>
    <updated>2016-12-12T14:36:22Z</updated>
    <published>2016-12-12T14:36:22Z</published>
    <title>Empirical Evaluation of A New Approach to Simplifying Long Short-term
  Memory (LSTM)</title>
    <summary>  The standard LSTM, although it succeeds in the modeling long-range
dependences, suffers from a highly complex structure that can be simplified
through modifications to its gate units. This paper was to perform an empirical
comparison between the standard LSTM and three new simplified variants that
were obtained by eliminating input signal, bias and hidden unit signal from
individual gates, on the tasks of modeling two sequence datasets. The
experiments show that the three variants, with reduced parameters, can achieve
comparable performance with the standard LSTM. Due attention should be paid to
turning the learning rate to achieve high accuracies
</summary>
    <author>
      <name>Yuzhen Lu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.03707v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.03707v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.05054v1</id>
    <updated>2016-12-15T13:24:41Z</updated>
    <published>2016-12-15T13:24:41Z</published>
    <title>Graphical RNN Models</title>
    <summary>  Many time series are generated by a set of entities that interact with one
another over time. This paper introduces a broad, flexible framework to learn
from multiple inter-dependent time series generated by such entities. Our
framework explicitly models the entities and their interactions through time.
It achieves this by building on the capabilities of Recurrent Neural Networks,
while also offering several ways to incorporate domain knowledge/constraints
into the model architecture. The capabilities of our approach are showcased
through an application to weather prediction, which shows gains over strong
baselines.
</summary>
    <author>
      <name>Ashish Bora</name>
    </author>
    <author>
      <name>Sugato Basu</name>
    </author>
    <author>
      <name>Joydeep Ghosh</name>
    </author>
    <link href="http://arxiv.org/abs/1612.05054v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.05054v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.03441v1</id>
    <updated>2017-01-12T18:12:05Z</updated>
    <published>2017-01-12T18:12:05Z</published>
    <title>Simplified Gating in Long Short-term Memory (LSTM) Recurrent Neural
  Networks</title>
    <summary>  The standard LSTM recurrent neural networks while very powerful in long-range
dependency sequence applications have highly complex structure and relatively
large (adaptive) parameters. In this work, we present empirical comparison
between the standard LSTM recurrent neural network architecture and three new
parameter-reduced variants obtained by eliminating combinations of the input
signal, bias, and hidden unit signals from individual gating signals. The
experiments on two sequence datasets show that the three new variants, called
simply as LSTM1, LSTM2, and LSTM3, can achieve comparable performance to the
standard LSTM model with less (adaptive) parameters.
</summary>
    <author>
      <name>Yuzhen Lu</name>
    </author>
    <author>
      <name>Fathi M. Salem</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 Figures, 3 Tables. arXiv admin note: substantial text
  overlap with arXiv:1612.03707</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.03441v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.03441v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.05159v1</id>
    <updated>2017-01-18T17:37:35Z</updated>
    <published>2017-01-18T17:37:35Z</published>
    <title>Temporal Overdrive Recurrent Neural Network</title>
    <summary>  In this work we present a novel recurrent neural network architecture
designed to model systems characterized by multiple characteristic timescales
in their dynamics. The proposed network is composed by several recurrent groups
of neurons that are trained to separately adapt to each timescale, in order to
improve the system identification process. We test our framework on time series
prediction tasks and we show some promising, preliminary results achieved on
synthetic data. To evaluate the capabilities of our network, we compare the
performance with several state-of-the-art recurrent architectures.
</summary>
    <author>
      <name>Filippo Maria Bianchi</name>
    </author>
    <author>
      <name>Michael Kampffmeyer</name>
    </author>
    <author>
      <name>Enrico Maiorino</name>
    </author>
    <author>
      <name>Robert Jenssen</name>
    </author>
    <link href="http://arxiv.org/abs/1701.05159v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.05159v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.05730v1</id>
    <updated>2017-01-20T09:17:52Z</updated>
    <published>2017-01-20T09:17:52Z</published>
    <title>Using LLVM-based JIT Compilation in Genetic Programming</title>
    <summary>  The paper describes an approach to implementing genetic programming, which
uses the LLVM library to just-in-time compile/interpret the evolved abstract
syntax trees. The solution is described in some detail, including a parser
(based on FlexC++ and BisonC++) that can construct the trees from a simple toy
language with C-like syntax. The approach is compared with a previous
implementation (based on direct execution of trees using polymorphic functors)
in terms of execution speed.
</summary>
    <author>
      <name>Michal Gregor</name>
    </author>
    <author>
      <name>Juraj Spalek</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ELEKTRO.2016.7512108</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ELEKTRO.2016.7512108" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Link to the IEEE published version:
  http://ieeexplore.ieee.org/document/7512108/</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.05730v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.05730v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.05818v1</id>
    <updated>2017-01-20T15:10:09Z</updated>
    <published>2017-01-20T15:10:09Z</published>
    <title>Fusion of Heterogeneous Data in Convolutional Networks for Urban
  Semantic Labeling (Invited Paper)</title>
    <summary>  In this work, we present a novel module to perform fusion of heterogeneous
data using fully convolutional networks for semantic labeling. We introduce
residual correction as a way to learn how to fuse predictions coming out of a
dual stream architecture. Especially, we perform fusion of DSM and IRRG optical
data on the ISPRS Vaihingen dataset over a urban area and obtain new
state-of-the-art results.
</summary>
    <author>
      <name>Nicolas Audebert</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Palaiseau, OBELIX</arxiv:affiliation>
    </author>
    <author>
      <name>Bertrand Le Saux</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Palaiseau</arxiv:affiliation>
    </author>
    <author>
      <name>Sébastien Lefèvre</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">OBELIX</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Joint Urban Remote Sensing Event (JURSE), Mar 2017, Dubai, United
  Arab Emirates. Joint Urban Remote Sensing Event 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.05818v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.05818v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.00993v1</id>
    <updated>2017-02-03T13:07:35Z</updated>
    <published>2017-02-03T13:07:35Z</published>
    <title>Robust Particle Swarm Optimizer based on Chemomimicry</title>
    <summary>  A particle swarm optimizer (PSO) loosely based on the phenomena of
crystallization and a chaos factor which follows the complimentary error
function is described. The method features three phases: diffusion, directed
motion, and nucleation. During the diffusion phase random walk is the only
contributor to particle motion. As the algorithm progresses the contribution
from chaos decreases and movement toward global best locations is pursued until
convergence has occurred. The algorithm was found to be more robust to local
minima in multimodal test functions than a standard PSO algorithm and is
designed for problems which feature experimental precision.
</summary>
    <author>
      <name>Casey Kneale</name>
    </author>
    <author>
      <name>Karl S. Booksh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be revised for formatting and submitted as a Letters style paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.00993v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.00993v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.02277v1</id>
    <updated>2017-02-08T04:27:11Z</updated>
    <published>2017-02-08T04:27:11Z</published>
    <title>A Historical Review of Forty Years of Research on CMAC</title>
    <summary>  The Cerebellar Model Articulation Controller (CMAC) is an influential
brain-inspired computing model in many relevant fields. Since its inception in
the 1970s, the model has been intensively studied and many variants of the
prototype, such as Kernel-CMAC, Self-Organizing Map CMAC, and Linguistic CMAC,
have been proposed. This review article focus on how the CMAC model is
gradually developed and refined to meet the demand of fast, adaptive, and
robust control. Two perspective, CMAC as a neural network and CMAC as a table
look-up technique are presented. Three aspects of the model: the architecture,
learning algorithms and applications are discussed. In the end, some potential
future research directions on this model are suggested.
</summary>
    <author>
      <name>Frank Z. Xing</name>
    </author>
    <link href="http://arxiv.org/abs/1702.02277v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.02277v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="A.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.03389v2</id>
    <updated>2017-03-30T12:53:54Z</updated>
    <published>2017-02-11T06:39:38Z</published>
    <title>Whale swarm algorithm for function optimization</title>
    <summary>  Increasing nature-inspired metaheuristic algorithms are applied to solving
the real-world optimization problems, as they have some advantages over the
classical methods of numerical optimization. This paper has proposed a new
nature-inspired metaheuristic called Whale Swarm Algorithm for function
optimization, which is inspired by the whales behavior of communicating with
each other via ultrasound for hunting. The proposed Whale Swarm Algorithm has
been compared with several popular metaheuristic algorithms on comprehensive
performance metrics. According to the experimental results, Whale Swarm
Algorithm has a quite competitive performance when compared with other
algorithms.
</summary>
    <author>
      <name>Bing Zeng</name>
    </author>
    <author>
      <name>Liang Gao</name>
    </author>
    <author>
      <name>Xinyu Li</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-63309-1_55</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-63309-1_55" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">LNCS. volume 10361. ICIC 2017: pp 624-639</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1702.03389v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.03389v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.08231v1</id>
    <updated>2017-02-27T11:10:54Z</updated>
    <published>2017-02-27T11:10:54Z</published>
    <title>Low-Precision Batch-Normalized Activations</title>
    <summary>  Artificial neural networks can be trained with relatively low-precision
floating-point and fixed-point arithmetic, using between one and 16 bits.
Previous works have focused on relatively wide-but-shallow, feed-forward
networks. We introduce a quantization scheme that is compatible with training
very deep neural networks. Quantizing the network activations in the middle of
each batch-normalization module can greatly reduce the amount of memory and
computational power needed, with little loss in accuracy.
</summary>
    <author>
      <name>Benjamin Graham</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.08231v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.08231v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.08727v1</id>
    <updated>2017-02-28T10:19:51Z</updated>
    <published>2017-02-28T10:19:51Z</published>
    <title>Improving the Neural GPU Architecture for Algorithm Learning</title>
    <summary>  Algorithm learning is a core problem in artificial intelligence with
significant implications on automation level that can be achieved by machines.
Recently deep learning methods are emerging for synthesizing an algorithm from
its input-output examples, the most successful being the Neural GPU, capable of
learning multiplication. We present several improvements to the Neural GPU that
substantially reduces training time and improves generalization. We introduce a
technique of general applicability to use hard nonlinearities with saturation
cost. We also introduce a technique of diagonal gates that can be applied to
active-memory models. The proposed architecture is the first capable of
learning decimal multiplication end-to-end.
</summary>
    <author>
      <name>Karlis Freivalds</name>
    </author>
    <author>
      <name>Renars Liepins</name>
    </author>
    <link href="http://arxiv.org/abs/1702.08727v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.08727v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.03768v1</id>
    <updated>2017-03-10T17:15:29Z</updated>
    <published>2017-03-10T17:15:29Z</published>
    <title>Integer Factorization with a Neuromorphic Sieve</title>
    <summary>  The bound to factor large integers is dominated by the computational effort
to discover numbers that are smooth, typically performed by sieving a
polynomial sequence. On a von Neumann architecture, sieving has log-log
amortized time complexity to check each value for smoothness. This work
presents a neuromorphic sieve that achieves a constant time check for
smoothness by exploiting two characteristic properties of neuromorphic
architectures: constant time synaptic integration and massively parallel
computation. The approach is validated by modifying msieve, one of the fastest
publicly available integer factorization implementations, to use the IBM
Neurosynaptic System (NS1e) as a coprocessor for the sieving stage.
</summary>
    <author>
      <name>John V. Monaco</name>
    </author>
    <author>
      <name>Manuel M. Vindiola</name>
    </author>
    <link href="http://arxiv.org/abs/1703.03768v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.03768v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.05955v1</id>
    <updated>2017-03-17T10:46:23Z</updated>
    <published>2017-03-17T10:46:23Z</published>
    <title>Implicit Gradient Neural Networks with a Positive-Definite Mass Matrix
  for Online Linear Equations Solving</title>
    <summary>  Motivated by the advantages achieved by implicit analogue net for solving
online linear equations, a novel implicit neural model is designed based on
conventional explicit gradient neural networks in this letter by introducing a
positive-definite mass matrix. In addition to taking the advantages of the
implicit neural dynamics, the proposed implicit gradient neural networks can
still achieve globally exponential convergence to the unique theoretical
solution of linear equations and also global stability even under no-solution
and multi-solution situations. Simulative results verify theoretical
convergence analysis on the proposed neural dynamics.
</summary>
    <author>
      <name>Ke Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Information Processing Letters</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.05955v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.05955v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.07841v1</id>
    <updated>2017-03-22T20:31:47Z</updated>
    <published>2017-03-22T20:31:47Z</published>
    <title>Classification-based RNN machine translation using GRUs</title>
    <summary>  We report the results of our classification-based machine translation model,
built upon the framework of a recurrent neural network using gated recurrent
units. Unlike other RNN models that attempt to maximize the overall conditional
log probability of sentences against sentences, our model focuses a
classification approach of estimating the conditional probability of the next
word given the input sequence. This simpler approach using GRUs was hoped to be
comparable with more complicated RNN models, but achievements in this
implementation were modest and there remains a lot of room for improving this
classification approach.
</summary>
    <author>
      <name>Ri Wang</name>
    </author>
    <author>
      <name>Maysum Panju</name>
    </author>
    <author>
      <name>Mahmood Gohari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 1 figure; graduate course research project</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.07841v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.07841v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.08481v1</id>
    <updated>2017-03-24T15:53:03Z</updated>
    <published>2017-03-24T15:53:03Z</published>
    <title>Long-Term Evolution of Genetic Programming Populations</title>
    <summary>  We evolve binary mux-6 trees for up to 100000 generations evolving some
programs with more than a hundred million nodes. Our unbounded Long-Term
Evolution Experiment LTEE GP appears not to evolve building blocks but does
suggests a limit to bloat. We do see periods of tens even hundreds of
generations where the population is 100 percent functionally converged. The
distribution of tree sizes is not as predicted by theory.
</summary>
    <author>
      <name>W. B. Langdon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Longer version of Langdon:2017:GECCO, July 2017, ACM, Berlin,
  RN/17/05
  http://www.cs.ucl.ac.uk/fileadmin/UCL-CS/research/Research_Notes/RN_17_05.pdf</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.08481v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.08481v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.09926v1</id>
    <updated>2017-03-29T08:10:26Z</updated>
    <published>2017-03-29T08:10:26Z</published>
    <title>Hierarchical Surrogate Modeling for Illumination Algorithms</title>
    <summary>  Evolutionary illumination is a recent technique that allows producing many
diverse, optimal solutions in a map of manually defined features. To support
the large amount of objective function evaluations, surrogate model assistance
was recently introduced. Illumination models need to represent many more,
diverse optimal regions than classical surrogate models. In this PhD thesis, we
propose to decompose the sample set, decreasing model complexity, by
hierarchically segmenting the training set according to their coordinates in
feature space. An ensemble of diverse models can then be trained to serve as a
surrogate to illumination.
</summary>
    <author>
      <name>Alexander Hagg</name>
    </author>
    <link href="http://arxiv.org/abs/1703.09926v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.09926v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.03477v4</id>
    <updated>2017-05-19T16:40:16Z</updated>
    <published>2017-04-11T18:09:01Z</published>
    <title>A Neural Representation of Sketch Drawings</title>
    <summary>  We present sketch-rnn, a recurrent neural network (RNN) able to construct
stroke-based drawings of common objects. The model is trained on thousands of
crude human-drawn images representing hundreds of classes. We outline a
framework for conditional and unconditional sketch generation, and describe new
robust training methods for generating coherent sketch drawings in a vector
format.
</summary>
    <author>
      <name>David Ha</name>
    </author>
    <author>
      <name>Douglas Eck</name>
    </author>
    <link href="http://arxiv.org/abs/1704.03477v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.03477v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.04879v1</id>
    <updated>2017-04-17T06:04:02Z</updated>
    <published>2017-04-17T06:04:02Z</published>
    <title>A Sport Tournament Scheduling by Genetic Algorithm with Swapping Method</title>
    <summary>  A sport tournament problem is considered the Traveling Tournament Problem
(TTP). One interesting type is the mirrored Traveling Tournament Problem
(mTTP). The objective of the problem is to minimize either the total number of
traveling or the total distances of traveling or both. This research aims to
find an optimized solution of the mirrored Traveling Tournament Problem with
minimum total number of traveling. The solutions consisting of traveling and
scheduling tables are solved by using genetic algorithm (GA) with swapping
method. The number of traveling of all teams from obtained solutions are close
to the lower bound theory of number of traveling. Moreover, this algorithm
generates better solutions than known results for most cases.
</summary>
    <author>
      <name>Tinnaluk Rutjanisarakul</name>
    </author>
    <author>
      <name>Thiradet Jiarasuksakun</name>
    </author>
    <link href="http://arxiv.org/abs/1704.04879v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.04879v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.05132v1</id>
    <updated>2017-04-17T21:31:14Z</updated>
    <published>2017-04-17T21:31:14Z</published>
    <title>A hybrid CPU-GPU parallelization scheme of variable neighborhood search
  for inventory optimization problems</title>
    <summary>  In this paper, we study various parallelization schemes for the Variable
Neighborhood Search (VNS) metaheuristic on a CPU-GPU system via OpenMP and
OpenACC. A hybrid parallel VNS method is applied to recent benchmark problem
instances for the multi-product dynamic lot sizing problem with product returns
and recovery, which appears in reverse logistics and is known to be NP-hard. We
report our findings regarding these parallelization approaches and present
promising computational results.
</summary>
    <author>
      <name>Nikolaos Antoniadis</name>
    </author>
    <author>
      <name>Angelo Sifaleras</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.endm.2017.03.007</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.endm.2017.03.007" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 1 figure</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Electronic Notes in Discrete Mathematics, Volume 58, April 2017,
  Pages 47-54, ISSN 1571-0653</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1704.05132v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.05132v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R99" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; I.2.8; D.1.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.05396v1</id>
    <updated>2017-04-18T15:33:10Z</updated>
    <published>2017-04-18T15:33:10Z</published>
    <title>A Study of Deep Learning Robustness Against Computation Failures</title>
    <summary>  For many types of integrated circuits, accepting larger failure rates in
computations can be used to improve energy efficiency. We study the performance
of faulty implementations of certain deep neural networks based on pessimistic
and optimistic models of the effect of hardware faults. After identifying the
impact of hyperparameters such as the number of layers on robustness, we study
the ability of the network to compensate for computational failures through an
increase of the network size. We show that some networks can achieve equivalent
performance under faulty implementations, and quantify the required increase in
computational complexity.
</summary>
    <author>
      <name>Jean-Charles Vialatte</name>
    </author>
    <author>
      <name>François Leduc-Primeau</name>
    </author>
    <link href="http://arxiv.org/abs/1704.05396v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.05396v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.05420v2</id>
    <updated>2017-04-19T23:36:18Z</updated>
    <published>2017-04-18T16:47:38Z</published>
    <title>Diagonal RNNs in Symbolic Music Modeling</title>
    <summary>  In this paper, we propose a new Recurrent Neural Network (RNN) architecture.
The novelty is simple: We use diagonal recurrent matrices instead of full. This
results in better test likelihood and faster convergence compared to regular
full RNNs in most of our experiments. We show the benefits of using diagonal
recurrent matrices with popularly used LSTM and GRU architectures as well as
with the vanilla RNN architecture, on four standard symbolic music datasets.
</summary>
    <author>
      <name>Y. Cem Subakan</name>
    </author>
    <author>
      <name>Paris Smaragdis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Waspaa 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.05420v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.05420v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.06016v1</id>
    <updated>2017-04-20T05:09:39Z</updated>
    <published>2017-04-20T05:09:39Z</published>
    <title>Genetic Algorithm Based Floor Planning System</title>
    <summary>  Genetic Algorithms are widely used in many different optimization problems
including layout design. The layout of the shelves play an important role in
the total sales metrics for superstores since this affects the customers'
shopping behaviour. This paper employed a genetic algorithm based approach to
design shelf layout of superstores. The layout design problem was tackled by
using a novel chromosome representation which takes many different parameters
to prevent dead-ends and improve shelf visibility into consideration. Results
show that the approach can produce reasonably good layout designs in very short
amounts of time.
</summary>
    <author>
      <name>Hamide Ozlem Dalgic</name>
    </author>
    <author>
      <name>Erkan Bostanci</name>
    </author>
    <author>
      <name>Mehmet Serdar Guzel</name>
    </author>
    <link href="http://arxiv.org/abs/1704.06016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.06016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.08774v1</id>
    <updated>2017-04-27T23:38:36Z</updated>
    <published>2017-04-27T23:38:36Z</published>
    <title>Genealogical Distance as a Diversity Estimate in Evolutionary Algorithms</title>
    <summary>  The evolutionary edit distance between two individuals in a population, i.e.,
the amount of applications of any genetic operator it would take the
evolutionary process to generate one individual starting from the other, seems
like a promising estimate for the diversity between said individuals. We
introduce genealogical diversity, i.e., estimating two individuals' degree of
relatedness by analyzing large, unused parts of their genome, as a
computationally efficient method to approximate that measure for diversity.
</summary>
    <author>
      <name>Thomas Gabor</name>
    </author>
    <author>
      <name>Lenz Belzner</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3067695.3082529</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3067695.3082529" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Measuring and Promoting Diversity in Evolutionary Algorithms @ GECCO
  2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.08774v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.08774v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.00280v1</id>
    <updated>2017-06-01T12:57:11Z</updated>
    <published>2017-06-01T12:57:11Z</published>
    <title>Integer Echo State Networks: Hyperdimensional Reservoir Computing</title>
    <summary>  We propose an integer approximation of Echo State Networks (ESN) based on the
mathematics of hyperdimensional computing. The reservoir of the proposed
Integer Echo State Network (intESN) contains only n-bits integers and replaces
the recurrent matrix multiply with an efficient cyclic shift operation. Such an
architecture results in dramatic improvements in memory footprint and
computational efficiency, with minimal performance loss. Our architecture
naturally supports the usage of the trained reservoir in symbolic processing
tasks of analogy making and logical inference.
</summary>
    <author>
      <name>Denis Kleyko</name>
    </author>
    <author>
      <name>Edward Paxon Frady</name>
    </author>
    <author>
      <name>Evgeny Osipov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.00280v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.00280v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.00504v1</id>
    <updated>2017-06-01T21:57:32Z</updated>
    <published>2017-06-01T21:57:32Z</published>
    <title>Dynamic Stripes: Exploiting the Dynamic Precision Requirements of
  Activation Values in Neural Networks</title>
    <summary>  Stripes is a Deep Neural Network (DNN) accelerator that uses bit-serial
computation to offer performance that is proportional to the fixed-point
precision of the activation values. The fixed-point precisions are determined a
priori using profiling and are selected at a per layer granularity. This paper
presents Dynamic Stripes, an extension to Stripes that detects precision
variance at runtime and at a finer granularity. This extra level of precision
reduction increases performance by 41% over Stripes.
</summary>
    <author>
      <name>Alberto Delmas</name>
    </author>
    <author>
      <name>Patrick Judd</name>
    </author>
    <author>
      <name>Sayeh Sharify</name>
    </author>
    <author>
      <name>Andreas Moshovos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.00504v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.00504v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.00648v1</id>
    <updated>2017-05-03T13:46:05Z</updated>
    <published>2017-05-03T13:46:05Z</published>
    <title>Dataflow Matrix Machines as a Model of Computations with Linear Streams</title>
    <summary>  We overview dataflow matrix machines as a Turing complete generalization of
recurrent neural networks and as a programming platform. We describe vector
space of finite prefix trees with numerical leaves which allows us to combine
expressive power of dataflow matrix machines with simplicity of traditional
recurrent neural networks.
</summary>
    <author>
      <name>Michael Bukatin</name>
    </author>
    <author>
      <name>Jon Anthony</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, accepted for presentation at LearnAut 2017: Learning and
  Automata workshop at LICS (Logic in Computer Science) 2017 conference.
  Preprint original version: April 9, 2017; minor correction: May 1, 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.00648v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.00648v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.02766v1</id>
    <updated>2017-06-08T20:49:34Z</updated>
    <published>2017-06-08T20:49:34Z</published>
    <title>Evolutionary Multitasking for Multiobjective Continuous Optimization:
  Benchmark Problems, Performance Metrics and Baseline Results</title>
    <summary>  In this report, we suggest nine test problems for multi-task multi-objective
optimization (MTMOO), each of which consists of two multiobjective optimization
tasks that need to be solved simultaneously. The relationship between tasks
varies between different test problems, which would be helpful to have a
comprehensive evaluation of the MO-MFO algorithms. It is expected that the
proposed test problems will germinate progress the field of the MTMOO research.
</summary>
    <author>
      <name>Yuan Yuan</name>
    </author>
    <author>
      <name>Yew-Soon Ong</name>
    </author>
    <author>
      <name>Liang Feng</name>
    </author>
    <author>
      <name>A. K. Qin</name>
    </author>
    <author>
      <name>Abhishek Gupta</name>
    </author>
    <author>
      <name>Bingshui Da</name>
    </author>
    <author>
      <name>Qingfu Zhang</name>
    </author>
    <author>
      <name>Kay Chen Tan</name>
    </author>
    <author>
      <name>Yaochu Jin</name>
    </author>
    <author>
      <name>Hisao Ishibuchi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 5 pages, technical report</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.02766v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.02766v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.03470v1</id>
    <updated>2017-06-12T05:25:40Z</updated>
    <published>2017-06-12T05:25:40Z</published>
    <title>Evolutionary Multitasking for Single-objective Continuous Optimization:
  Benchmark Problems, Performance Metric, and Baseline Results</title>
    <summary>  In this report, we suggest nine test problems for multi-task single-objective
optimization (MTSOO), each of which consists of two single-objective
optimization tasks that need to be solved simultaneously. The relationship
between tasks varies between different test problems, which would be helpful to
have a comprehensive evaluation of the MFO algorithms. It is expected that the
proposed test problems will germinate progress the field of the MTSOO research.
</summary>
    <author>
      <name>Bingshui Da</name>
    </author>
    <author>
      <name>Yew-Soon Ong</name>
    </author>
    <author>
      <name>Liang Feng</name>
    </author>
    <author>
      <name>A. K. Qin</name>
    </author>
    <author>
      <name>Abhishek Gupta</name>
    </author>
    <author>
      <name>Zexuan Zhu</name>
    </author>
    <author>
      <name>Chuan-Kang Ting</name>
    </author>
    <author>
      <name>Ke Tang</name>
    </author>
    <author>
      <name>Xin Yao</name>
    </author>
    <link href="http://arxiv.org/abs/1706.03470v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.03470v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.06920v1</id>
    <updated>2017-06-21T14:14:13Z</updated>
    <published>2017-06-21T14:14:13Z</published>
    <title>Genetic Algorithm with Optimal Recombination for the Asymmetric
  Travelling Salesman Problem</title>
    <summary>  We propose a new genetic algorithm with optimal recombination for the
asymmetric instances of travelling salesman problem. The algorithm incorporates
several new features that contribute to its effectiveness: (i) Optimal
recombination problem is solved within crossover operator. (ii) A new mutation
operator performs a random jump within 3-opt or 4-opt neighborhood. (iii)
Greedy constructive heuristic of W.Zhang and 3-opt local search heuristic are
used to generate the initial population. A computational experiment on TSPLIB
instances shows that the proposed algorithm yields competitive results to other
well-known memetic algorithms for asymmetric travelling salesman problem.
</summary>
    <author>
      <name>A. V. Eremeev</name>
    </author>
    <author>
      <name>Yu. V. Kovalenko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Proc. of The 11th International Conference on
  Large-Scale Scientific Computations (LSSC-17), June 5 - 9, 2017, Sozopol,
  Bulgaria</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.06920v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.06920v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.00081v1</id>
    <updated>2017-07-01T01:30:21Z</updated>
    <published>2017-07-01T01:30:21Z</published>
    <title>Synthesizing Deep Neural Network Architectures using Biological Synaptic
  Strength Distributions</title>
    <summary>  In this work, we perform an exploratory study on synthesizing deep neural
networks using biological synaptic strength distributions, and the potential
influence of different distributions on modelling performance particularly for
the scenario associated with small data sets. Surprisingly, a CNN with
convolutional layer synaptic strengths drawn from biologically-inspired
distributions such as log-normal or correlated center-surround distributions
performed relatively well suggesting a possibility for designing deep neural
network architectures that do not require many data samples to learn, and can
sidestep current training procedures while maintaining or boosting modelling
performance.
</summary>
    <author>
      <name>A. H. Karimi</name>
    </author>
    <author>
      <name>M. J. Shafiee</name>
    </author>
    <author>
      <name>A. Ghodsi</name>
    </author>
    <author>
      <name>A. Wong</name>
    </author>
    <link href="http://arxiv.org/abs/1707.00081v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.00081v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.00451v1</id>
    <updated>2017-07-03T09:04:52Z</updated>
    <published>2017-07-03T09:04:52Z</published>
    <title>A Distance Between Populations for n-Points Crossover in Genetic
  Algorithms</title>
    <summary>  Genetic algorithms (GAs) are an optimization technique that has been
successfully used on many real-world problems. There exist different approaches
to their theoretical study. In this paper we complete a recently presented
approach to model one-point crossover using pretopologies (or Cech topologies)
in two ways. First, we extend it to the case of n-points crossover. Then, we
experimentally study how the distance distribution changes when the number of
crossover points increases.
</summary>
    <author>
      <name>Mauro Castelli</name>
    </author>
    <author>
      <name>Gianpiero Cattaneo</name>
    </author>
    <author>
      <name>Luca Manzoni</name>
    </author>
    <author>
      <name>Leonardo Vanneschi</name>
    </author>
    <link href="http://arxiv.org/abs/1707.00451v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.00451v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.00884v1</id>
    <updated>2017-07-04T09:56:24Z</updated>
    <published>2017-07-04T09:56:24Z</published>
    <title>Identification of non-linear behavior models with restricted or
  redundant data</title>
    <summary>  This study presents a new strategy for the identification of material
parameters in the case of restricted or redundant data, based on a hybrid
approach combining a genetic algorithm and the Levenberg-Marquardt method. The
proposed methodology consists essentially in a statistically based topological
analysis of the search domain, after this one has been reduced by the analysis
of the parameters ranges. This is used to identify the parameters of a model
representing the behavior of damaged elastic, visco-elastic, plastic and
visco-plastic composite laminates. Optimization of the experimental tests on
tubular samples leads to the selective identification of these parameters.
</summary>
    <author>
      <name>S. Carbillet</name>
    </author>
    <author>
      <name>V. Guicheret-Retel</name>
    </author>
    <author>
      <name>F. Trivaudey</name>
    </author>
    <author>
      <name>F. Richard</name>
    </author>
    <author>
      <name>M. L. Boubakar</name>
    </author>
    <link href="http://arxiv.org/abs/1707.00884v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.00884v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.04853v2</id>
    <updated>2017-07-20T21:37:54Z</updated>
    <published>2017-07-16T10:12:13Z</published>
    <title>Overcoming Catastrophic Interference by Conceptors</title>
    <summary>  Catastrophic interference has been a major roadblock in the research of
continual learning. Here we propose a variant of the back-propagation
algorithm, "conceptor-aided back-prop" (CAB), in which gradients are shielded
by conceptors against degradation of previously learned tasks. Conceptors have
their origin in reservoir computing, where they have been previously shown to
overcome catastrophic forgetting. CAB extends these results to deep feedforward
networks. On the disjoint MNIST task CAB outperforms two other methods for
coping with catastrophic interference that have recently been proposed in the
deep learning field.
</summary>
    <author>
      <name>Xu He</name>
    </author>
    <author>
      <name>Herbert Jaeger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.04853v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.04853v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.00587v1</id>
    <updated>2017-08-02T03:23:17Z</updated>
    <published>2017-08-02T03:23:17Z</published>
    <title>Geometric Convolutional Neural Network for Analyzing Surface-Based
  Neuroimaging Data</title>
    <summary>  The conventional CNN, widely used for two-dimensional images, however, is not
directly applicable to non-regular geometric surface, such as a cortical
thickness. We propose Geometric CNN (gCNN) that deals with data representation
over a spherical surface and renders pattern recognition in a multi-shell mesh
structure. The classification accuracy for sex was significantly higher than
that of SVM and image based CNN. It only uses MRI thickness data to classify
gender but this method can expand to classify disease from other MRI or fMRI
data
</summary>
    <author>
      <name>Si-Baek Seong</name>
    </author>
    <author>
      <name>Chongwon Pae</name>
    </author>
    <author>
      <name>Hae-Jeong Park</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.00587v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.00587v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.01368v1</id>
    <updated>2017-08-04T03:21:23Z</updated>
    <published>2017-08-04T03:21:23Z</published>
    <title>A novel metaheuristic method for solving constrained engineering
  optimization problems: Drone Squadron Optimization</title>
    <summary>  Several constrained optimization problems have been adequately solved over
the years thanks to advances in the metaheuristics area. In this paper, we
evaluate a novel self-adaptive and auto-constructive metaheuristic called Drone
Squadron Optimization (DSO) in solving constrained engineering design problems.
This paper evaluates DSO with death penalty on three widely tested engineering
design problems. Results show that the proposed approach is competitive with
some very popular metaheuristics.
</summary>
    <author>
      <name>Vinícius Veloso de Melo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.01368v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.01368v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.03417v1</id>
    <updated>2017-08-11T00:41:56Z</updated>
    <published>2017-08-11T00:41:56Z</published>
    <title>GlobeNet: Convolutional Neural Networks for Typhoon Eye Tracking from
  Remote Sensing Imagery</title>
    <summary>  Advances in remote sensing technologies have made it possible to use
high-resolution visual data for weather observation and forecasting tasks. We
propose the use of multi-layer neural networks for understanding complex
atmospheric dynamics based on multichannel satellite images. The capability of
our model was evaluated by using a linear regression task for single typhoon
coordinates prediction. A specific combination of models and different
activation policies enabled us to obtain an interesting prediction result in
the northeastern hemisphere (ENH).
</summary>
    <author>
      <name>Seungkyun Hong</name>
    </author>
    <author>
      <name>Seongchan Kim</name>
    </author>
    <author>
      <name>Minsu Joh</name>
    </author>
    <author>
      <name>Sa-kwang Song</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Under review as a workshop paper at CI 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.03417v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.03417v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.06008v1</id>
    <updated>2017-08-20T19:29:44Z</updated>
    <published>2017-08-20T19:29:44Z</published>
    <title>Boltzmann machines and energy-based models</title>
    <summary>  We review Boltzmann machines and energy-based models. A Boltzmann machine
defines a probability distribution over binary-valued patterns. One can learn
parameters of a Boltzmann machine via gradient based approaches in a way that
log likelihood of data is increased. The gradient and Laplacian of a Boltzmann
machine admit beautiful mathematical representations, although computing them
is in general intractable. This intractability motivates approximate methods,
including Gibbs sampler and contrastive divergence, and tractable alternatives,
namely energy-based models.
</summary>
    <author>
      <name>Takayuki Osogami</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">36 pages. The topics covered in this paper are presented in Part I of
  IJCAI-17 tutorial on energy-based machine learning.
  https://researcher.watson.ibm.com/researcher/view_group.php?id=7834</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.06008v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.06008v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.08557v2</id>
    <updated>2017-09-11T19:04:57Z</updated>
    <published>2017-08-28T23:08:21Z</published>
    <title>A parameterized activation function for learning fuzzy logic operations
  in deep neural networks</title>
    <summary>  We present a deep learning architecture for learning fuzzy logic expressions.
Our model uses an innovative, parameterized, differentiable activation function
that can learn a number of logical operations by gradient descent. This
activation function allows a neural network to determine the relationships
between its input variables and provides insight into the logical significance
of learned network parameters. We provide a theoretical basis for this
parameterization and demonstrate its effectiveness and utility by successfully
applying our model to five classification problems from the UCI Machine
Learning Repository.
</summary>
    <author>
      <name>Luke B. Godfrey</name>
    </author>
    <author>
      <name>Michael S. Gashler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures, IEEE SMC 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.08557v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.08557v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.09116v1</id>
    <updated>2017-08-30T04:48:33Z</updated>
    <published>2017-08-30T04:48:33Z</published>
    <title>Slope Stability Analysis with Geometric Semantic Genetic Programming</title>
    <summary>  Genetic programming has been widely used in the engineering field. Compared
with the conventional genetic programming and artificial neural network,
geometric semantic genetic programming (GSGP) is superior in astringency and
computing efficiency. In this paper, GSGP is adopted for the classification and
regression analysis of a sample dataset. Furthermore, a model for slope
stability analysis is established on the basis of geometric semantics.
According to the results of the study based on GSGP, the method can analyze
slope stability objectively and is highly precise in predicting slope stability
and safety factors. Hence, the predicted results can be used as a reference for
slope safety design.
</summary>
    <author>
      <name>Juncai Xu</name>
    </author>
    <author>
      <name>Zhenzhong Shen</name>
    </author>
    <author>
      <name>Qingwen Ren</name>
    </author>
    <author>
      <name>Xin Xie</name>
    </author>
    <author>
      <name>Zhengyu Yang</name>
    </author>
    <link href="http://arxiv.org/abs/1708.09116v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.09116v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.00410v1</id>
    <updated>2017-09-01T09:01:39Z</updated>
    <published>2017-09-01T09:01:39Z</published>
    <title>Visual art inspired by the collective feeding behavior of sand-bubbler
  crabs</title>
    <summary>  Sand-bubbler crabs of the genera Dotilla and Scopimera are known to produce
remarkable patterns and structures at tropical beaches. From these
pattern-making abilities, we may draw inspiration for digital visual art. A
simple mathematical model of sand--bubbler patterns is proposed and an
algorithm is designed that may create such patterns artificially. In addition,
design parameters to modify the patterns are identified and analyzed by
computational aesthetic measures. Finally, an extension of the algorithm is
discussed that may enable controlling and guiding generative evolution of the
art-making process.
</summary>
    <author>
      <name>Hendrik Richter</name>
    </author>
    <link href="http://arxiv.org/abs/1709.00410v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.00410v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.03247v1</id>
    <updated>2017-09-11T05:37:53Z</updated>
    <published>2017-09-11T05:37:53Z</published>
    <title>Evolution of Convolutional Highway Networks</title>
    <summary>  Convolutional highways are deep networks based on multiple stacked
convolutional layers for feature preprocessing. We introduce an evolutionary
algorithm (EA) for optimization of the structure and hyperparameters of
convolutional highways and demonstrate the potential of this optimization
setting on the well-known MNIST data set. The (1+1)-EA employs Rechenberg's
mutation rate control and a niching mechanism to overcome local optima adapts
the optimization approach. An experimental study shows that the EA is capable
of improving the state-of-the-art network contribution and of evolving highway
networks from scratch.
</summary>
    <author>
      <name>Oliver Kramer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.03247v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.03247v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.07432v1</id>
    <updated>2017-09-21T17:50:04Z</updated>
    <published>2017-09-21T17:50:04Z</published>
    <title>Dynamic Evaluation of Neural Sequence Models</title>
    <summary>  We present methodology for using dynamic evaluation to improve neural
sequence models. Models are adapted to recent history via a gradient descent
based mechanism, allowing them to assign higher probabilities to re-occurring
sequential patterns. Dynamic evaluation is demonstrated to compare favourably
with existing adaptation approaches for language modelling. We apply dynamic
evaluation to improve the state of the art word-level perplexities on the Penn
Treebank and WikiText-2 datasets to 51.1 and 44.3 respectively, and the state
of the art character-level cross-entropy on the Hutter prize dataset to 1.17
bits/character.
</summary>
    <author>
      <name>Ben Krause</name>
    </author>
    <author>
      <name>Emmanuel Kahembwe</name>
    </author>
    <author>
      <name>Iain Murray</name>
    </author>
    <author>
      <name>Steve Renals</name>
    </author>
    <link href="http://arxiv.org/abs/1709.07432v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.07432v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.09227v1</id>
    <updated>2017-09-26T19:20:31Z</updated>
    <published>2017-09-26T19:20:31Z</published>
    <title>Optimizing PID parameters with machine learning</title>
    <summary>  This paper examines the Evolutionary programming (EP) method for optimizing
PID parameters. PID is the most common type of regulator within control theory,
partly because it's relatively simple and yields stable results for most
applications. The p, i and d parameters vary for each application; therefore,
choosing the right parameters is crucial for obtaining good results but also
somewhat difficult. EP is a derivative-free optimization algorithm which makes
it suitable for PID optimization. The experiments in this paper demonstrate the
power of EP to solve the problem of optimizing PID parameters without getting
stuck in local minimums.
</summary>
    <author>
      <name>Adam Nyberg</name>
    </author>
    <link href="http://arxiv.org/abs/1709.09227v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.09227v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.09840v1</id>
    <updated>2017-09-28T08:02:16Z</updated>
    <published>2017-09-28T08:02:16Z</published>
    <title>PSA: A novel optimization algorithm based on survival rules of porcellio
  scaber</title>
    <summary>  Bio-inspired algorithms have received a significant amount of attention in
both academic and engineering societies. In this paper, based on the
observation of two major survival rules of a species of woodlice, i.e.,
porcellio scaber, we design and propose an algorithm called the porcellio
scaber algorithm (PSA) for solving optimization problems, including
differentiable and non-differential ones as well as the case with local
optimums. Numerical results based on benchmark problems are presented to
validate the efficacy of PSA.
</summary>
    <author>
      <name>Yinyan Zhang</name>
    </author>
    <author>
      <name>Shuai Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.09840v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.09840v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.01013v1</id>
    <updated>2017-10-03T07:21:03Z</updated>
    <published>2017-10-03T07:21:03Z</published>
    <title>Training Feedforward Neural Networks with Standard Logistic Activations
  is Feasible</title>
    <summary>  Training feedforward neural networks with standard logistic activations is
considered difficult because of the intrinsic properties of these sigmoidal
functions. This work aims at showing that these networks can be trained to
achieve generalization performance comparable to those based on hyperbolic
tangent activations. The solution consists on applying a set of conditions in
parameter initialization, which have been derived from the study of the
properties of a single neuron from an information-theoretic perspective. The
proposed initialization is validated through an extensive experimental
analysis.
</summary>
    <author>
      <name>Emanuele Sansone</name>
    </author>
    <author>
      <name>Francesco G. B. De Natale</name>
    </author>
    <link href="http://arxiv.org/abs/1710.01013v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.01013v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0808.1000v3</id>
    <updated>2010-03-07T12:56:29Z</updated>
    <published>2008-08-07T11:07:22Z</published>
    <title>Fitness Landscape Analysis for Dynamic Resource Allocation in Multiuser
  OFDM Based Cognitive Radio Systems</title>
    <summary>  This paper has been withdrawn.
</summary>
    <author>
      <name>Dong Huang</name>
    </author>
    <author>
      <name>Chunyan Miao</name>
    </author>
    <author>
      <name>Cyril Leung</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn</arxiv:comment>
    <link href="http://arxiv.org/abs/0808.1000v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0808.1000v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.4295v1</id>
    <updated>2012-05-19T04:25:04Z</updated>
    <published>2012-05-19T04:25:04Z</published>
    <title>Efficient Methods for Unsupervised Learning of Probabilistic Models</title>
    <summary>  In this thesis I develop a variety of techniques to train, evaluate, and
sample from intractable and high dimensional probabilistic models. Abstract
exceeds arXiv space limitations -- see PDF.
</summary>
    <author>
      <name>Jascha Sohl-Dickstein</name>
    </author>
    <link href="http://arxiv.org/abs/1205.4295v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.4295v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.00036v2</id>
    <updated>2015-04-14T22:55:08Z</updated>
    <published>2015-02-27T23:50:22Z</published>
    <title>Norm-Based Capacity Control in Neural Networks</title>
    <summary>  We investigate the capacity, convexity and characterization of a general
family of norm-constrained feed-forward networks.
</summary>
    <author>
      <name>Behnam Neyshabur</name>
    </author>
    <author>
      <name>Ryota Tomioka</name>
    </author>
    <author>
      <name>Nathan Srebro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.00036v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.00036v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.00556v2</id>
    <updated>2016-02-08T07:06:58Z</updated>
    <published>2015-10-02T10:48:56Z</published>
    <title>Autonomous Perceptron Neural Network Inspired from Quantum computing</title>
    <summary>  This abstract will be modified after correcting the minor error in Eq.(2)
</summary>
    <author>
      <name>M. Zidan</name>
    </author>
    <author>
      <name>A. Sagheer</name>
    </author>
    <author>
      <name>N. Metwally</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by the author due to a crucial sign
  error in equation 2</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.00556v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.00556v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09556v1</id>
    <updated>2017-06-29T02:43:37Z</updated>
    <published>2017-06-29T02:43:37Z</published>
    <title>Vision-based Detection of Acoustic Timed Events: a Case Study on
  Clarinet Note Onsets</title>
    <summary>  Acoustic events often have a visual counterpart. Knowledge of visual
information can aid the understanding of complex auditory scenes, even when
only a stereo mixdown is available in the audio domain, \eg identifying which
musicians are playing in large musical ensembles. In this paper, we consider a
vision-based approach to note onset detection. As a case study we focus on
challenging, real-world clarinetist videos and carry out preliminary
experiments on a 3D convolutional neural network based on multiple streams and
purposely avoiding temporal pooling. We release an audiovisual dataset with 4.5
hours of clarinetist videos together with cleaned annotations which include
about 36,000 onsets and the coordinates for a number of salient points and
regions of interest. By performing several training trials on our dataset, we
learned that the problem is challenging. We found that the CNN model is highly
sensitive to the optimization algorithm and hyper-parameters, and that treating
the problem as binary classification may prevent the joint optimization of
precision and recall. To encourage further research, we publicly share our
dataset, annotations and all models and detail which issues we came across
during our preliminary experiments.
</summary>
    <author>
      <name>A. Bazzica</name>
    </author>
    <author>
      <name>J. C. van Gemert</name>
    </author>
    <author>
      <name>C. C. S. Liem</name>
    </author>
    <author>
      <name>A. Hanjalic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the First International Conference on Deep Learning
  and Music, Anchorage, US, May, 2017 (arXiv:1706.08675v1 [cs.NE])</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc of the First Int Workshop on Deep Learning and Music.
  Anchorage, US. 1(1). pp 31-36 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1706.09556v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09556v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3; H.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0608073v1</id>
    <updated>2006-08-18T08:28:23Z</updated>
    <published>2006-08-18T08:28:23Z</published>
    <title>Parametrical Neural Networks and Some Other Similar Architectures</title>
    <summary>  A review of works on associative neural networks accomplished during last
four years in the Institute of Optical Neural Technologies RAS is given. The
presentation is based on description of parametrical neural networks (PNN). For
today PNN have record recognizing characteristics (storage capacity, noise
immunity and speed of operation). Presentation of basic ideas and principles is
accentuated.
</summary>
    <author>
      <name>Leonid B. Litinskii</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 2 figures, accepted for publication in "Optical Memory &amp;
  Neural Networks" (2006)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0608073v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0608073v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/nlin/0204038v1</id>
    <updated>2002-04-16T12:58:26Z</updated>
    <published>2002-04-16T12:58:26Z</published>
    <title>Neutrality: A Necessity for Self-Adaptation</title>
    <summary>  Self-adaptation is used in all main paradigms of evolutionary computation to
increase efficiency. We claim that the basis of self-adaptation is the use of
neutrality. In the absence of external control neutrality allows a variation of
the search distribution without the risk of fitness loss.
</summary>
    <author>
      <name>Marc Toussaint</name>
    </author>
    <author>
      <name>Christian Igel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures, LaTeX</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Congress on Evolutionary Computation (CEC
  2002), 1354-1359.</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/nlin/0204038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/nlin/0204038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0709.0883v5</id>
    <updated>2011-07-08T01:54:35Z</updated>
    <published>2007-09-06T16:04:42Z</published>
    <title>Liquid State Machines in Adbiatic Quantum Computers for General
  Computation</title>
    <summary>  Major mistakes do not read
</summary>
    <author>
      <name>Joshua Jay Herman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Totally wrong</arxiv:comment>
    <link href="http://arxiv.org/abs/0709.0883v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0709.0883v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3; F.1.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0805.0197v1</id>
    <updated>2008-05-02T09:20:11Z</updated>
    <published>2008-05-02T09:20:11Z</published>
    <title>Flatness of the Energy Landscape for Horn Clauses</title>
    <summary>  The Little-Hopfield neural network programmed with Horn clauses is studied.
We argue that the energy landscape of the system, corresponding to the
inconsistency function for logical interpretations of the sets of Horn clauses,
has minimal ruggedness. This is supported by computer simulations.
</summary>
    <author>
      <name>Saratha Sathasivam</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">USM</arxiv:affiliation>
    </author>
    <author>
      <name>Wan Ahmad Tajuddin Wan Abdullah</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Univ. Malaya</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Matematika 23 (2007) 147-156</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0805.0197v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0805.0197v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0808.0549v5</id>
    <updated>2010-02-26T07:41:18Z</updated>
    <published>2008-08-05T04:27:00Z</published>
    <title>Resource Allocation of MU-OFDM Based Cognitive Radio Systems Under
  Partial Channel State Information</title>
    <summary>  This paper has been withdrawn by the author due to some errors.
</summary>
    <author>
      <name>Dong Huang</name>
    </author>
    <author>
      <name>Chunyan Miao</name>
    </author>
    <author>
      <name>Cyril Leung</name>
    </author>
    <author>
      <name>Zhiqi Shen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn</arxiv:comment>
    <link href="http://arxiv.org/abs/0808.0549v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0808.0549v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1107.1564v3</id>
    <updated>2014-03-12T07:08:13Z</updated>
    <published>2011-07-08T06:26:03Z</published>
    <title>Polyceptron: A Polyhedral Learning Algorithm</title>
    <summary>  In this paper we propose a new algorithm for learning polyhedral classifiers
which we call as Polyceptron. It is a Perception like algorithm which updates
the parameters only when the current classifier misclassifies any training
data. We give both batch and online version of Polyceptron algorithm. Finally
we give experimental results to show the effectiveness of our approach.
</summary>
    <author>
      <name>Naresh Manwani</name>
    </author>
    <author>
      <name>P. S. Sastry</name>
    </author>
    <link href="http://arxiv.org/abs/1107.1564v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.1564v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.5643v1</id>
    <updated>2011-08-29T16:36:05Z</updated>
    <published>2011-08-29T16:36:05Z</published>
    <title>Collective Adaptive Systems: Challenges Beyond Evolvability</title>
    <summary>  This position paper overviews several challenges of collective adaptive
systems, which are beyond the research objectives of current top-projects in
ICT, and especially in FET, initiatives. The attention is paid not only to
challenges and new research topics, but also to their impact and potential
breakthroughs in information and communication technologies.
</summary>
    <author>
      <name>Serge Kernbach</name>
    </author>
    <author>
      <name>Thomas Schmickl</name>
    </author>
    <author>
      <name>Jon Timmis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Workshop "Fundamentals of Collective Adaptive Systems", European
  Commission, 3-4 November, 2009, Brussels</arxiv:comment>
    <link href="http://arxiv.org/abs/1108.5643v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.5643v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.5340v1</id>
    <updated>2012-08-27T08:58:46Z</updated>
    <published>2012-08-27T08:58:46Z</published>
    <title>New results of ant algorithms for the Linear Ordering Problem</title>
    <summary>  Ant-based algorithms are successful tools for solving complex problems. One
of these problems is the Linear Ordering Problem (LOP). The paper shows new
results on some LOP instances, using Ant Colony System (ACS) and the Step-Back
Sensitive Ant Model (SB-SAM).
</summary>
    <author>
      <name>Camelia-M. Pintea</name>
    </author>
    <author>
      <name>Camelia Chira</name>
    </author>
    <author>
      <name>D. Dumitrescu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures Zbl:06048718</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">An. Univ. Vest Timis., Ser. Mat.-Inform. 48, No. 3, 139-150 (2010)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1208.5340v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.5340v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.6031v1</id>
    <updated>2013-02-25T10:13:09Z</updated>
    <published>2013-02-25T10:13:09Z</published>
    <title>Phoneme discrimination using KS algebra I</title>
    <summary>  In our work we define a new algebra of operators as a substitute for fuzzy
logic. Its primary purpose is for construction of binary discriminators for
phonemes based on spectral content. It is optimized for design of
non-parametric computational circuits, and makes uses of 4 operations: $\min$,
$\max$, the difference and generalized additively homogenuous means.
</summary>
    <author>
      <name>Ondrej Such</name>
    </author>
    <link href="http://arxiv.org/abs/1302.6031v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.6031v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; I.5.2; I.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.5853v4</id>
    <updated>2014-02-18T21:35:13Z</updated>
    <published>2013-12-20T08:45:07Z</published>
    <title>Multi-GPU Training of ConvNets</title>
    <summary>  In this work we evaluate different approaches to parallelize computation of
convolutional neural networks across several GPUs.
</summary>
    <author>
      <name>Omry Yadan</name>
    </author>
    <author>
      <name>Keith Adams</name>
    </author>
    <author>
      <name>Yaniv Taigman</name>
    </author>
    <author>
      <name>Marc'Aurelio Ranzato</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Machine Learning, Deep Learning, Convolutional Networks, Computer
  Vision, GPU, CUDA</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.5853v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.5853v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.7351v1</id>
    <updated>2014-02-28T19:12:50Z</updated>
    <published>2014-02-28T19:12:50Z</published>
    <title>A Machine Learning Model for Stock Market Prediction</title>
    <summary>  Stock market prediction is the act of trying to determine the future value of
a company stock or other financial instrument traded on a financial exchange.
</summary>
    <author>
      <name>Osman Hegazy</name>
    </author>
    <author>
      <name>Omar S. Soliman</name>
    </author>
    <author>
      <name>Mustafa Abdul Salam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 Pages. arXiv admin note: substantial text overlap with
  arXiv:1402.6366</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science and Telecommunications
  [Volume 4, Issue 12, December 2013]</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1402.7351v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.7351v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.0968v1</id>
    <updated>2014-06-04T08:25:56Z</updated>
    <published>2014-06-04T08:25:56Z</published>
    <title>Integration of a Predictive, Continuous Time Neural Network into
  Securities Market Trading Operations</title>
    <summary>  This paper describes recent development and test implementation of a
continuous time recurrent neural network that has been configured to predict
rates of change in securities. It presents outcomes in the context of popular
technical analysis indicators and highlights the potential impact of continuous
predictive capability on securities market trading operations.
</summary>
    <author>
      <name>Christopher S Kirk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.0968v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.0968v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.CP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.CP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.3191v2</id>
    <updated>2014-12-14T03:18:33Z</updated>
    <published>2014-12-10T04:06:38Z</published>
    <title>Bach in 2014: Music Composition with Recurrent Neural Network</title>
    <summary>  We propose a framework for computer music composition that uses resilient
propagation (RProp) and long short term memory (LSTM) recurrent neural network.
In this paper, we show that LSTM network learns the structure and
characteristics of music pieces properly by demonstrating its ability to
recreate music. We also show that predicting existing music using RProp
outperforms Back propagation through time (BPTT).
</summary>
    <author>
      <name>I-Ting Liu</name>
    </author>
    <author>
      <name>Bhiksha Ramakrishnan</name>
    </author>
    <link href="http://arxiv.org/abs/1412.3191v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.3191v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.6061v1</id>
    <updated>2014-12-15T06:55:28Z</updated>
    <published>2014-12-15T06:55:28Z</published>
    <title>CITlab ARGUS for Arabic Handwriting</title>
    <summary>  In the recent years it turned out that multidimensional recurrent neural
networks (MDRNN) perform very well for offline handwriting recognition tasks
like the OpenHaRT 2013 evaluation DIR. With suitable writing preprocessing and
dictionary lookup, our ARGUS software completed this task with an error rate of
26.27% in its primary setup.
</summary>
    <author>
      <name>Gundram Leifert</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Rostock - CITlab</arxiv:affiliation>
    </author>
    <author>
      <name>Roger Labahn</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Rostock - CITlab</arxiv:affiliation>
    </author>
    <author>
      <name>Tobias Strauß</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Rostock - CITlab</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">http://www.nist.gov/itl/iad/mig/upload/OpenHaRT2013_SysDesc_CITLAB.pdf</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.6061v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.6061v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T10, 68T05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.04163v1</id>
    <updated>2015-02-14T03:23:53Z</updated>
    <published>2015-02-14T03:23:53Z</published>
    <title>A Distributional Representation Model For Collaborative Filtering</title>
    <summary>  In this paper, we propose a very concise deep learning approach for
collaborative filtering that jointly models distributional representation for
users and items. The proposed framework obtains better performance when
compared against current state-of-art algorithms and that made the
distributional representation model a promising direction for further research
in the collaborative filtering.
</summary>
    <author>
      <name>Zhang Junlin</name>
    </author>
    <author>
      <name>Cai Heng</name>
    </author>
    <author>
      <name>Huang Tongwen</name>
    </author>
    <author>
      <name>Xue Huiping</name>
    </author>
    <link href="http://arxiv.org/abs/1502.04163v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.04163v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.06410v1</id>
    <updated>2015-03-22T11:32:34Z</updated>
    <published>2015-03-22T11:32:34Z</published>
    <title>What the F-measure doesn't measure: Features, Flaws, Fallacies and Fixes</title>
    <summary>  The F-measure or F-score is one of the most commonly used single number
measures in Information Retrieval, Natural Language Processing and Machine
Learning, but it is based on a mistake, and the flawed assumptions render it
unsuitable for use in most contexts! Fortunately, there are better
alternatives.
</summary>
    <author>
      <name>David M. W. Powers</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.06410v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.06410v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.07571v1</id>
    <updated>2015-04-28T17:08:18Z</updated>
    <published>2015-04-28T17:08:18Z</published>
    <title>Can Machines Truly Think</title>
    <summary>  Can machines truly think? This question and its answer have many implications
that depend, in large part, on any number of assumptions underlying how the
issue has been addressed or considered previously. A crucial question, and one
that is almost taken for granted, is the starting point for this discussion:
Can "thought" be achieved or emulated by algorithmic procedures?
</summary>
    <author>
      <name>Murat Okandan</name>
    </author>
    <link href="http://arxiv.org/abs/1504.07571v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.07571v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.08215v1</id>
    <updated>2015-04-30T13:26:46Z</updated>
    <published>2015-04-30T13:26:46Z</published>
    <title>Lateral Connections in Denoising Autoencoders Support Supervised
  Learning</title>
    <summary>  We show how a deep denoising autoencoder with lateral connections can be used
as an auxiliary unsupervised learning task to support supervised learning. The
proposed model is trained to minimize simultaneously the sum of supervised and
unsupervised cost functions by back-propagation, avoiding the need for
layer-wise pretraining. It improves the state of the art significantly in the
permutation-invariant MNIST classification task.
</summary>
    <author>
      <name>Antti Rasmus</name>
    </author>
    <author>
      <name>Harri Valpola</name>
    </author>
    <author>
      <name>Tapani Raiko</name>
    </author>
    <link href="http://arxiv.org/abs/1504.08215v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.08215v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.05053v1</id>
    <updated>2015-07-17T17:48:49Z</updated>
    <published>2015-07-17T17:48:49Z</published>
    <title>Massively Deep Artificial Neural Networks for Handwritten Digit
  Recognition</title>
    <summary>  Greedy Restrictive Boltzmann Machines yield an fairly low 0.72% error rate on
the famous MNIST database of handwritten digits. All that was required to
achieve this result was a high number of hidden layers consisting of many
neurons, and a graphics card to greatly speed up the rate of learning.
</summary>
    <author>
      <name>Keiron O'Shea</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.05053v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.05053v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.07374v1</id>
    <updated>2015-07-27T11:50:44Z</updated>
    <published>2015-07-27T11:50:44Z</published>
    <title>A genetic algorithm for autonomous navigation in partially observable
  domain</title>
    <summary>  The problem of autonomous navigation is one of the basic problems for
robotics. Although, in general, it may be challenging when an autonomous
vehicle is placed into partially observable domain. In this paper we consider
simplistic environment model and introduce a navigation algorithm based on
Learning Classifier System.
</summary>
    <author>
      <name>Maxim Borisyak</name>
    </author>
    <author>
      <name>Andrey Ustyuzhanin</name>
    </author>
    <link href="http://arxiv.org/abs/1507.07374v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.07374v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.07285v1</id>
    <updated>2016-03-23T17:52:21Z</updated>
    <published>2016-03-23T17:52:21Z</published>
    <title>A guide to convolution arithmetic for deep learning</title>
    <summary>  We introduce a guide to help deep learning practitioners understand and
manipulate convolutional neural network architectures. The guide clarifies the
relationship between various properties (input shape, kernel shape, zero
padding, strides and output shape) of convolutional, pooling and transposed
convolutional layers, as well as the relationship between convolutional and
transposed convolutional layers. Relationships are derived for various cases,
and are illustrated in order to make them intuitive.
</summary>
    <author>
      <name>Vincent Dumoulin</name>
    </author>
    <author>
      <name>Francesco Visin</name>
    </author>
    <link href="http://arxiv.org/abs/1603.07285v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.07285v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.08776v2</id>
    <updated>2016-05-19T11:58:22Z</updated>
    <published>2016-03-29T14:10:14Z</published>
    <title>COCO: The Experimental Procedure</title>
    <summary>  We present a budget-free experimental setup and procedure for benchmarking
numericaloptimization algorithms in a black-box scenario. This procedure can be
applied with the COCO benchmarking platform. We describe initialization of and
input to the algorithm and touch upon therelevance of termination and restarts.
</summary>
    <author>
      <name>Nikolaus Hansen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Inria</arxiv:affiliation>
    </author>
    <author>
      <name>Tea Tusar</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Inria</arxiv:affiliation>
    </author>
    <author>
      <name>Olaf Mersmann</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Inria</arxiv:affiliation>
    </author>
    <author>
      <name>Anne Auger</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Inria</arxiv:affiliation>
    </author>
    <author>
      <name>Dimo Brockhoff</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Inria</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ArXiv e-prints, arXiv:1603.08776</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.08776v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.08776v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.00404v1</id>
    <updated>2016-05-02T09:33:46Z</updated>
    <published>2016-05-02T09:33:46Z</published>
    <title>Simple2Complex: Global Optimization by Gradient Descent</title>
    <summary>  A method named simple2complex for modeling and training deep neural networks
is proposed. Simple2complex train deep neural networks by smoothly adding more
and more layers to the shallow networks, as the learning procedure going on,
the network is just like growing. Compared with learning by end2end,
simple2complex is with less possibility trapping into local minimal, namely,
owning ability for global optimization. Cifar10 is used for verifying the
superiority of simple2complex.
</summary>
    <author>
      <name>Ming Li</name>
    </author>
    <link href="http://arxiv.org/abs/1605.00404v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.00404v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
