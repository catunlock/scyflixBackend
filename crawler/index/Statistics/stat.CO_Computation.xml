<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Astat.CO%26id_list%3D%26start%3D0%26max_results%3D500" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:stat.CO&amp;id_list=&amp;start=0&amp;max_results=500</title>
  <id>http://arxiv.org/api/i9VO1kv8ndHyWQCpeWOYUXRJAb8</id>
  <updated>2017-10-08T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">3046</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">500</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/0905.4131v1</id>
    <updated>2009-05-26T08:20:50Z</updated>
    <published>2009-05-26T08:20:50Z</published>
    <title>Maximum Likelihood Estimation for Markov Chains</title>
    <summary>  A new approach for optimal estimation of Markov chains with sparse transition
matrices is presented.
</summary>
    <author>
      <name>Iuliana Teodorescu</name>
    </author>
    <link href="http://arxiv.org/abs/0905.4131v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0905.4131v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1011.1745v1</id>
    <updated>2010-11-08T10:01:57Z</updated>
    <published>2010-11-08T10:01:57Z</published>
    <title>Online Expectation-Maximisation</title>
    <summary>  Tutorial chapter on the Online EM algorithm to appear in the volume
'Mixtures' edited by Kerrie Mengersen, Mike Titterington and Christian P.
Robert.
</summary>
    <author>
      <name>Olivier Cappé</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LTCI</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1011.1745v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.1745v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.4148v1</id>
    <updated>2012-04-18T17:46:32Z</updated>
    <published>2012-04-18T17:46:32Z</published>
    <title>Algorithm for multivariate data standardization up to third moment</title>
    <summary>  An algorithm for transforming multivariate data to a form with normalized
first, second and third moments is presented.
</summary>
    <author>
      <name>Vadim Asnin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.4148v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.4148v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.6684v1</id>
    <updated>2013-06-28T00:10:19Z</updated>
    <published>2013-06-28T00:10:19Z</published>
    <title>Supplement to "Markov Chain Monte Carlo Based on Deterministic
  Transformations"</title>
    <summary>  This is a supplement to the article "Markov Chain Monte Carlo Based on
Deterministic Transformations" available at http://arxiv.org/abs/1106.5850
</summary>
    <author>
      <name>Somak Dutta</name>
    </author>
    <author>
      <name>Sourabh Bhattacharya</name>
    </author>
    <link href="http://arxiv.org/abs/1306.6684v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.6684v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.5098v1</id>
    <updated>2007-10-26T14:28:08Z</updated>
    <published>2007-10-26T14:28:08Z</published>
    <title>Particle Filters for Multiscale Diffusions</title>
    <summary>  We consider multiscale stochastic systems that are partially observed at
discrete points of the slow time scale. We introduce a particle filter that
takes advantage of the multiscale structure of the system to efficiently
approximate the optimal filter.
</summary>
    <author>
      <name>Anastasia Papavasiliou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear in ESAIM Proceedings (Workshop on Sequential Monte Carlo
  Methods: filtering and other applications, Oxford, 2006)</arxiv:comment>
    <link href="http://arxiv.org/abs/0710.5098v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.5098v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0804.1392v2</id>
    <updated>2009-01-30T17:34:22Z</updated>
    <published>2008-04-09T02:07:36Z</published>
    <title>Coverage Probability of Wald Interval for Binomial Parameters</title>
    <summary>  In this paper, we develop an exact method for computing the minimum coverage
probability of Wald interval for estimation of binomial parameters. Similar
approach can be used for other type of confidence intervals.
</summary>
    <author>
      <name>Xinjia Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, no figure</arxiv:comment>
    <link href="http://arxiv.org/abs/0804.1392v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0804.1392v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0804.1393v2</id>
    <updated>2009-01-30T17:38:08Z</updated>
    <published>2008-04-09T02:14:49Z</published>
    <title>Optimal Explicit Binomial Confidence Interval with Guaranteed Coverage
  Probability</title>
    <summary>  In this paper, we develop an approach for optimizing the explicit binomial
confidence interval recently derived by Chen et al. The optimization reduces
conservativeness while guaranteeing prescribed coverage probability.
</summary>
    <author>
      <name>Xinjia Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, no figure</arxiv:comment>
    <link href="http://arxiv.org/abs/0804.1393v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0804.1393v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0902.4117v1</id>
    <updated>2009-02-24T11:02:31Z</updated>
    <published>2009-02-24T11:02:31Z</published>
    <title>A Gibbs Sampling Alternative to Reversible Jump MCMC</title>
    <summary>  This note presents a simple and elegant sampler which could be used as an
alternative to the reversible jump MCMC methodology.
</summary>
    <author>
      <name>Stephen G. Walker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to the Electronic Journal of Statistics
  (http://www.i-journals.org/ejs/) by the Institute of Mathematical Statistics
  (http://www.imstat.org)</arxiv:comment>
    <link href="http://arxiv.org/abs/0902.4117v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0902.4117v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.2355v1</id>
    <updated>2010-05-13T15:53:10Z</updated>
    <published>2010-05-13T15:53:10Z</published>
    <title>On a Multiplicative Algorithm for Computing Bayesian D-optimal Designs</title>
    <summary>  We use the minorization-maximization principle (Lange, Hunter and Yang 2000)
to establish the monotonicity of a multiplicative algorithm for computing
Bayesian D-optimal designs. This proves a conjecture of Dette, Pepelyshev and
Zhigljavsky (2008).
</summary>
    <author>
      <name>Yaming Yu</name>
    </author>
    <link href="http://arxiv.org/abs/1005.2355v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.2355v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.3254v1</id>
    <updated>2011-09-15T03:48:23Z</updated>
    <published>2011-09-15T03:48:23Z</published>
    <title>Rigorous Computing of Rectangle Scan Probabilities for Markov Increments</title>
    <summary>  Extending recent work of Corrado, we derive an algorithm that computes
rigorous upper and lower bounds for rectangle scan probabilities for Markov
increments. We experimentally examine the closeness of the bounds computed by
the algorithm and we examine the range of tractable input variables.
</summary>
    <author>
      <name>Jannis Dimitriadis</name>
    </author>
    <link href="http://arxiv.org/abs/1109.3254v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.3254v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.1893v1</id>
    <updated>2012-01-09T20:24:52Z</updated>
    <published>2012-01-09T20:24:52Z</published>
    <title>Discussions on Fernhead and Prangle (2012)</title>
    <summary>  Two contributions to the discussion of Fearnhead P. and D. Prangle (2012).
Constructing summary statistics for approximate Bayesian computation:
Semi-automatic approx- imate Bayesian computation, J. Roy. Statist. Soc. B, 74
(3).
</summary>
    <author>
      <name>D. J. Nott</name>
    </author>
    <author>
      <name>Y. Fan</name>
    </author>
    <author>
      <name>S. A. Sisson</name>
    </author>
    <link href="http://arxiv.org/abs/1201.1893v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.1893v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.03386v1</id>
    <updated>2015-01-14T16:04:05Z</updated>
    <published>2015-01-14T16:04:05Z</published>
    <title>Discussion of "Sequential Quasi-Monte Carlo" by Mathieu Gerber and
  Nicolas Chopin</title>
    <summary>  A discussion on the possibility of reducing the variance of quasi-Monte Carlo
estimators in applications. Further details are provided in the accompanying
paper "Variance Reduction for Quasi-Monte Carlo".
</summary>
    <author>
      <name>Chris. J. Oates</name>
    </author>
    <author>
      <name>Daniel Simpson</name>
    </author>
    <author>
      <name>Mark Girolami</name>
    </author>
    <link href="http://arxiv.org/abs/1501.03386v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.03386v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.06336v1</id>
    <updated>2015-07-22T20:40:55Z</updated>
    <published>2015-07-22T20:40:55Z</published>
    <title>Hessian corrections to the Metropolis Adjusted Langevin Algorithm</title>
    <summary>  A natural method for the introduction of second-order derivatives of the log
likelihood into MCMC algorithms is introduced, based on Taylor expansion of the
Langevin equation followed by exact solution of the truncated system.
</summary>
    <author>
      <name>Thomas House</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.06336v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.06336v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.04923v1</id>
    <updated>2015-10-16T15:42:25Z</updated>
    <published>2015-10-16T15:42:25Z</published>
    <title>Simpler Online Updates for Arbitrary-Order Central Moments</title>
    <summary>  Statistical moments are widely used in descriptive statistics. Therefore
efficient and numerically stable implementations are important in practice.
Pebay [1] derives online update formulas for arbitrary-order central moments.
We present a simpler version that is also easier to implement.
</summary>
    <author>
      <name>Xiangrui Meng</name>
    </author>
    <link href="http://arxiv.org/abs/1510.04923v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.04923v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.08102v1</id>
    <updated>2016-04-27T15:07:48Z</updated>
    <published>2016-04-27T15:07:48Z</published>
    <title>An ABC interpretation of the multiple auxiliary variable method</title>
    <summary>  We show that the auxiliary variable method (M{\o}ller et al., 2006; Murray et
al., 2006) for inference of Markov random fields can be viewed as an
approximate Bayesian computation method for likelihood estimation.
</summary>
    <author>
      <name>Dennis Prangle</name>
    </author>
    <author>
      <name>Richard G. Everitt</name>
    </author>
    <link href="http://arxiv.org/abs/1604.08102v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.08102v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.04182v1</id>
    <updated>2016-06-14T00:50:30Z</updated>
    <published>2016-06-14T00:50:30Z</published>
    <title>KoulMde: An R Package for Koul's Minimum Distance Estimation</title>
    <summary>  This article provides a full description of the R package KoulMde which is
designed for Koul's minimum distance estimation method. When we encounter
estimation problems in the linear regression and autogressive models, this
package provides more efficient estimators than other R packages.
</summary>
    <author>
      <name>Jiwoong Kim</name>
    </author>
    <link href="http://arxiv.org/abs/1606.04182v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04182v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.05615v1</id>
    <updated>2016-09-19T07:24:21Z</updated>
    <published>2016-09-19T07:24:21Z</published>
    <title>Discussion of "Fast Approximate Inference for Arbitrarily Large
  Semiparametric Regression Models via Message Passing"</title>
    <summary>  Discussion paper on "Fast Approximate Inference for Arbitrarily Large
Semiparametric Regression Models via Message Passing" by Wand
[arXiv:1602.07412].
</summary>
    <author>
      <name>Dustin Tran</name>
    </author>
    <author>
      <name>David M. Blei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Journal of the American Statistical Association</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.05615v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.05615v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.08251v1</id>
    <updated>2017-02-27T12:12:46Z</updated>
    <published>2017-02-27T12:12:46Z</published>
    <title>Hessian corrections to Hybrid Monte Carlo</title>
    <summary>  A method for the introduction of second-order derivatives of the log
likelihood into HMC algorithms is introduced, which does not require the
Hessian to be evaluated at each leapfrog step but only at the start and end of
trajectories.
</summary>
    <author>
      <name>Thomas House</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.08251v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.08251v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.00554v1</id>
    <updated>2017-05-01T14:59:56Z</updated>
    <published>2017-05-01T14:59:56Z</published>
    <title>A weaker structural Markov property for decomposable graph laws</title>
    <summary>  We present a new kind of structural Markov property for probabilistic laws on
decomposable graphs, prove the equivalence of an exponential family assumption,
and discuss identifiability, modelling and computational implications.
</summary>
    <author>
      <name>Peter J Green</name>
    </author>
    <author>
      <name>Alun Thomas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.00554v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.00554v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62H05, 05C80, 05C90, 68T30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0711.2893v2</id>
    <updated>2008-02-13T09:05:18Z</updated>
    <published>2007-11-19T10:36:53Z</published>
    <title>The Method of Normalized Correlations - A Fast Alternative to Maximum
  Likelihood Estimation for Random Processes and Isotropic Random Fields with
  Short-Range Dependence</title>
    <summary>  This paper has been withdrawn by the authors, due the copyright policy of the
journal it has been submited to.
</summary>
    <author>
      <name>Milan Zukovic</name>
    </author>
    <author>
      <name>Dionissios T. Hristopulos</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1198/TECH.2009.0018</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1198/TECH.2009.0018" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Technometrics 51 173 (2009)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0711.2893v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0711.2893v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0805.3079v1</id>
    <updated>2008-05-20T13:38:03Z</updated>
    <published>2008-05-20T13:38:03Z</published>
    <title>A note on the ABC-PRC algorithm of Sissons et al. (2007)</title>
    <summary>  This note describes the results of some tests of the ABC-PRC algorithm of
Sissons et al. (PNAS, 2007), and demonstrates with a toy example that the
method does not converge on the true posterior distribution.
</summary>
    <author>
      <name>Mark A. Beaumont</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This note was circulated to colleagues 11 October 2007. The PDF
  document is 14 pages. There are 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0805.3079v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0805.3079v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0805.3602v2</id>
    <updated>2009-02-13T14:47:14Z</updated>
    <published>2008-05-23T09:34:11Z</published>
    <title>Marginal Likelihood Integrals for Mixtures of Independence Models</title>
    <summary>  Inference in Bayesian statistics involves the evaluation of marginal
likelihood integrals. We present algebraic algorithms for computing such
integrals exactly for discrete data of small sample size. Our methods apply to
both uniform priors and Dirichlet priors. The underlying statistical models are
mixtures of independent distributions, or, in geometric language, secant
varieties of Segre-Veronese varieties.
</summary>
    <author>
      <name>Shaowei Lin</name>
    </author>
    <author>
      <name>Bernd Sturmfels</name>
    </author>
    <author>
      <name>Zhiqiang Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages. Journal of Machine Learning Research, to appear</arxiv:comment>
    <link href="http://arxiv.org/abs/0805.3602v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0805.3602v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0807.4719v2</id>
    <updated>2016-02-12T16:43:13Z</updated>
    <published>2008-07-29T18:37:17Z</published>
    <title>On an Auxiliary Function for Log-Density Estimation</title>
    <summary>  In this note we provide explicit expressions and expansions for a special
function which appears in nonparametric estimation of log-densities. This
function returns the integral of a log-linear function on a simplex of
arbitrary dimension. In particular it is used in the R-package "LogCondDEAD" by
Cule et al. (2007).
</summary>
    <author>
      <name>Madeleine L. Cule</name>
    </author>
    <author>
      <name>Lutz Duembgen</name>
    </author>
    <link href="http://arxiv.org/abs/0807.4719v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0807.4719v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.4627v2</id>
    <updated>2011-08-27T19:19:12Z</updated>
    <published>2008-09-26T13:31:13Z</published>
    <title>Solving the 100 Swiss Francs Problem</title>
    <summary>  Sturmfels offered 100 Swiss Francs in 2005 to a conjecture, which deals with
a special case of the maximum likelihood estimation for a latent class model.
This paper confirms the conjecture positively.
</summary>
    <author>
      <name>Mingfu Zhu</name>
    </author>
    <author>
      <name>Guangran Jiang</name>
    </author>
    <author>
      <name>Shuhong Gao</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11786-011-0068-3</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11786-011-0068-3" rel="related"/>
    <link href="http://arxiv.org/abs/0809.4627v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.4627v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.AG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65H10 (Primary), 62P10, 62F30 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.1835v1</id>
    <updated>2009-07-10T14:37:58Z</updated>
    <published>2009-07-10T14:37:58Z</published>
    <title>Information geometry for testing pseudorandom number generators</title>
    <summary>  The information geometry of the 2-manifold of gamma probability density
functions provides a framework in which pseudorandom number generators may be
evaluated using a neighbourhood of the curve of exponential density functions.
The process is illustrated using the pseudorandom number generator in
Mathematica. This methodology may be useful to add to the current family of
test procedures in real applications to finite sampling data.
</summary>
    <author>
      <name>C. T. J. Dodson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages,</arxiv:comment>
    <link href="http://arxiv.org/abs/0907.1835v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.1835v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.1997v2</id>
    <updated>2011-10-27T11:53:22Z</updated>
    <published>2009-07-12T13:20:32Z</published>
    <title>Statistical estimation requires unbounded memory</title>
    <summary>  We investigate the existence of bounded-memory consistent estimators of
various statistical functionals. This question is resolved in the negative in a
rather strong sense. We propose various bounded-memory approximations, using
techniques from automata theory and stochastic processes. Some questions of
potential interest are raised for future work.
</summary>
    <author>
      <name> Leonid</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Aryeh</arxiv:affiliation>
    </author>
    <author>
      <name> Kontorovich</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">this is an old version, with a mistake in the proof of Thm. 6.1.
  Please see my homepage for an updated version</arxiv:comment>
    <link href="http://arxiv.org/abs/0907.1997v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.1997v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.4010v1</id>
    <updated>2009-07-23T09:07:22Z</updated>
    <published>2009-07-23T09:07:22Z</published>
    <title>Simulation of truncated normal variables</title>
    <summary>  We provide in this paper simulation algorithms for one-sided and two-sided
truncated normal distributions. These algorithms are then used to simulate
multivariate normal variables with restricted parameter space for any
covariance structure.
</summary>
    <author>
      <name>Christian P. Robert</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/BF00143942</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/BF00143942" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This 1992 paper appeared in 1995 in Statistics and Computing and the
  gist of it is contained in Monte Carlo Statistical Methods (2004), but I
  receive weekly requests for reprints so here it is!</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Statistics and Computing, 1995, 5, 121-125</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0907.4010v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.4010v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.5123v1</id>
    <updated>2009-07-29T13:33:51Z</updated>
    <published>2009-07-29T13:33:51Z</published>
    <title>Computational methods for Bayesian model choice</title>
    <summary>  In this note, we shortly survey some recent approaches on the approximation
of the Bayes factor used in Bayesian hypothesis testing and in Bayesian model
choice. In particular, we reassess importance sampling, harmonic mean sampling,
and nested sampling from a unified perspective.
</summary>
    <author>
      <name>Christian P. Robert</name>
    </author>
    <author>
      <name>Darren Wraith</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1063/1.3275622</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1063/1.3275622" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 4 figures, submitted to the proceedings of MaxEnt 2009,
  July 05-10, 2009, to be published by the American Institute of Physics</arxiv:comment>
    <link href="http://arxiv.org/abs/0907.5123v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.5123v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.0389v1</id>
    <updated>2009-09-02T11:25:53Z</updated>
    <published>2009-09-02T11:25:53Z</published>
    <title>Monte Carlo Methods in Statistics</title>
    <summary>  Monte Carlo methods are now an essential part of the statistician's toolbox,
to the point of being more familiar to graduate students than the measure
theoretic notions upon which they are based! We recall in this note some of the
advances made in the design of Monte Carlo techniques towards their use in
Statistics, referring to Robert and Casella (2004,2010) for an in-depth
coverage.
</summary>
    <author>
      <name>Christian P. Robert</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Entry submitted to the International Handbook of Statistical Methods</arxiv:comment>
    <link href="http://arxiv.org/abs/0909.0389v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.0389v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.4129v1</id>
    <updated>2009-09-23T03:47:31Z</updated>
    <published>2009-09-23T03:47:31Z</published>
    <title>Efficient Simulation of a Bivariate Exponential Conditionals
  Distribution</title>
    <summary>  The bivariate distribution with exponential conditionals (BEC) is introduced
by Arnold and Strauss [Bivariate distributions with exponential conditionals,
J. Amer. Statist. Assoc. 83 (1988) 522--527]. This work presents a simple and
fast algorithm for simulating random variates from this density.
</summary>
    <author>
      <name>Yaming Yu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.csda.2007.10.013</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.csda.2007.10.013" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">only 5 pages!</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computational Statistics &amp; Data Analysis 52 (2008) 2273-2276</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0909.4129v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.4129v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0910.1819v1</id>
    <updated>2009-10-09T19:15:42Z</updated>
    <published>2009-10-09T19:15:42Z</published>
    <title>Importance Sampling for rare events and conditioned random walks</title>
    <summary>  This paper introduces a new Importance Sampling scheme, called Adaptive
Twisted Importance Sampling, which is adequate for the improved estimation of
rare event probabilities in he range of moderate deviations pertaining to the
empirical mean of real i.i.d. summands. It is based on a sharp approximation of
the density of long runs extracted from a random walk conditioned on its end
value.
</summary>
    <author>
      <name>Michel Broniatowski</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LSTA</arxiv:affiliation>
    </author>
    <author>
      <name>Ya'Acov Ritov</name>
    </author>
    <link href="http://arxiv.org/abs/0910.1819v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0910.1819v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0911.0412v2</id>
    <updated>2009-11-09T22:41:21Z</updated>
    <published>2009-11-02T21:33:22Z</published>
    <title>Probability matrices, non-negative rank, and parameterizations of
  mixture models</title>
    <summary>  In this paper we parameterize non-negative matrices of sum one and rank at
most two. More precisely, we give a family of parameterizations using the least
possible number of parameters. We also show how these parameterizations relate
to a class of statistical models, known in Probability and Statistics as
mixture models for contingency tables.
</summary>
    <author>
      <name>Enrico Carlini</name>
    </author>
    <author>
      <name>Fabio Rapallo</name>
    </author>
    <link href="http://arxiv.org/abs/0911.0412v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.0412v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0911.0985v1</id>
    <updated>2009-11-05T07:10:03Z</updated>
    <published>2009-11-05T07:10:03Z</published>
    <title>Comments on "Particle Markov chain Monte Carlo" by C. Andrieu, A.
  Doucet, and R. Hollenstein</title>
    <summary>  This is the compilation of our comments submitted to the Journal of the Royal
Statistical Society, Series B, to be published within the discussion of the
Read Paper of Andrieu, Doucet and Hollenstein.
</summary>
    <author>
      <name>Pierre Jacob</name>
    </author>
    <author>
      <name>Nicolas Chopin</name>
    </author>
    <author>
      <name>Christian P. Robert</name>
    </author>
    <author>
      <name>Havard Rue</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0911.0985v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.0985v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.1297v1</id>
    <updated>2010-01-08T15:05:01Z</updated>
    <published>2010-01-08T15:05:01Z</published>
    <title>BSA - exact algorithm computing LTS estimate</title>
    <summary>  The main result of this paper is a new exact algorithm computing the estimate
given by the Least Trimmed Squares (LTS). The algorithm works under very weak
assumptions. To prove that, we study the respective objective function using
basic techniques of analysis and linear algebra.
</summary>
    <author>
      <name>Karel Klouda</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.csda.2014.11.001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.csda.2014.11.001" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 1 figure</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computational Statistics &amp; Data Analysis, 84 (2015), 27-40</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1001.1297v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.1297v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.2797v1</id>
    <updated>2010-01-16T00:55:16Z</updated>
    <published>2010-01-16T00:55:16Z</published>
    <title>Adaptive Gibbs samplers</title>
    <summary>  We consider various versions of adaptive Gibbs and Metropolis within-Gibbs
samplers, which update their selection probabilities (and perhaps also their
proposal distributions) on the fly during a run, by learning as they go in an
attempt to optimise the algorithm. We present a cautionary example of how even
a simple-seeming adaptive Gibbs sampler may fail to converge. We then present
various positive results guaranteeing convergence of adaptive Gibbs samplers
under certain conditions.
</summary>
    <author>
      <name>Krzysztof Latuszynski</name>
    </author>
    <author>
      <name>Jeffrey S. Rosenthal</name>
    </author>
    <link href="http://arxiv.org/abs/1001.2797v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.2797v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1010.6098v1</id>
    <updated>2010-10-28T21:25:31Z</updated>
    <published>2010-10-28T21:25:31Z</published>
    <title>Maximum Likelihood Estimation of Nonnegative Trigonometric Sum Models
  Using a Newton-like Algorithm on Manifolds</title>
    <summary>  In Fern\'andez-Dur\'an (2004), a new family of circular distributions based
on nonnegative trigonometric sums (NNTS models) is developed. Because the
parameter space of this family is the surface of the hypersphere, an efficient
Newton-like algorithm on manifolds is generated in order to obtain the maximum
likelihood estimates of the parameters.
</summary>
    <author>
      <name> Fernández-Durán</name>
    </author>
    <author>
      <name>J. J.</name>
    </author>
    <author>
      <name> Gregorio-Domínguez</name>
    </author>
    <author>
      <name>M. M</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1010.6098v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1010.6098v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="Primary 49M15, 62G07, secondary 49Q99" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1011.0175v1</id>
    <updated>2010-10-31T16:16:30Z</updated>
    <published>2010-10-31T16:16:30Z</published>
    <title>A Comparison of Methods for Computing Autocorrelation Time</title>
    <summary>  This paper describes four methods for estimating autocorrelation time and
evaluates these methods with a test set of seven series. Fitting an
autoregressive process appears to be the most accurate method of the four. An R
package is provided for extending the comparison to more methods and test
series.
</summary>
    <author>
      <name>Madeleine B. Thompson</name>
    </author>
    <link href="http://arxiv.org/abs/1011.0175v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.0175v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1011.0834v1</id>
    <updated>2010-11-03T09:56:21Z</updated>
    <published>2010-11-03T09:56:21Z</published>
    <title>Discussions on "Riemann manifold Langevin and Hamiltonian Monte Carlo
  methods"</title>
    <summary>  This is a collection of discussions of `Riemann manifold Langevin and
Hamiltonian Monte Carlo methods" by Girolami and Calderhead, to appear in the
Journal of the Royal Statistical Society, Series B.
</summary>
    <author>
      <name>Simon Barthelme</name>
    </author>
    <author>
      <name>Magali Beffy</name>
    </author>
    <author>
      <name>Nicolas Chopin</name>
    </author>
    <author>
      <name>Arnaud Doucet</name>
    </author>
    <author>
      <name>Pierre Jacob</name>
    </author>
    <author>
      <name>Adam M. Johansen</name>
    </author>
    <author>
      <name>Jean-Michel Marin</name>
    </author>
    <author>
      <name>Christian P. Robert</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, one figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1011.0834v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.0834v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1101.1136v1</id>
    <updated>2011-01-06T04:30:27Z</updated>
    <published>2011-01-06T04:30:27Z</published>
    <title>Marginal Likelihood Computation via Arrogance Sampling</title>
    <summary>  This paper describes a method for estimating the marginal likelihood or Bayes
factors of Bayesian models using non-parametric importance sampling ("arrogance
sampling"). This method can also be used to compute the normalizing constant of
probability distributions. Because the required inputs are samples from the
distribution to be normalized and the scaled density at those samples, this
method may be a convenient replacement for the harmonic mean estimator. The
method has been implemented in the open source R package margLikArrogance.
</summary>
    <author>
      <name>Benedict Escoto</name>
    </author>
    <link href="http://arxiv.org/abs/1101.1136v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1101.1136v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.6096v1</id>
    <updated>2011-03-31T06:42:26Z</updated>
    <published>2011-03-31T06:42:26Z</published>
    <title>Counting with Combined Splitting and Capture-Recapture Methods</title>
    <summary>  We apply the splitting method to three well-known counting problems, namely
3-SAT, random graphs with prescribed degrees, and binary contingency tables. We
present an enhanced version of the splitting method based on the
capture-recapture technique, and show by experiments the superiority of this
technique for SAT problems in terms of variance of the associated estimators,
and speed of the algorithms.
</summary>
    <author>
      <name>Paul Dupuis</name>
    </author>
    <author>
      <name>Bahar Kaynar</name>
    </author>
    <author>
      <name>Ad Ridder</name>
    </author>
    <author>
      <name>Reuven Rubinstein</name>
    </author>
    <author>
      <name>Radislav Vaisman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1103.6096v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.6096v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1104.0323v1</id>
    <updated>2011-04-02T17:11:16Z</updated>
    <published>2011-04-02T17:11:16Z</published>
    <title>Exact Enumeration and Sampling of Matrices with Specified Margins</title>
    <summary>  We describe a dynamic programming algorithm for exact counting and exact
uniform sampling of matrices with specified row and column sums. The algorithm
runs in polynomial time when the column sums are bounded. Binary or
non-negative integer matrices are handled. The method is distinguished by
applicability to non-regular margins, tractability on large matrices, and the
capacity for exact sampling.
</summary>
    <author>
      <name>Jeffrey W. Miller</name>
    </author>
    <author>
      <name>Matthew T. Harrison</name>
    </author>
    <link href="http://arxiv.org/abs/1104.0323v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.0323v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.1476v2</id>
    <updated>2012-09-07T09:31:45Z</updated>
    <published>2011-05-07T21:46:34Z</published>
    <title>EM algorithm and variants: an informal tutorial</title>
    <summary>  The expectation-maximization (EM) algorithm introduced by Dempster et al in
1977 is a very general method to solve maximum likelihood estimation problems.
In this informal report, we review the theory behind EM as well as a number of
EM variants, suggesting that beyond the current state of the art is an even
much wider territory still to be discovered.
</summary>
    <author>
      <name>Alexis Roche</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Unpublished</arxiv:comment>
    <link href="http://arxiv.org/abs/1105.1476v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.1476v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62-01" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.4823v1</id>
    <updated>2011-05-24T16:35:59Z</updated>
    <published>2011-05-24T16:35:59Z</published>
    <title>Simulation in Statistics</title>
    <summary>  Simulation has become a standard tool in statistics because it may be the
only tool available for analysing some classes of probabilistic models. We
review in this paper simulation tools that have been specifically derived to
address statistical challenges and, in particular, recent advances in the areas
of adaptive Markov chain Monte Carlo (MCMC) algorithms, and approximate
Bayesian calculation (ABC) algorithms.
</summary>
    <author>
      <name>Christian P. Robert</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University Paris-Dauphine, IUF, and CREST</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Draft of an advanced tutorial paper for the Proceedings of the 2011
  Winter Simulation Conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1105.4823v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.4823v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.2999v1</id>
    <updated>2011-08-15T14:07:59Z</updated>
    <published>2011-08-15T14:07:59Z</published>
    <title>Dual $φ$-divergences estimation in normal models</title>
    <summary>  A class of robust estimators which are obtained from dual representation of
$\phi$-divergences, are studied empirically for the normal location model.
Members of this class of estimators are compared, and it is found that they are
efficient at the true model and offer an attractive alternative to the maximum
likelihood, in term of robustness .
</summary>
    <author>
      <name>Mohamed Cherfi</name>
    </author>
    <link href="http://arxiv.org/abs/1108.2999v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.2999v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.5232v2</id>
    <updated>2012-10-07T21:21:51Z</updated>
    <published>2012-06-22T19:28:39Z</published>
    <title>Extending Monte Carlo Methods to Factor Graphs with Negative and Complex
  Factors</title>
    <summary>  The partition function of a factor graph can sometimes be accurately
estimated by Monte Carlo methods. In this paper, such methods are extended to
factor graphs with negative and complex factors.
</summary>
    <author>
      <name>Mehdi Molkaraie</name>
    </author>
    <author>
      <name>Hans-Andrea Loeliger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. IEEE Information Theory Workshop (ITW), Lausanne, Switzerland,
  Sept. 3-7, 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.5232v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.5232v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.2611v1</id>
    <updated>2012-07-11T12:25:48Z</updated>
    <published>2012-07-11T12:25:48Z</published>
    <title>A simple and numerical stable algorithm for solving the cone projection
  problem based on a Gram-Schmidt process</title>
    <summary>  We are presenting a simple and numerical stable algorithm for the solution of
the cone projection problem which is suitable for relative small data sets and
for simulation purposes needed for convexity tests. Not even one pseudo-inverse
matrix is computed because of a proper Gram-Schmidt orthonormalization process
that is used.
</summary>
    <author>
      <name>Demetris T. Christopoulos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1207.2611v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.2611v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.5359v1</id>
    <updated>2012-09-24T18:35:58Z</updated>
    <published>2012-09-24T18:35:58Z</published>
    <title>On Simulations from the Two-Parameter Poisson-Dirichlet Process and the
  Normalized Inverse-Gaussian Process</title>
    <summary>  In this paper, we develop simple, yet efficient, procedures for sampling
approximations of the two-Parameter Poisson-Dirichlet Process and the
normalized inverse-Gaussian process. We compare the efficiency of the new
approximations to the corresponding stick-breaking approximations, in which we
demonstrate a substantial improvement.
</summary>
    <author>
      <name>Luai Al Labadi</name>
    </author>
    <author>
      <name>Mahmoud Zarepour</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.5359v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.5359v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.6559v1</id>
    <updated>2013-01-28T14:40:52Z</updated>
    <published>2013-01-28T14:40:52Z</published>
    <title>Clustering Via Nonparametric Density Estimation: the R Package
  pdfCluster</title>
    <summary>  The R package pdfCluster performs cluster analysis based on a nonparametric
estimate of the density of the observed variables. After summarizing the main
aspects of the methodology, we describe the features and the usage of the
package, and finally illustrate its working with the aid of two datasets.
</summary>
    <author>
      <name>Adelchi Azzalini</name>
    </author>
    <author>
      <name>Giovanna Menardi</name>
    </author>
    <link href="http://arxiv.org/abs/1301.6559v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.6559v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.3775v1</id>
    <updated>2013-03-15T14:01:16Z</updated>
    <published>2013-03-15T14:01:16Z</published>
    <title>Approximation for the Distribution of Three-dimensional Discrete Scan
  Statistic</title>
    <summary>  We consider the discrete three dimensional scan statistics. Viewed as the
maximum of an 1-dependent stationary r.v.'s sequence, we provide approximations
and error bounds for the probability distribution of the three dimensional scan
statistics. Importance sampling algorithm is used to obtains sharp bounds for
the simulation error. Simulation results and comparisons with other
approximations are presented for the binomial and Poisson models.
</summary>
    <author>
      <name>Alexandru Amarioarei</name>
    </author>
    <author>
      <name>Cristian Preda</name>
    </author>
    <link href="http://arxiv.org/abs/1303.3775v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.3775v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62E17, 62M30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.3697v1</id>
    <updated>2013-05-16T07:39:18Z</updated>
    <published>2013-05-16T07:39:18Z</published>
    <title>Random Latin squares and Sudoku designs generation</title>
    <summary>  Uniform random generation of Latin squares is a classical problem. In this
paper we prove that both Latin squares and Sudoku designs are maximum cliques
of properly defined graphs. We have developed a simple algorithm for uniform
random sampling of Latin squares and Sudoku designs. It makes use of recent
tools for graph analysis. The corresponding SAS code is annexed.
</summary>
    <author>
      <name>Roberto Fontana</name>
    </author>
    <link href="http://arxiv.org/abs/1305.3697v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.3697v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.5017v1</id>
    <updated>2013-05-22T04:01:38Z</updated>
    <published>2013-05-22T04:01:38Z</published>
    <title>PAWL-Forced Simulated Tempering</title>
    <summary>  In this short note, we show how the parallel adaptive Wang-Landau (PAWL)
algorithm of Bornn et al. (2013) can be used to automate and improve simulated
tempering algorithms. While Wang-Landau and other stochastic approximation
methods have frequently been applied within the simulated tempering framework,
this note demonstrates through a simple example the additional improvements
brought about by parallelization, adaptive proposals and automated bin
splitting.
</summary>
    <author>
      <name>Luke Bornn</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of BAYSM, 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1305.5017v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.5017v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.5626v4</id>
    <updated>2014-02-16T12:09:00Z</updated>
    <published>2013-07-22T09:15:33Z</published>
    <title>SSM: Inference for time series analysis with State Space Models</title>
    <summary>  The main motivation behind the open source library SSM is to reduce the
technical friction that prevents modellers from sharing their work, quickly
iterating in crisis situations, and making their work directly usable by public
authorities to serve decision-making.
</summary>
    <author>
      <name>Joseph Dureau</name>
    </author>
    <author>
      <name>Sébastien Ballesteros</name>
    </author>
    <author>
      <name>Tiffany Bogich</name>
    </author>
    <link href="http://arxiv.org/abs/1307.5626v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.5626v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.0399v1</id>
    <updated>2013-08-02T03:46:33Z</updated>
    <published>2013-08-02T03:46:33Z</published>
    <title>Spatial Process Generation</title>
    <summary>  The generation of random spatial data on a computer is an important tool for
understanding the behavior of spatial processes. In this paper we describe how
to generate realizations from the main types of spatial processes, including
Gaussian and Markov random fields, point processes, spatial Wiener processes,
and Levy fields. Concrete MATLAB code is provided.
</summary>
    <author>
      <name>Dirk P. Kroese</name>
    </author>
    <author>
      <name>Zdravko I. Botev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">41 pages, 31 figures, 13 matlab programs, 6 algorithms</arxiv:comment>
    <link href="http://arxiv.org/abs/1308.0399v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.0399v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65C05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.5717v1</id>
    <updated>2013-08-26T22:53:02Z</updated>
    <published>2013-08-26T22:53:02Z</published>
    <title>A Modified Gibbs Sampler on General State Spaces</title>
    <summary>  We present a modified Gibbs sampler for general state spaces. We establish
that this modification can lead to substantial gains in statistical efficiency
while maintaining the overall quality of convergence. We illustrate our results
in two examples including a toy Normal-Normal model and a Bayesian version of
the random effects model.
</summary>
    <author>
      <name>Alicia A. Johnson</name>
    </author>
    <author>
      <name>James M. Flegal</name>
    </author>
    <link href="http://arxiv.org/abs/1308.5717v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.5717v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.2335v1</id>
    <updated>2013-11-11T02:38:44Z</updated>
    <published>2013-11-11T02:38:44Z</published>
    <title>A First-Order Algorithm for the A-Optimal Experimental Design Problem: A
  Mathematical Programming Approach</title>
    <summary>  We develop and analyse a first-order algorithm for the A-optimal experimental
design problem. The problem is first presented as a special case of a
parametric family of optimal design problems for which duality results and
optimality conditions are given. Then, two first-order (Frank-Wolfe type)
algorithms are presented, accompanied by a detailed time-complexity analysis of
the algorithms and computational results on various sized problems.
</summary>
    <author>
      <name>Selin Damla Ahipasaoglu</name>
    </author>
    <link href="http://arxiv.org/abs/1311.2335v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.2335v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.6486v1</id>
    <updated>2013-11-25T21:21:57Z</updated>
    <published>2013-11-25T21:21:57Z</published>
    <title>Online inference in Markov modulated nonlinear dynamic systems: a
  Rao-Blackwellized particle filtering approach</title>
    <summary>  The Markov modulated (switching) state space is an important model paradigm
in applied statistics. In this article, we specifically consider Markov
modulated nonlinear state-space models and address the online Bayesian
inference problem for such models. In particular, we propose a new
Rao-Blackwellized particle filter for the inference task which is our main
contribution here. The detailed descriptions including an algorithmic summary
are subsequently presented.
</summary>
    <author>
      <name>Saikat Saha</name>
    </author>
    <author>
      <name>Gustaf Hendeby</name>
    </author>
    <link href="http://arxiv.org/abs/1311.6486v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.6486v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.2822v1</id>
    <updated>2014-01-13T12:57:57Z</updated>
    <published>2014-01-13T12:57:57Z</published>
    <title>Approximations for two-dimensional discrete scan statistics in some
  block-factor type dependent models</title>
    <summary>  We consider the two-dimensional discrete scan statistic generated by a
block-factor type model obtained from i.i.d. sequence. We present an
approximation for the distribution of the scan statistics and the corresponding
error bounds. A simulation study illustrates our methodology.
</summary>
    <author>
      <name>Alexandru Amarioarei</name>
    </author>
    <author>
      <name>Cristian Preda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.2822v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.2822v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62E17, 62M30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.3105v1</id>
    <updated>2014-01-14T09:12:47Z</updated>
    <published>2014-01-14T09:12:47Z</published>
    <title>Maximum likelihood estimation in the two-state Markovian arrival process</title>
    <summary>  The Markovian arrival process (MAP) has proven a versatile model for fitting
dependent and non-exponential interarrival times, with a number of applications
to queueing, teletraffic, reliability or finance. Despite theoretical
properties of MAPs and models involving MAPs are well studied, their estimation
remains less explored. This paper examines maximum likelihood estimation of the
second-order MAP using a recently obtained parameterization of the two-state
MAPs.
</summary>
    <author>
      <name>Emilio Carrizosa</name>
    </author>
    <author>
      <name>Pepa Ramírez-Cobo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 PAGES, 2 FIGURES</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.3105v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.3105v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.5617v1</id>
    <updated>2014-01-22T10:47:46Z</updated>
    <published>2014-01-22T10:47:46Z</published>
    <title>Exploring Hoover and Perez's experimental designs using global
  sensitivity analysis</title>
    <summary>  This paper investigates variable-selection procedures in regression that make
use of global sensitivity analysis. The approach is combined with existing
algorithms and it is applied to the time series regression designs proposed by
Hoover and Perez. A comparison of an algorithm employing global sensitivity
analysis and the (optimized) algorithm of Hoover and Perez shows that the
former significantly improves the recovery rates of original specifications.
</summary>
    <author>
      <name>William Becker</name>
    </author>
    <author>
      <name>Paolo Paruolo</name>
    </author>
    <author>
      <name>Andrea Saltelli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.5617v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.5617v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.0506v1</id>
    <updated>2014-05-02T20:37:59Z</updated>
    <published>2014-05-02T20:37:59Z</published>
    <title>Sampling Polya-Gamma random variates: alternate and approximate
  techniques</title>
    <summary>  Efficiently sampling from the P\'olya-Gamma distribution, ${PG}(b,z)$, is an
essential element of P\'olya-Gamma data augmentation. Polson et. al (2013) show
how to efficiently sample from the ${PG}(1,z)$ distribution. We build two new
samplers that offer improved performance when sampling from the ${PG}(b,z)$
distribution and $b$ is not unity.
</summary>
    <author>
      <name>Jesse Windle</name>
    </author>
    <author>
      <name>Nicholas G. Polson</name>
    </author>
    <author>
      <name>James G. Scott</name>
    </author>
    <link href="http://arxiv.org/abs/1405.0506v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.0506v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.2780v1</id>
    <updated>2014-06-11T05:15:59Z</updated>
    <published>2014-06-11T05:15:59Z</published>
    <title>An R Implementation of the Polya-Aeppli Distribution</title>
    <summary>  An efficient implementation of the Polya-Aeppli, or geometirc compound
Poisson, distribution in the statistical programming language R is presented.
The implementation is available as the package polyaAeppli and consists of
functions for the mass function, cumulative distribution function, quantile
function and random variate generation with those parameters conventionally
provided for standard univatiate probability distributions in the stats package
in R
</summary>
    <author>
      <name>Conrad J. Burden</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.2780v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.2780v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.0251v2</id>
    <updated>2015-03-18T13:12:42Z</updated>
    <published>2014-07-31T10:29:00Z</published>
    <title>Modeling Cassava Yield: A Response Surface Approach</title>
    <summary>  This paper reports on application of theory of experimental design using
graphical techniques in R programming language and application of nonlinear
bootstrap regression method to demonstrate the invariant property of parameter
estimates of the Inverse polynomial Model (IPM) in a nonlinear surface.
</summary>
    <author>
      <name>Adeshina Oyedele Bello</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal on Computational Sciences &amp; Applications
  (IJCSA) Vol.4, No.3, June 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1408.0251v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.0251v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.4438v1</id>
    <updated>2014-08-19T19:37:29Z</updated>
    <published>2014-08-19T19:37:29Z</published>
    <title>Understanding the Hastings Algorithm</title>
    <summary>  The Hastings algorithm is a key tool in computational science. While
mathematically justified by detailed balance, it can be conceptually difficult
to grasp. Here, we present two complementary and intuitive ways to derive and
understand the algorithm. In our framework, it is straightforward to see that
the celebrated Metropolis-Hastings algorithm has the highest acceptance
probability of all Hastings algorithms.
</summary>
    <author>
      <name>David D. L. Minh</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Paul</arxiv:affiliation>
    </author>
    <author>
      <name>Do Le</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Paul</arxiv:affiliation>
    </author>
    <author>
      <name> Minh</name>
    </author>
    <link href="http://arxiv.org/abs/1408.4438v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.4438v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.2967v1</id>
    <updated>2014-12-09T14:10:01Z</updated>
    <published>2014-12-09T14:10:01Z</published>
    <title>BayesDccGarch - An Implementation of Multivariate GARCH DCC Models</title>
    <summary>  Multivariate GARCH models are important tools to describe the dynamics of
multivariate times series of financial returns. Nevertheless, these models have
been much less used in practice due to the lack of reliable software. This
paper describes the {\tt R} package {\bf BayesDccGarch} which was developed to
implement recently proposed inference procedures to estimate and compare
multivariate GARCH models allowing for asymmetric and heavy tailed
distributions.
</summary>
    <author>
      <name>Jose A. Fioruci</name>
    </author>
    <author>
      <name>Ricardo S. Ehlers</name>
    </author>
    <author>
      <name>Francisco Louzada</name>
    </author>
    <link href="http://arxiv.org/abs/1412.2967v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.2967v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.02438v1</id>
    <updated>2015-02-09T11:14:24Z</updated>
    <published>2015-02-09T11:14:24Z</published>
    <title>Ergodicity of Fuzzy Markov Chains Based on Simulation Using Sequences</title>
    <summary>  As shown in [1], we reduce periodicity of fuzzy Markov chains using the
Halton quasi-random generator. In this paper, we employ two different
quasi-random sequences namely Faure and Kronecker to generate the membership
values of fuzzy Markov chain. Using simulation it is revealed that the number
of ergodic fuzzy Markov chain simulated by Kronecker sequences is more than the
one obtained by Faure sequences.
</summary>
    <author>
      <name>Behrouz Fathi Vajargah</name>
    </author>
    <author>
      <name>Maryam Gharehdaghi</name>
    </author>
    <link href="http://arxiv.org/abs/1502.02438v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.02438v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.04202v1</id>
    <updated>2015-02-14T12:28:30Z</updated>
    <published>2015-02-14T12:28:30Z</published>
    <title>A fast Mixed Model B-splines algorithm</title>
    <summary>  A fast algorithm for B-splines in mixed models is presented. B-splines have
local support and are computational attractive, because the corresponding
matrices are sparse. A key element of the new algorithm is that the local
character of B-splines is preserved, while in other existing methods this local
character is lost. The computation time for the fast algorithm is linear in the
number of B-splines, while computation time scales cubically for existing
transformations.
</summary>
    <author>
      <name>Martin P. Boer</name>
    </author>
    <link href="http://arxiv.org/abs/1502.04202v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.04202v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.04363v3</id>
    <updated>2016-06-05T09:32:11Z</updated>
    <published>2015-03-14T23:55:23Z</published>
    <title>Fast calculation of boundary crossing probabilities for Poisson
  processes</title>
    <summary>  The boundary crossing probability of a Poisson process with $n$ jumps is a
fundamental quantity with numerous applications. We present a fast $O(n^2 \log
n)$ algorithm to calculate this probability for arbitrary upper and lower
boundaries.
</summary>
    <author>
      <name>Amit Moscovich</name>
    </author>
    <author>
      <name>Boaz Nadler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures, associated C++ code is available at
  http://www.wisdom.weizmann.ac.il/~amitmo</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.04363v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.04363v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62E15, 60G51, 62G30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.01896v3</id>
    <updated>2016-01-27T13:21:39Z</updated>
    <published>2015-04-08T10:13:16Z</published>
    <title>The Metropolis-Hastings algorithm</title>
    <summary>  This short note is a self-contained and basic introduction to the
Metropolis-Hastings algorithm, this ubiquitous tool used for producing
dependent simulations from an arbitrary distribution. The document illustrates
the principles of the methodology on simple examples with R codes and provides
references to the recent extensions of the method.
</summary>
    <author>
      <name>Christian P. Robert</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">U. Paris-Dauphine PSL &amp; U. Warwick</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 7 figures, corrections of errors in R code</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.01896v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.01896v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.03152v1</id>
    <updated>2015-04-13T12:40:55Z</updated>
    <published>2015-04-13T12:40:55Z</published>
    <title>Bayesian computational algorithms for social network analysis</title>
    <summary>  In this chapter we review some of the most recent computational advances in
the rapidly expanding field of statistical social network analysis using the R
open-source software. In particular we will focus on Bayesian estimation for
two important families of models: exponential random graph models (ERGMs) and
latent space models (LSMs).
</summary>
    <author>
      <name>Alberto Caimo</name>
    </author>
    <author>
      <name>Isabella Gollini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Book chapter to appear in "Challenges of Computational Network
  Analysis With R"</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.03152v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.03152v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.03467v1</id>
    <updated>2015-04-14T09:20:21Z</updated>
    <published>2015-04-14T09:20:21Z</published>
    <title>A note on one of the Markov chain Monte Carlo novice's questions</title>
    <summary>  We introduce a novel time-homogeneous Markov embedding of a class of time
inhomogeneous Markov chains widely used in the context of Monte Carlo sampling
algorithms which allows us to answer one of the most basic, yet hard, question
about the practical implementation of these techniques. We also show that this
embedding sheds some light on the recent result of [#maire-douc-olsson2013]. We
discuss further applications of the technique.
</summary>
    <author>
      <name>Christophe Andrieu</name>
    </author>
    <link href="http://arxiv.org/abs/1504.03467v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.03467v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65C40" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.05079v1</id>
    <updated>2015-07-17T19:22:41Z</updated>
    <published>2015-07-17T19:22:41Z</published>
    <title>Riemann Manifold Langevin Methods on Stochastic Volatility Estimation</title>
    <summary>  In this paper we perform Bayesian estimation of stochastic volatility models
with heavy tail distributions using Metropolis adjusted Langevin (MALA) and
Riemman manifold Langevin (MMALA) methods. We provide analytical expressions
for the application of these methods, assess the performance of these
methodologies in simulated data and illustrate their use on two financial time
series data sets.
</summary>
    <author>
      <name>Mauricio Zevallos</name>
    </author>
    <author>
      <name>Loretta Gasco</name>
    </author>
    <author>
      <name>Ricardo Ehlers</name>
    </author>
    <link href="http://arxiv.org/abs/1507.05079v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.05079v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.01745v1</id>
    <updated>2015-09-05T22:31:05Z</updated>
    <published>2015-09-05T22:31:05Z</published>
    <title>A Turning Band Approach to Kernel Convolution for Arbitrary Surfaces</title>
    <summary>  One of the most efficient ways to produce unconditional simulations is with
the spectral method using fast Fourier transform (FFT) [1]. But this approach
is not applicable to arbitrary surfaces because no regular grid exists.
However, points on the arbitrary surface can be generated randomly using
uniform distribution to replace a regular grid. This paper will describe a
nonstationary kernel convolution approach for data on arbitrary surfaces.
</summary>
    <author>
      <name>Alexander Gribov</name>
    </author>
    <link href="http://arxiv.org/abs/1509.01745v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.01745v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.06772v1</id>
    <updated>2015-10-22T21:27:38Z</updated>
    <published>2015-10-22T21:27:38Z</published>
    <title>Models for generalized spherical and related distributions</title>
    <summary>  A flexible model is developed for multivariate generalized spherical
distributions, i.e. ones with level sets that are star shaped. To work in
dimension above 2 requires tools from computational geometry and multivariate
numerical integration. In order to simulate from these star shaped contours, an
algorithm to simulate from general tessellations has been developed that has
applications in other situations. These techniques are implemented in an R
package gensphere.
</summary>
    <author>
      <name>John P Nolan</name>
    </author>
    <link href="http://arxiv.org/abs/1510.06772v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.06772v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.01633v1</id>
    <updated>2015-12-05T07:11:26Z</updated>
    <published>2015-12-05T07:11:26Z</published>
    <title>Robust Estimation of the Generalized Loggamma Model. The R Package
  robustloggamma</title>
    <summary>  robustloggamma is an R package for robust estimation and inference in the
generalized loggamma model. We briefly introduce the model, the estimation
procedures and the computational algorithms. Then, we illustrate the use of the
package with the help of a real data set.
</summary>
    <author>
      <name>Claudio Agostinelli</name>
    </author>
    <author>
      <name>Alfio Marazzi</name>
    </author>
    <author>
      <name>Victor J. Yohai</name>
    </author>
    <author>
      <name>Alex Randriamiharisoa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in Journal of Statistical Software</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.01633v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.01633v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.04065v1</id>
    <updated>2016-01-13T21:05:46Z</updated>
    <published>2016-01-13T21:05:46Z</published>
    <title>Efficient Kernel Convolution for Smooth Surfaces without Edge Effects</title>
    <summary>  One of the most efficient ways to produce unconditional simulations is with
the kernel convolution using fast Fourier transform (FFT) [1]. However, when
data is located on a surface, this approach is not efficient because data needs
to be processed in a three-dimensional enclosing box. This paper describes a
novel approach based on integer transformation to reduce the volume of the
enclosing box.
</summary>
    <author>
      <name>Alexander Gribov</name>
    </author>
    <link href="http://arxiv.org/abs/1601.04065v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.04065v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.00990v1</id>
    <updated>2016-08-02T20:22:25Z</updated>
    <published>2016-08-02T20:22:25Z</published>
    <title>Barrett: out-of-core processing of MultiNest output</title>
    <summary>  Barrett is a Python package for processing and visualising statistical
inferences made using the nested sampling algorithm MultiNest. The main
differential feature from competitors are full out-of-core processing allowing
barrett to handle arbitrarily large datasets. This is achieved by using the
HDF5 data format.
</summary>
    <author>
      <name>Sebastian Liem</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.00990v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.00990v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.00984v1</id>
    <updated>2016-10-01T01:08:50Z</updated>
    <published>2016-10-01T01:08:50Z</published>
    <title>On the State of Computing in Statistics Education: Tools for Learning
  and for Doing</title>
    <summary>  This paper lays out the current landscape of tools used in statistics
education. In particular, it considers graphing calculators, spreadsheets,
applets and microworlds, standalone educational software, statistical
programming tools, tools for reproducible research and bespoke tools. The
strengths and weaknesses of the tools are considered, particularly in the
context of McNamara (2016)'s list of attributes for a statistical computing
tool. Best practices for computing in introductory statistics are suggested.
</summary>
    <author>
      <name>Amelia McNamara</name>
    </author>
    <link href="http://arxiv.org/abs/1610.00984v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.00984v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.06752v1</id>
    <updated>2016-10-21T12:07:47Z</updated>
    <published>2016-10-21T12:07:47Z</published>
    <title>Comments on "Bayesian Solution Uncertainty Quantification for
  Differential Equations" by Chkrebtii, Campbell, Calderhead &amp; Girolami</title>
    <summary>  We commend the authors for an exciting paper which provides a strong
contribution to the emerging field of probabilistic numerics (PN). Below, we
discuss aspects of prior modelling which need to be considered thoroughly in
future work.
</summary>
    <author>
      <name>Francois-Xavier Briol</name>
    </author>
    <author>
      <name>Jon Cockayne</name>
    </author>
    <author>
      <name>Onur Teymur</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Bayesian Analysis, Vol 11, Num 4, pp1285-1293, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1610.06752v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.06752v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.01069v1</id>
    <updated>2016-11-03T15:45:06Z</updated>
    <published>2016-11-03T15:45:06Z</published>
    <title>Maxima Units Search (MUS) algorithm: methodology and applications</title>
    <summary>  An algorithm for extracting identity submatrices of small rank and pivotal
units from large and sparse matrices is proposed. The procedure has already
been satisfactorily applied for solving the label switching problem in Bayesian
mixture models. Here we introduce it on its own and explore possible
applications in different contexts.
</summary>
    <author>
      <name>Leonardo Egidi</name>
    </author>
    <author>
      <name>Roberta Pappadà</name>
    </author>
    <author>
      <name>Francesco Pauli</name>
    </author>
    <author>
      <name>Nicola Torelli</name>
    </author>
    <link href="http://arxiv.org/abs/1611.01069v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.01069v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.03969v1</id>
    <updated>2016-11-12T08:18:38Z</updated>
    <published>2016-11-12T08:18:38Z</published>
    <title>An Introduction to MM Algorithms for Machine Learning and Statistical</title>
    <summary>  MM (majorization--minimization) algorithms are an increasingly popular tool
for solving optimization problems in machine learning and statistical
estimation. This article introduces the MM algorithm framework in general and
via three popular example applications: Gaussian mixture regressions,
multinomial logistic regressions, and support vector machines. Specific
algorithms for the three examples are derived and numerical demonstrations are
presented. Theoretical and practical aspects of MM algorithm design are
discussed.
</summary>
    <author>
      <name>Hien D. Nguyen</name>
    </author>
    <link href="http://arxiv.org/abs/1611.03969v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.03969v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.09252v1</id>
    <updated>2016-11-23T15:35:40Z</updated>
    <published>2016-11-23T15:35:40Z</published>
    <title>Fast Mixing Random Walks and Regularity of Incompressible Vector Fields</title>
    <summary>  We show sufficient conditions under which the \textsc{BallWalk} algorithm
mixes fast in a bounded connected subset of $\Real^n$. In particular, we show
fast mixing if the space is the transformation of a convex space under a smooth
incompressible flow. Construction of such smooth flows is in turn reduced to
the study of the regularity of the solution of the Dirichlet problem for
Laplace's equation.
</summary>
    <author>
      <name>Yasin Abbasi-Yadkori</name>
    </author>
    <link href="http://arxiv.org/abs/1611.09252v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.09252v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.06492v4</id>
    <updated>2017-08-29T14:02:27Z</updated>
    <published>2016-12-20T02:54:51Z</published>
    <title>Chunked-and-Averaged Estimators for Vector Parameters</title>
    <summary>  A divide-and-conquer method for parameter estimation is the
chunked-and-averaged (CA) estimator. CA estimators have been studied for
univariate parameters under independent and identically distributed (IID)
sampling. We study the CA estimators of vector parameters and under non-IID
sampling.
</summary>
    <author>
      <name>Hien D. Nguyen</name>
    </author>
    <author>
      <name>Geoffrey J. McLachlan</name>
    </author>
    <link href="http://arxiv.org/abs/1612.06492v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.06492v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.03405v1</id>
    <updated>2017-01-12T16:56:29Z</updated>
    <published>2017-01-12T16:56:29Z</published>
    <title>New Flexible Compact Covariance Model on a Sphere</title>
    <summary>  We discuss how the kernel convolution approach can be used to accurately
approximate the spatial covariance model on a sphere using spherical distances
between points. A detailed derivation of the required formulas is provided. The
proposed covariance model approximation can be used for non-stationary spatial
prediction and simulation in the case when the dataset is large and the
covariance model can be estimated separately in the data subsets.
</summary>
    <author>
      <name>Alexander Gribov</name>
    </author>
    <author>
      <name>Konstantin Krivoruchko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.03405v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.03405v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.06826v1</id>
    <updated>2017-03-16T14:09:50Z</updated>
    <published>2017-03-16T14:09:50Z</published>
    <title>RatingScaleReduction package: stepwise rating scale item reduction
  without predictability loss</title>
    <summary>  This study presents an innovative method for reducing the number of rating
scale items without predictability loss. The "area under the re- ceiver
operator curve method" (AUC ROC) is used to implement in the
RatingScaleReduction package posted on CRAN. Several cases have been used to
illustrate how the stepwise method has reduced the number of rating scale items
(variables).
</summary>
    <author>
      <name>Waldemar W. Koczkodaj</name>
    </author>
    <author>
      <name>Alicja Wolny-Dominiak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.06826v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.06826v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="94A50, 62C25, 62C99, 62P10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.08627v1</id>
    <updated>2017-03-24T23:49:33Z</updated>
    <published>2017-03-24T23:49:33Z</published>
    <title>Random sampling of Latin squares via binary contingency tables and
  probabilistic divide-and-conquer</title>
    <summary>  We demonstrate a novel approach for the random sampling of Latin squares of
order~$n$ via probabilistic divide-and-conquer. The algorithm divides the
entries of the table modulo powers of $2$, and samples a corresponding binary
contingency table at each level. The sampling distribution is based on the
Boltzmann sampling heuristic, along with probabilistic divide-and-conquer.
</summary>
    <author>
      <name>Stephen DeSalvo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.08627v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.08627v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.09382v1</id>
    <updated>2017-03-28T03:08:35Z</updated>
    <published>2017-03-28T03:08:35Z</published>
    <title>Exact computation of GMM estimators for instrumental variable quantile
  regression models</title>
    <summary>  We show that the generalized method of moments (GMM) estimation problem in
instrumental variable quantile regression (IVQR) models can be equivalently
formulated as a mixed integer quadratic programming problem. This enables exact
computation of the GMM estimators for the IVQR models. We illustrate the
usefulness of our algorithm via Monte Carlo experiments and an application to
demand for fish.
</summary>
    <author>
      <name>Le-Yu Chen</name>
    </author>
    <author>
      <name>Sokbae Lee</name>
    </author>
    <link href="http://arxiv.org/abs/1703.09382v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.09382v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.09746v1</id>
    <updated>2017-05-27T00:38:10Z</updated>
    <published>2017-05-27T00:38:10Z</published>
    <title>simmer: Discrete-Event Simulation for R</title>
    <summary>  The simmer package brings discrete-event simulation to R. It is designed as a
generic yet powerful process-oriented framework. The architecture encloses a
robust and fast simulation core written in C++ with automatic monitoring
capabilities. It provides a rich and flexible R API that revolves around the
concept of trajectory, a common path in the simulation model for entities of
the same type.
</summary>
    <author>
      <name>Iñaki Ucar</name>
    </author>
    <author>
      <name>Bart Smeets</name>
    </author>
    <author>
      <name>Arturo Azcorra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages, 3 figures. Submitted to the Journal of Statistical Software</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.09746v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.09746v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.09998v1</id>
    <updated>2017-05-28T22:25:21Z</updated>
    <published>2017-05-28T22:25:21Z</published>
    <title>Bayesian Bootstraps for Massive Data</title>
    <summary>  Recently, two scalable adaptations of the bootstrap have been proposed: the
bag of little bootstraps (BLB; Kleiner et al., 2014) and the subsampled double
bootstrap (SDB; Sengupta et al., 2016). In this paper, we introduce Bayesian
bootstrap analogues to the BLB and SDB that have similar theoretical and
computational properties, a strategy to perform lossless inference for a class
of functionals of the Bayesian bootstrap, and briefly discuss extensions for
Dirichlet Processes.
</summary>
    <author>
      <name>Andrés F. Barrientos</name>
    </author>
    <author>
      <name>Víctor Peña</name>
    </author>
    <link href="http://arxiv.org/abs/1705.09998v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.09998v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.04464v1</id>
    <updated>2017-07-14T11:28:41Z</updated>
    <published>2017-07-14T11:28:41Z</published>
    <title>Hierarchical EM algorithm for estimating the parameters of Mixture of
  Bivariate Generalized Exponential distributions</title>
    <summary>  This paper provides a mixture modeling framework using the bivariate
generalized exponential distribution. Hierarchical EM algorithm is developed
for finding the estimates of the parameters. The algorithm takes very large
sample size to work as it contains many stages of approximation.
</summary>
    <author>
      <name>Arabin Kumar Dey</name>
    </author>
    <author>
      <name>Debasis Kundu</name>
    </author>
    <link href="http://arxiv.org/abs/1707.04464v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.04464v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.1823v1</id>
    <updated>2009-07-10T13:46:00Z</updated>
    <published>2009-07-10T13:46:00Z</published>
    <title>Improvement of random LHD for high dimensions</title>
    <summary>  Designs of experiments for multivariate case are reviewed. Fast algorithm of
construction of good Latin hypercube designs is developed.
</summary>
    <author>
      <name>Andrey Pepelyshev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, Proceedings of the 6th St. Petersburg Workshop on
  Simulation, 1091-1096</arxiv:comment>
    <link href="http://arxiv.org/abs/0907.1823v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.1823v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.08363v1</id>
    <updated>2016-10-25T11:48:39Z</updated>
    <published>2016-10-25T11:48:39Z</published>
    <title>Comments on "Bayesian Solution Uncertainty Quantification for
  Differential Equations" by Chkrebtii, Campbell, Calderhead &amp; Girolami</title>
    <summary>  I would like to thank the authors for their interesting and very clearly
presented paper discussing probabilistic solvers for ODEs and PDEs.
</summary>
    <author>
      <name>Jon Cockayne</name>
    </author>
    <link href="http://arxiv.org/abs/1610.08363v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.08363v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0706.3520v1</id>
    <updated>2007-06-24T15:10:12Z</updated>
    <published>2007-06-24T15:10:12Z</published>
    <title>On probabilities for separating sets of order statistics</title>
    <summary>  Consider a set of order statistics that arise from sorting samples from two
different populations, each with their own, possibly different distribution
function. The probability that these order statistics fall in disjoint, ordered
intervals, and that of the smallest statistics, a certain number come from the
first populations, are given in terms of the two distribution functions. The
result is applied to computing the joint probability of the number of
rejections and the number of false rejections for the Benjamini-Hochberg false
discovery rate procedure.
</summary>
    <author>
      <name>Deborah H. Glueck</name>
    </author>
    <author>
      <name>Anis Karimpour-Fard</name>
    </author>
    <author>
      <name>Jan Mandel</name>
    </author>
    <author>
      <name>Keith E. Muller</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0706.3520v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0706.3520v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0708.1051v1</id>
    <updated>2007-08-08T07:15:05Z</updated>
    <published>2007-08-08T07:15:05Z</published>
    <title>Deconvolution by simulation</title>
    <summary>  Given samples (x_1,...,x_m) and (z_1,...,z_n) which we believe are
independent realizations of random variables X and Z respectively, where we
further believe that Z=X+Y with Y independent of X, the problem is to estimate
the distribution of Y. We present a new method for doing this, involving
simulation. Experiments suggest that the method provides useful estimates.
</summary>
    <author>
      <name>Colin Mallows</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1214/074921707000000021</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1214/074921707000000021" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at http://dx.doi.org/10.1214/074921707000000021 in the IMS
  Lecture Notes Monograph Series
  (http://www.imstat.org/publications/lecnotes.htm) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IMS Lecture Notes Monograph Series 2007, Vol. 54, 1-11</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0708.1051v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0708.1051v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60J10, 62G05, 94C99 (Primary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0709.1721v1</id>
    <updated>2007-09-11T20:43:15Z</updated>
    <published>2007-09-11T20:43:15Z</published>
    <title>Parallel marginalization Monte Carlo with applications to conditional
  path sampling</title>
    <summary>  Monte Carlo sampling methods often suffer from long correlation times.
Consequently, these methods must be run for many steps to generate an
independent sample. In this paper a method is proposed to overcome this
difficulty. The method utilizes information from rapidly equilibrating coarse
Markov chains that sample marginal distributions of the full system. This is
accomplished through exchanges between the full chain and the auxiliary coarse
chains. Results of numerical tests on the bridge sampling and
filtering/smoothing problems for a stochastic differential equation are
presented.
</summary>
    <author>
      <name>Jonathan Weare</name>
    </author>
    <link href="http://arxiv.org/abs/0709.1721v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0709.1721v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.5670v2</id>
    <updated>2008-03-12T23:38:20Z</updated>
    <published>2007-10-30T15:05:58Z</published>
    <title>An Elegant Method for Generating Multivariate Poisson Random Variable</title>
    <summary>  Generating multivariate Poisson data is essential in many applications.
Current simulation methods suffer from limitations ranging from computational
complexity to restrictions on the structure of the correlation matrix. We
propose a computationally efficient and conceptually appealing method for
generating multivariate Poisson data. The method is based on simulating
multivariate Normal data and converting them to achieve a specific correlation
matrix and Poisson rate vector. This allows for generating data that have
positive or negative correlations as well as different rates.
</summary>
    <author>
      <name>Inbal Yahav</name>
    </author>
    <author>
      <name>Galit Shmueli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0710.5670v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.5670v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0712.4166v1</id>
    <updated>2007-12-27T19:59:41Z</updated>
    <published>2007-12-27T19:59:41Z</published>
    <title>Simulation of the matrix Bingham-von Mises-Fisher distribution, with
  applications to multivariate and relational data</title>
    <summary>  Orthonormal matrices play an important role in reduced-rank matrix
approximations and the analysis of matrix-valued data. A matrix Bingham-von
Mises-Fisher distribution is a probability distribution on the set of
orthonormal matrices that includes linear and quadratic terms, and arises as a
posterior distribution in latent factor models for multivariate and relational
data. This article describes rejection and Gibbs sampling algorithms for
sampling from this family of distributions, and illustrates their use in the
analysis of a protein-protein interaction network.
</summary>
    <author>
      <name>Peter Hoff</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 3 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/0712.4166v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0712.4166v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0801.1864v1</id>
    <updated>2008-01-11T22:53:12Z</updated>
    <published>2008-01-11T22:53:12Z</published>
    <title>Adaptive Independent Metropolis-Hastings by Fast Estimation of Mixtures
  of Normals</title>
    <summary>  We construct an adaptive independent Metropolis-Hastings sampler that uses a
mixture of normals as a proposal distribution. To take full advantage of the
potential of adaptive sampling our algorithm updates the mixture of normals
frequently, starting early in the chain. The algorithm is built for speed and
reliability and its sampling performance is evaluated with real and simulated
examples. Our article outlines conditions for adaptive sampling to hold and
gives a readily accessible proof that under these conditions the sampling
scheme generates iterates that converge to the target distribution.
</summary>
    <author>
      <name>P. Giordani</name>
    </author>
    <author>
      <name>R. Kohn</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages and 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0801.1864v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.1864v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0802.2581v2</id>
    <updated>2008-05-28T01:07:24Z</updated>
    <published>2008-02-19T03:00:55Z</published>
    <title>A Localization Approach to Improve Iterative Proportional Scaling in
  Gaussian Graphical Models</title>
    <summary>  We discuss an efficient implementation of the iterative proportional scaling
procedure in the multivariate Gaussian graphical models. We show that the
computational cost can be reduced by localization of the update procedure in
each iterative step by using the structure of a decomposable model obtained by
triangulation of the graph associated with the model. Some numerical
experiments demonstrate the competitive performance of the proposed algorithm.
</summary>
    <author>
      <name>Hisayuki Hara</name>
    </author>
    <author>
      <name>Akimichi Takemura</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1080/03610920802238662</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1080/03610920802238662" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Communications in Statistics Theory and Methods, 39, No.8,
  1643-1654, 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0802.2581v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0802.2581v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0802.3690v1</id>
    <updated>2008-02-25T20:13:58Z</updated>
    <published>2008-02-25T20:13:58Z</published>
    <title>On variance stabilisation by double Rao-Blackwellisation</title>
    <summary>  Population Monte Carlo has been introduced as a sequential importance
sampling technique to overcome poor fit of the importance function. In this
paper, we compare the performances of the original Population Monte Carlo
algorithm with a modified version that eliminates the influence of the
transition particle via a double Rao-Blackwellisation. This modification is
shown to improve the exploration of the modes through an large simulation
experiment on posterior distributions of mean mixtures of distributions.
</summary>
    <author>
      <name>Alessandra Iacobucci</name>
    </author>
    <author>
      <name>Jean-Michel Marin</name>
    </author>
    <author>
      <name>Christian Robert</name>
    </author>
    <link href="http://arxiv.org/abs/0802.3690v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0802.3690v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0804.0390v1</id>
    <updated>2008-04-02T16:24:48Z</updated>
    <published>2008-04-02T16:24:48Z</published>
    <title>A practical procedure to find matching priors for frequentist inference</title>
    <summary>  In the manuscript, we present a practical way to find the matching priors
proposed by Welch &amp; Peers (1963) and Peers (1965). We investigate the use of
saddlepoint approximations combined with matching priors and obtain p-values of
the test of an interest parameter in the presence of nuisance parameter. The
advantage of our procedure is the flexibility of choosing different initial
conditions so that one can adjust the performance of the test. Two examples
have been studied, with coverage verified via Monte Carlo simulation. One
relates to the ratio of two exponential means, and the other relates the
logistic regression model. Particularly, we are interested in small sample
settings.
</summary>
    <author>
      <name>Juan Zhang</name>
    </author>
    <author>
      <name>John E. Kolassa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/0804.0390v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0804.0390v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0804.2247v1</id>
    <updated>2008-04-14T19:25:53Z</updated>
    <published>2008-04-14T19:25:53Z</published>
    <title>On central tendency and dispersion measures for intervals and hypercubes</title>
    <summary>  The uncertainty or the variability of the data may be treated by considering,
rather than a single value for each data, the interval of values in which it
may fall. This paper studies the derivation of basic description statistics for
interval-valued datasets. We propose a geometrical approach in the
determination of summary statistics (central tendency and dispersion measures)
for interval-valued variables.
</summary>
    <author>
      <name>Marie Chavent</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IMB</arxiv:affiliation>
    </author>
    <author>
      <name>Jérôme Saracco</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">GREThA</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1080/03610920701678984</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1080/03610920701678984" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Communications in Statistics - Theory and Methods 37, 9 (2008)
  1471 - 1482</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0804.2247v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0804.2247v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0804.2413v1</id>
    <updated>2008-04-15T15:45:03Z</updated>
    <published>2008-04-15T15:45:03Z</published>
    <title>Bayesian Inference on Mixtures of Distributions</title>
    <summary>  This survey covers state-of-the-art Bayesian techniques for the estimation of
mixtures. It complements the earlier Marin, Mengersen and Robert (2005) by
studying new types of distributions, the multinomial, latent class and t
distributions. It also exhibits closed form solutions for Bayesian inference in
some discrete setups. Lastly, it sheds a new light on the computation of Bayes
factors via the approximation of Chib (1995).
</summary>
    <author>
      <name>Kate Lee</name>
    </author>
    <author>
      <name>Jean-Michel Marin</name>
    </author>
    <author>
      <name>Kerrie Mengersen</name>
    </author>
    <author>
      <name>Christian P. Robert</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0804.2413v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0804.2413v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0806.0129v1</id>
    <updated>2008-06-01T07:33:27Z</updated>
    <published>2008-06-01T07:33:27Z</published>
    <title>Symbolic computation of moments of sampling distributions</title>
    <summary>  By means of the notion of umbrae indexed by multisets, a general method to
express estimators and their products in terms of power sums is derived. A
connection between the notion of multiset and integer partition leads
immediately to a way to speed up the procedures. Comparisons of computational
times with known procedures show how this approach turns out to be more
efficient in eliminating much unnecessary computation.
</summary>
    <author>
      <name>E. Di Nardo</name>
    </author>
    <author>
      <name>G. Guarino</name>
    </author>
    <author>
      <name>D. Senato</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 7 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computational Statistics and Data Analysis (2008)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0806.0129v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0806.0129v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="05A40, 65C60, 62H12, 68W30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0806.3301v2</id>
    <updated>2009-05-12T04:46:56Z</updated>
    <published>2008-06-20T00:44:53Z</published>
    <title>Fast computation of the median by successive binning</title>
    <summary>  This paper describes a new median algorithm and a median approximation
algorithm. The former has O(n) average running time and the latter has O(n)
worst-case running time. These algorithms are highly competitive with the
standard algorithm when computing the median of a single data set, but are
significantly faster in updating the median when more data is added.
</summary>
    <author>
      <name>Ryan J. Tibshirani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 1 Postscript figure</arxiv:comment>
    <link href="http://arxiv.org/abs/0806.3301v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0806.3301v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0808.3466v2</id>
    <updated>2009-11-11T08:45:17Z</updated>
    <published>2008-08-26T09:22:48Z</published>
    <title>On sequential Monte Carlo, partial rejection control and approximate
  Bayesian computation</title>
    <summary>  We present a sequential Monte Carlo sampler variant of the partial rejection
control algorithm, and show that this variant can be considered as a sequential
Monte Carlo sampler with a modified mutation kernel. We prove that the new
sampler can reduce the variance of the incremental importance weights when
compared with standard sequential Monte Carlo samplers. We provide a study of
theoretical properties of the new algorithm, and make connections with some
existing algorithms. Finally, the sampler is adapted for application under the
challenging "likelihood free," approximate Bayesian computation modelling
framework, where we demonstrate superior performance over existing
likelihood-free samplers.
</summary>
    <author>
      <name>G. W. Peters</name>
    </author>
    <author>
      <name>Y. Fan</name>
    </author>
    <author>
      <name>S. A. Sisson</name>
    </author>
    <link href="http://arxiv.org/abs/0808.3466v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0808.3466v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.4047v1</id>
    <updated>2008-09-23T23:35:02Z</updated>
    <published>2008-09-23T23:35:02Z</published>
    <title>Improved Sequential Stopping Rule for Monte Carlo Simulation</title>
    <summary>  This paper presents an improved result on the negative-binomial Monte Carlo
technique analyzed in a previous paper for the estimation of an unknown
probability p. Specifically, the confidence level associated to a relative
interval [p/\mu_2, p\mu_1], with \mu_1, \mu_2 &gt; 1, is proved to exceed its
asymptotic value for a broader range of intervals than that given in the
referred paper, and for any value of p. This extends the applicability of the
estimator, relaxing the conditions that guarantee a given confidence level.
</summary>
    <author>
      <name>Luis Mendo</name>
    </author>
    <author>
      <name>Jose M. Hernando</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 figures. Paper accepted in IEEE Transactions on Communications</arxiv:comment>
    <link href="http://arxiv.org/abs/0809.4047v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.4047v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0812.1484v1</id>
    <updated>2008-12-08T13:47:45Z</updated>
    <published>2008-12-08T13:47:45Z</published>
    <title>Parallel hierarchical sampling: a practical multiple-chains sampler for
  Bayesian model selection</title>
    <summary>  This paper introduces the parallel hierarchical sampler (PHS), a Markov chain
Monte Carlo algorithm using several chains simultaneously. The connections
between PHS and the parallel tempering (PT) algorithm are illustrated,
convergence of PHS joint transition kernel is proved and and its practical
advantages are emphasized. We illustrate the inferences obtained using PHS,
parallel tempering and the Metropolis-Hastings algorithm for three Bayesian
model selection problems, namely Gaussian clustering, the selection of
covariates for a linear regression model and the selection of the structure of
a treed survival model.
</summary>
    <author>
      <name>Fabio Rigat</name>
    </author>
    <link href="http://arxiv.org/abs/0812.1484v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0812.1484v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0901.4752v2</id>
    <updated>2014-10-08T16:55:20Z</updated>
    <published>2009-01-29T19:21:33Z</published>
    <title>Estimation of Gaussian mixtures in small sample studies using $l_1$
  penalization</title>
    <summary>  Many experiments in medicine and ecology can be conveniently modeled by
finite Gaussian mixtures but face the problem of dealing with small data sets.
We propose a robust version of the estimator based on self-regression and
sparsity promoting penalization in order to estimate the components of Gaussian
mixtures in such contexts. A space alternating version of the penalized EM
algorithm is obtained and we prove that its cluster points satisfy the
Karush-Kuhn-Tucker conditions. Monte Carlo experiments are presented in order
to compare the results obtained by our method and by standard maximum
likelihood estimation. In particular, our estimator is seen to perform better
than the maximum likelihood estimator.
</summary>
    <author>
      <name>Stephane Chretien</name>
    </author>
    <link href="http://arxiv.org/abs/0901.4752v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0901.4752v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0902.0117v1</id>
    <updated>2009-02-01T08:45:44Z</updated>
    <published>2009-02-01T08:45:44Z</published>
    <title>Fixed Point Iteration for Estimating The Parameters of Extreme Value
  Distributions</title>
    <summary>  Maximum likelihood estimations for the parameters of extreme value
distributions are discussed in this paper using fixed point iteration. The
commonly used numerical approach for addressing this problem is the
Newton-Raphson approach which requires differentiation unlike the fixed point
iteration which is also easier to implement. Graphical approaches are also
usually proposed in the literature. We prove that these reduce in fact to the
fixed point solution proposed in this paper.
</summary>
    <author>
      <name>Tewfik Kernane</name>
    </author>
    <author>
      <name>Zohrh A. Raizah</name>
    </author>
    <link href="http://arxiv.org/abs/0902.0117v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0902.0117v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0902.0442v1</id>
    <updated>2009-02-03T08:07:17Z</updated>
    <published>2009-02-03T08:07:17Z</published>
    <title>On the Permutation Distribution of Independence Tests</title>
    <summary>  One of the most popular class of tests for independence between two random
variables is the general class of rank statistics which are invariant under
permutations. This class contains Spearman's coefficient of rank correlation
statistic, Fisher-Yates statistic, weighted Mann statistic and others. Under
the null hypothesis of independence these test statistics have a permutation
distribution that usually the normal asymptotic theory used to approximate the
p-values for these tests. In this note we suggest using a saddlepoint approach
that almost exact and need no extensive simulation calculations to calculate
the p-value of such class of tests.
</summary>
    <author>
      <name>Ehab F. Abd-Elfattah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to the Electronic Journal of Statistics
  (http://www.i-journals.org/ejs/) by the Institute of Mathematical Statistics
  (http://www.imstat.org)</arxiv:comment>
    <link href="http://arxiv.org/abs/0902.0442v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0902.0442v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0905.2646v4</id>
    <updated>2010-10-05T07:24:29Z</updated>
    <published>2009-05-18T18:53:53Z</published>
    <title>Monotonic convergence of a general algorithm for computing optimal
  designs</title>
    <summary>  Monotonic convergence is established for a general class of multiplicative
algorithms introduced by Silvey, Titterington and Torsney [Comm. Statist.
Theory Methods 14 (1978) 1379--1389] for computing optimal designs. A
conjecture of Titterington [Appl. Stat. 27 (1978) 227--234] is confirmed as a
consequence. Optimal designs for logistic regression are used as an
illustration.
</summary>
    <author>
      <name>Yaming Yu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1214/09-AOS761</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1214/09-AOS761" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in at http://dx.doi.org/10.1214/09-AOS761 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Annals of Statistics 2010, Vol. 38, No. 3, 1593-1606</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0905.2646v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0905.2646v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.0198v1</id>
    <updated>2009-06-01T15:11:28Z</updated>
    <published>2009-06-01T15:11:28Z</published>
    <title>Some Numerical Results on the Rank of Generic Three-Way Arrays over R</title>
    <summary>  The aim of this paper is the introduction of a new method for the numerical
computation of the rank of a three-way array. We show that the rank of a
three-way array over R is intimately related to the real solution set of a
system of polynomial equations. Using this, we present some numerical results
based on the computation of Grobner bases.
  Key words: Tensors; three-way arrays; Candecomp/Parafac; Indscal; generic
rank; typical rank; Veronese variety; Segre variety; Grobner bases.
  AMS classification: Primary 15A69; Secondary 15A72, 15A18.
</summary>
    <author>
      <name>Vartan Choulakian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0906.0198v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.0198v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.1004v2</id>
    <updated>2013-01-25T20:32:08Z</updated>
    <published>2009-06-04T21:36:56Z</published>
    <title>A Dynamic Programming Approach for Approximate Uniform Generation of
  Binary Matrices with Specified Margins</title>
    <summary>  Consider the collection of all binary matrices having a specific sequence of
row and column sums and consider sampling binary matrices uniformly from this
collection. Practical algorithms for exact uniform sampling are not known, but
there are practical algorithms for approximate uniform sampling. Here it is
shown how dynamic programming and recent asymptotic enumeration results can be
used to simplify and improve a certain class of approximate uniform samplers.
The dynamic programming perspective suggests interesting generalizations.
</summary>
    <author>
      <name>Matthew T. Harrison</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, minor typographic corrections from previous version,
  superseded by arXiv:1301.3928</arxiv:comment>
    <link href="http://arxiv.org/abs/0906.1004v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.1004v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.4160v2</id>
    <updated>2010-05-04T16:21:21Z</updated>
    <published>2009-07-24T18:02:47Z</published>
    <title>Notes on Using Control Variates for Estimation with Reversible MCMC
  Samplers</title>
    <summary>  A general methodology is presented for the construction and effective use of
control variates for reversible MCMC samplers. The values of the coefficients
of the optimal linear combination of the control variates are computed, and
adaptive, consistent MCMC estimators are derived for these optimal
coefficients. All methodological and asymptotic arguments are rigorously
justified. Numerous MCMC simulation examples from Bayesian inference
applications demonstrate that the resulting variance reduction can be quite
dramatic.
</summary>
    <author>
      <name>Ioannis Kontoyiannis</name>
    </author>
    <author>
      <name>Petros Dellaportas</name>
    </author>
    <link href="http://arxiv.org/abs/0907.4160v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.4160v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0910.0526v1</id>
    <updated>2009-10-03T09:03:31Z</updated>
    <published>2009-10-03T09:03:31Z</published>
    <title>A path algorithm for the Fused Lasso Signal Approximator</title>
    <summary>  The Lasso is a very well known penalized regression model, which adds an
$L_{1}$ penalty with parameter $\lambda_{1}$ on the coefficients to the squared
error loss function. The Fused Lasso extends this model by also putting an
$L_{1}$ penalty with parameter $\lambda_{2}$ on the difference of neighboring
coefficients, assuming there is a natural ordering. In this paper, we develop a
fast path algorithm for solving the Fused Lasso Signal Approximator that
computes the solutions for all values of $\lambda_1$ and $\lambda_2$. In the
supplement, we also give an algorithm for the general Fused Lasso for the case
with predictor matrix $\bX \in \mathds{R}^{n \times p}$ with
$\text{rank}(\bX)=p$.
</summary>
    <author>
      <name>Holger Hoefling</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">38 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0910.0526v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0910.0526v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0910.2325v1</id>
    <updated>2009-10-13T14:48:38Z</updated>
    <published>2009-10-13T14:48:38Z</published>
    <title>Importance sampling methods for Bayesian discrimination between embedded
  models</title>
    <summary>  This paper surveys some well-established approaches on the approximation of
Bayes factors used in Bayesian model choice, mostly as covered in Chen et al.
(2000). Our focus here is on methods that are based on importance sampling
strategies rather than variable dimension techniques like reversible jump MCMC,
including: crude Monte Carlo, maximum likelihood based importance sampling,
bridge and harmonic mean sampling, as well as Chib's method based on the
exploitation of a functional equality. We demonstrate in this survey how these
different methods can be efficiently implemented for testing the significance
of a predictive variable in a probit model. Finally, we compare their
performances on a real dataset.
</summary>
    <author>
      <name>Jean-Michel Marin</name>
    </author>
    <author>
      <name>Christian P. Robert</name>
    </author>
    <link href="http://arxiv.org/abs/0910.2325v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0910.2325v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0910.4472v2</id>
    <updated>2010-01-19T10:00:37Z</updated>
    <published>2009-10-23T13:03:35Z</published>
    <title>Tutorial on ABC rejection and ABC SMC for parameter estimation and model
  selection</title>
    <summary>  In this tutorial we schematically illustrate four algorithms:
  (1) ABC rejection for parameter estimation
  (2) ABC SMC for parameter estimation
  (3) ABC rejection for model selection on the joint space
  (4) ABC SMC for model selection on the joint space.
</summary>
    <author>
      <name>Tina Toni</name>
    </author>
    <author>
      <name>Michael P. H. Stumpf</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This tutorial forms a part of the supplementary material of the paper
  "T. Toni, M. P. H. Stumpf, Simulation-based model selection for dynamical
  systems in systems and population biology, Bioinformatics, 2009 (in press)"</arxiv:comment>
    <link href="http://arxiv.org/abs/0910.4472v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0910.4472v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.0567v2</id>
    <updated>2010-02-03T07:56:25Z</updated>
    <published>2010-02-02T20:31:52Z</published>
    <title>A New Approximation to the Normal Distribution Quantile Function</title>
    <summary>  We present a new approximation to the normal distribution quantile function.
It has a similar form to the approximation of Beasley and Springer [3],
providing a maximum absolute error of less than $2.5 \cdot 10^{-5}$. This is
less accurate than [3], but still sufficient for many applications. However it
is faster than [3]. This is its primary benefit, which can be crucial to many
applications, including in financial markets.
</summary>
    <author>
      <name>Paul M. Voutier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">added contact details</arxiv:comment>
    <link href="http://arxiv.org/abs/1002.0567v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.0567v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.CP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.2684v2</id>
    <updated>2010-02-25T07:12:50Z</updated>
    <published>2010-02-15T15:50:53Z</published>
    <title>On computational tools for Bayesian data analysis</title>
    <summary>  While Robert and Rousseau (2010) addressed the foundational aspects of
Bayesian analysis, the current chapter details its practical aspects through a
review of the computational methods available for approximating Bayesian
procedures. Recent innovations like Monte Carlo Markov chain, sequential Monte
Carlo methods and more recently Approximate Bayesian Computation techniques
have considerably increased the potential for Bayesian applications and they
have also opened new avenues for Bayesian inference, first and foremost
Bayesian model choice.
</summary>
    <author>
      <name>Christian P. Robert</name>
    </author>
    <author>
      <name>Jean-Michel Marin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is a chapter for the book "Bayesian Methods and Expert
  Elicitation" edited by Klaus Bocker, 23 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1002.2684v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.2684v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.2702v1</id>
    <updated>2010-02-13T15:36:25Z</updated>
    <published>2010-02-13T15:36:25Z</published>
    <title>Bayesian computational methods</title>
    <summary>  In this chapter, we will first present the most standard computational
challenges met in Bayesian Statistics, focussing primarily on mixture
estimation and on model choice issues, and then relate these problems with
computational solutions. Of course, this chapter is only a terse introduction
to the problems and solutions related to Bayesian computations. For more
complete references, see Robert and Casella (2004, 2009), or Marin and Robert
(2007), among others. We also restrain from providing an introduction to
Bayesian Statistics per se and for comprehensive coverage, address the reader
to Robert (2007), (again) among others.
</summary>
    <author>
      <name>Christian P. Robert</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is a revised version of a chapter written for the Handbook of
  Computational Statistics, edited by J. Gentle, W. Hardle and Y. Mori in 2003,
  in preparation for the second edition</arxiv:comment>
    <link href="http://arxiv.org/abs/1002.2702v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.2702v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.2706v1</id>
    <updated>2010-02-13T13:57:26Z</updated>
    <published>2010-02-13T13:57:26Z</published>
    <title>Evolutionary Stochastic Search for Bayesian model exploration</title>
    <summary>  Implementing Bayesian variable selection for linear Gaussian regression
models for analysing high dimensional data sets is of current interest in many
fields. In order to make such analysis operational, we propose a new sampling
algorithm based upon Evolutionary Monte Carlo and designed to work under the
"large p, small n" paradigm, thus making fully Bayesian multivariate analysis
feasible, for example, in genetics/genomics experiments. Two real data examples
in genomics are presented, demonstrating the performance of the algorithm in a
space of up to 10,000 covariates. Finally the methodology is compared with a
recently proposed search algorithms in an extensive simulation study.
</summary>
    <author>
      <name>Leonardo Bottolo</name>
    </author>
    <author>
      <name>Sylvia Richardson</name>
    </author>
    <link href="http://arxiv.org/abs/1002.2706v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.2706v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.1771v1</id>
    <updated>2010-03-09T02:38:43Z</updated>
    <published>2010-03-09T02:38:43Z</published>
    <title>Data Driven Computing by the Morphing Fast Fourier Transform Ensemble
  Kalman Filter in Epidemic Spread Simulations</title>
    <summary>  The FFT EnKF data assimilation method is proposed and applied to a stochastic
cell simulation of an epidemic, based on the S-I-R spread model. The FFT EnKF
combines spatial statistics and ensemble filtering methodologies into a
localized and computationally inexpensive version of EnKF with a very small
ensemble, and it is further combined with the morphing EnKF to assimilate
changes in the position of the epidemic.
</summary>
    <author>
      <name>Jan Mandel</name>
    </author>
    <author>
      <name>Jonathan D. Beezley</name>
    </author>
    <author>
      <name>Loren Cobb</name>
    </author>
    <author>
      <name>Ashok Krishnamurthy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 3 figures. Submitted to ICCS 2010</arxiv:comment>
    <link href="http://arxiv.org/abs/1003.1771v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.1771v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60G35" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.3201v1</id>
    <updated>2010-03-16T17:45:16Z</updated>
    <published>2010-03-16T17:45:16Z</published>
    <title>Covariance-Adaptive Slice Sampling</title>
    <summary>  We describe two slice sampling methods for taking multivariate steps using
the crumb framework. These methods use the gradients at rejected proposals to
adapt to the local curvature of the log-density surface, a technique that can
produce much better proposals when parameters are highly correlated. We
evaluate our methods on four distributions and compare their performance to
that of a non-adaptive slice sampling method and a Metropolis method. The
adaptive methods perform favorably on low-dimensional target distributions with
highly-correlated parameters.
</summary>
    <author>
      <name>Madeleine Thompson</name>
    </author>
    <author>
      <name>Radford M. Neal</name>
    </author>
    <link href="http://arxiv.org/abs/1003.3201v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.3201v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65C05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.3357v2</id>
    <updated>2010-03-22T12:55:08Z</updated>
    <published>2010-03-17T12:26:05Z</published>
    <title>Computational Methods in Bayesian Statistics</title>
    <summary>  This paper focuses on utilizing two different Bayesian methods to deal with a
variety of toy problems which occur in data analysis. In particular we
implement the Variational Bayesian and Nested Sampling methods to tackle the
problems of polynomial selection and Gaussian Mixture Models, comparing the
algorithms in terms of processing speed and accuracy. In the problems tackled
here it is the Variational Bayesian algorithms which are the faster though both
results give similar results.
</summary>
    <author>
      <name>Alan Tua</name>
    </author>
    <author>
      <name>Kristian Zarb Adami</name>
    </author>
    <link href="http://arxiv.org/abs/1003.3357v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.3357v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.5338v3</id>
    <updated>2017-02-13T06:01:37Z</updated>
    <published>2010-03-28T04:49:41Z</published>
    <title>Ideal-Theoretic Strategies for Asymptotic Approximation of Marginal
  Likelihood Integrals</title>
    <summary>  The accurate asymptotic evaluation of marginal likelihood integrals is a
fundamental problem in Bayesian statistics. Following the approach introduced
by Watanabe, we translate this into a problem of computational algebraic
geometry, namely, to determine the real log canonical threshold of a polynomial
ideal, and we present effective methods for solving this problem. Our results
are based on resolution of singularities. They apply to parametric models where
the Kullback-Leibler distance is upper and lower bounded by scalar multiples of
some sum of squared real analytic functions. Such models include finite state
discrete models.
</summary>
    <author>
      <name>Shaowei Lin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.18409/jas.v8i1.47</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.18409/jas.v8i1.47" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">34 pages. Moved technical parts of the computation for the statistics
  example to the appendix</arxiv:comment>
    <link href="http://arxiv.org/abs/1003.5338v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.5338v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62E99, 14E15, 62G20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.4347v1</id>
    <updated>2010-04-25T12:25:14Z</updated>
    <published>2010-04-25T12:25:14Z</published>
    <title>Exact posterior distributions over the segmentation space and model
  selection for multiple change-point detection problems</title>
    <summary>  In segmentation problems, inference on change-point position and model
selection are two difficult issues due to the discrete nature of change-points.
In a Bayesian context, we derive exact, non-asymptotic, explicit and tractable
formulae for the posterior distribution of variables such as the number of
change-points or their positions. We also derive a new selection criterion that
accounts for the reliability of the results. All these results are based on an
efficient strategy to explore the whole segmentation space, which is very
large. We illustrate our methodology on both simulated data and a comparative
genomic hybridisation profile.
</summary>
    <author>
      <name>Guillem Rigaill</name>
    </author>
    <author>
      <name>Emilie Lebarbier</name>
    </author>
    <author>
      <name>Stéphane Robin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Statistics and Computing, Volume 22, Issue 4, pp 917-929 (2012)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1004.4347v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.4347v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.5201v1</id>
    <updated>2010-05-28T02:03:34Z</updated>
    <published>2010-05-28T02:03:34Z</published>
    <title>A note on target distribution ambiguity of likelihood-free samplers</title>
    <summary>  Methods for Bayesian simulation in the presence of computationally
intractable likelihood functions are of growing interest. Termed
likelihood-free samplers, standard simulation algorithms such as Markov chain
Monte Carlo have been adapted for this setting. In this article, by presenting
generalisations of existing algorithms, we demonstrate that likelihood-free
samplers can be ambiguous over the form of the target distribution. We also
consider the theoretical justification of these samplers. Distinguishing
between the forms of the target distribution may have implications for the
future development of likelihood-free samplers.
</summary>
    <author>
      <name>S. A. Sisson</name>
    </author>
    <author>
      <name>G. W. Peters</name>
    </author>
    <author>
      <name>M. Briers</name>
    </author>
    <author>
      <name>Y. Fan</name>
    </author>
    <link href="http://arxiv.org/abs/1005.5201v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.5201v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1007.1032v1</id>
    <updated>2010-07-07T01:24:41Z</updated>
    <published>2010-07-07T01:24:41Z</published>
    <title>Approximating quantiles in very large datasets</title>
    <summary>  Very large datasets are often encountered in climatology, either from a
multiplicity of observations over time and space or outputs from deterministic
models (sometimes in petabytes= 1 million gigabytes). Loading a large data
vector and sorting it, is impossible sometimes due to memory limitations or
computing power. We show that a proposed algorithm to approximating the median,
"the median of the median" performs poorly. Instead we develop an algorithm to
approximate quantiles of very large datasets which works by partitioning the
data or use existing partitions (possibly of non-equal size). We show the
deterministic precision of this algorithm and how it can be adjusted to get
customized precisions.
</summary>
    <author>
      <name>Reza Hosseini</name>
    </author>
    <link href="http://arxiv.org/abs/1007.1032v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1007.1032v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1007.4580v2</id>
    <updated>2010-11-21T14:52:10Z</updated>
    <published>2010-07-26T22:07:03Z</published>
    <title>Cases for the nugget in modeling computer experiments</title>
    <summary>  Most surrogate models for computer experiments are interpolators, and the
most common interpolator is a Gaussian process (GP) that deliberately omits a
small-scale (measurement) error term called the nugget. The explanation is that
computer experiments are, by definition, "deterministic", and so there is no
measurement error. We think this is too narrow a focus for a computer
experiment and a statistically inefficient way to model them. We show that
estimating a (non-zero) nugget can lead to surrogate models with better
statistical properties, such as predictive accuracy and coverage, in a variety
of common situations.
</summary>
    <author>
      <name>Robert B. Gramacy</name>
    </author>
    <author>
      <name>Herbert K. H. Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 4 figures, 3 tables; revised</arxiv:comment>
    <link href="http://arxiv.org/abs/1007.4580v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1007.4580v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1008.5246v1</id>
    <updated>2010-08-31T08:05:28Z</updated>
    <published>2010-08-31T08:05:28Z</published>
    <title>Rate estimation in partially observed Markov jump processes with
  measurement errors</title>
    <summary>  We present a simulation methodology for Bayesian estimation of rate
parameters in Markov jump processes arising for example in stochastic kinetic
models. To handle the problem of missing components and measurement errors in
observed data, we embed the Markov jump process into the framework of a general
state space model. We do not use diffusion approximations. Markov chain Monte
Carlo and particle filter type algorithms are introduced, which allow sampling
from the posterior distribution of the rate parameters and the Markov jump
process also in data-poor scenarios. The algorithms are illustrated by applying
them to rate estimation in a model for prokaryotic auto-regulation and in the
stochastic Oregonator, respectively.
</summary>
    <author>
      <name>Michael Amrein</name>
    </author>
    <author>
      <name>Hans R. Kuensch</name>
    </author>
    <link href="http://arxiv.org/abs/1008.5246v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1008.5246v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.2260v3</id>
    <updated>2011-12-22T17:37:48Z</updated>
    <published>2010-09-12T19:22:10Z</published>
    <title>Computing the confidence levels for a root-mean-square test of
  goodness-of-fit, II</title>
    <summary>  This paper extends our earlier article, "Computing the confidence levels for
a root-mean-square test of goodness-of-fit;" unlike in the earlier article, the
models in the present paper involve parameter estimation -- both the null and
alternative hypotheses in the associated tests are composite. We provide
efficient black-box algorithms for calculating the asymptotic confidence levels
of a variant on the classic chi-squared test. In some circumstances, it is also
feasible to compute the exact confidence levels via Monte Carlo simulation.
</summary>
    <author>
      <name>William Perkins</name>
    </author>
    <author>
      <name>Mark Tygert</name>
    </author>
    <author>
      <name>Rachel Ward</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 3 figures (each with two parts), 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1009.2260v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.2260v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.2698v2</id>
    <updated>2011-01-24T17:26:33Z</updated>
    <published>2010-09-14T15:37:23Z</published>
    <title>Approximate variances for tapered spectral estimates</title>
    <summary>  We propose an approximation of the asymptotic variance that removes a certain
discontinuity in the usual formula for the raw and the smoothed periodogram in
case a data taper is used. It is based on an approximation of the covariance of
the (tapered) periodogram at two arbitrary frequencies. Exact computations of
the variances for a Gaussian white noise and an AR(4) process show that the
approximation is more accurate than the usual formula.
</summary>
    <author>
      <name>Michael Amrein</name>
    </author>
    <author>
      <name>Hans R. Künsch</name>
    </author>
    <link href="http://arxiv.org/abs/1009.2698v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.2698v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.4959v1</id>
    <updated>2010-09-25T00:41:33Z</updated>
    <published>2010-09-25T00:41:33Z</published>
    <title>Bayesian Tracking of Emerging Epidemics Using Ensemble Optimal
  Statistical Interpolation (EnOSI)</title>
    <summary>  We explore the use of the optimal statistical interpolation (OSI) data
assimilation method for the statistical tracking of emerging epidemics and to
study the spatial dynamics of a disease. The epidemic models that we used for
this study are spatial variants of the common susceptible-infectious-removed
(S-I-R) compartmental model of epidemiology. The spatial S-I-R epidemic model
is illustrated by application to simulated spatial dynamic epidemic data from
the historic "Black Death" plague of 14th century Europe. Bayesian statistical
tracking of emerging epidemic diseases using the OSI as it unfolds is
illustrated for a simulated epidemic wave originating in Santa Fe, New Mexico.
</summary>
    <author>
      <name>Ashok Krishnamurthy</name>
    </author>
    <author>
      <name>Loren Cobb</name>
    </author>
    <author>
      <name>Jan Mandel</name>
    </author>
    <author>
      <name>Jonathan Beezley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 5 figures. JSM 2010</arxiv:comment>
    <link href="http://arxiv.org/abs/1009.4959v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.4959v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62L12, 60H30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1011.2932v1</id>
    <updated>2010-11-12T14:58:32Z</updated>
    <published>2010-11-12T14:58:32Z</published>
    <title>Simulation-based Bayesian analysis for multiple changepoints</title>
    <summary>  This paper presents a Markov chain Monte Carlo method to generate approximate
posterior samples in retrospective multiple changepoint problems where the
number of changes is not known in advance. The method uses conjugate models
whereby the marginal likelihood for the data between consecutive changepoints
is tractable. Inclusion of hyperpriors gives a near automatic algorithm
providing a robust alternative to popular filtering recursions approaches in
cases which may be sensitive to prior information. Three real examples are used
to demonstrate the proposed approach.
</summary>
    <author>
      <name>Jason Wyse</name>
    </author>
    <author>
      <name>Nial Friel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1011.2932v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.2932v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1011.2948v1</id>
    <updated>2010-11-12T15:29:40Z</updated>
    <published>2010-11-12T15:29:40Z</published>
    <title>Block clustering with collapsed latent block models</title>
    <summary>  We introduce a Bayesian extension of the latent block model for model-based
block clustering of data matrices. Our approach considers a block model where
block parameters may be integrated out. The result is a posterior defined over
the number of clusters in rows and columns and cluster memberships. The number
of row and column clusters need not be known in advance as these are sampled
along with cluster memberhips using Markov chain Monte Carlo. This differs from
existing work on latent block models, where the number of clusters is assumed
known or is chosen using some information criteria. We analyze both simulated
and real data to validate the technique.
</summary>
    <author>
      <name>Jason Wyse</name>
    </author>
    <author>
      <name>Nial Friel</name>
    </author>
    <link href="http://arxiv.org/abs/1011.2948v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.2948v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1011.4457v1</id>
    <updated>2010-11-19T16:11:27Z</updated>
    <published>2010-11-19T16:11:27Z</published>
    <title>Graphical Comparison of MCMC Performance</title>
    <summary>  This paper presents a graphical method for comparing performance of Markov
Chain Monte Carlo methods. Most researchers present comparisons of MCMC methods
using tables of figures of merit; this paper presents a graphical alternative.
It first discusses the computation of autocorrelation time, then uses this to
construct a figure of merit, log density function evaluations per independent
observation. Then, it demonstrates how one can plot this figure of merit
against a tuning parameter in a grid of plots where columns represent sampling
methods and rows represent distributions. This type of visualization makes it
possible to convey a greater depth of information without overwhelming the user
with numbers, allowing researchers to put their contributions into a broader
context than is possible with a textual presentation.
</summary>
    <author>
      <name>Madeleine B. Thompson</name>
    </author>
    <link href="http://arxiv.org/abs/1011.4457v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.4457v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1011.4604v1</id>
    <updated>2010-11-20T19:49:58Z</updated>
    <published>2010-11-20T19:49:58Z</published>
    <title>An Alternating Direction Method for Finding Dantzig Selectors</title>
    <summary>  In this paper, we study the alternating direction method for finding the
Dantzig selectors, which are first introduced in [8]. In particular, at each
iteration we apply the nonmonotone gradient method proposed in [17] to
approximately solve one subproblem of this method. We compare our approach with
a first-order method proposed in [3]. The computational results show that our
approach usually outperforms that method in terms of CPU time while producing
solutions of comparable quality.
</summary>
    <author>
      <name>Zhaosong Lu</name>
    </author>
    <author>
      <name>Ting Kei Pong</name>
    </author>
    <author>
      <name>Yong Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1011.4604v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.4604v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1011.5038v2</id>
    <updated>2011-06-03T12:47:57Z</updated>
    <published>2010-11-23T09:32:20Z</published>
    <title>Approximate simulation-free Bayesian inference for multiple changepoint
  models with dependence within segments</title>
    <summary>  This paper proposes approaches for the analysis of multiple changepoint
models when dependency in the data is modelled through a hierarchical Gaussian
Markov random field. Integrated nested Laplace approximations are used to
approximate data quantities, and an approximate filtering recursions approach
is proposed for savings in compuational cost when detecting changepoints. All
of these methods are simulation free. Analysis of real data demonstrates the
usefulness of the approach in general. The new models which allow for data
dependence are compared with conventional models where data within segments is
assumed independent.
</summary>
    <author>
      <name>Jason Wyse</name>
    </author>
    <author>
      <name>Nial Friel</name>
    </author>
    <author>
      <name>Håvard Rue</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1011.5038v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.5038v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.1086v3</id>
    <updated>2010-12-14T18:35:32Z</updated>
    <published>2010-12-06T06:56:55Z</published>
    <title>Two Proposals for Robust PCA using Semidefinite Programming</title>
    <summary>  The performance of principal component analysis (PCA) suffers badly in the
presence of outliers. This paper proposes two novel approaches for robust PCA
based on semidefinite programming. The first method, maximum mean absolute
deviation rounding (MDR), seeks directions of large spread in the data while
damping the effect of outliers. The second method produces a low-leverage
decomposition (LLD) of the data that attempts to form a low-rank model for the
data by separating out corrupted observations. This paper also presents
efficient computational methods for solving these SDPs. Numerical experiments
confirm the value of these new techniques.
</summary>
    <author>
      <name>Michael McCoy</name>
    </author>
    <author>
      <name>Joel Tropp</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1214/11-EJS636</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1214/11-EJS636" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Electronic Journal of Statistics 5 (2011), 1123--1160</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1012.1086v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.1086v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1101.0955v2</id>
    <updated>2011-05-27T12:56:57Z</updated>
    <published>2011-01-05T12:52:12Z</published>
    <title>Approximate Bayesian Computational methods</title>
    <summary>  Also known as likelihood-free methods, approximate Bayesian computational
(ABC) methods have appeared in the past ten years as the most satisfactory
approach to untractable likelihood problems, first in genetics then in a
broader spectrum of applications. However, these methods suffer to some degree
from calibration difficulties that make them rather volatile in their
implementation and thus render them suspicious to the users of more traditional
Monte Carlo methods. In this survey, we study the various improvements and
extensions made to the original ABC algorithm over the recent years.
</summary>
    <author>
      <name>Jean-Michel Marin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">I3M</arxiv:affiliation>
    </author>
    <author>
      <name>Pierre Pudlo</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">I3M</arxiv:affiliation>
    </author>
    <author>
      <name>Christian P. Robert</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University Paris-Dauphine and CREST</arxiv:affiliation>
    </author>
    <author>
      <name>Robin Ryder</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CREST</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1101.0955v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1101.0955v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.0365v1</id>
    <updated>2011-03-02T08:48:21Z</updated>
    <published>2011-03-02T08:48:21Z</published>
    <title>Diagonal Based Feature Extraction for Handwritten Alphabets Recognition
  System using Neural Network</title>
    <summary>  An off-line handwritten alphabetical character recognition system using
multilayer feed forward neural network is described in the paper. A new method,
called, diagonal based feature extraction is introduced for extracting the
features of the handwritten alphabets. Fifty data sets, each containing 26
alphabets written by various people, are used for training the neural network
and 570 different handwritten alphabetical characters are used for testing. The
proposed recognition system performs quite well yielding higher levels of
recognition accuracy compared to the systems employing the conventional
horizontal and vertical methods of feature extraction. This system will be
suitable for converting handwritten documents into structural text form and
recognizing handwritten names.
</summary>
    <author>
      <name>J. Pradeep</name>
    </author>
    <author>
      <name>E. Srinivasan</name>
    </author>
    <author>
      <name>S. Himavathi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijcsit.2011.3103</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijcsit.2011.3103" rel="related"/>
    <link href="http://arxiv.org/abs/1103.0365v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.0365v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.3970v1</id>
    <updated>2011-03-21T11:06:58Z</updated>
    <published>2011-03-21T11:06:58Z</published>
    <title>Sequential Monte Carlo samplers: error bounds and insensitivity to
  initial conditions</title>
    <summary>  This paper addresses finite sample stability properties of sequential Monte
Carlo methods for approximating sequences of probability distributions. The
results presented herein are applicable in the scenario where the start and end
distributions in the sequence are fixed and the number of intermediate steps is
a parameter of the algorithm. Under assumptions which hold on non-compact
spaces, it is shown that the effect of the initial distribution decays
exponentially fast in the number of intermediate steps and the corresponding
stochastic error is stable in \mathbb{L}_{p} norm.
</summary>
    <author>
      <name>Nick Whiteley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">36 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1103.3970v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.3970v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.2982v1</id>
    <updated>2011-05-16T00:23:49Z</updated>
    <published>2011-05-16T00:23:49Z</published>
    <title>Fast approximate inference with INLA: the past, the present and the
  future</title>
    <summary>  Latent Gaussian models are an extremely popular, flexible class of models.
Bayesian inference for these models is, however, tricky and time consuming.
Recently, Rue, Martino and Chopin introduced the Integrated Nested Laplace
Approximation (INLA) method for deterministic fast approximate inference. In
this paper, we outline the INLA approximation and its related R package. We
will discuss the newer components of the r-INLA program as well as some
possible extensions.
</summary>
    <author>
      <name>Daniel Simpson</name>
    </author>
    <author>
      <name>Finn Lindgren</name>
    </author>
    <author>
      <name>Håvard Rue</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 Pages, 2 Figures. Presented at ISI 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1105.2982v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.2982v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.5256v1</id>
    <updated>2011-05-26T10:52:54Z</updated>
    <published>2011-05-26T10:52:54Z</published>
    <title>Parameter estimation in high dimensional Gaussian distributions</title>
    <summary>  In order to compute the log-likelihood for high dimensional spatial Gaussian
models, it is necessary to compute the determinant of the large, sparse,
symmetric positive definite precision matrix, Q. Traditional methods for
evaluating the log-likelihood for very large models may fail due to the massive
memory requirements. We present a novel approach for evaluating such
likelihoods when the matrix-vector product, Qv, is fast to compute. In this
approach we utilise matrix functions, Krylov subspaces, and probing vectors to
construct an iterative method for computing the log-likelihood.
</summary>
    <author>
      <name>Erlend Aune</name>
    </author>
    <author>
      <name>Daniel P. Simpson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1105.5256v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.5256v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.1980v2</id>
    <updated>2011-11-04T14:48:05Z</updated>
    <published>2011-06-10T08:46:53Z</published>
    <title>How do Markov approximations compare with other methods for large
  spatial data sets?</title>
    <summary>  The Mat\'ern covariance function is a popular choice for modeling dependence
in spatial environmental data. Standard Mat\'ern covariance models are,
however, often computationally infeasible for large data sets. In this work,
recent results for Markov approximations of Gaussian Mat\'{e}rn fields based on
Hilbert space approximations are extended using wavelet basis functions. These
Markov approximations are compared with two of the most popular methods for
efficient covariance approximations; covariance tapering and the process
convolution method. The results show that, for a given computational cost, the
Markov methods have a substantial gain in accuracy compared with the other
methods.
</summary>
    <author>
      <name>David Bolin</name>
    </author>
    <author>
      <name>Finn Lindgren</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Updated title and revised Section 4 to clarify the simulation setup</arxiv:comment>
    <link href="http://arxiv.org/abs/1106.1980v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.1980v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.2245v3</id>
    <updated>2012-08-09T15:19:26Z</updated>
    <published>2011-08-10T18:54:53Z</published>
    <title>Generalized Direct Sampling for Hierarchical Bayesian Models</title>
    <summary>  We develop a new method to sample from posterior distributions in
hierarchical models without using Markov chain Monte Carlo. This method, which
is a variant of importance sampling ideas, is generally applicable to
high-dimensional models involving large data sets. Samples are independent, so
they can be collected in parallel, and we do not need to be concerned with
issues like chain convergence and autocorrelation. Additionally, the method can
be used to compute marginal likelihoods.
</summary>
    <author>
      <name>Michael Braun</name>
    </author>
    <author>
      <name>Paul Damien</name>
    </author>
    <link href="http://arxiv.org/abs/1108.2245v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.2245v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62C10" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3; I.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.6497v1</id>
    <updated>2011-10-29T05:23:36Z</updated>
    <published>2011-10-29T05:23:36Z</published>
    <title>Bayesian Optimization for Adaptive MCMC</title>
    <summary>  This paper proposes a new randomized strategy for adaptive MCMC using
Bayesian optimization. This approach applies to non-differentiable objective
functions and trades off exploration and exploitation to reduce the number of
potentially costly objective function evaluations. We demonstrate the strategy
in the complex setting of sampling from constrained, discrete and densely
connected probabilistic graphical models where, for each variation of the
problem, one needs to adjust the parameters of the proposal mechanism
automatically to ensure efficient mixing of the Markov chains.
</summary>
    <author>
      <name>Nimalan Mahendran</name>
    </author>
    <author>
      <name>Ziyu Wang</name>
    </author>
    <author>
      <name>Firas Hamze</name>
    </author>
    <author>
      <name>Nando de Freitas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper contains 12 pages and 6 figures. A similar version of this
  paper has been submitted to AISTATS 2012 and is currently under review</arxiv:comment>
    <link href="http://arxiv.org/abs/1110.6497v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.6497v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.0574v2</id>
    <updated>2012-04-06T16:42:56Z</updated>
    <published>2011-11-02T17:32:23Z</published>
    <title>Particle algorithms for optimization on binary spaces</title>
    <summary>  We discuss a unified approach to stochastic optimization of pseudo-Boolean
objective functions based on particle methods, including the cross-entropy
method and simulated annealing as special cases. We point out the need for
auxiliary sampling distributions, that is parametric families on binary spaces,
which are able to reproduce complex dependency structures, and illustrate their
usefulness in our numerical experiments. We provide numerical evidence that
particle-driven optimization algorithms based on parametric families yield
superior results on strongly multi-modal optimization problems while local
search heuristics outperform them on easier problems.
</summary>
    <author>
      <name>Christian Schäfer</name>
    </author>
    <link href="http://arxiv.org/abs/1111.0574v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.0574v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1112.3777v1</id>
    <updated>2011-12-16T12:22:29Z</updated>
    <published>2011-12-16T12:22:29Z</published>
    <title>Parameter estimation for the discretely observed fractional
  Ornstein-Uhlenbeck process and the Yuima R package</title>
    <summary>  This paper proposes consistent and asymptotically Gaussian estimators for the
drift, the diffusion coefficient and the Hurst exponent of the discretely
observed fractional Ornstein-Uhlenbeck process. For the estimation of the
drift, the results are obtained only in the case when 1/2 &lt; H &lt; 3/4. This paper
also provides ready-to-use software for the R statistical environment based on
the YUIMA package.
</summary>
    <author>
      <name>Alexandre Brouste</name>
    </author>
    <author>
      <name>Stefano M. Iacus</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1112.3777v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1112.3777v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62M" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1112.6162v1</id>
    <updated>2011-12-28T18:31:21Z</updated>
    <published>2011-12-28T18:31:21Z</published>
    <title>Exactly computing bivariate projection depth contours and median</title>
    <summary>  Among their competitors, projection depth and its induced estimators are very
favorable because they can enjoy very high breakdown point robustness without
having to pay the price of low efficiency, meanwhile providing a promising
center-outward ordering of multi-dimensional data. However, their further
applications have been severely hindered due to their computational challenge
in practice. In this paper, we derive a simple form of the projection depth
function, when (\mu, \sigma) = (Med, MAD). This simple form enables us to
extend the existing result of point-wise exact computation of projection depth
(PD) of Zuo and Lai (2011) to depth contours and median for bivariate data.
</summary>
    <author>
      <name>Xiaohui Liu</name>
    </author>
    <author>
      <name>Yijun Zuo</name>
    </author>
    <author>
      <name>Zhizhong Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 10 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1112.6162v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1112.6162v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62F10, 62F40, 62F35" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.0646v3</id>
    <updated>2013-02-28T14:28:35Z</updated>
    <published>2012-01-03T14:38:14Z</published>
    <title>On the flexibility of the design of Multiple Try Metropolis schemes</title>
    <summary>  The Multiple Try Metropolis (MTM) method is a generalization of the classical
Metropolis-Hastings algorithm in which the next state of the chain is chosen
among a set of samples, according to normalized weights. In the literature,
several extensions have been proposed. In this work, we show and remark upon
the flexibility of the design of MTM-type methods, fulfilling the detailed
balance condition. We discuss several possibilities and show different
numerical results.
</summary>
    <author>
      <name>Luca Martino</name>
    </author>
    <author>
      <name>Jesse Read</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s00180-013-0429-2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s00180-013-0429-2" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computational Statistics, Volume 28, Issue 6, Pages: 2797-2823,
  2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1201.0646v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.0646v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.1320v1</id>
    <updated>2012-01-05T22:43:16Z</updated>
    <published>2012-01-05T22:43:16Z</published>
    <title>Simply Explicitly Invertible Approximations to 4 Decimals of Error
  Function and Normal Cumulative Distribution Function</title>
    <summary>  We improve the Modified Winitzki's Approximation of the error function
$erf(x)\cong \sqrt{1-e^{-x^2\frac{\frac{4}{\pi}+0.147x^2}{1+0.147x^2}}}$ which
has error $|\varepsilon (x)| &lt; 1.25 \cdot 10^{-4}$ $\forall x \ge 0$ till
reaching 4 decimals of precision with $|\varepsilon (x)| &lt; 2.27 \cdot 10^{-5}$;
also reducing slightly the relative error. Old formula and ours are both
explicitly invertible, essentially solving a biquadratic equation, after
obvious substitutions. Then we derive approximations to 4 decimals of normal
cumulative distribution function $\Phi (x)$, of erfc$(x)$ and of the $Q$
function (or cPhi).
</summary>
    <author>
      <name>A. Soranzo</name>
    </author>
    <author>
      <name>E. Epure</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 1 figure, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1201.1320v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.1320v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="33B20, 33F05, 65D20, 97N50" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.2375v4</id>
    <updated>2012-11-14T12:05:34Z</updated>
    <published>2012-01-11T18:53:45Z</published>
    <title>Mixed Beta Regression: A Bayesian Perspective</title>
    <summary>  This paper builds on recent research that focuses on regression modeling of
continuous bounded data, such as proportions measured on a continuous scale.
Specifically, it deals with beta regression models with mixed effects from a
Bayesian approach. We use a suitable parameterization of the beta law in terms
of its mean and a precision parameter, and allow both parameters to be modeled
through regression structures that may involve fixed and random effects.
Specification of prior distributions is discussed, computational implementation
via Gibbs sampling is provided, and illustrative examples are presented.
</summary>
    <author>
      <name>Jorge I. Figueroa-Zuñiga</name>
    </author>
    <author>
      <name>Reinaldo B. Arellano-Valle</name>
    </author>
    <author>
      <name>Silvia L. P. Ferrari</name>
    </author>
    <link href="http://arxiv.org/abs/1201.2375v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.2375v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.2770v3</id>
    <updated>2014-01-28T16:45:25Z</updated>
    <published>2012-01-13T09:04:47Z</published>
    <title>Bergm: Bayesian Exponential Random Graphs in R</title>
    <summary>  In this paper we describe the main featuress of the Bergm package for the
open-source R software which provides a comprehensive framework for Bayesian
analysis for exponential random graph models: tools for parameter estimation,
model selection and goodness-of-fit diagnostics. We illustrate the capabilities
of this package describing the algorithms through a tutorial analysis of two
well-known network datasets.
</summary>
    <author>
      <name>Alberto Caimo</name>
    </author>
    <author>
      <name>Nial Friel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1201.2770v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.2770v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.5912v1</id>
    <updated>2012-01-27T23:12:51Z</updated>
    <published>2012-01-27T23:12:51Z</published>
    <title>On EM algorithms and their proximal generalizations</title>
    <summary>  In this paper, we analyze the celebrated EM algorithm from the point of view
of proximal point algorithms. More precisely, we study a new type of
generalization of the EM procedure introduced in \cite{Chretien&amp;Hero:98} and
called Kullback-proximal algorithms. The proximal framework allows us to prove
new results concerning the cluster points. An essential contribution is a
detailed analysis of the case where some cluster points lie on the boundary of
the parameter space.
</summary>
    <author>
      <name>Stéphane Chrétien</name>
    </author>
    <author>
      <name>Alfred O. Hero</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ESAIM: Probability and Statistics (2008) 12 pp. 308--326</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1201.5912v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.5912v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.5913v1</id>
    <updated>2012-01-27T23:19:11Z</updated>
    <published>2012-01-27T23:19:11Z</published>
    <title>A Component-wise EM Algorithm for Mixtures</title>
    <summary>  In some situations, EM algorithm shows slow convergence problems. One
possible reason is that standard procedures update the parameters
simultaneously. In this paper we focus on finite mixture estimation. In this
framework, we propose a component-wise EM, which updates the parameters
sequentially. We give an interpretation of this procedure as a proximal point
algorithm and use it to prove the convergence. Illustrative numerical
experiments show how our algorithm compares to EM and a version of the SAGE
algorithm.
</summary>
    <author>
      <name>Gilles Celeux</name>
    </author>
    <author>
      <name>Stéphane Chrétien</name>
    </author>
    <author>
      <name>Florence Forbes</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Computational and Graphical Statistics. (2001), 10 no.4
  pp. 697--712</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1201.5913v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.5913v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.6140v1</id>
    <updated>2012-01-30T09:39:35Z</updated>
    <published>2012-01-30T09:39:35Z</published>
    <title>Fast simulation of truncated Gaussian distributions</title>
    <summary>  We consider the problem of simulating a Gaussian vector X, conditional on the
fact that each component of X belongs to a finite interval [a_i,b_i], or a
semi-finite interval [a_i,+infty). In the one-dimensional case, we design a
table-based algorithm that is computationally faster than alternative
algorithms. In the two-dimensional case, we design an accept-reject algorithm.
According to our calculations and our numerical studies, the acceptance rate of
this algorithm is bounded from below by 0.5 for semi-finite truncation
intervals, and by 0.47 for finite intervals. Extension to 3 or more dimensions
is discussed.
</summary>
    <author>
      <name>Nicolas Chopin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11222-009-9168-1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11222-009-9168-1" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Statistics and Computing 2011, Volume 21, Number 2, 275-288</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1201.6140v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.6140v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65C10" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.5093v1</id>
    <updated>2012-02-23T06:17:39Z</updated>
    <published>2012-02-23T06:17:39Z</published>
    <title>A measure of skewness for testing departures from normality</title>
    <summary>  We propose a new skewness test statistic for normality based on the Pearson
measure of skewness. We obtain asymptotic first four moments of the null
distribution for this statistic by using a computer algebra system and its
normalizing transformation based on the Johnson $S_{U}$ system. Finally the
performance of the proposed statistic is shown by comparing the powers of
several skewness test statistics against some alternative hypotheses.
</summary>
    <author>
      <name>Shigekazu Nakagawa</name>
    </author>
    <author>
      <name>Hiroki Hashiguchi</name>
    </author>
    <author>
      <name>Naoto Niki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1202.5093v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.5093v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.0919v1</id>
    <updated>2012-03-05T13:58:04Z</updated>
    <published>2012-03-05T13:58:04Z</published>
    <title>Finite approximations to coherent choice</title>
    <summary>  This paper studies and bounds the effects of approximating loss functions and
credal sets on choice functions, under very weak assumptions. In particular,
the credal set is assumed to be neither convex nor closed. The main result is
that the effects of approximation can be bounded, although in general,
approximation of the credal set may not always be practically possible. In case
of pairwise choice, I demonstrate how the situation can be improved by showing
that only approximations of the extreme points of the closure of the convex
hull of the credal set need to be taken into account, as expected.
</summary>
    <author>
      <name>Matthias C. M. Troffaes</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.ijar.2008.07.001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.ijar.2008.07.001" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 1 figure, 2 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Approximate Reasoning 50 (2009) 655-665</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1203.0919v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.0919v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="91B06, 62C99, 65G30" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3; G.1.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.3880v1</id>
    <updated>2012-03-17T18:06:25Z</updated>
    <published>2012-03-17T18:06:25Z</published>
    <title>Parameter Estimation from Censored Samples using the
  Expectation-Maximization Algorithm</title>
    <summary>  This paper deals with parameter estimation when the data are randomly right
censored. The maximum likelihood estimates from censored samples are obtained
by using the expectation-maximization (EM) and Monte Carlo EM (MCEM)
algorithms. We introduce the concept of the EM and MCEM algorithms and develop
parameter estimation methods for a variety of distributions such as normal,
Laplace and Rayleigh distributions. These proposed methods are illustrated with
three examples.
</summary>
    <author>
      <name>Chanseok Park</name>
    </author>
    <author>
      <name>Seong Beom Lee</name>
    </author>
    <link href="http://arxiv.org/abs/1203.3880v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.3880v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.3187v1</id>
    <updated>2012-04-14T17:11:30Z</updated>
    <published>2012-04-14T17:11:30Z</published>
    <title>Driving Markov chain Monte Carlo with a dependent random stream</title>
    <summary>  Markov chain Monte Carlo is a widely-used technique for generating a
dependent sequence of samples from complex distributions. Conventionally, these
methods require a source of independent random variates. Most implementations
use pseudo-random numbers instead because generating true independent variates
with a physical system is not straightforward. In this paper we show how to
modify some commonly used Markov chains to use a dependent stream of random
numbers in place of independent uniform variates. The resulting Markov chains
have the correct invariant distribution without requiring detailed knowledge of
the stream's dependencies or even its marginal distribution. As a side-effect,
sometimes far fewer random numbers are required to obtain accurate results.
</summary>
    <author>
      <name>Iain Murray</name>
    </author>
    <author>
      <name>Lloyd T. Elliott</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.3187v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.3187v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.4248v1</id>
    <updated>2012-04-19T04:44:41Z</updated>
    <published>2012-04-19T04:44:41Z</published>
    <title>A new two parameter lifetime distribution: model and properties</title>
    <summary>  In this paper a new lifetime distribution which is obtained by compounding
Lindley and geometric distributions, named Lindley-geometric (LG) distribution,
is introduced. Several properties of the new distribution such as density,
failure rate, mean lifetime, moments, and order statistics are derived.
Furthermore, estimation by maximum likelihood and inference for large sample
are discussed. The paper is motivated by two applications to real data sets and
we hope that this model be able to attract wider applicability in survival and
reliability.
</summary>
    <author>
      <name>Hojjatollah Zakerzadeh</name>
    </author>
    <author>
      <name>Eisa Mahmoudi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1007.0238</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.4248v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.4248v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60E05, 62F10, 62P99" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.1076v1</id>
    <updated>2012-05-04T22:01:04Z</updated>
    <published>2012-05-04T22:01:04Z</published>
    <title>Adaptive parallel tempering algorithm</title>
    <summary>  Parallel tempering is a generic Markov chain Monte Carlo sampling method
which allows good mixing with multimodal target distributions, where
conventional Metropolis-Hastings algorithms often fail. The mixing properties
of the sampler depend strongly on the choice of tuning parameters, such as the
temperature schedule and the proposal distribution used for local exploration.
We propose an adaptive algorithm which tunes both the temperature schedule and
the parameters of the random-walk Metropolis kernel automatically. We prove the
convergence of the adaptation and a strong law of large numbers for the
algorithm. We illustrate the performance of our method with examples. Our
empirical findings indicate that the algorithm can cope well with different
kind of scenarios without prior tuning.
</summary>
    <author>
      <name>Blazej Miasojedow</name>
    </author>
    <author>
      <name>Eric Moulines</name>
    </author>
    <author>
      <name>Matti Vihola</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">33 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1205.1076v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.1076v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.4062v1</id>
    <updated>2012-05-18T00:12:53Z</updated>
    <published>2012-05-18T00:12:53Z</published>
    <title>On optimal direction gibbs sampling</title>
    <summary>  Generalized Gibbs kernels are those that may take any direction not
necessarily bounded to each axis along the parameters of the objective
function. We study how to optimally choose such directions in a Directional,
random scan, Gibbs sampler setting. The optimal direction is chosen by
minimizing to the mutual information (Kullback-Leibler divergence) of two steps
of the MCMC for a truncated Normal objective function. The result is
generalized to be used when a Multivariate Normal (local) approximation is
available for the objective function. Three Gibbs direction distributions are
tested in highly skewed non-normal objective functions.
</summary>
    <author>
      <name>J. Andrés Christen</name>
    </author>
    <author>
      <name>Colin Fox</name>
    </author>
    <author>
      <name>Diego Andrés Pérez-Ruiz</name>
    </author>
    <author>
      <name>Mario Santana-Cibrian</name>
    </author>
    <link href="http://arxiv.org/abs/1205.4062v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.4062v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62F15" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.4120v1</id>
    <updated>2012-05-18T09:33:29Z</updated>
    <published>2012-05-18T09:33:29Z</published>
    <title>Two New Algorithms for Solving Covariance Graphical Lasso Based on
  Coordinate Descent and ECM</title>
    <summary>  Covariance graphical lasso applies a lasso penalty on the elements of the
covariance matrix. This method is useful because it not only produces sparse
estimation of covariance matrix but also discovers marginal independence
structures by generating zeros in the covariance matrix. We propose and explore
two new algorithms for solving the covariance graphical lasso problem. Our new
algorithms are based on coordinate descent and ECM. We show that these two
algorithms are more attractive than the only existing competing algorithm of
Bien and Tibshirani (2011) in terms of simplicity, speed and stability. We also
discuss convergence properties of our algorithms.
</summary>
    <author>
      <name>Hao Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1205.4120v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.4120v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.4503v1</id>
    <updated>2012-05-21T07:32:50Z</updated>
    <published>2012-05-21T07:32:50Z</published>
    <title>Parameter Exploration in Simulation Experiments: A Bayesian Framework</title>
    <summary>  Simulations often involve the use of model parameters which are unknown or
uncertain. For this reason, simulation experiments are often repeated for
multiple combinations of parameter values, often iterating through parameter
values lying on a fixed grid. However, the use of a discrete grid places limits
on the dimension of the parameter space and creates the potential to miss
important parameter combinations which fall in the gaps between grid points.
Here we draw parallels with strategies for numerical integration and describe a
Markov chain Monte-Carlo strategy for exploring parameter values. We illustrate
the approach using examples from phylogenetics, archaeology, and epidemiology.
</summary>
    <author>
      <name>Jessica W. Leigh</name>
    </author>
    <author>
      <name>David Bryant</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages incl. supplementary information. 4 figures, 2 supp. figures,
  2 supp. tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1205.4503v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.4503v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.3421v1</id>
    <updated>2012-06-15T10:48:20Z</updated>
    <published>2012-06-15T10:48:20Z</published>
    <title>Linear Latent Variable Models: The lava-package</title>
    <summary>  An R package for specifying and estimating linear latent variable models is
presented. The philosophy of the implementation is to separate the model
specification from the actual data, which leads to a dynamic and easy way of
modeling complex hierarchical structures. Several advanced features are
implemented including robust standard errors for clustered correlated data,
multigroup analyses, non-linear parameter constraints, inference with
incomplete data, maximum likelihood estimation with censored and binary
observations, and instrumental variable estimators. In addition an extensive
simulation interface covering a broad range of non-linear generalized
structural equation models is described. The model and software are
demonstrated in data of measurements of the serotonin transporter in the human
brain.
</summary>
    <author>
      <name>Klaus K. Holst</name>
    </author>
    <author>
      <name>Esben Budtz-Jørgensen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s00180-012-0344-y</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s00180-012-0344-y" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computational Statistics, Volume 28, Issue 4 , pp 1385-1452</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1206.3421v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.3421v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.5239v1</id>
    <updated>2012-06-20T14:51:32Z</updated>
    <published>2012-06-20T14:51:32Z</published>
    <title>Large-Flip Importance Sampling</title>
    <summary>  We propose a new Monte Carlo algorithm for complex discrete distributions.
The algorithm is motivated by the N-Fold Way, which is an ingenious
event-driven MCMC sampler that avoids rejection moves at any specific state.
The N-Fold Way can however get "trapped" in cycles. We surmount this problem by
modifying the sampling process. This correction does introduce bias, but the
bias is subsequently corrected with a carefully engineered importance sampler.
</summary>
    <author>
      <name>Firas Hamze</name>
    </author>
    <author>
      <name>Nando de Freitas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Twenty-Third Conference on Uncertainty
  in Artificial Intelligence (UAI2007)</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.5239v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.5239v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.5387v1</id>
    <updated>2012-06-23T12:37:20Z</updated>
    <published>2012-06-23T12:37:20Z</published>
    <title>Moments Calculation For the Doubly Truncated Multivariate Normal Density</title>
    <summary>  In the present article we derive an explicit expression for the trun- cated
mean and variance for the multivariate normal distribution with ar- bitrary
rectangular double truncation. We use the moment generating ap- proach of
Tallis (1961) and extend it to general {\mu}, {\Sigma} and all combinations of
truncation. As part of the solution we also give a formula for the bivari- ate
marginal density of truncated multinormal variates. We also prove an invariance
property of some elements of the inverse covariance after trunca- tion.
Computer algorithms for computing the truncated mean, variance and the
bivariate marginal probabilities for doubly truncated multivariate normal
variates have been written in R and are presented along with three examples.
</summary>
    <author>
      <name>Manjunath B G</name>
    </author>
    <author>
      <name>Stefan Wilhelm</name>
    </author>
    <link href="http://arxiv.org/abs/1206.5387v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.5387v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.5758v1</id>
    <updated>2012-07-24T18:02:25Z</updated>
    <published>2012-07-24T18:02:25Z</published>
    <title>Bayesian inference for Gibbs random fields using composite likelihoods</title>
    <summary>  Gibbs random fields play an important role in statistics, for example the
autologistic model is commonly used to model the spatial distribution of binary
variables defined on a lattice. However they are complicated to work with due
to an intractability of the likelihood function. It is therefore natural to
consider tractable approximations to the likelihood function. Composite
likelihoods offer a principled approach to constructing such approximation. The
contribution of this paper is to examine the performance of a collection of
composite likelihood approximations in the context of Bayesian inference.
</summary>
    <author>
      <name>Nial Friel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in the proceedings of the 2012 Winter Simulation Conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1207.5758v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.5758v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.7311v1</id>
    <updated>2012-07-31T16:37:26Z</updated>
    <published>2012-07-31T16:37:26Z</published>
    <title>Tests for exponentiality against NBUE alternatives: a Monte Carlo
  comparison</title>
    <summary>  Testing of various classes of life distributions has been addressed in the
literature for more than 45 years. In this paper, we consider the problem of
testing exponentiality (which essentially implies no ageing) against positive
ageing which is captured by the fairly large class of new better than used in
expectation (NBUE) distributions. These tests of exponentiality against NBUE
alternatives are discussed and compared. The empirical size of the tests is
obtained by simulations. Power comparisons for different popular alternatives
are done using Monte Carlo simulations. These comparisons are made for both
small and large sample sizes. The paper concludes with a discussion in which
suggestions are made regarding the choices of the test when a particular
alternative is suspected.
</summary>
    <author>
      <name>M. Z. Anis</name>
    </author>
    <author>
      <name>Kinjal Basu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1080/00949655.2012.704517</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1080/00949655.2012.704517" rel="related"/>
    <link href="http://arxiv.org/abs/1207.7311v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.7311v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G10, 62G20, 90B25" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.1728v1</id>
    <updated>2012-08-08T18:28:31Z</updated>
    <published>2012-08-08T18:28:31Z</published>
    <title>Statistical Analysis of Autoregressive Fractionally Integrated Moving
  Average Models</title>
    <summary>  In practice, several time series exhibit long-range dependence or persistence
in their observations, leading to the development of a number of estimation and
prediction methodologies to account for the slowly decaying autocorrelations.
The autoregressive fractionally integrated moving average (ARFIMA) process is
one of the best-known classes of long-memory models. In the package afmtools
for R, we have implemented some of these statistical tools for analyzing ARFIMA
models. In particular, this package contains functions for parameter
estimation, exact autocovariance calculation, predictive ability testing, and
impulse response function, amongst others. Finally, the implemented methods are
illustrated with applications to real-life time series.
</summary>
    <author>
      <name>Javier E. Contreras-Reyes</name>
    </author>
    <author>
      <name>Wilfredo Palma</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s00180-013-0408-7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s00180-013-0408-7" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 8 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computational Statistics (2013), 28(5), 2309-2331</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1208.1728v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.1728v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.2651v1</id>
    <updated>2012-08-13T18:01:17Z</updated>
    <published>2012-08-13T18:01:17Z</published>
    <title>A Plea for Neutral Comparison Studies in Computational Sciences</title>
    <summary>  In a context where most published articles are devoted to the development of
"new methods", comparison studies are generally appreciated by readers but
surprisingly given poor consideration by many scientific journals. In
connection with recent articles on over-optimism and epistemology published in
Bioinformatics, this letter stresses the importance of neutral comparison
studies for the objective evaluation of existing methods and the establishment
of standards by drawing parallels with clinical research.
</summary>
    <author>
      <name>Anne-Laure Boulesteix</name>
    </author>
    <author>
      <name>Manuel J. A. Eugster</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1371/journal.pone.0061562</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1371/journal.pone.0061562" rel="related"/>
    <link href="http://arxiv.org/abs/1208.2651v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.2651v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.4275v1</id>
    <updated>2012-08-21T14:20:05Z</updated>
    <published>2012-08-21T14:20:05Z</published>
    <title>Composite likelihood estimation of sparse Gaussian graphical models with
  symmetry</title>
    <summary>  In this article, we discuss the composite likelihood estimation of sparse
Gaussian graphical models. When there are symmetry constraints on the
concentration matrix or partial correlation matrix, the likelihood estimation
can be computational intensive. The composite likelihood offers an alternative
formulation of the objective function and yields consistent estimators. When a
sparse model is considered, the penalized composite likelihood estimation can
yield estimates satisfying both the symmetry and sparsity constraints and
possess ORACLE property. Application of the proposed method is demonstrated
through simulation studies and a network analysis of a biological data set.
</summary>
    <author>
      <name>Xin Gao</name>
    </author>
    <author>
      <name>Helene Massam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">1 figure 4 tables 28 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1208.4275v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.4275v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.0876v4</id>
    <updated>2015-11-10T10:20:13Z</updated>
    <published>2012-09-05T07:10:45Z</published>
    <title>Fitting directed acyclic graphs with latent nodes as finite mixtures
  models, with application to education transmission</title>
    <summary>  This paper describes an efficient EM algorithm for maximum likelihood
estimation of a system of nonlinear structural equations corresponding to a
directed acyclic graph model that can contain an arbitrary number of latent
variables. The endogenous variables in the model must be categorical, while the
exogenous variables may be arbitrary. The models discussed in this paper are an
extended version of finite mixture models suitable for causal inference. An
application to the problem of education transmission is presented as an
illustration.
</summary>
    <author>
      <name>Antonio Forcina</name>
    </author>
    <author>
      <name>Salvatore Modica</name>
    </author>
    <link href="http://arxiv.org/abs/1209.0876v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.0876v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.0333v2</id>
    <updated>2013-02-20T12:33:22Z</updated>
    <published>2012-10-01T10:06:54Z</published>
    <title>Bayesian computing with INLA: new features</title>
    <summary>  The INLA approach for approximate Bayesian inference for latent Gaussian
models has been shown to give fast and accurate estimates of posterior
marginals and also to be a valuable tool in practice via the R-package R-INLA.
In this paper we formalize new developments in the R-INLA package and show how
these features greatly extend the scope of models that can be analyzed by this
interface. We also discuss the current default method in R-INLA to approximate
posterior marginals of the hyperparameters using only a modest number of
evaluations of the joint posterior distribution of the hyperparameters, without
any need for numerical integration.
</summary>
    <author>
      <name>Thiago G. Martins</name>
    </author>
    <author>
      <name>Daniel Simpson</name>
    </author>
    <author>
      <name>Finn Lindgren</name>
    </author>
    <author>
      <name>Håvard Rue</name>
    </author>
    <link href="http://arxiv.org/abs/1210.0333v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.0333v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.0815v1</id>
    <updated>2012-11-05T10:26:28Z</updated>
    <published>2012-11-05T10:26:28Z</published>
    <title>A note on a universal random variate generator for integer-valued random
  variables</title>
    <summary>  A universal generator for integer-valued square-integrable random variables
is introduced. The generator relies on a rejection technique based on a
generalization of the inversion formula for integer-valued random variables.
The proposal gives rise to a simple algorithm which may be implemented in a few
code lines and which may show good performance when the classical families of
distributions - such as the Poisson and the Binomial - are considered. In
addition, the method is suitable for the computer generation of integer-valued
random variables which display closed-form characteristic functions, but do not
possess a probability function expressible in a simple analytical way. As an
example of such a framework, an application to the Poisson-Tweedie distribution
is provided.
</summary>
    <author>
      <name>Lucio Barabesi</name>
    </author>
    <author>
      <name>Luca Pratelli</name>
    </author>
    <link href="http://arxiv.org/abs/1211.0815v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.0815v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.2442v1</id>
    <updated>2012-11-11T17:50:58Z</updated>
    <published>2012-11-11T17:50:58Z</published>
    <title>Testing goodness-of-fit of random graph models</title>
    <summary>  Random graphs are matrices with independent 0, 1 elements with probabilities
determined by a small number of parameters. One of the oldest model is the
Rasch model where the odds are ratios of positive numbers scaling the rows and
columns. Later Persi Diaconis with his coworkers rediscovered the model for
symmetric matrices and called the model beta. Here we give goodnes-of-fit tests
for the model and extend the model to a version of the block model introduced
by Holland, Laskey, and Leinhard.
</summary>
    <author>
      <name>Villö Csiszár</name>
    </author>
    <author>
      <name>Péter Hussami</name>
    </author>
    <author>
      <name>János Komlós</name>
    </author>
    <author>
      <name>Tamás F. Móri</name>
    </author>
    <author>
      <name>Lídia Rejtö</name>
    </author>
    <author>
      <name>Gábor Tusnády</name>
    </author>
    <link href="http://arxiv.org/abs/1211.2442v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.2442v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G99, 60B20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.4801v1</id>
    <updated>2012-11-20T16:52:23Z</updated>
    <published>2012-11-20T16:52:23Z</published>
    <title>MCMC inference for Markov Jump Processes via the Linear Noise
  Approximation</title>
    <summary>  Bayesian analysis for Markov jump processes is a non-trivial and challenging
problem. Although exact inference is theoretically possible, it is
computationally demanding thus its applicability is limited to a small class of
problems. In this paper we describe the application of Riemann manifold MCMC
methods using an approximation to the likelihood of the Markov jump process
which is valid when the system modelled is near its thermodynamic limit. The
proposed approach is both statistically and computationally efficient while the
convergence rate and mixing of the chains allows for fast MCMC inference. The
methodology is evaluated using numerical simulations on two problems from
chemical kinetics and one from systems biology.
</summary>
    <author>
      <name>Vassilios Stathopoulos</name>
    </author>
    <author>
      <name>Mark A. Girolami</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1098/rsta.2011.0541</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1098/rsta.2011.0541" rel="related"/>
    <link href="http://arxiv.org/abs/1211.4801v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.4801v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.0122v3</id>
    <updated>2013-03-15T13:28:20Z</updated>
    <published>2012-12-01T15:05:59Z</published>
    <title>Fully Adaptive Gaussian Mixture Metropolis-Hastings Algorithm</title>
    <summary>  Markov Chain Monte Carlo methods are widely used in signal processing and
communications for statistical inference and stochastic optimization. In this
work, we introduce an efficient adaptive Metropolis-Hastings algorithm to draw
samples from generic multi-modal and multi-dimensional target distributions.
The proposal density is a mixture of Gaussian densities with all parameters
(weights, mean vectors and covariance matrices) updated using all the
previously generated samples applying simple recursive rules. Numerical results
for the one and two-dimensional cases are provided.
</summary>
    <author>
      <name>David Luengo</name>
    </author>
    <author>
      <name>Luca Martino</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICASSP.2013.6638846</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICASSP.2013.6638846" rel="related"/>
    <link href="http://arxiv.org/abs/1212.0122v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.0122v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.1038v1</id>
    <updated>2012-12-05T14:29:56Z</updated>
    <published>2012-12-05T14:29:56Z</published>
    <title>A note on marginal posterior simulation via higher-order tail area
  approximations</title>
    <summary>  We explore the use of higher-order tail area approximations for Bayesian
simulation. These approximations give rise to an alternative simulation scheme
to MCMC for Bayesian computation of marginal posterior distributions for a
scalar parameter of interest, in the presence of nuisance parameters. Its
advantage over MCMC methods is that samples are drawn independently with lower
computational time and the implementation requires only standard maximum
likelihood routines. The method is illustrated by a genetic linkage model, a
normal regression with censored data and a logistic regression model.
</summary>
    <author>
      <name>Erlis Ruli</name>
    </author>
    <author>
      <name>Nicola Sartori</name>
    </author>
    <author>
      <name>Laura Ventura</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1214/13-BA851</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1214/13-BA851" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Bayesian Analysis 09 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1212.1038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.1038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.2393v1</id>
    <updated>2012-12-11T11:56:34Z</updated>
    <published>2012-12-11T11:56:34Z</published>
    <title>Simulating the Continuation of a Time Series in R</title>
    <summary>  The simulation of the continuation of a given time series is useful for many
practical applications. But no standard procedure for this task is suggested in
the literature. It is therefore demonstrated how to use the seasonal ARIMA
process to simulate the continuation of an observed time series. The R-code
presented uses well-known modeling procedures for ARIMA models and conditional
simulation of a SARIMA model with known parameters. A small example
demonstrates the correctness and practical relevance of the new idea.
</summary>
    <author>
      <name>Halis Sak</name>
    </author>
    <author>
      <name>Wolfgang Hörmann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.2393v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.2393v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65C05" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.6.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.5669v2</id>
    <updated>2013-01-04T08:59:48Z</updated>
    <published>2012-12-22T09:07:35Z</published>
    <title>Estimation, Testing, and Prediction Regions of the Fixed and Random
  Effects by Solving the Henderson's Mixed Model Equations</title>
    <summary>  We present a brief overview of the methods for making statistical inference
(testing statistical hypotheses, construction of confidence and/or prediction
intervals and regions) about linear functions of the fixed effects and/or about
the fixed and random effects simultaneously, in conventional simple linear
mixed model. The presented approach is based on solutions from the Henderson's
mixed model equations.
</summary>
    <author>
      <name>Viktor Witkovský</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.2478/v10048-012-0033-6</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.2478/v10048-012-0033-6" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Measurement Science Review, Vol. 12, No. 6, 2012, 234-248</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1212.5669v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.5669v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62J07, 62J10, 62F10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.3928v1</id>
    <updated>2013-01-16T21:34:22Z</updated>
    <published>2013-01-16T21:34:22Z</published>
    <title>Importance sampling for weighted binary random matrices with specified
  margins</title>
    <summary>  A sequential importance sampling algorithm is developed for the distribution
that results when a matrix of independent, but not identically distributed,
Bernoulli random variables is conditioned on a given sequence of row and column
sums. This conditional distribution arises in a variety of applications and
includes as a special case the uniform distribution over zero-one tables with
specified margins. The algorithm uses dynamic programming to combine hard
margin constraints, combinatorial approximations, and additional non-uniform
weighting in a principled way to give state-of-the-art results.
</summary>
    <author>
      <name>Matthew T. Harrison</name>
    </author>
    <author>
      <name>Jeffrey W. Miller</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">39 pages (13 pages main text, 26 pages supplementary material);
  supersedes arXiv:0906.1004</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.3928v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.3928v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.5035v1</id>
    <updated>2013-01-21T23:14:39Z</updated>
    <published>2013-01-21T23:14:39Z</published>
    <title>Computing Robust Leverage Diagnostics when the Design Matrix Contains
  Coded Categorical Variables</title>
    <summary>  For a robust leverage diagnostic in linear regression, Rousseeuw and van
Zomeren [1990] proposed using robust distance (Mahalanobis distance computed
using robust estimates of location and covariance). However, a design matrix X
that contains coded categorical predictor variables is often sufficiently
sparse that robust estimates of location and covariance cannot be computed.
Specifically, matrices formed by taking subsets of the rows of X are likely to
be singular, causing algorithms that rely on subsampling to fail. Following the
spirit of Maronna and Yohai [2000], we observe that extreme leverage points are
extreme in the continuous predictor variables. We therefore propose a robust
leverage diagnostic that combines a robust analysis of the continuous predictor
variables and the classical definition of leverage.
</summary>
    <author>
      <name>Kjell Konis</name>
    </author>
    <link href="http://arxiv.org/abs/1301.5035v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.5035v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.6064v2</id>
    <updated>2013-06-25T16:27:56Z</updated>
    <published>2013-01-25T15:21:39Z</published>
    <title>Geodesic Monte Carlo on Embedded Manifolds</title>
    <summary>  Markov chain Monte Carlo methods explicitly defined on the manifold of
probability distributions have recently been established. These methods are
constructed from diffusions across the manifold and the solution of the
equations describing geodesic flows in the Hamilton--Jacobi representation.
This paper takes the differential geometric basis of Markov chain Monte Carlo
further by considering methods to simulate from probability distributions that
themselves are defined on a manifold, with common examples being classes of
distributions describing directional statistics. Proposal mechanisms are
developed based on the geodesic flows over the manifolds of support for the
distributions and illustrative examples are provided for the hypersphere and
Stiefel manifold of orthonormal matrices.
</summary>
    <author>
      <name>Simon Byrne</name>
    </author>
    <author>
      <name>Mark Girolami</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1111/sjos.12036</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1111/sjos.12036" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Scandinavian Journal of Statistics (2013), 40 (4), pages 825-845</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1301.6064v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.6064v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.1095v1</id>
    <updated>2013-02-05T16:26:24Z</updated>
    <published>2013-02-05T16:26:24Z</published>
    <title>The TimeMachine for Inference on Stochastic Trees</title>
    <summary>  The simulation of genealogical trees backwards in time, from observations up
to the most recent common ancestor (MRCA), is hindered by the fact that, while
approaching the root of the tree, coalescent events become rarer, with a
corresponding increase in computation time. The recently proposed "Time
Machine" tackles this issue by stopping the simulation of the tree before
reaching the MRCA and correcting for the induced bias. We present a
computationally efficient implementation of this approach that exploits
multithreading.
</summary>
    <author>
      <name>Gianluca Campanella</name>
    </author>
    <author>
      <name>Maria De Iorio</name>
    </author>
    <author>
      <name>Ajay Jasra</name>
    </author>
    <author>
      <name>Marc Chadeau-Hyam</name>
    </author>
    <link href="http://arxiv.org/abs/1302.1095v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.1095v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.1884v3</id>
    <updated>2015-04-07T17:34:17Z</updated>
    <published>2013-02-07T21:22:44Z</published>
    <title>Simulating from a gamma distribution with small shape parameter</title>
    <summary>  Simulating from a gamma distribution with small shape parameter is a
challenging problem. Towards an efficient method, we obtain a limiting
distribution for a suitably normalized gamma distribution when the shape
parameter tends to zero. Then this limiting distribution provides insight to
the construction of a new, simple, and highly efficient acceptance--rejection
algorithm. Comparisons based on acceptance rates show that the proposed
procedure is more efficient than existing acceptance--rejection methods.
</summary>
    <author>
      <name>Chuanhai Liu</name>
    </author>
    <author>
      <name>Ryan Martin</name>
    </author>
    <author>
      <name>Nick Syring</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1302.1884v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.1884v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.2102v1</id>
    <updated>2013-02-08T18:05:16Z</updated>
    <published>2013-02-08T18:05:16Z</published>
    <title>Estimating Common Principal Components in High Dimensions</title>
    <summary>  We consider the problem of minimizing an objective function that depends on
an orthonormal matrix. This situation is encountered when looking for common
principal components, for example, and the Flury method is a popular approach.
However, the Flury method is not effective for higher dimensional problems. We
obtain several simple majorization-minizmation (MM) algorithms that provide
solutions to this problem and are effective in higher dimensions. We then use
simulated data to compare them with other approaches in terms of convergence
and computational time.
</summary>
    <author>
      <name>Ryan P. Browne</name>
    </author>
    <author>
      <name>Paul D. McNicholas</name>
    </author>
    <link href="http://arxiv.org/abs/1302.2102v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.2102v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62H30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.6529v2</id>
    <updated>2013-03-28T11:49:05Z</updated>
    <published>2013-03-26T15:29:24Z</published>
    <title>Random generation of optimal saturated designs</title>
    <summary>  Efficient algorithms for searching for optimal saturated designs are widely
available. They maximize a given efficiency measure (such as D-optimality) and
provide an optimum design. Nevertheless, they do not guarantee a \emph{global}
optimal design. Indeed, they start from an initial random design and find a
local optimal design. If the initial design is changed the optimum found will,
in general, be different. A natural question arises. Should we stop at the
design found or should we run the algorithm again in search of a better design?
This paper uses very recent methods and software for discovery probability to
support the decision to continue or stop the sampling. A software tool written
in SAS has been developed.
</summary>
    <author>
      <name>Roberto Fontana</name>
    </author>
    <link href="http://arxiv.org/abs/1303.6529v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.6529v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62K05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.1350v1</id>
    <updated>2013-04-04T12:36:16Z</updated>
    <published>2013-04-04T12:36:16Z</published>
    <title>A Direct Sampler for G-Wishart Variates</title>
    <summary>  The G-Wishart distribution is the conjugate prior for precision matrices that
encode the conditional independencies of a Gaussian graphical model. While the
distribution has received considerable attention, posterior inference has
proven computationally challenging, in part due to the lack of a direct
sampler. In this note, we rectify this situation. The existence of a direct
sampler offers a host of new possibilities for the use of G-Wishart variates.
We discuss one such development by outlining a new transdimensional model
search algorithm--which we term double reversible jump--that leverages this
sampler to avoid normalizing constant calculation when comparing graphical
models. We conclude with two short studies meant to investigate our algorithm's
validity.
</summary>
    <author>
      <name>Alex Lenkoski</name>
    </author>
    <link href="http://arxiv.org/abs/1304.1350v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.1350v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.3673v1</id>
    <updated>2013-04-12T16:28:41Z</updated>
    <published>2013-04-12T16:28:41Z</published>
    <title>Bayesian analysis of matrix data with rstiefel</title>
    <summary>  We illustrate the use of the R-package "rstiefel" for matrix-variate data
analysis in the context of two examples. The first example considers estimation
of a reduced-rank mean matrix in the presence of normally distributed noise.
The second example considers the modeling of a social network of friendships
among teenagers. Bayesian estimation for these models requires the ability to
simulate from the matrix-variate von Mises-Fisher distributions and the
matrix-variate Bingham distributions on the Stiefel manifold.
</summary>
    <author>
      <name>Peter D. Hoff</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is a vignette for the R-package "rstiefel"</arxiv:comment>
    <link href="http://arxiv.org/abs/1304.3673v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.3673v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62H11, 62H25, 65C40" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.3800v3</id>
    <updated>2013-06-25T23:14:41Z</updated>
    <published>2013-04-13T11:31:37Z</published>
    <title>Extremely efficient generation of Gamma random variables for α&gt;= 1</title>
    <summary>  The Gamma distribution is well-known and widely used in many signal
processing and communications applications. In this letter, a simple and
extremely efficient accept/reject algorithm is introduced for the generation of
independent random variables from a Gamma distribution with any shape parameter
\alpha &gt;= 1. The proposed method uses another Gamma distribution with integer
\alpha_p &lt;= \alpha, from which samples can be easily drawn, as proposal
function. For this reason, the new technique attains a higher acceptance rate
(AR) for \alpha &gt;= 3 than all the methods currently available in the
literature, with AR tends to 1 as \alpha\ diverges.
</summary>
    <author>
      <name>Luca Martino</name>
    </author>
    <author>
      <name>David Luengo</name>
    </author>
    <link href="http://arxiv.org/abs/1304.3800v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.3800v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.7808v1</id>
    <updated>2013-04-29T21:15:07Z</updated>
    <published>2013-04-29T21:15:07Z</published>
    <title>Initializing adaptive importance sampling with Markov chains</title>
    <summary>  Adaptive importance sampling is a powerful tool to sample from complicated
target densities, but its success depends sensitively on the initial proposal
density. An algorithm is presented to automatically perform the initialization
using Markov chains and hierarchical clustering. The performance is checked on
challenging multimodal examples in up to 20 dimensions and compared to results
from nested sampling. Our approach yields a proposal that leads to rapid
convergence and accurate estimation of overall normalization and marginal
distributions.
</summary>
    <author>
      <name>Frederik Beaujean</name>
    </author>
    <author>
      <name>Allen Caldwell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1304.7808v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.7808v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.7973v1</id>
    <updated>2013-04-30T12:22:17Z</updated>
    <published>2013-04-30T12:22:17Z</published>
    <title>Calculating the normalising constant of the Bingham distribution on the
  sphere using the holonomic gradient method</title>
    <summary>  In this paper we implement the holonomic gradient method to exactly compute
the normalising constant of Bingham distributions. This idea is originally
applied for general Fisher-Bingham distributions in Nakayama et al. (2011). In
this paper we explicitly apply this algorithm to show the exact calculation of
the normalising constant; derive explicitly the Pfaffian system for this
parametric case; implement the general approach for the maximum likelihood
solution search and finally adjust the method for degenerate cases, namely when
the parameter values have multiplicities.
</summary>
    <author>
      <name>Tomonari Sei</name>
    </author>
    <author>
      <name>Alfred Kume</name>
    </author>
    <link href="http://arxiv.org/abs/1304.7973v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.7973v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.0040v1</id>
    <updated>2013-05-31T21:57:12Z</updated>
    <published>2013-05-31T21:57:12Z</published>
    <title>Expectation-maximization for logistic regression</title>
    <summary>  We present a family of expectation-maximization (EM) algorithms for binary
and negative-binomial logistic regression, drawing a sharp connection with the
variational-Bayes algorithm of Jaakkola and Jordan (2000). Indeed, our results
allow a version of this variational-Bayes approach to be re-interpreted as a
true EM algorithm. We study several interesting features of the algorithm, and
of this previously unrecognized connection with variational Bayes. We also
generalize the approach to sparsity-promoting priors, and to an online method
whose convergence properties are easily established. This latter method
compares favorably with stochastic-gradient descent in situations with marked
collinearity.
</summary>
    <author>
      <name>James G. Scott</name>
    </author>
    <author>
      <name>Liang Sun</name>
    </author>
    <link href="http://arxiv.org/abs/1306.0040v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.0040v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.1265v3</id>
    <updated>2014-10-01T22:16:02Z</updated>
    <published>2013-06-05T23:16:31Z</published>
    <title>Sparse Covers for Sums of Indicators</title>
    <summary>  For all $n, \epsilon &gt;0$, we show that the set of Poisson Binomial
distributions on $n$ variables admits a proper $\epsilon$-cover in total
variation distance of size $n^2+n \cdot (1/\epsilon)^{O(\log^2 (1/\epsilon))}$,
which can also be computed in polynomial time. We discuss the implications of
our construction for approximation algorithms and the computation of
approximate Nash equilibria in anonymous games.
</summary>
    <author>
      <name>Constantinos Daskalakis</name>
    </author>
    <author>
      <name>Christos Papadimitriou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PTRF, to appear</arxiv:comment>
    <link href="http://arxiv.org/abs/1306.1265v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.1265v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.6028v1</id>
    <updated>2013-06-25T16:49:11Z</updated>
    <published>2013-06-25T16:49:11Z</published>
    <title>Adaptive MC^3 and Gibbs algorithms for Bayesian Model Averaging in
  Linear Regression Models</title>
    <summary>  The MC$^3$ (Madigan and York, 1995) and Gibbs (George and McCulloch, 1997)
samplers are the most widely implemented algorithms for Bayesian Model
Averaging (BMA) in linear regression models. These samplers draw a variable at
random in each iteration using uniform selection probabilities and then propose
to update that variable. This may be computationally inefficient if the number
of variables is large and many variables are redundant. In this work, we
introduce adaptive versions of these samplers that retain their simplicity in
implementation and reduce the selection probabilities of the many redundant
variables. The improvements in efficiency for the adaptive samplers are
illustrated in real and simulated datasets.
</summary>
    <author>
      <name>Demetris Lamnisos</name>
    </author>
    <author>
      <name>Jim E. Griffin</name>
    </author>
    <author>
      <name>Mark F. J. Steel</name>
    </author>
    <link href="http://arxiv.org/abs/1306.6028v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.6028v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.6408v1</id>
    <updated>2013-06-27T05:19:38Z</updated>
    <published>2013-06-27T05:19:38Z</published>
    <title>Using interpolation to reduce computing time for analysis of large but
  simple data sets with application to design of epidemiological studies</title>
    <summary>  One way to investigate the precision of estimates likely to result from
planned experiments and planned epidemiological studies is to simulate a large
number of possible outcomes and analyse the sets of possible results. This
appears to be computationally expensive for some multi-stage designs, so choice
of designs is instead based on theoretical derivation of expected information.
This paper shows that for some types of studies the analysis of large numbers
of simulated outcomes can be achieved more rapidly by making use of
interpolation.
</summary>
    <author>
      <name>G. K. Robinson</name>
    </author>
    <author>
      <name>L. M. Ryan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, no figures or tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1306.6408v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.6408v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.1799v2</id>
    <updated>2013-12-28T10:50:34Z</updated>
    <published>2013-07-06T16:18:48Z</published>
    <title>The Containment Condition and AdapFail algorithms</title>
    <summary>  This short note investigates convergence of adaptive MCMC algorithms, i.e.\
algorithms which modify the Markov chain update probabilities on the fly. We
focus on the Containment condition introduced in \cite{roberts2007coupling}. We
show that if the Containment condition is \emph{not} satisfied, then the
algorithm will perform very poorly. Specifically, with positive probability,
the adaptive algorithm will be asymptotically less efficient then \emph{any}
nonadaptive ergodic MCMC algorithm. We call such algorithms \texttt{AdapFail},
and conclude that they should not be used.
</summary>
    <author>
      <name>Krzysztof Latuszynski</name>
    </author>
    <author>
      <name>Jeffrey S. Rosenthal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">slight revision and with referees comments incorporated</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.1799v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.1799v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.2668v1</id>
    <updated>2013-07-10T04:31:06Z</updated>
    <published>2013-07-10T04:31:06Z</published>
    <title>Bayesian Quantile Regression for Partially Linear Additive Models</title>
    <summary>  In this article, we develop a semiparametric Bayesian estimation and model
selection approach for partially linear additive models in conditional quantile
regression. The asymmetric Laplace distribution provides a mechanism for
Bayesian inferences of quantile regression models based on the check loss. The
advantage of this new method is that nonlinear, linear and zero function
components can be separated automatically and simultaneously during model
fitting without the need of pre-specification or parameter tuning. This is
achieved by spike-and-slab priors using two sets of indicator variables. For
posterior inferences, we design an effective partially collapsed Gibbs sampler.
Simulation studies are used to illustrate our algorithm. The proposed approach
is further illustrated by applications to two real data sets.
</summary>
    <author>
      <name>Yuao Hu</name>
    </author>
    <author>
      <name>Kaifeng Zhao</name>
    </author>
    <author>
      <name>Heng Lian</name>
    </author>
    <link href="http://arxiv.org/abs/1307.2668v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.2668v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.3180v2</id>
    <updated>2014-01-29T10:37:15Z</updated>
    <published>2013-07-11T16:45:47Z</published>
    <title>Path storage in the particle filter</title>
    <summary>  This article considers the problem of storing the paths generated by a
particle filter and more generally by a sequential Monte Carlo algorithm. It
provides a theoretical result bounding the expected memory cost by $T + C N
\log N$ where $T$ is the time horizon, $N$ is the number of particles and $C$
is a constant, as well as an efficient algorithm to realise this. The
theoretical result and the algorithm are illustrated with numerical
experiments.
</summary>
    <author>
      <name>Pierre E. Jacob</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">National University of Singapore</arxiv:affiliation>
    </author>
    <author>
      <name>Lawrence Murray</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CSIRO Mathematics, Informatics &amp; Statistics</arxiv:affiliation>
    </author>
    <author>
      <name>Sylvain Rubenthaler</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Univ. Nice Sophia Antipolis</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11222-013-9445-x</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11222-013-9445-x" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 5 figures. To appear in Statistics and Computing</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.3180v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.3180v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65C35, 65M75" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.8270v1</id>
    <updated>2013-07-31T10:21:39Z</updated>
    <published>2013-07-31T10:21:39Z</published>
    <title>Applying least absolute deviation regression to regression-type
  estimation of the index of a stable distribution using the characteristic
  function</title>
    <summary>  Least absolute deviation regression is applied using a fixed number of points
for all values of the index to estimate the index and scale parameter of the
stable distribution using regression methods based on the empirical
characteristic function. The recognized fixed number of points estimation
procedure uses ten points in the interval zero to one, and least squares
estimation. It is shown that using the more robust least absolute regression
based on iteratively re-weighted least squares outperforms the least squares
procedure with respect to bias and also mean square error in smaller samples.
</summary>
    <author>
      <name>J. Martin van Zyl</name>
    </author>
    <link href="http://arxiv.org/abs/1307.8270v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.8270v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.1246v1</id>
    <updated>2013-09-05T07:20:28Z</updated>
    <published>2013-09-05T07:20:28Z</published>
    <title>Holonomic Decent Minimization Method for Restricted Maximum Likelihood
  Estimation</title>
    <summary>  Recently, the school of Takemura and Takayama have developed a quite
interesting minimization method called holonomic gradient descent method (HGD).
It works by a mixed use of Pfaffian differential equation satisfied by an
objective holonomic function and an iterative optimization method. They
successfully applied the method to several maximum likelihood estimation (MLE)
problems, which have been intractable in the past. On the other hand, in
statistical models, it is not rare that parameters are constrained and
therefore the MLE with constraints has been surely one of fundamental topics in
statistics. In this paper we develop HGD with constraints for MLE .
</summary>
    <author>
      <name>Rieko Sakurai</name>
    </author>
    <author>
      <name>Toshio Sakata</name>
    </author>
    <link href="http://arxiv.org/abs/1309.1246v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.1246v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.1296v1</id>
    <updated>2013-09-05T09:57:33Z</updated>
    <published>2013-09-05T09:57:33Z</published>
    <title>Regression with an infinite number of observations applied to estimating
  the parameters of the stable distribution using the empirical characteristic
  function</title>
    <summary>  A function of the empirical characteristic function,exists for the stable
distribution, which leads to a linear regression and can be used to estimate
the parameters. Two approaches are often used, one to find optimal values of t,
but these points are dependent on the unknown parameters. And using a fixed
number of values for t. In this work the results when all points in an interval
is used, thus where least squares using an infinite number of observations,is
approximated. It was found that this procedure performs good in small samples.
</summary>
    <author>
      <name>J. Martin van Zyl</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1080/03610926.2014.901382</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1080/03610926.2014.901382" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Communications in Statistics - Theory and Methods Volume 45, Issue
  11, 2016 pgs 3323-3331</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1309.1296v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.1296v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.5117v1</id>
    <updated>2013-09-19T22:59:44Z</updated>
    <published>2013-09-19T22:59:44Z</published>
    <title>Diagnostics for Variational Bayes approximations</title>
    <summary>  Variational Bayes (VB) has shown itself to be a powerful approximation method
in many application areas. This paper describes some diagnostics methods which
can assess how well the VB approximates the true posterior, particularly with
regards to its covariance structure. The methods proposed also allow us to
generate simple corrections when the approximation error is large. It looks at
joint, marginal and conditional aspects of the approximate posterior and shows
how to apply these techniques in both simulated and real data examples.
</summary>
    <author>
      <name>Hui Zhao</name>
    </author>
    <author>
      <name>Paul Marriott</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.5117v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.5117v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.5122v1</id>
    <updated>2013-09-20T00:43:50Z</updated>
    <published>2013-09-20T00:43:50Z</published>
    <title>Variational Bayes inference and Dirichlet process priors</title>
    <summary>  This paper shows how the variational Bayes method provides a computational
efficient technique in the context of hierarchical modelling using Dirichlet
process priors, in particular without requiring conjugate prior assumption. It
shows, using the so called parameter separation parameterization, a simple
criterion under which the variational method works well. Based on this
framework, its provides a full variational solution for the Dirichlet process.
The numerical results show that the method is very computationally efficient
when compared to MCMC. Finally, we propose an empirical method to estimate the
truncation level for the truncated Dirichlet process.
</summary>
    <author>
      <name>Hui Zhao</name>
    </author>
    <author>
      <name>Paul Marriott</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.5122v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.5122v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.5489v1</id>
    <updated>2013-09-21T15:18:34Z</updated>
    <published>2013-09-21T15:18:34Z</published>
    <title>Computational Aspects of Optional Pólya Tree</title>
    <summary>  Optional P\'{o}lya Tree (OPT) is a flexible non-parametric Bayesian model for
density estimation. Despite its merits, the computation for OPT inference is
challenging. In this paper we present time complexity analysis for OPT
inference and propose two algorithmic improvements. The first improvement,
named Limited-Lookahead Optional P\'{o}lya Tree (LL-OPT), aims at greatly
accelerate the computation for OPT inference. The second improvement modifies
the output of OPT or LL-OPT and produces a continuous piecewise linear density
estimate. We demonstrate the performance of these two improvements using
simulations.
</summary>
    <author>
      <name>Hui Jiang</name>
    </author>
    <author>
      <name>John C. Mu</name>
    </author>
    <author>
      <name>Kun Yang</name>
    </author>
    <author>
      <name>Chao Du</name>
    </author>
    <author>
      <name>Luo Lu</name>
    </author>
    <author>
      <name>Wing Hung Wong</name>
    </author>
    <link href="http://arxiv.org/abs/1309.5489v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.5489v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.5808v1</id>
    <updated>2013-09-23T14:01:32Z</updated>
    <published>2013-09-23T14:01:32Z</published>
    <title>Efficient goodness-of-fit tests in multi-dimensional vine copula models</title>
    <summary>  We introduce a new goodness-of-fit test for regular vine (R-vine) copula
models, a flexible class of multivariate copulas based on a pair-copula
construction (PCC). The test arises from the information matrix ratio. The
corresponding test statistic is derived and its asymptotic normality is proven.
The test's power is investigated and compared to 14 other goodness-of-fit
tests, adapted from the bivariate copula case, in a high dimensional setting.
The extensive simulation study shows the excellent performance with respect to
size and power as well as the superiority of the information matrix ratio based
test against most other goodness-of-fit tests. The best performing tests are
applied to a portfolio of stock indices and their related volatility indices
validating different R-vine specifications.
</summary>
    <author>
      <name>Ulf Schepsmeier</name>
    </author>
    <link href="http://arxiv.org/abs/1309.5808v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.5808v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.1034v1</id>
    <updated>2013-10-03T17:01:34Z</updated>
    <published>2013-10-03T17:01:34Z</published>
    <title>Computing Exact Clustering Posteriors with Subset Convolution</title>
    <summary>  An exponential-time exact algorithm is provided for the task of clustering n
items of data into k clusters. Instead of seeking one partition, posterior
probabilities are computed for summary statistics: the number of clusters, and
pairwise co-occurrence. The method is based on subset convolution, and yields
the posterior distribution for the number of clusters in O(n * 3^n) operations,
or O(n^3 * 2^n) using fast subset convolution. Pairwise co-occurrence
probabilities are then obtained in O(n^3 * 2^n) operations. This is
considerably faster than exhaustive enumeration of all partitions.
</summary>
    <author>
      <name>Jukka Kohonen</name>
    </author>
    <author>
      <name>Jukka Corander</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1310.1034v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.1034v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.3596v1</id>
    <updated>2013-10-14T08:53:15Z</updated>
    <published>2013-10-14T08:53:15Z</published>
    <title>Semiparametric Cross Entropy for rare-event simulation</title>
    <summary>  The Cross Entropy method is a well-known adaptive importance sampling method
for rare-event probability estimation, which requires estimating an optimal
importance sampling density within a parametric class. In this article we
estimate an optimal importance sampling density within a wider semiparametric
class of distributions. We show that this semiparametric version of the Cross
Entropy method frequently yields efficient estimators. We illustrate the
excellent practical performance of the method with numerical experiments and
show that for the problems we consider it typically outperforms alternative
schemes by orders of magnitude.
</summary>
    <author>
      <name>Z. I. Botev</name>
    </author>
    <author>
      <name>A. Ridder</name>
    </author>
    <author>
      <name>L. Rojas-Nandayapa</name>
    </author>
    <link href="http://arxiv.org/abs/1310.3596v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.3596v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.4411v1</id>
    <updated>2013-10-16T15:11:22Z</updated>
    <published>2013-10-16T15:11:22Z</published>
    <title>The Danger of a Big Data Episteme and the Need to Evolve GIS</title>
    <summary>  The emergence of "Big Data" as a dominant technology meme challenges
Geography's technical underpinnings, found in GIS, while engaging the
discipline in a conversation about the meme's impact on society. This allows
scholars to engage collaboratively from both a computationally quantitative and
critically qualitative perspective. For Geography there is an opportunity to
point out these shortcomings through critical appraisals of "Big Data" and its
reflection of society. Complimentarily this opens the door to developing
methodologies that will allow for a more realistic interpretation of "Big Data"
analysis in the context of an unfiltered societal view.
</summary>
    <author>
      <name>Sean P. Gorman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AAG Conference 2013 - Los angeles, CA - More data, more problems?
  Geography and the future of 'big data'</arxiv:comment>
    <link href="http://arxiv.org/abs/1310.4411v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.4411v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.0674v2</id>
    <updated>2014-01-11T18:05:36Z</updated>
    <published>2013-11-04T12:30:46Z</published>
    <title>On the use of marginal posteriors in marginal likelihood estimation via
  importance-sampling</title>
    <summary>  We investigate the efficiency of a marginal likelihood estimator where the
product of the marginal posterior distributions is used as an
importance-sampling function. The approach is generally applicable to
multi-block parameter vector settings, does not require additional Markov Chain
Monte Carlo (MCMC) sampling and is not dependent on the type of MCMC scheme
used to sample from the posterior. The proposed approach is applied to normal
regression models, finite normal mixtures and longitudinal Poisson models, and
leads to accurate marginal likelihood estimates.
</summary>
    <author>
      <name>K. Perrakis</name>
    </author>
    <author>
      <name>I. Ntzoufras</name>
    </author>
    <author>
      <name>E. G. Tsionas</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.csda.2014.03.004</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.csda.2014.03.004" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computational Statistics &amp; Data Analysis Volume 77, September
  2014, Pages 54-69</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1311.0674v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.0674v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.1129v1</id>
    <updated>2013-11-05T17:17:15Z</updated>
    <published>2013-11-05T17:17:15Z</published>
    <title>Discussion of "Geodesic Monte Carlo on Embedded Manifolds"</title>
    <summary>  Contributed discussion and rejoinder to "Geodesic Monte Carlo on Embedded
Manifolds" (arXiv:1301.6064)
</summary>
    <author>
      <name>Simon Byrne</name>
    </author>
    <author>
      <name>Mark Girolami</name>
    </author>
    <author>
      <name>Persi Diaconis</name>
    </author>
    <author>
      <name>Christof Seiler</name>
    </author>
    <author>
      <name>Susan Holmes</name>
    </author>
    <author>
      <name>Ian L. Dryden</name>
    </author>
    <author>
      <name>John T. Kent</name>
    </author>
    <author>
      <name>Marcelo Pereyra</name>
    </author>
    <author>
      <name>Babak Shahbaba</name>
    </author>
    <author>
      <name>Shiwei Lan</name>
    </author>
    <author>
      <name>Jeffrey Streets</name>
    </author>
    <author>
      <name>Daniel Simpson</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1111/sjos.12081</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1111/sjos.12081" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Discussion of arXiv:1301.6064. To appear in the Scandinavian Journal
  of Statistics. 18 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Scandinavian Journal of Statistics (2014) 41 (1), pages 1-21</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1311.1129v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.1129v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.2166v2</id>
    <updated>2015-10-12T14:40:38Z</updated>
    <published>2013-11-09T12:59:17Z</published>
    <title>Auxiliary-variable Exact Hamiltonian Monte Carlo Samplers for Binary
  Distributions</title>
    <summary>  We present a new approach to sample from generic binary distributions, based
on an exact Hamiltonian Monte Carlo algorithm applied to a piecewise continuous
augmentation of the binary distribution of interest. An extension of this idea
to distributions over mixtures of binary and possibly-truncated Gaussian or
exponential variables allows us to sample from posteriors of linear and probit
regression models with spike-and-slab priors and truncated parameters. We
illustrate the advantages of these algorithms in several examples in which they
outperform the Metropolis or Gibbs samplers.
</summary>
    <author>
      <name>Ari Pakman</name>
    </author>
    <author>
      <name>Liam Paninski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 4 figures. Proceedings of the 27th Annual Conference Neural
  Information Processing Systems (NIPS), 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.2166v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.2166v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.4117v1</id>
    <updated>2013-11-17T05:27:56Z</updated>
    <published>2013-11-17T05:27:56Z</published>
    <title>Parameter Estimation in Hidden Markov Models with Intractable
  Likelihoods Using Sequential Monte Carlo</title>
    <summary>  We propose sequential Monte Carlo based algorithms for maximum likelihood
estimation of the static parameters in hidden Markov models with an intractable
likelihood using ideas from approximate Bayesian computation. The static
parameter estimation algorithms are gradient based and cover both offline and
online estimation. We demonstrate their performance by estimating the
parameters of three intractable models, namely the alpha-stable distribution,
g-and-k distribution, and the stochastic volatility model with alpha-stable
returns, using both real and synthetic data.
</summary>
    <author>
      <name>Sinan Yildirim</name>
    </author>
    <author>
      <name>Sumeetpal Singh</name>
    </author>
    <author>
      <name>Thomas Dean</name>
    </author>
    <author>
      <name>Ajay Jasra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 8 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.4117v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.4117v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.6529v1</id>
    <updated>2013-11-26T00:57:30Z</updated>
    <published>2013-11-26T00:57:30Z</published>
    <title>A Blockwise Descent Algorithm for Group-penalized Multiresponse and
  Multinomial Regression</title>
    <summary>  In this paper we purpose a blockwise descent algorithm for group-penalized
multiresponse regression. Using a quasi-newton framework we extend this to
group-penalized multinomial regression. We give a publicly available
implementation for these in R, and compare the speed of this algorithm to a
competing algorithm --- we show that our implementation is an order of
magnitude faster than its competitor, and can solve gene-expression-sized
problems in real time.
</summary>
    <author>
      <name>Noah Simon</name>
    </author>
    <author>
      <name>Jerome Friedman</name>
    </author>
    <author>
      <name>Trevor Hastie</name>
    </author>
    <link href="http://arxiv.org/abs/1311.6529v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.6529v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.1903v2</id>
    <updated>2014-08-29T08:38:58Z</updated>
    <published>2013-12-06T16:06:38Z</published>
    <title>A sequential reduction method for inference in generalized linear mixed
  models</title>
    <summary>  The likelihood for the parameters of a generalized linear mixed model
involves an integral which may be of very high dimension. Because of this
intractability, many approximations to the likelihood have been proposed, but
all can fail when the model is sparse, in that there is only a small amount of
information available on each random effect. The sequential reduction method
described in this paper exploits the dependence structure of the posterior
distribution of the random effects to reduce substantially the cost of finding
an accurate approximation to the likelihood in models with sparse structure.
</summary>
    <author>
      <name>Helen Ogden</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 3 Figures, 2 Tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.1903v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.1903v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.2556v1</id>
    <updated>2013-12-06T11:35:14Z</updated>
    <published>2013-12-06T11:35:14Z</published>
    <title>A method for importance sampling through Markov chain Monte Carlo with
  post sampling variational estimate</title>
    <summary>  We propose a method to efficiently integrate truncated probability densities.
The method uses Markov chain Monte Carlo method to sample from a probability
density matching the function being integrated. The required normalisation or
equivalently the result is obtained by constructing a function with known
integral, through non-parametric kernel density estimation and variational
procedure. The method is demonstrated with numerical case studies. Possible
enhancements to the method and limitations are discussed.
</summary>
    <author>
      <name>A. John Arul</name>
    </author>
    <author>
      <name>Kannan Iyer</name>
    </author>
    <link href="http://arxiv.org/abs/1312.2556v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.2556v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.3027v1</id>
    <updated>2013-12-11T03:21:53Z</updated>
    <published>2013-12-11T03:21:53Z</published>
    <title>Rare-event Probability Estimation via Empirical Likelihood Maximization</title>
    <summary>  We explore past and recent developments in rare-event probability estimation
with a particular focus on a novel Monte Carlo technique Empirical Likelihood
Maximization (ELM). This is a versatile method that involves sampling from a
sequence of densities using MCMC and maximizing an empirical likelihood. The
quantity of interest, the probability of a given rare-event, is estimated by
solving a convex optimization program related to likelihood maximization.
Numerical experiments are performed using this new technique and benchmarks are
given against existing robust algorithms and estimators.
</summary>
    <author>
      <name>A. Huang</name>
    </author>
    <author>
      <name>Z. I. Botev</name>
    </author>
    <link href="http://arxiv.org/abs/1312.3027v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.3027v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.7827v1</id>
    <updated>2013-12-30T19:09:52Z</updated>
    <published>2013-12-30T19:09:52Z</published>
    <title>Surface response analysis and determination of confidence regions for
  atmospheric CO2: a global warming study for U.S.A. data</title>
    <summary>  Starting from the atmospheric CO2 measurements taken in Hawaii between 1959
and 2008, a quadratic model with interactions was fitted, using 5 attributable
variables. Surface response analysis returned the eigenvalues and eigenvectors
at the critical point, which turns out to be of mixed type, with two positive
eigenvalues, one null, and the rest negative. From these data, it is derived
that the confidence regions in two variables are of various types (elliptic,
hyperbolic, and degenerate). Based on these results we indicate how to
determine two-dimensional confidence regions for statistically-significant
variables which are relevant contributors to the atmospheric CO2 emissions.
</summary>
    <author>
      <name>Iuliana Teodorescu</name>
    </author>
    <author>
      <name>Chris Tsokos</name>
    </author>
    <link href="http://arxiv.org/abs/1312.7827v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.7827v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.0265v1</id>
    <updated>2014-01-01T08:59:39Z</updated>
    <published>2014-01-01T08:59:39Z</published>
    <title>Approximate Bayesian Computation for a Class of Time Series Models</title>
    <summary>  In the following article we consider approximate Bayesian computation (ABC)
for certain classes of time series models. In particular, we focus upon
scenarios where the likelihoods of the observations and parameter are
intractable, by which we mean that one cannot evaluate the likelihood even
up-to a positive unbiased estimate. This paper reviews and develops a class of
approximation procedures based upon the idea of ABC, but, specifically
maintains the probabilistic structure of the original statistical model. This
idea is useful, in that it can facilitate an analysis of the bias of the
approximation and the adaptation of established computational methods for
parameter inference. Several existing results in the literature are surveyed
and novel developments with regards to computation are given.
</summary>
    <author>
      <name>Ajay Jasra</name>
    </author>
    <link href="http://arxiv.org/abs/1401.0265v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.0265v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.2135v1</id>
    <updated>2014-01-09T19:44:36Z</updated>
    <published>2014-01-09T19:44:36Z</published>
    <title>Implementing and Automating Fixed-Form Variational Posterior
  Approximation through Stochastic Linear Regression</title>
    <summary>  We recently proposed a general algorithm for approximating nonstandard
Bayesian posterior distributions by minimization of their Kullback-Leibler
divergence with respect to a more convenient approximating distribution. In
this note we offer details on how to efficiently implement this algorithm in
practice. We also suggest default choices for the form of the posterior
approximation, the number of iterations, the step size, and other user choices.
By using these defaults it becomes possible to construct good posterior
approximations for hierarchical models completely automatically.
</summary>
    <author>
      <name>Tim Salimans</name>
    </author>
    <link href="http://arxiv.org/abs/1401.2135v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.2135v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.3812v1</id>
    <updated>2014-01-16T02:21:05Z</updated>
    <published>2014-01-16T02:21:05Z</published>
    <title>Estimating Box-Cox power transformation parameter via goodness of fit
  tests</title>
    <summary>  Box-Cox power transformation is a commonly used methodology to transform the
distribution of a non-normal data into a normal one. Estimation of the
transformation parameter is crucial in this methodology. In this study, the
estimation process is hold via a searching algorithm and is integrated into
well-known seven goodness of fit tests for normal distribution. An artificial
covariate method is also included for comparative purposes. Simulation studies
are implemented to compare the effectiveness of the proposed methods. The
methods are also illustrated on two different real life data applications.
Moreover, an R package AID is proposed for implementation.
</summary>
    <author>
      <name>Ozgur Asar</name>
    </author>
    <author>
      <name>Ozlem Ilk</name>
    </author>
    <author>
      <name>Osman Dag</name>
    </author>
    <link href="http://arxiv.org/abs/1401.3812v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.3812v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.4912v5</id>
    <updated>2015-02-05T15:20:25Z</updated>
    <published>2014-01-20T14:19:09Z</published>
    <title>An Importance Sampling Scheme on Dual Factor Graphs. I. Models in a
  Strong External Field</title>
    <summary>  We propose an importance sampling scheme to estimate the partition function
of the two-dimensional ferromagnetic Ising model and the two-dimensional
ferromagnetic $q$-state Potts model, both in the presence of an external
magnetic field. The proposed scheme operates in the dual Forney factor graph
and is capable of efficiently computing an estimate of the partition function
under a wide range of model parameters. In particular, we consider models that
are in a strong external magnetic field.
</summary>
    <author>
      <name>Mehdi Molkaraie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.4912v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.4912v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.5548v1</id>
    <updated>2014-01-22T03:33:45Z</updated>
    <published>2014-01-22T03:33:45Z</published>
    <title>On Bayesian inference for the M/G/1 queue with efficient MCMC sampling</title>
    <summary>  We introduce an efficient MCMC sampling scheme to perform Bayesian inference
in the M/G/1 queueing model given only observations of interdeparture times.
Our MCMC scheme uses a combination of Gibbs sampling and simple Metropolis
updates together with three novel "shift" and "scale" updates. We show that our
novel updates improve the speed of sampling considerably, by factors of about
60 to about 180 on a variety of simulated data sets.
</summary>
    <author>
      <name>Alexander Y. Shestopaloff</name>
    </author>
    <author>
      <name>Radford M. Neal</name>
    </author>
    <link href="http://arxiv.org/abs/1401.5548v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.5548v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.2828v2</id>
    <updated>2016-01-25T17:02:42Z</updated>
    <published>2014-02-12T14:15:56Z</published>
    <title>Decomposition Sampling applied to Parallelization of Metropolis-Hastings</title>
    <summary>  This paper presents an algorithm for sampling random variables that allows to
separation of the sampling process into subproblems by dividing the sample
space into overlapping parts. The subproblems can be solved independently of
each other and are thus well suited for parallelization. Furthermore, on each
of these subproblems it is possible to use distinct and independent sampling
methods. In other words, specific samplers can be designed for specific parts
of the sample space. The algorithms are demonstrated on a particle marginal
Metropolis-Hastings sampler applied to calibration of a volatility model and
two toy examples. Significant speedup and decrease of total variation is
observed in experiments.
</summary>
    <author>
      <name>Jonas Hallgren</name>
    </author>
    <author>
      <name>Timo Koski</name>
    </author>
    <link href="http://arxiv.org/abs/1402.2828v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.2828v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.3410v1</id>
    <updated>2014-02-14T09:43:04Z</updated>
    <published>2014-02-14T09:43:04Z</published>
    <title>Wmixnet: Software for Clustering the Nodes of Binary and Valued Graphs
  using the Stochastic Block Model</title>
    <summary>  Clustering the nodes of a graph allows the analysis of the topology of a
network.
  The stochastic block model is a clustering method based on a probabilistic
model. Initially developed for binary networks it has recently been extended to
valued networks possibly with covariates on the edges.
  We present an implementation of a variational EM algorithm. It is written
using C++, parallelized, available under a GNU General Public License (version
3), and can select the optimal number of clusters using the ICL criteria. It
allows us to analyze networks with ten thousand nodes in a reasonable amount of
time.
</summary>
    <author>
      <name>Jean-Benoist Leger</name>
    </author>
    <link href="http://arxiv.org/abs/1402.3410v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.3410v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.3466v2</id>
    <updated>2014-07-28T18:08:28Z</updated>
    <published>2014-02-14T14:06:40Z</published>
    <title>Kernel density estimates in particle filter</title>
    <summary>  The paper deals with kernel density estimates of filtering densities in the
particle filter. The convergence of the estimates is investigated by means of
Fourier analysis. It is shown that the estimates converge to the theoretical
filtering densities in the mean integrated squared error under a certain
assumption on the Sobolev character of the filtering densities. A sufficient
condition is presented for the persistence of this Sobolev character over time.
Both results are extended to partial derivatives of the estimates and filtering
densities.
</summary>
    <author>
      <name>David Coufal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Substantially revised version. The extension of results to partial
  derivatives has been provided</arxiv:comment>
    <link href="http://arxiv.org/abs/1402.3466v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.3466v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.3569v2</id>
    <updated>2014-06-23T10:22:49Z</updated>
    <published>2014-02-14T20:18:22Z</published>
    <title>A Fast Algorithm for Sampling from the Posterior of a von Mises
  distribution</title>
    <summary>  Motivated by molecular biology, there has been an upsurge of research
activities in directional statistics in general and its Bayesian aspect in
particular. The central distribution for the circular case is von Mises
distribution which has two parameters (mean and concentration) akin to the
univariate normal distribution. However, there has been a challenge to sample
efficiently from the posterior distribution of the concentration parameter. We
describe a novel, highly efficient algorithm to sample from the posterior
distribution and fill this long-standing gap.
</summary>
    <author>
      <name>Peter G. M. Forbes</name>
    </author>
    <author>
      <name>Kanti V. Mardia</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1080/00949655.2014.928711</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1080/00949655.2014.928711" rel="related"/>
    <link href="http://arxiv.org/abs/1402.3569v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.3569v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62F15" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.0387v1</id>
    <updated>2014-03-03T11:04:20Z</updated>
    <published>2014-03-03T11:04:20Z</published>
    <title>Approximate Integrated Likelihood via ABC methods</title>
    <summary>  We propose a novel use of a recent new computational tool for Bayesian
inference, namely the Approximate Bayesian Computation (ABC) methodology. ABC
is a way to handle models for which the likelihood function may be intractable
or even unavailable and/or too costly to evaluate; in particular, we consider
the problem of eliminating the nuisance parameters from a complex statistical
model in order to produce a likelihood function depending on the quantity of
interest only. Given a proper prior for the entire vector parameter, we propose
to approximate the integrated likelihood by the ratio of kernel estimators of
the marginal posterior and prior for the quantity of interest. We present
several examples.
</summary>
    <author>
      <name>Clara Grazian</name>
    </author>
    <author>
      <name>Brunero Liseo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1403.0387v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.0387v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62F15" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.0532v1</id>
    <updated>2014-03-03T19:27:33Z</updated>
    <published>2014-03-03T19:27:33Z</published>
    <title>Visualization of Skewed Data: A Tool in R</title>
    <summary>  In this work we present a visualization tool specifically tailored to deal
with skewed data. The technique is based upon the use of two types of notched
boxplots (the usual one, and one which is tuned for the skewness of the data),
the violin plot, the histogram and a nonparametric estimate of the density. The
data is assumed to lie on the same line, so the plots are compatible. We show
that a good deal of information can be extracted from the inspection of this
tool; in particular, we apply the technique to analyze data from synthetic
aperture radar images. We provide the implementation in R.
</summary>
    <author>
      <name>R. Ospina</name>
    </author>
    <author>
      <name>A. M. Larangeiras</name>
    </author>
    <author>
      <name>A. C. Frery</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to the Revista Colombiana de Estad\'istica</arxiv:comment>
    <link href="http://arxiv.org/abs/1403.0532v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.0532v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.5537v1</id>
    <updated>2014-03-21T18:41:09Z</updated>
    <published>2014-03-21T18:41:09Z</published>
    <title>Randomized pick-freeze for sparse Sobol indices estimation in high
  dimension</title>
    <summary>  This article investigates a new procedure to estimate the influence of each
variable of a given function defined on a high-dimensional space. More
precisely, we are concerned with describing a function of a large number $p$ of
parameters that depends only on a small number $s$ of them. Our proposed method
is an unconstrained $\ell_{1}$-minimization based on the Sobol's method. We
prove that, with only $\mathcal O(s\log p)$ evaluations of $f$, one can find
which are the relevant parameters.
</summary>
    <author>
      <name>Yohann De Castro</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LM-Orsay</arxiv:affiliation>
    </author>
    <author>
      <name>Alexandre Janon</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LM-Orsay, - Méthodes d'Analyse Stochastique des Codes et Traitements Numériques</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1403.5537v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.5537v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.6886v2</id>
    <updated>2014-10-22T14:50:05Z</updated>
    <published>2014-03-26T23:05:57Z</published>
    <title>Scalable Inference for Markov Processes with Intractable Likelihoods</title>
    <summary>  Bayesian inference for Markov processes has become increasingly relevant in
recent years. Problems of this type often have intractable likelihoods and
prior knowledge about model rate parameters is often poor. Markov Chain Monte
Carlo (MCMC) techniques can lead to exact inference in such models but in
practice can suffer performance issues including long burn-in periods and poor
mixing. On the other hand approximate Bayesian computation techniques can allow
rapid exploration of a large parameter space but yield only approximate
posterior distributions. Here we consider the combined use of approximate
Bayesian computation (ABC) and MCMC techniques for improved computational
efficiency while retaining exact inference on parallel hardware.
</summary>
    <author>
      <name>Jamie Owen</name>
    </author>
    <author>
      <name>Darren J. Wilkinson</name>
    </author>
    <author>
      <name>Colin S. Gillespie</name>
    </author>
    <link href="http://arxiv.org/abs/1403.6886v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.6886v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.0651v1</id>
    <updated>2014-04-02T18:26:02Z</updated>
    <published>2014-04-02T18:26:02Z</published>
    <title>On parameter identification in stochastic differential equations by
  penalized maximum likelihood</title>
    <summary>  In this paper we present nonparametric estimators for coefficients in
stochastic differential equation if the data are described by independent,
identically distributed random variables. The problem is formulated as a
nonlinear ill-posed operator equation with a deterministic forward operator
described by the Fokker-Planck equation. We derive convergence rates of the
risk for penalized maximum likelihood estimators with convex penalty terms and
for Newton-type methods. The assumptions of our general convergence results are
verified for estimation of the drift coefficient. The advantages of
log-likelihood compared to quadratic data fidelity terms are demonstrated in
Monte-Carlo simulations.
</summary>
    <author>
      <name>Fabian Dunker</name>
    </author>
    <author>
      <name>Thorsten Hohage</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/0266-5611/30/9/095001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/0266-5611/30/9/095001" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Inverse Problems, 2014, 30, 095001</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1404.0651v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.0651v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.ST" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.3177v2</id>
    <updated>2014-09-16T12:48:04Z</updated>
    <published>2014-04-11T18:29:57Z</published>
    <title>Fast Estimation of Multinomial Logit Models: R Package mnlogit</title>
    <summary>  We present R package mnlogit for training multinomial logistic regression
models, particularly those involving a large number of classes and features.
Compared to existing software, mnlogit offers speedups of 10x-50x for modestly
sized problems and more than 100x for larger problems. Running mnlogit in
parallel mode on a multicore machine gives an additional 2x-4x speedup on up to
8 processor cores. Computational efficiency is achieved by drastically speeding
up calculation of the log-likelihood function's Hessian matrix by exploiting
structure in matrices that arise in intermediate calculations.
</summary>
    <author>
      <name>Asad Hasan</name>
    </author>
    <author>
      <name>Wang Zhiyu</name>
    </author>
    <author>
      <name>Alireza S. Mahani</name>
    </author>
    <link href="http://arxiv.org/abs/1404.3177v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.3177v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.4185v1</id>
    <updated>2014-04-16T10:13:57Z</updated>
    <published>2014-04-16T10:13:57Z</published>
    <title>Simulation based sequential Monte Carlo methods for discretely observed
  Markov processes</title>
    <summary>  Parameter estimation for discretely observed Markov processes is a
challenging problem. However, simulation of Markov processes is straightforward
using the Gillespie algorithm. We exploit this ease of simulation to develop an
effective sequential Monte Carlo (SMC) algorithm for obtaining samples from the
posterior distribution of the parameters. In particular, we introduce two key
innovations, coupled simulations, which allow us to study multiple parameter
values on the basis of a single simulation, and a simple, yet effective,
importance sampling scheme for steering simulations towards the observed data.
These innovations substantially improve the efficiency of the SMC algorithm
with minimal effect on the speed of the simulation process. The SMC algorithm
is successfully applied to two examples, a Lotka-Volterra model and a
Repressilator model.
</summary>
    <author>
      <name>Peter Neal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1404.4185v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.4185v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.4272v1</id>
    <updated>2014-04-16T14:44:53Z</updated>
    <published>2014-04-16T14:44:53Z</published>
    <title>Fast and exact implementation of 3-dimensional Tukey depth regions</title>
    <summary>  Tukey depth regions are important notions in nonparametric multivariate data
analysis. A $\tau$-th Tukey depth region $\mathcal{D}_{\tau}$ is the set of all
points that have at least depth $\tau$. While the Tukey depth regions are
easily defined and interpreted as $p$-variate quantiles, their practical
applications is impeded by the lack of efficient computational procedures in
dimensions with $p &gt; 2$. Feasible algorithms are available, but practically
very slow. In this paper we present a new exact algorithm for 3-dimensional
data. An efficient implementation is also provided. Data examples indicate that
the proposed algorithm runs much faster than the existing ones.
</summary>
    <author>
      <name>Xiaohui Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This manuscript is still under a revision. Any comment would be
  deeply appreciated!</arxiv:comment>
    <link href="http://arxiv.org/abs/1404.4272v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.4272v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.0362v2</id>
    <updated>2014-09-22T21:12:12Z</updated>
    <published>2014-05-02T09:00:01Z</published>
    <title>An efficient algorithm for T-estimation</title>
    <summary>  We introduce an efficient and exact algorithm, together with a faster but
approximate version, which implements with a sub-quadratic complexity the
hold-out derived from T-estimation. We study empirically the performance of
this hold-out in the context of density estimation considering well-known
competitors (hold-out derived from least-squares or Kullback-Leibler
divergence, model selection procedures, etc.) and classical problems including
histogram or bandwidth selection. Our algorithms are integrated in a companion
R-package called {\it Density.T.HoldOut} available on the CRAN:
{\url{http://cran.r-project.org/web/packages/Density.T.HoldOut/index.html}}.
</summary>
    <author>
      <name>Nelo Magalhães</name>
    </author>
    <author>
      <name>Yves Rozenholc</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.0362v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.0362v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G05 62G07" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.1491v1</id>
    <updated>2014-05-07T02:42:55Z</updated>
    <published>2014-05-07T02:42:55Z</published>
    <title>Demonstration of Enhanced Monte Carlo Computation of the Fisher
  Information for Complex Problems</title>
    <summary>  The Fisher information matrix summarizes the amount of information in a set
of data relative to the quantities of interest. There are many applications of
the information matrix in statistical modeling, system identification and
parameter estimation. This short paper reviews a feedback-based method and an
independent perturbation approach for computing the information matrix for
complex problems, where a closed form of the information matrix is not
achievable. We show through numerical examples how these methods improve the
accuracy of the estimate of the information matrix compared to the basic
resampling-based approach. Some relevant theory is summarized.
</summary>
    <author>
      <name>Xumeng Cao</name>
    </author>
    <link href="http://arxiv.org/abs/1405.1491v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.1491v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.1796v1</id>
    <updated>2014-05-08T03:18:27Z</updated>
    <published>2014-05-08T03:18:27Z</published>
    <title>Comparisons of penalized least squares methods by simulations</title>
    <summary>  Penalized least squares methods are commonly used for simultaneous estimation
and variable selection in high-dimensional linear models. In this paper we
compare several prevailing methods including the lasso, nonnegative garrote,
and SCAD in this area through Monte Carlo simulations. Criterion for evaluating
these methods in terms of variable selection and estimation are presented. This
paper focuses on the traditional n &gt; p cases. For larger p, our results are
still helpful to practitioners after the dimensionality is reduced by a
screening method. K
</summary>
    <author>
      <name>Ke Zhang</name>
    </author>
    <author>
      <name>Fan Yin</name>
    </author>
    <author>
      <name>Shifeng Xiong</name>
    </author>
    <link href="http://arxiv.org/abs/1405.1796v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.1796v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.2673v1</id>
    <updated>2014-05-12T08:43:56Z</updated>
    <published>2014-05-12T08:43:56Z</published>
    <title>Particle MCMC for Bayesian Microwave Control</title>
    <summary>  We consider the problem of local radioelectric property estimation from
global electromagnetic scattering measurements. This challenging ill-posed high
dimensional inverse problem can be explored by intensive computations of a
parallel Maxwell solver on a petaflopic supercomputer. Then, it is shown how
Bayesian inference can be perfomed with a Particle Marginal Metropolis-Hastings
(PMMH) approach, which includes a Rao-Blackwellised Sequential Monte Carlo
algorithm with interacting Kalman filters. Material properties, including a
multiple components "Debye relaxation"/"Lorenzian resonant" material model, are
estimated; it is illustrated on synthetic data. Eventually, we propose
different ways to deal with higher dimensional problems, from parallelization
to the original introduction of efficient sequential data assimilation
techniques, widely used in weather forecasting, oceanography, geophysics, etc.
</summary>
    <author>
      <name>P. Minvielle</name>
    </author>
    <author>
      <name>A. Todeschini</name>
    </author>
    <author>
      <name>F. Caron</name>
    </author>
    <author>
      <name>P. Del Moral</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/1742-6596/542/1/012007</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/1742-6596/542/1/012007" rel="related"/>
    <link href="http://arxiv.org/abs/1405.2673v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.2673v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.3222v2</id>
    <updated>2014-11-03T16:44:50Z</updated>
    <published>2014-05-13T16:42:45Z</published>
    <title>Efficient Implementations of the Generalized Lasso Dual Path Algorithm</title>
    <summary>  We consider efficient implementations of the generalized lasso dual path
algorithm of Tibshirani and Taylor (2011). We first describe a generic approach
that covers any penalty matrix D and any (full column rank) matrix X of
predictor variables. We then describe fast implementations for the special
cases of trend filtering problems, fused lasso problems, and sparse fused lasso
problems, both with X=I and a general matrix X. These specialized
implementations offer a considerable improvement over the generic
implementation, both in terms of numerical stability and efficiency of the
solution path computation. These algorithms are all available for use in the
genlasso R package, which can be found in the CRAN repository.
</summary>
    <author>
      <name>Taylor Arnold</name>
    </author>
    <author>
      <name>Ryan Tibshirani</name>
    </author>
    <link href="http://arxiv.org/abs/1405.3222v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.3222v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.5841v1</id>
    <updated>2014-05-22T17:58:37Z</updated>
    <published>2014-05-22T17:58:37Z</published>
    <title>Parameter Estimates of General Failure Rate Model: A Bayesian Approach</title>
    <summary>  The failure rate function plays an important role in studying the lifetime
distributions in reliability theory and life testing models. A study of the
general failure rate model $r(t)=a+bt^{\theta-1}$, under squared error loss
function taking $a$ and $b$ independent exponential random variables has been
analyzed in the literature. In this article, we consider $a$ and $b$ not
necessarily independent. The estimates of the parameters $a$ and $b$ under
squared error loss, linex loss and entropy loss functions are obtained here.
</summary>
    <author>
      <name>Asok K. Nanda</name>
    </author>
    <author>
      <name>Sudhansu S. Maiti</name>
    </author>
    <author>
      <name>Chanchal Kundu</name>
    </author>
    <author>
      <name>Amarjit Kundu</name>
    </author>
    <link href="http://arxiv.org/abs/1405.5841v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.5841v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.6397v1</id>
    <updated>2014-05-25T15:37:52Z</updated>
    <published>2014-05-25T15:37:52Z</published>
    <title>Efficient Evaluation of the Probability Density Function of a Wrapped
  Normal Distribution</title>
    <summary>  The wrapped normal distribution arises when a the density of a
one-dimensional normal distribution is wrapped around the circle infinitely
many times. At first look, evaluation of its probability density function
appears tedious as an infinite series is involved. In this paper, we
investigate the evaluation of two truncated series representations. As one
representation performs well for small uncertainties whereas the other performs
well for large uncertainties, we show that in all cases a small number of
summands is sufficient to achieve high accuracy.
</summary>
    <author>
      <name>Gerhard Kurz</name>
    </author>
    <author>
      <name>Igor Gilitschenski</name>
    </author>
    <author>
      <name>Uwe D. Hanebeck</name>
    </author>
    <link href="http://arxiv.org/abs/1405.6397v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.6397v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.6460v1</id>
    <updated>2014-05-26T05:05:54Z</updated>
    <published>2014-05-26T05:05:54Z</published>
    <title>Bayesian likelihood-free localisation of a biochemical source using
  multiple dispersion models</title>
    <summary>  Localisation of a source of a toxic release of biochemical aerosols in the
atmosphere is a problem of great importance for public safety. Two main
practical difficulties are encountered in this problem: the lack of knowledge
of the likelihood function of measurements collected by biochemical sensors,
and the plethora of candidate dispersion models, developed under various
assumptions (e.g. meteorological conditions, terrain). Aiming to overcome these
two difficulties, the paper proposes a likelihood-free approximate Bayesian
computation method, which simultaneously uses a set of candidate dispersion
models, to localise the source. This estimation framework is implemented via
the Monte Carlo method and tested using two experimental datasets.
</summary>
    <author>
      <name>Branko Ristic</name>
    </author>
    <author>
      <name>Ajith Gunatilaka</name>
    </author>
    <author>
      <name>Ralph Gailis</name>
    </author>
    <author>
      <name>Alex Skvortsov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 7 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Signal Processing 108 (2015): 13-24</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1405.6460v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.6460v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.3218v2</id>
    <updated>2014-06-24T11:12:40Z</updated>
    <published>2014-06-12T12:44:12Z</published>
    <title>State dependent swap strategies and adaptive adjusting of number of
  temperatures in Parallel Tempering algorithms</title>
    <summary>  In this paper we present extensions to the original adaptive parallel
tempering algorithm. Two different approaches are presented. In the first one
we introduce state-dependent strategies using current information to perform a
swap step. It encompasses a wide family of potential moves including the
standard one and Equi Energy type move, without any loss in tractability. In
the second one, we introduce online adjustment of the number of temperatures.
Numerical experiments demonstrate the effectiveness of the proposed method.
</summary>
    <author>
      <name>Mateusz Krzysztof Łącki</name>
    </author>
    <author>
      <name>Błażej Miasojedow</name>
    </author>
    <link href="http://arxiv.org/abs/1406.3218v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.3218v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.3843v1</id>
    <updated>2014-06-15T19:03:46Z</updated>
    <published>2014-06-15T19:03:46Z</published>
    <title>Semi-Separable Hamiltonian Monte Carlo for Inference in Bayesian
  Hierarchical Models</title>
    <summary>  Sampling from hierarchical Bayesian models is often difficult for MCMC
methods, because of the strong correlations between the model parameters and
the hyperparameters. Recent Riemannian manifold Hamiltonian Monte Carlo (RMHMC)
methods have significant potential advantages in this setting, but are
computationally expensive. We introduce a new RMHMC method, which we call
semi-separable Hamiltonian Monte Carlo, which uses a specially designed mass
matrix that allows the joint Hamiltonian over model parameters and
hyperparameters to decompose into two simpler Hamiltonians. This structure is
exploited by a new integrator which we call the alternating blockwise leapfrog
algorithm. The resulting method can mix faster than simpler Gibbs sampling
while being simpler and more efficient than previous instances of RMHMC.
</summary>
    <author>
      <name>Yichuan Zhang</name>
    </author>
    <author>
      <name>Charles Sutton</name>
    </author>
    <link href="http://arxiv.org/abs/1406.3843v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.3843v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.2864v1</id>
    <updated>2014-07-10T17:04:38Z</updated>
    <published>2014-07-10T17:04:38Z</published>
    <title>Asynchronous Anytime Sequential Monte Carlo</title>
    <summary>  We introduce a new sequential Monte Carlo algorithm we call the particle
cascade. The particle cascade is an asynchronous, anytime alternative to
traditional particle filtering algorithms. It uses no barrier synchronizations
which leads to improved particle throughput and memory efficiency. It is an
anytime algorithm in the sense that it can be run forever to emit an unbounded
number of particles while keeping within a fixed memory budget. We prove that
the particle cascade is an unbiased marginal likelihood estimator which means
that it can be straightforwardly plugged into existing pseudomarginal methods.
</summary>
    <author>
      <name>Brooks Paige</name>
    </author>
    <author>
      <name>Frank Wood</name>
    </author>
    <author>
      <name>Arnaud Doucet</name>
    </author>
    <author>
      <name>Yee Whye Teh</name>
    </author>
    <link href="http://arxiv.org/abs/1407.2864v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.2864v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.2954v1</id>
    <updated>2014-07-10T20:11:23Z</updated>
    <published>2014-07-10T20:11:23Z</published>
    <title>Prioritized Data Compression using Wavelets</title>
    <summary>  The volume of data and the velocity with which it is being generated by com-
putational experiments on high performance computing (HPC) systems is quickly
outpacing our ability to effectively store this information in its full
fidelity. There- fore, it is critically important to identify and study
compression methodologies that retain as much information as possible,
particularly in the most salient regions of the simulation space. In this
paper, we cast this in terms of a general decision-theoretic problem and
discuss a wavelet-based compression strategy for its solution. We pro- vide a
heuristic argument as justification and illustrate our methodology on several
examples. Finally, we will discuss how our proposed methodology may be utilized
in an HPC environment on large-scale computational experiments.
</summary>
    <author>
      <name>Henry Scharf</name>
    </author>
    <author>
      <name>Ryan Elmore</name>
    </author>
    <author>
      <name>Kenny Gruchalla</name>
    </author>
    <link href="http://arxiv.org/abs/1407.2954v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.2954v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.3492v1</id>
    <updated>2014-07-13T17:30:19Z</updated>
    <published>2014-07-13T17:30:19Z</published>
    <title>Fitting heavy tailed distributions: the poweRlaw package</title>
    <summary>  Over the last few years, the power law distribution has been used as the data
generating mechanism in many disparate fields. However, at times the techniques
used to fit the power law distribution have been inappropriate. This paper
describes the poweRlaw R package, which makes fitting power laws and other
heavy-tailed distributions straightforward. This package contains R functions
for fitting, comparing and visualising heavy tailed distributions. Overall, it
provides a principled approach to power law fitting.
</summary>
    <author>
      <name>Colin S Gillespie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The code for this paper can be found at
  https://github.com/csgillespie/poweRlaw</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.3492v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.3492v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.1742v1</id>
    <updated>2014-08-08T01:15:36Z</updated>
    <published>2014-08-08T01:15:36Z</published>
    <title>Discrepancy Estimates for Acceptance-Rejection Samplers Using Stratified
  Inputs</title>
    <summary>  In this paper we propose an acceptance-rejection sampler using stratified
inputs as diver sequence. We estimate the discrepancy of the points generated
by this algorithm. First we show an upper bound on the star discrepancy of
order $N^{-1/2-1/(2s)}$. Further we prove an upper bound on the $q$-th moment
of the $L_q$-discrepancy $(\mathbb{E}[N^{q}L^{q}_{q,N}])^{1/q}$ for $2\le q\le
\infty$, which is of order $N^{(1-1/s)(1-1/q)}$. We also present an improved
convergence rate for a deterministic acceptance-rejection algorithm using
$(t,m,s)-$nets as driver sequence.
</summary>
    <author>
      <name>Houying Zhu</name>
    </author>
    <author>
      <name>Josef Dick</name>
    </author>
    <link href="http://arxiv.org/abs/1408.1742v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.1742v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62F15, 11K45" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.2698v1</id>
    <updated>2014-08-12T12:02:31Z</updated>
    <published>2014-08-12T12:02:31Z</published>
    <title>Approximate D-optimal Experimental Design with Simultaneous Size and
  Cost Constraints</title>
    <summary>  Consider an experiment with a finite set of design points representing
permissible trial conditions. Suppose that each trial is associated with a cost
that depends on the selected design point. In this paper, we study the problem
of constructing an approximate D-optimal experimental design with simultaneous
restrictions on the size and on the total cost. For the problem of
size-and-cost constrained D-optimality, we formulate an equivalence theorem and
rules for the removal of redundant design points. We also propose a simple
monotonically convergent "barycentric" algorithm that allows us to numerically
compute a size-and-cost constrained approximate D-optimal design.
</summary>
    <author>
      <name>Radoslav Harman</name>
    </author>
    <author>
      <name>Eva Benková</name>
    </author>
    <link href="http://arxiv.org/abs/1408.2698v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.2698v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62K05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.3027v1</id>
    <updated>2014-09-10T11:27:15Z</updated>
    <published>2014-09-10T11:27:15Z</published>
    <title>Implementation of Lévy CARMA model in Yuima package</title>
    <summary>  The paper shows how to use the R package yuima available on CRAN for the
simulation and the estimation of a general L\'evy Continuous Autoregressive
Moving Average (CARMA) model. The flexibility of the package is due to the fact
that the user is allowed to choose several parametric L\'evy distribution for
the increments. Some numerical examples are given in order to explain the main
classes and the corresponding methods implemented in yuima package for the
CARMA model.
</summary>
    <author>
      <name>Stefano M. Iacus</name>
    </author>
    <author>
      <name>Lorenzo Mercuri</name>
    </author>
    <link href="http://arxiv.org/abs/1409.3027v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.3027v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.3821v3</id>
    <updated>2015-07-30T19:35:44Z</updated>
    <published>2014-09-12T18:57:01Z</published>
    <title>Computational Implications of Reducing Data to Sufficient Statistics</title>
    <summary>  Given a large dataset and an estimation task, it is common to pre-process the
data by reducing them to a set of sufficient statistics. This step is often
regarded as straightforward and advantageous (in that it simplifies statistical
analysis). I show that -on the contrary- reducing data to sufficient statistics
can change a computationally tractable estimation problem into an intractable
one. I discuss connections with recent work in theoretical computer science,
and implications for some techniques to estimate graphical models.
</summary>
    <author>
      <name>Andrea Montanari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.3821v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.3821v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.3901v3</id>
    <updated>2016-10-05T22:14:54Z</updated>
    <published>2014-09-13T02:18:47Z</published>
    <title>Fast implementation of the Tukey depth</title>
    <summary>  Tukey depth function is one of the most famous multivariate tools serving
robust purposes. It is also very well known for its computability problems in
dimensions $p \ge 3$. In this paper, we address this computing issue by
presenting two combinatorial algorithms. The first is naive and calculates the
Tukey depth of a single point with complexity $O\left(n^{p-1}\log(n)\right)$,
while the second further utilizes the quasiconcave of the Tukey depth function
and hence is more efficient than the first. Both require very minimal memory
and run much faster than the existing ones. All experiments indicate that they
compute the exact Tukey depth.
</summary>
    <author>
      <name>Xiaohui Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.3901v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.3901v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62F10, 62F40, 62F35" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.4302v1</id>
    <updated>2014-09-15T15:50:26Z</updated>
    <published>2014-09-15T15:50:26Z</published>
    <title>Exact Estimation for Markov Chain Equilibrium Expectations</title>
    <summary>  We introduce a new class of Monte Carlo methods, which we call exact
estimation algorithms. Such algorithms provide unbiased estimators for
equilibrium expectations associated with real- valued functionals defined on a
Markov chain. We provide easily implemented algorithms for the class of
positive Harris recurrent Markov chains, and for chains that are contracting on
average. We further argue that exact estimation in the Markov chain setting
provides a significant theoretical relaxation relative to exact simulation
methods.
</summary>
    <author>
      <name>Peter W. Glynn</name>
    </author>
    <author>
      <name>Chang-han Rhee</name>
    </author>
    <link href="http://arxiv.org/abs/1409.4302v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.4302v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.5827v1</id>
    <updated>2014-09-19T22:34:29Z</updated>
    <published>2014-09-19T22:34:29Z</published>
    <title>Software Alchemy: Turning Complex Statistical Computations into
  Embarrassingly-Parallel Ones</title>
    <summary>  The growth in the use of computationally intensive statistical procedures,
especially with Big Data, has necessitated the usage of parallel computation on
diverse platforms such as multicore, GPU, clusters and clouds. However,
slowdown due to interprocess communication costs typically limits such methods
to "embarrassingly parallel" (EP) algorithms, especially on non-shared memory
platforms. This paper develops a broadly-applicable method for converting many
non-EP algorithms into statistically equivalent EP ones. The method is shown to
yield excellent levels of speedup for a variety of statistical computations. It
also overcomes certain problems of memory limitations.
</summary>
    <author>
      <name>Norman Matloff</name>
    </author>
    <link href="http://arxiv.org/abs/1409.5827v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.5827v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62-04" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.8047v1</id>
    <updated>2014-09-29T09:35:46Z</updated>
    <published>2014-09-29T09:35:46Z</published>
    <title>Asymptotic Normality of the Maximum Pseudolikelihood Estimator for Fully
  Visible Boltzmann Machines</title>
    <summary>  Boltzmann machines (BMs) are a class of binary neural networks for which
there have been numerous proposed methods of estimation. Recently, it has been
shown that in the fully visible case of the BM, the method of maximum
pseudolikelihood estimation (MPLE) results in parameter estimates which are
consistent in the probabilistic sense. In this article, we investigate the
properties of MPLE for the fully visible BMs further, and prove that MPLE also
yields an asymptotically normal parameter estimator. These results can be used
to construct confidence intervals and to test statistical hypotheses. We
support our theoretical results by showing that the estimator behaves as
expected in a simulation study.
</summary>
    <author>
      <name>Hien D. Nguyen</name>
    </author>
    <author>
      <name>Ian A. Wood</name>
    </author>
    <link href="http://arxiv.org/abs/1409.8047v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.8047v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.8083v1</id>
    <updated>2014-09-29T11:45:36Z</updated>
    <published>2014-09-29T11:45:36Z</published>
    <title>Variational Inference For Probabilistic Latent Tensor Factorization with
  KL Divergence</title>
    <summary>  Probabilistic Latent Tensor Factorization (PLTF) is a recently proposed
probabilistic framework for modelling multi-way data. Not only the common
tensor factorization models but also any arbitrary tensor factorization
structure can be realized by the PLTF framework. This paper presents full
Bayesian inference via variational Bayes that facilitates more powerful
modelling and allows more sophisticated inference on the PLTF framework. We
illustrate our approach on model order selection and link prediction.
</summary>
    <author>
      <name>Beyza Ermis</name>
    </author>
    <author>
      <name>Y. Kenan Yılmaz</name>
    </author>
    <author>
      <name>A. Taylan Cemgil</name>
    </author>
    <author>
      <name>Evrim Acar</name>
    </author>
    <link href="http://arxiv.org/abs/1409.8083v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.8083v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.0793v1</id>
    <updated>2014-10-03T09:36:21Z</updated>
    <published>2014-10-03T09:36:21Z</published>
    <title>On the maximization of likelihoods belonging to the exponential family
  using ideas related to the Levenberg-Marquardt approach</title>
    <summary>  The Levenberg-Marquardt algorithm is a flexible iterative procedure used to
solve non-linear least squares problems. In this work we study how a class of
possible adaptations of this procedure can be used to solve maximum likelihood
problems when the underlying distributions are in the exponential family. We
formally demonstrate a local convergence property and we discuss a possible
implementation of the penalization involved in this class of algorithms.
Applications to real and simulated compositional data show the stability and
efficiency of this approach.
</summary>
    <author>
      <name>Marco Giordan</name>
    </author>
    <author>
      <name>Federico Vaggi</name>
    </author>
    <author>
      <name>Ron Wehrens</name>
    </author>
    <link href="http://arxiv.org/abs/1410.0793v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.0793v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.4534v2</id>
    <updated>2014-12-23T17:40:10Z</updated>
    <published>2014-10-16T18:53:20Z</published>
    <title>Bayesian Inference for Generalized Extreme Value Distributions via
  Hamiltonian Monte Carlo</title>
    <summary>  In this paper we propose to evaluate and compare Markov chain Monte Carlo
(MCMC) methods to estimate the parameters in a generalized extreme value model.
We employed the Bayesian approach using traditional Metropolis-Hastings
methods, Hamiltonian Monte Carlo (HMC) and Riemann manifold HMC (RMHMC) methods
to obtain the approximations to the posterior marginal distributions of
interest. Applications to real datasets of maxima illustrate illustrate how HMC
can be much more efficient computationally than traditional MCMC and simulation
studies are conducted to compare the algorithms in terms of how fast they get
close enough to the stationary distribution so as to provide good estimates
with a smaller number of iterations.
</summary>
    <author>
      <name>Marcelo Hartmann</name>
    </author>
    <author>
      <name>Ricardo Ehlers</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1080/03610918.2016.1152365</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1080/03610918.2016.1152365" rel="related"/>
    <link href="http://arxiv.org/abs/1410.4534v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.4534v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.6131v3</id>
    <updated>2015-05-04T15:54:42Z</updated>
    <published>2014-10-22T18:30:38Z</published>
    <title>Penalized versus constrained generalized eigenvalue problems</title>
    <summary>  We investigate the difference between using an $\ell_1$ penalty versus an
$\ell_1$ constraint in generalized eigenvalue problems, such as principal
component analysis and discriminant analysis. Our main finding is that an
$\ell_1$ penalty may fail to provide very sparse solutions; a severe
disadvantage for variable selection that can be remedied by using an $\ell_1$
constraint. Our claims are supported both by empirical evidence and theoretical
analysis. Finally, we illustrate the advantages of an $\ell_1$ constraint in
the context of discriminant analysis and principal component analysis.
</summary>
    <author>
      <name>Irina Gaynanova</name>
    </author>
    <author>
      <name>James Booth</name>
    </author>
    <author>
      <name>Martin T. Wells</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.6131v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.6131v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.7348v1</id>
    <updated>2014-10-27T18:39:20Z</updated>
    <published>2014-10-27T18:39:20Z</published>
    <title>Fractional Bi-Spectrum</title>
    <summary>  A signal with discrete frequency components, has a zero bispectrum if no
linear combination of the frequencies equals one of the frequency components.
We introduce fractional bispectrum in which for such signals the fractional
bispectrum is nonzero. It is shown that fractional bispectrum has the same
property as bispectrum for Gaussian signals: the fractional bispectrum of a
zero mean Gaussian signal is zero; therefore it can be used to eliminate or
reduce the Gaussian noise.
</summary>
    <author>
      <name>Mehrdad Abolbashari</name>
    </author>
    <author>
      <name>Gelareh Babaie</name>
    </author>
    <author>
      <name>Jonathan Babaie</name>
    </author>
    <author>
      <name>Faramarz Farahi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 Pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.7348v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.7348v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.8276v1</id>
    <updated>2014-10-30T07:23:33Z</updated>
    <published>2014-10-30T07:23:33Z</published>
    <title>Functional regression approximate Bayesian computation for Gaussian
  process density estimation</title>
    <summary>  We propose a novel Bayesian nonparametric method for hierarchical modelling
on a set of related density functions, where grouped data in the form of
samples from each density function are available. Borrowing strength across the
groups is a major challenge in this context. To address this problem, we
introduce a hierarchically structured prior, defined over a set of univariate
density functions, using convenient transformations of Gaussian processes.
Inference is performed through approximate Bayesian computation (ABC), via a
novel functional regression adjustment. The performance of the proposed method
is illustrated via a simulation study and an analysis of rural high school exam
performance in Brazil.
</summary>
    <author>
      <name>G. S. Rodrigues</name>
    </author>
    <author>
      <name>David J. Nott</name>
    </author>
    <author>
      <name>S. A. Sisson</name>
    </author>
    <link href="http://arxiv.org/abs/1410.8276v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.8276v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G07" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.6927v3</id>
    <updated>2016-01-12T15:31:38Z</updated>
    <published>2014-11-25T17:20:06Z</published>
    <title>Exact computation of the halfspace depth</title>
    <summary>  For computing the exact value of the halfspace depth of a point w.r.t. a data
cloud of $n$ points in arbitrary dimension, a theoretical framework is
suggested. Based on this framework a whole class of algorithms can be derived.
In all of these algorithms the depth is calculated as the minimum over a finite
number of depth values w.r.t. proper projections of the data cloud. Three
variants of this class are studied in more detail. All of these algorithms are
capable of dealing with data that are not in general position and even with
data that contain ties. As is shown by simulations, all proposed algorithms
prove to be very efficient.
</summary>
    <author>
      <name>Rainer Dyckerhoff</name>
    </author>
    <author>
      <name>Pavlo Mozharovskyi</name>
    </author>
    <link href="http://arxiv.org/abs/1411.6927v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.6927v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.3013v1</id>
    <updated>2014-12-09T16:35:10Z</updated>
    <published>2014-12-09T16:35:10Z</published>
    <title>Efficient Bayesian inference for stochastic volatility models with
  ensemble MCMC methods</title>
    <summary>  In this paper, we introduce efficient ensemble Markov Chain Monte Carlo
(MCMC) sampling methods for Bayesian computations in the univariate stochastic
volatility model. We compare the performance of our ensemble MCMC methods with
an improved version of a recent sampler of Kastner and Fruwirth-Schnatter
(2014). We show that ensemble samplers are more efficient than this state of
the art sampler by a factor of about 3.1, on a data set simulated from the
stochastic volatility model. This performance gain is achieved without the
ensemble MCMC sampler relying on the assumption that the latent process is
linear and Gaussian, unlike the sampler of Kastner and Fruwirth-Schnatter.
</summary>
    <author>
      <name>Alexander Y. Shestopaloff</name>
    </author>
    <author>
      <name>Radford M. Neal</name>
    </author>
    <link href="http://arxiv.org/abs/1412.3013v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.3013v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.3779v1</id>
    <updated>2014-12-11T19:51:34Z</updated>
    <published>2014-12-11T19:51:34Z</published>
    <title>Biips: Software for Bayesian Inference with Interacting Particle Systems</title>
    <summary>  Biips is a software platform for automatic Bayesian inference with
interacting particle systems. Biips allows users to define their statistical
model in the probabilistic programming BUGS language, as well as to add custom
functions or samplers within this language. Then it runs sequential Monte Carlo
based algorithms (particle filters, particle independent Metropolis-Hastings,
particle marginal Metropolis-Hastings) in a black-box manner so that to
approximate the posterior distribution of interest as well as the marginal
likelihood. The software is developed in C++ with interfaces with the softwares
R, Matlab and Octave.
</summary>
    <author>
      <name>Adrien Todeschini</name>
    </author>
    <author>
      <name>François Caron</name>
    </author>
    <author>
      <name>Marc Fuentes</name>
    </author>
    <author>
      <name>Pierrick Legrand</name>
    </author>
    <author>
      <name>Pierre Del Moral</name>
    </author>
    <link href="http://arxiv.org/abs/1412.3779v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.3779v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.4128v1</id>
    <updated>2014-12-12T21:04:15Z</updated>
    <published>2014-12-12T21:04:15Z</published>
    <title>Expanded Alternating Optimization of Nonconvex Functions with
  Applications to Matrix Factorization and Penalized Regression</title>
    <summary>  We propose a general technique for improving alternating optimization (AO) of
nonconvex functions. Starting from the solution given by AO, we conduct another
sequence of searches over subspaces that are both meaningful to the
optimization problem at hand and different from those used by AO. To
demonstrate the utility of our approach, we apply it to the matrix
factorization (MF) algorithm for recommender systems and the coordinate descent
algorithm for penalized regression (PR), and show meaningful improvements using
both real-world (for MF) and simulated (for PR) data sets. Moreover, we
demonstrate for MF that, by constructing search spaces customized to the given
data set, we can significantly increase the convergence rate of our technique.
</summary>
    <author>
      <name>W. James Murdoch</name>
    </author>
    <author>
      <name>Mu Zhu</name>
    </author>
    <link href="http://arxiv.org/abs/1412.4128v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.4128v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.5122v1</id>
    <updated>2014-12-16T18:56:04Z</updated>
    <published>2014-12-16T18:56:04Z</published>
    <title>Fast computation of Tukey trimmed regions in dimension $p&gt;2$</title>
    <summary>  Given data in $\mathbb{R}^{p}$, a Tukey $\tau$-trimmed region is the set of
all points that have at least Tukey depth $\tau$ in the data. As they are
visual, affine equivariant and robust, Tukey trimmed regions are useful tools
in nonparametric multivariate analysis. While these regions are easily defined
and can be interpreted as $p$-variate quantiles, their practical application is
impeded by the lack of efficient computational procedures in dimension $p&gt;2$.
We construct two algorithms that exploit certain combinatorial properties of
the regions. Both calculate the exact region. They run much faster and require
substantially less RAM than existing ones.
</summary>
    <author>
      <name>Xiaohui Liu</name>
    </author>
    <author>
      <name>Karl Mosler</name>
    </author>
    <author>
      <name>Pavlo Mozharovskyi</name>
    </author>
    <link href="http://arxiv.org/abs/1412.5122v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.5122v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62F10, 62F40, 62F35" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.6760v2</id>
    <updated>2014-12-29T07:52:32Z</updated>
    <published>2014-12-21T10:29:28Z</published>
    <title>Individual adaptation: an adaptive MCMC scheme for variable selection
  problems</title>
    <summary>  The increasing size of data sets has lead to variable selection in regression
becoming increasingly important. Bayesian approaches are attractive since they
allow uncertainty about the choice of variables to be formally included in the
analysis. The application of fully Bayesian variable selection methods to large
data sets is computationally challenging. We describe an adaptive Markov chain
Monte Carlo approach called Individual Adaptation which adjusts a general
proposal to the data. We show that the algorithm is ergodic and discuss its use
within parallel tempering and sequential Monte Carlo approaches. We illustrate
the use of the method on two data sets including a gene expression analysis
with 22 577 variables.
</summary>
    <author>
      <name>Jim Griffin</name>
    </author>
    <author>
      <name>Krzysztof Latuszynski</name>
    </author>
    <author>
      <name>Mark Steel</name>
    </author>
    <link href="http://arxiv.org/abs/1412.6760v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.6760v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.7550v1</id>
    <updated>2014-12-23T21:25:24Z</updated>
    <published>2014-12-23T21:25:24Z</published>
    <title>Efficient particle-based online smoothing in general hidden Markov
  models: the PaRIS algorithm</title>
    <summary>  This paper presents a novel algorithm, the particle-based, rapid incremental
smoother (PaRIS), for efficient online approximation of smoothed expectations
of additive state functionals in general hidden Markov models. The algorithm,
which has a linear computational complexity under weak assumptions and very
limited memory requirements, is furnished with a number of convergence results,
including a central limit theorem. An interesting feature of PaRIS, which
samples on-the-fly from the retrospective dynamics induced by the particle
filter, is that it requires two or more backward draws per particle in order to
cope with degeneracy of the sampled trajectories and to stay numerically stable
in the long run with an asymptotic variance that grows only linearly with time.
</summary>
    <author>
      <name>Jimmy Olsson</name>
    </author>
    <author>
      <name>Johan Westerborn</name>
    </author>
    <link href="http://arxiv.org/abs/1412.7550v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.7550v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62M09 (Primary) 62F12 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.01898v1</id>
    <updated>2015-01-08T16:29:16Z</updated>
    <published>2015-01-08T16:29:16Z</published>
    <title>Fast Estimation of Diffusion Tensors under Rician noise by the EM
  algorithm</title>
    <summary>  This paper presents a fast computational method, the Expectation Maximization
algorithm, for Maximum Likelihood (ML) estimation in diffusion tensor imaging
under the Rice noise model. We further extend the ML framework to the maximum a
posterior (MAP) estimation and describe the numerical similarities of both ML
and MAP estimators. This novel method is implemented and applied using both
synthetic and real data in a wide range of b amplitudes. The comparison with
other popular methods are made in accuracy, methodology and computation.
</summary>
    <author>
      <name>Jia Liu</name>
    </author>
    <author>
      <name>Dario Gasbarra</name>
    </author>
    <author>
      <name>Juha Railavo</name>
    </author>
    <link href="http://arxiv.org/abs/1501.01898v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.01898v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.05128v1</id>
    <updated>2015-01-21T11:07:45Z</updated>
    <published>2015-01-21T11:07:45Z</published>
    <title>Estimating confidence regions of common measures of (baseline, treatment
  effect) on dichotomous outcome of a population</title>
    <summary>  In this article we estimate confidence regions of the common measures of
(baseline, treatment effect) in observational studies, where the measure of
baseline is baseline risk or baseline odds while the measure of treatment
effect is odds ratio, risk difference, risk ratio or attributable fraction, and
where confounding is controlled in estimation of both baseline and treatment
effect. To avoid high complexity of the normal approximation method and the
parametric or non-parametric bootstrap method, we obtain confidence regions for
measures of (baseline, treatment effect) by generating approximate
distributions of the ML estimates of these measures based on one logistic
model.
</summary>
    <author>
      <name>Li Yin</name>
    </author>
    <author>
      <name>Xiaoqin Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1501.05128v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.05128v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.05144v1</id>
    <updated>2015-01-21T11:48:52Z</updated>
    <published>2015-01-21T11:48:52Z</published>
    <title>Lazier ABC</title>
    <summary>  ABC algorithms involve a large number of simulations from the model of
interest, which can be very computationally costly. This paper summarises the
lazy ABC algorithm of Prangle (2015), which reduces the computational demand by
abandoning many unpromising simulations before completion. By using a random
stopping decision and reweighting the output sample appropriately, the target
distribution is the same as for standard ABC. Lazy ABC is also extended here to
the case of non-uniform ABC kernels, which is shown to simplify the process of
tuning the algorithm effectively.
</summary>
    <author>
      <name>Dennis Prangle</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented as contributed paper at "ABC in Montreal" NIPS workshop in
  December 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1501.05144v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.05144v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.05478v2</id>
    <updated>2016-09-13T17:06:05Z</updated>
    <published>2015-01-22T12:45:32Z</published>
    <title>Relabelling in Bayesian mixture models by pivotal units</title>
    <summary>  In this paper a simple procedure to deal with label switching when exploring
complex posterior distributions by MCMC algorithms is proposed. Although it
cannot be generalized to any situation, it may be handy in many applications
because of its simplicity and very low computational burden. A possible area
where it proves to be useful is when deriving a sample for the posterior
distribution arising from finite mixture models when no simple or rational
ordering between the components is available.
</summary>
    <author>
      <name>Leonardo Egidi</name>
    </author>
    <author>
      <name>Roberta Pappadà</name>
    </author>
    <author>
      <name>Francesco Pauli</name>
    </author>
    <author>
      <name>Nicola Torelli</name>
    </author>
    <link href="http://arxiv.org/abs/1501.05478v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.05478v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.06117v3</id>
    <updated>2015-06-14T09:30:58Z</updated>
    <published>2015-01-25T05:22:10Z</published>
    <title>Nonparametric estimation of the entropy using a ranked set sample</title>
    <summary>  This paper is concerned with non-parametric estimation of the entropy in
ranked set sampling. Theoretical properties of the proposed estimator are
studied. The proposed estimator is compared with the rival estimator in simple
random sampling. The applications of the proposed estimator to the mutual
information estimation as well as estimation of the Kullback-Leibler divergence
are provided. Several Monte-Carlo simulation studies are conducted to examine
the performance of the estimator. The results are applied to the long-leaf pine
(pinus palustris) trees and the body fat percentage data sets to illustrate
applicability of theoretical results.
</summary>
    <author>
      <name>Morteza Amini</name>
    </author>
    <author>
      <name>Mahdi Mahdizadeh</name>
    </author>
    <link href="http://arxiv.org/abs/1501.06117v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.06117v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.00318v1</id>
    <updated>2015-02-01T21:43:51Z</updated>
    <published>2015-02-01T21:43:51Z</published>
    <title>Setting the stage for data science: integration of data management
  skills in introductory and second courses in statistics</title>
    <summary>  Many have argued that statistics students need additional facility to express
statistical computations. By introducing students to commonplace tools for data
management, visualization, and reproducible analysis in data science and
applying these to real-world scenarios, we prepare them to think statistically.
In an era of increasingly big data, it is imperative that students develop
data-related capacities, beginning with the introductory course. We believe
that the integration of these precursors to data science into our
curricula-early and often-will help statisticians be part of the dialogue
regarding "Big Data" and "Big Questions".
</summary>
    <author>
      <name>Nicholas J. Horton</name>
    </author>
    <author>
      <name>Benjamin S. Baumer</name>
    </author>
    <author>
      <name>Hadley Wickham</name>
    </author>
    <link href="http://arxiv.org/abs/1502.00318v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.00318v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62-01" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.05287v2</id>
    <updated>2015-04-17T11:17:13Z</updated>
    <published>2015-02-18T16:08:27Z</published>
    <title>Optimal Regular Graph Designs</title>
    <summary>  A typical problem in optimal design theory is finding an experimental design
that is optimal with respect to some criteria in a class of designs. The most
popular criteria include the A- and D-criteria. Regular graph designs occur in
many optimality results and, if the number of blocks is large enough, they are
A- and D-optimal. We present the results of an exact computer search for the
best regular graph designs in large systems for up to 20 points, k&lt;=r&lt;=10 and
r(k-1)-(v-1)[r(k-1)/(v-1)]&lt;=9.
</summary>
    <author>
      <name>Sera Aylin Cakiroglu</name>
    </author>
    <link href="http://arxiv.org/abs/1502.05287v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.05287v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.01631v1</id>
    <updated>2015-03-05T13:37:13Z</updated>
    <published>2015-03-05T13:37:13Z</published>
    <title>Application of Sequential Quasi-Monte Carlo to Autonomous Positioning</title>
    <summary>  Sequential Monte Carlo algorithms (also known as particle filters) are
popular methods to approximate filtering (and related) distributions of
state-space models. However, they converge at the slow $1/\sqrt{N}$ rate, which
may be an issue in real-time data-intensive scenarios. We give a brief outline
of SQMC (Sequential Quasi-Monte Carlo), a variant of SMC based on
low-discrepancy point sets proposed by Gerber and Chopin (2015), which
converges at a faster rate, and we illustrate the greater performance of SQMC
on autonomous positioning problems.
</summary>
    <author>
      <name>Nicolas Chopin</name>
    </author>
    <author>
      <name>Mathieu Gerber</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.01631v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.01631v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.01842v1</id>
    <updated>2015-03-06T03:43:34Z</updated>
    <published>2015-03-06T03:43:34Z</published>
    <title>CEoptim: Cross-Entropy R Package for Optimization</title>
    <summary>  The cross-entropy (CE) method is simple and versatile technique for
optimization, based on Kullback-Leibler (or cross-entropy) minimization. The
method can be applied to a wide range of optimization tasks, including
continuous, discrete, mixed and constrained optimization problems. The new
package CEoptim provides the R implementation of the CE method for
optimization. We describe the general CE methodology for optimization and well
as some useful modifications. The usage and efficacy of CEoptim is demonstrated
through a variety of optimization examples, including model fitting,
combinatorial optimization, and maximum likelihood estimation.
</summary>
    <author>
      <name>Tim Benham</name>
    </author>
    <author>
      <name>Qibin Duan</name>
    </author>
    <author>
      <name>Dirk P. Kroese</name>
    </author>
    <author>
      <name>Benoit Liquet</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.01842v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.01842v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.05210v1</id>
    <updated>2015-03-17T20:20:15Z</updated>
    <published>2015-03-17T20:20:15Z</published>
    <title>Speeding up lower bound estimation in powerlaw distributions</title>
    <summary>  The traditional lower bound estimation method for powerlaw distributions
based on the Kolmogorov-Smirnov distance proved to perform better than other
competing methods. However, if applied to very large collections of data, such
a method can be computationally demanding. In this paper, we propose two
alternative methods with the aim to reduce the time required by the estimation
procedure. We apply the traditional method and the two proposed methods to
large collections of data ($N = 500,000$) with varying values of the true lower
bound. Both the proposed methods yield a significantly better performance and
accuracy than the traditional method.
</summary>
    <author>
      <name>Alessandro Bessi</name>
    </author>
    <link href="http://arxiv.org/abs/1503.05210v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.05210v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.00964v2</id>
    <updated>2015-12-28T21:31:37Z</updated>
    <published>2015-04-04T01:05:36Z</published>
    <title>Efficient Computation of the Bergsma-Dassios Sign Covariance</title>
    <summary>  In an extension of Kendall's $\tau$, Bergsma and Dassios (2014) introduced a
covariance measure $\tau^*$ for two ordinal random variables that vanishes if
and only if the two variables are independent. For a sample of size $n$, a
direct computation of $t^*$, the empirical version of $\tau^*$, requires
$O(n^4)$ operations. We derive an algorithm that computes the statistic using
only $O(n^2\log(n))$ operations.
</summary>
    <author>
      <name>Luca Weihs</name>
    </author>
    <author>
      <name>Mathias Drton</name>
    </author>
    <author>
      <name>Dennis Leung</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Improved formatting, added reference to R package implementing the
  algorithm, and added additional experiments</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.00964v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.00964v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.06226v1</id>
    <updated>2015-04-23T15:39:29Z</updated>
    <published>2015-04-23T15:39:29Z</published>
    <title>Optimal design of experiments via linear programming</title>
    <summary>  We investigate the possibility of extending some results of Pazman and
Pronzato (2014) to a larger set of optimality criteria. Namely, in a linear
regression model the problem of computing D-, A-, E_k-optimal designs, of
combining these optimality criteria, and the "criterion robust" problem of
Harman (2004) are reformulated here as "infinite-dimensional" linear
programming problems. Approximate optimum designs can then be computed by a
modified cutting-plane method, and this is checked on examples. Finally, the
expressions for these criteria are reformulated in terms of the response
function of an even nonlinear model.
</summary>
    <author>
      <name>Katarina Burclova</name>
    </author>
    <author>
      <name>Andrej Pazman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.06226v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.06226v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62K05, 90C05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.01979v1</id>
    <updated>2015-05-08T10:15:51Z</updated>
    <published>2015-05-08T10:15:51Z</published>
    <title>The efficiency of the likelihood ratio to choose between a
  t-distribution and a normal distribution</title>
    <summary>  A decision must often be made between heavy-tailed and Gaussian errors for a
regression or a time series model, and the t-distribution is frequently used
when it is assumed that the errors are heavy-tailed distributed. The
performance of the likelihood ratio to choose between the two distributions is
investigated using entropy properties and a simulation study. The proportion of
times or probability that the likelihood of the correct assumption will be
bigger than the likelihood of the incorrect assumption is estimated.
</summary>
    <author>
      <name>J. Martin van Zyl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.01979v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.01979v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.02556v1</id>
    <updated>2015-05-11T10:36:00Z</updated>
    <published>2015-05-11T10:36:00Z</published>
    <title>Extending Bayesian analysis of circular data to comparison of multiple
  groups</title>
    <summary>  Circular data are data measured in angles and occur in a variety of
scientific disciplines. Bayesian methods promise to allow for flexible analysis
of circular data. Three existing MCMC methods (Gibbs, Metropolis-Hastings, and
Rejection) for a single group of circular data were extended to be used in a
between-subjects design, providing a novel procedure to compare groups of
circular data. Investigating the performance of the methods by simulation
study, all methods were found to overestimate the concentration parameter of
the posterior, while coverage was reasonable. The rejection sampler performed
best. In future research, the MCMC method may be extended to include
covariates, or a within-subjects design.
</summary>
    <author>
      <name>Kees Tim Mulder</name>
    </author>
    <author>
      <name>Irene Klugkist</name>
    </author>
    <link href="http://arxiv.org/abs/1505.02556v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.02556v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62F15 62M05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.03506v1</id>
    <updated>2015-05-13T19:41:04Z</updated>
    <published>2015-05-13T19:41:04Z</published>
    <title>Subset Simulation Method for Rare Event Estimation: An Introduction</title>
    <summary>  This paper provides a detailed introductory description of Subset Simulation,
an advanced stochastic simulation method for estimation of small probabilities
of rare failure events. A simple and intuitive derivation of the method is
given along with the discussion on its implementation. The method is
illustrated with several easy-to-understand examples. For demonstration
purposes, the MATLAB code for the considered examples is provided. The reader
is assumed to be familiar only with elementary probability theory and
statistics.
</summary>
    <author>
      <name>Konstantin Zuev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 page, 12 figures, MATLAB code in Zuev K.: Subset Simulation Method
  for Rare Event Estimation: An Introduction. In: Beer M. et al (Ed.)
  Encyclopedia of Earthquake Engineering: SpringerReference
  (www.springerreference.com), 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.03506v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.03506v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.05391v1</id>
    <updated>2015-05-20T14:13:39Z</updated>
    <published>2015-05-20T14:13:39Z</published>
    <title>Efficient Multiple Importance Sampling Estimators</title>
    <summary>  Multiple importance sampling (MIS) methods use a set of proposal
distributions from which samples are drawn. Each sample is then assigned an
importance weight that can be obtained according to different strategies. This
work is motivated by the trade-off between variance reduction and computational
complexity of the different approaches (classical vs. deterministic mixture)
available for the weight calculation. A new method that achieves an efficient
compromise between both factors is introduced in this paper. It is based on
forming a partition of the set of proposal distributions and computing the
weights accordingly. Computer simulations show the excellent performance of the
associated \mbox{\emph{partial deterministic mixture} MIS estimator.
</summary>
    <author>
      <name>Víctor Elvira</name>
    </author>
    <author>
      <name>Luca Martino</name>
    </author>
    <author>
      <name>David Luengo</name>
    </author>
    <author>
      <name>Mónica F. Bugallo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/LSP.2015.2432078</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/LSP.2015.2432078" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Signal Processing Letters, VOL. 22, NO. 10, OCTOBER 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1505.05391v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.05391v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.06473v2</id>
    <updated>2015-05-26T15:34:59Z</updated>
    <published>2015-05-24T20:16:11Z</published>
    <title>Three discussions of the paper "sequential quasi-Monte Carlo sampling",
  by M. Gerber and N. Chopin</title>
    <summary>  This is a collection of three written discussions of the paper "sequential
quasi-Monte Carlo sampling" by M. Gerber and N. Chopin, following the
presentation given before the Royal Statistical Society in London on December
10th, 2014.
</summary>
    <author>
      <name>Julyan Arbel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Torino</arxiv:affiliation>
    </author>
    <author>
      <name>Igor Prunster</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Torino</arxiv:affiliation>
    </author>
    <author>
      <name>Christian P. Robert</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Universite Paris-Dauphine</arxiv:affiliation>
    </author>
    <author>
      <name>Robin J. Ryder</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Universite Paris-Dauphine</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1111/rssb.12104</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1111/rssb.12104" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in the Journal of the Royal Statistical Society, volume
  77(3), pages 559, 569 and 570</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.06473v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.06473v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.01117v1</id>
    <updated>2015-06-03T04:28:39Z</updated>
    <published>2015-06-03T04:28:39Z</published>
    <title>Estimating Residual Connectivity for Random Graphs</title>
    <summary>  Computation of the probability that a random graph is connected is a
challenging problem, so it is natural to turn to approximations such as Monte
Carlo methods. We describe sequential importance resampling and splitting
algorithms for the estimation of these probabilities. The importance sampling
steps of these algorithms involve identifying vertices that must be present in
order for the random graph to be connected, and conditioning on the
corresponding events. We provide numerical results demonstrating the
effectiveness of the proposed algorithm.
</summary>
    <author>
      <name>Rohan Shah</name>
    </author>
    <author>
      <name>Dirk P. Kroese</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.01117v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.01117v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65C05, 60K10" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.07044v3</id>
    <updated>2015-07-21T13:57:16Z</updated>
    <published>2015-06-23T15:12:11Z</published>
    <title>Efficient Monte Carlo Methods for the Potts Model at Low Temperature</title>
    <summary>  We consider the problem of estimating the partition function of the
ferromagnetic $q$-state Potts model. We propose an importance sampling
algorithm in the dual of the normal factor graph representing the model. The
algorithm can efficiently compute an estimate of the partition function when
the coupling parameters of the model are strong (corresponding to models at low
temperature) or when the model contains a mixture of strong and weak couplings.
We show that, in this setting, the proposed algorithm significantly outperforms
the state of the art methods.
</summary>
    <author>
      <name>Mehdi Molkaraie</name>
    </author>
    <author>
      <name>Vicenc Gomez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.07044v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.07044v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.08640v1</id>
    <updated>2015-06-29T14:14:38Z</updated>
    <published>2015-06-29T14:14:38Z</published>
    <title>Leave Pima Indians alone: binary regression as a benchmark for Bayesian
  computation</title>
    <summary>  Abstract. Whenever a new approach to perform Bayesian computation is
introduced, a common practice is to showcase this approach on a binary
regression model and datasets of moderate size. This paper discusses to which
extent this practice is sound. It also reviews the current state of the art of
Bayesian computation, using binary regression as a running example. Both
sampling-based algorithms (importance sampling, MCMC and SMC) and fast
approximations (Laplace and EP) are covered. Extensive numerical results are
provided, some of which might go against conventional wisdom regarding the
effectiveness of certain algorithms. Implications for other problems (variable
selection) and other models are also discussed.
</summary>
    <author>
      <name>Nicolas Chopin</name>
    </author>
    <author>
      <name>James Ridgway</name>
    </author>
    <link href="http://arxiv.org/abs/1506.08640v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.08640v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.09035v1</id>
    <updated>2015-06-30T11:17:40Z</updated>
    <published>2015-06-30T11:17:40Z</published>
    <title>Bayesian model averaging in model-based clustering and density
  estimation</title>
    <summary>  We propose Bayesian model averaging (BMA) as a method for postprocessing the
results of model-based clustering. Given a number of competing models,
appropriate model summaries are averaged, using the posterior model
probabilities, instead of being taken from a single "best" model. We
demonstrate the use of BMA in model-based clustering for a number of datasets.
We show that BMA provides a useful summary of the clustering of observations
while taking model uncertainty into account. Further, we show that BMA in
conjunction with model-based clustering gives a competitive method for density
estimation in a multivariate setting. Applying BMA in the model-based context
is fast and can give enhanced modeling performance.
</summary>
    <author>
      <name>Niamh Russell</name>
    </author>
    <author>
      <name>Thomas Brendan Murphy</name>
    </author>
    <author>
      <name>Adrian E Raftery</name>
    </author>
    <link href="http://arxiv.org/abs/1506.09035v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.09035v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.02646v4</id>
    <updated>2016-09-26T08:34:49Z</updated>
    <published>2015-07-09T18:43:28Z</published>
    <title>Pareto Smoothed Importance Sampling</title>
    <summary>  Importance weighting is a convenient general way to adjust for draws from the
wrong distribution, but the resulting ratio estimate can be noisy when the
importance weights have a heavy right tail, as routinely occurs when there are
aspects of the target distribution not well captured by the approximating
distribution. More stable estimates can be obtained by truncating the
importance ratios. Here we present a new method for stabilizing importance
weights using a generalized Pareto distribution fit to the upper tail of the
distribution of the simulated importance ratios.
</summary>
    <author>
      <name>Aki Vehtari</name>
    </author>
    <author>
      <name>Andrew Gelman</name>
    </author>
    <author>
      <name>Jonah Gabry</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Three new examples, updated references</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.02646v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.02646v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.03887v1</id>
    <updated>2015-07-14T15:24:52Z</updated>
    <published>2015-07-14T15:24:52Z</published>
    <title>An SVM-like Approach for Expectile Regression</title>
    <summary>  Expectile regression is a nice tool for investigating conditional
distributions beyond the conditional mean. It is well-known that expectiles can
be described with the help of the asymmetric least square loss function, and
this link makes it possible to estimate expectiles in a non-parametric
framework by a support vector machine like approach. In this work we develop an
efficient sequential-minimal-optimization-based solver for the underlying
optimization problem. The behavior of the solver is investigated by conducting
various experiments and the results are compared with the recent R-package
ER-Boost.
</summary>
    <author>
      <name>Muhammad Farooq</name>
    </author>
    <author>
      <name>Ingo Steinwart</name>
    </author>
    <link href="http://arxiv.org/abs/1507.03887v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.03887v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.08050v1</id>
    <updated>2015-07-29T08:20:22Z</updated>
    <published>2015-07-29T08:20:22Z</published>
    <title>Probabilistic Programming in Python using PyMC</title>
    <summary>  Probabilistic programming (PP) allows flexible specification of Bayesian
statistical models in code. PyMC3 is a new, open-source PP framework with an
intutive and readable, yet powerful, syntax that is close to the natural syntax
statisticians use to describe models. It features next-generation Markov chain
Monte Carlo (MCMC) sampling algorithms such as the No-U-Turn Sampler (NUTS;
Hoffman, 2014), a self-tuning variant of Hamiltonian Monte Carlo (HMC; Duane,
1987). Probabilistic programming in Python confers a number of advantages
including multi-platform compatibility, an expressive yet clean and readable
syntax, easy integration with other scientific libraries, and extensibility via
C, C++, Fortran or Cython. These features make it relatively straightforward to
write and use custom statistical distributions, samplers and transformation
functions, as required by Bayesian analysis.
</summary>
    <author>
      <name>John Salvatier</name>
    </author>
    <author>
      <name>Thomas Wiecki</name>
    </author>
    <author>
      <name>Christopher Fonnesbeck</name>
    </author>
    <link href="http://arxiv.org/abs/1507.08050v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.08050v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.08563v2</id>
    <updated>2016-07-12T10:36:18Z</updated>
    <published>2015-07-30T16:24:56Z</published>
    <title>Metropolized Randomized Maximum Likelihood for sampling from multimodal
  distributions</title>
    <summary>  This article describes a method for using optimization to derive efficient
independent transition functions for Markov chain Monte Carlo simulations. Our
interest is in sampling from a posterior density $\pi(x)$ for problems in which
the dimension of the model space is large, $\pi(x)$ is multimodal with regions
of low probability separating the modes, and evaluation of the likelihood is
expensive. We restrict our attention to the special case for which the target
density is the product of a multivariate Gaussian prior and a likelihood
function for which the errors in observations are additive and Gaussian.
</summary>
    <author>
      <name>Dean S. Oliver</name>
    </author>
    <link href="http://arxiv.org/abs/1507.08563v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.08563v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.00219v1</id>
    <updated>2015-08-02T11:06:33Z</updated>
    <published>2015-08-02T11:06:33Z</published>
    <title>On Bivariate Generalized Exponential-Power Series Class of Distributions</title>
    <summary>  In this paper, we introduce a new class of bivariate distributions by
compounding the bivariate generalized exponential and power-series
distributions.
  This new class contains some new sub-models such as the bivariate generalized
exponential distribution, the bivariate generalized exponential-poisson,
-logarithmic, -binomial and -negative binomial distributions. We derive
different properties of the new class of distributions. The EM algorithm is
used to determine the maximum likelihood estimates of the parameters. We
illustrate the usefulness of the new distributions by means of an application
to a real data set.
</summary>
    <author>
      <name>Ali Akbar Jafari</name>
    </author>
    <author>
      <name>Rasool Roozegar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1507.07535</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.00219v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.00219v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.03884v4</id>
    <updated>2015-09-28T06:31:00Z</updated>
    <published>2015-08-17T00:28:54Z</published>
    <title>A simple sampler for the horseshoe estimator</title>
    <summary>  In this note we derive a simple Bayesian sampler for linear regression with
the horseshoe hierarchy. A new interpretation of the horseshoe model is
presented, and extensions to logistic regression and alternative hierarchies,
such as horseshoe$+$, are discussed. Due to the conjugacy of the proposed
hierarchy, Chib's algorithm may be used to easily compute the marginal
likelihood of the model.
</summary>
    <author>
      <name>Enes Makalic</name>
    </author>
    <author>
      <name>Daniel F. Schmidt</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/LSP.2015.2503725</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/LSP.2015.2503725" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Signal Processing Letters, Vol. 23(1), pp. 179-182, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1508.03884v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.03884v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.04253v2</id>
    <updated>2016-02-19T19:32:32Z</updated>
    <published>2015-08-18T09:18:53Z</published>
    <title>Issues in the Multiple Try Metropolis mixing</title>
    <summary>  The multiple Try Metropolis (MTM) algorithm is an advanced MCMC technique
based on drawing and testing several candidates at each iteration of the
algorithm. One of them is selected according to certain weights and then it is
tested according to a suitable acceptance probability. Clearly, since the
computational cost increases as the employed number of tries grows, one expects
that the performance of an MTM scheme improves as the number of tries
increases, as well. However, there are scenarios where the increase of number
of tries does not produce a corresponding enhancement of the performance. In
this work, we describe these scenarios and then we introduce possible solutions
for solving these issues.
</summary>
    <author>
      <name>L. Martino</name>
    </author>
    <author>
      <name>F. Louzada</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s00180-016-0643-9</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s00180-016-0643-9" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computational Statistics, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1508.04253v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.04253v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.03253v1</id>
    <updated>2015-09-10T18:11:48Z</updated>
    <published>2015-09-10T18:11:48Z</published>
    <title>Comparison of hazard rate estimation in R</title>
    <summary>  We give an overview of eight different software packages and functions
available in R for semi- or non-parametric estimation of the hazard rate for
right-censored survival data. Of particular interest is the accuracy of the
estimation of the hazard rate in the presence of covariates, as well as the
user-friendliness of the packages. In addition, we investigate the ability to
incorporate covariates under both the proportional and the non-proportional
hazards assumptions. We contrast the robustness, variability and precision of
the functions through simulations, and then further compare differences between
the functions by analyzing the "cancer" and "TRACE" survival data sets
available in R, including covariates under the proportional and
non-proportional hazards settings.
</summary>
    <author>
      <name>Yolanda Hagar</name>
    </author>
    <author>
      <name>Vanja Dukic</name>
    </author>
    <link href="http://arxiv.org/abs/1509.03253v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.03253v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.03495v1</id>
    <updated>2015-09-11T13:14:31Z</updated>
    <published>2015-09-11T13:14:31Z</published>
    <title>Gradient Scan Gibbs Sampler: an efficient algorithm for high-dimensional
  Gaussian distributions</title>
    <summary>  This paper deals with Gibbs samplers that include high dimensional
conditional Gaussian distributions. It proposes an efficient algorithm that
avoids the high dimensional Gaussian sampling and relies on a random excursion
along a small set of directions. The algorithm is proved to converge, i.e. the
drawn samples are asymptotically distributed according to the target
distribution. Our main motivation is in inverse problems related to general
linear observation models and their solution in a hierarchical Bayesian
framework implemented through sampling algorithms. It finds direct applications
in semi-blind/unsupervised methods as well as in some non-Gaussian methods. The
paper provides an illustration focused on the unsupervised estimation for
super-resolution methods.
</summary>
    <author>
      <name>Olivier Féron</name>
    </author>
    <author>
      <name>François Orieux</name>
    </author>
    <author>
      <name>Jean-François Giovannelli</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/JSTSP.2015.2510961</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/JSTSP.2015.2510961" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.03495v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.03495v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.05005v1</id>
    <updated>2015-09-16T19:22:19Z</updated>
    <published>2015-09-16T19:22:19Z</published>
    <title>Comments on "Detecting Outliers in Gamma Distribution" by M. Jabbari
  Nooghabi et al. (2010)</title>
    <summary>  This note shows that the results presented by Jabbari Nooghabi et al. (2010)
do not hold in all expected cases. With this, the technique proposed by Kumar
and Lalhita (2012) for detecting upper outliers in Gamma samples is also not
valid. Specifically, this note shows that the probability density functions
(pdf) under the null hypothesis of the test statistics therein proposed are not
always valid.
</summary>
    <author>
      <name>M. Magdalena Lucini</name>
    </author>
    <author>
      <name>Alejandro C. Frery</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in Communications in Statistics - Theory and
  Methods</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.05005v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.05005v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.06365v1</id>
    <updated>2015-09-21T14:56:35Z</updated>
    <published>2015-09-21T14:56:35Z</published>
    <title>Expanding the Computation of Mixture Models by the use of Hermite
  Polynomials and Ideals</title>
    <summary>  Mixture models have found uses in many areas. To list a few: unsupervised
learning, empirical Bayes, latent class and trait models. The current
applications of mixture models to empirical data is limited to computing a
mixture model from the same parametric family, e.g. Gaussians or Poissons. In
this paper it is shown that by using Hermite polynomials and ideals, the
modeling of a mixture process can be extended to include different families in
terms of their cumulative distribution functions (cdfs)
</summary>
    <author>
      <name>Andrew Clark</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The use of algebraic geometry to the solution of the mixture problem
  expands the application of algebra to statistics. The algebraic method used
  is a well known. It is its application to statistics that is different</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.06365v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.06365v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62-07, 14Q99" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.07376v1</id>
    <updated>2015-09-24T13:58:17Z</updated>
    <published>2015-09-24T13:58:17Z</published>
    <title>A hybrid sampler for Poisson-Kingman mixture models</title>
    <summary>  This paper concerns the introduction of a new Markov Chain Monte Carlo scheme
for posterior sampling in Bayesian nonparametric mixture models with priors
that belong to the general Poisson-Kingman class. We present a novel compact
way of representing the infinite dimensional component of the model such that
while explicitly representing this infinite component it has less memory and
storage requirements than previous MCMC schemes. We describe comparative
simulation results demonstrating the efficacy of the proposed MCMC algorithm
against existing marginal and conditional MCMC samplers.
</summary>
    <author>
      <name>Maria Lomeli</name>
    </author>
    <author>
      <name>Stefano Favaro</name>
    </author>
    <author>
      <name>Yee Whye Teh</name>
    </author>
    <link href="http://arxiv.org/abs/1509.07376v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.07376v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.07985v1</id>
    <updated>2015-09-26T14:16:25Z</updated>
    <published>2015-09-26T14:16:25Z</published>
    <title>Adaptive Rejection Sampling with fixed number of nodes</title>
    <summary>  The adaptive rejection sampling (ARS) algorithm is a universal random
generator for drawing samples efficiently from a univariate log-concave target
probability density function (pdf). ARS generates independent samples from the
target via rejection sampling with high acceptance rates. Indeed, ARS yields a
sequence of proposal functions that converge toward the target pdf, so that the
probability of accepting a sample approaches one. However, sampling from the
proposal pdf becomes more computational demanding each time it is updated. In
this work, we propose a novel ARS scheme, called Cheap Adaptive Rejection
Sampling (CARS), where the computational effort for drawing from the proposal
remains constant, decided in advance by the user. For generating a large number
of desired samples, CARS is faster than ARS.
</summary>
    <author>
      <name>L. Martino</name>
    </author>
    <author>
      <name>F. Louzada</name>
    </author>
    <link href="http://arxiv.org/abs/1509.07985v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.07985v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.07993v1</id>
    <updated>2015-09-26T15:16:50Z</updated>
    <published>2015-09-26T15:16:50Z</published>
    <title>Parallel Metropolis chains with cooperative adaptation</title>
    <summary>  Monte Carlo methods, such as Markov chain Monte Carlo (MCMC) algorithms, have
become very popular in signal processing over the last years. In this work, we
introduce a novel MCMC scheme where parallel MCMC chains interact, adapting
cooperatively the parameters of their proposal functions. Furthermore, the
novel algorithm distributes the computational effort adaptively, rewarding the
chains which are providing better performance and, possibly even stopping other
ones. These extinct chains can be reactivated if the algorithm considers
necessary. Numerical simulations shows the benefits of the novel scheme.
</summary>
    <author>
      <name>L. Martino</name>
    </author>
    <author>
      <name>V. Elvira</name>
    </author>
    <author>
      <name>D. Luengo</name>
    </author>
    <author>
      <name>F. Louzada</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICASSP.2016.7472423</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICASSP.2016.7472423" rel="related"/>
    <link href="http://arxiv.org/abs/1509.07993v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.07993v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.00041v2</id>
    <updated>2016-04-07T17:49:16Z</updated>
    <published>2015-09-30T21:31:42Z</published>
    <title>iotools: High-Performance I/O Tools for R</title>
    <summary>  The iotools package provides a set of tools for Input/Output (I/O) intensive
datasets processing in R (R Core Team, 2014). Efficent parsing methods are
included which minimize copying and avoid the use of intermediate string
representations whenever possible. Functions for applying chunk-wise operations
allow for computing on streaming input as well as arbitrarily large files. We
present a set of example use cases for iotools, as well as extensive benchmarks
comparing comparable functions provided in both core-R as well as other
contributed packages.
</summary>
    <author>
      <name>Taylor Arnold</name>
    </author>
    <author>
      <name>Michael Kane</name>
    </author>
    <author>
      <name>Simon Urbanek</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.00041v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.00041v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="03-04" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.00755v1</id>
    <updated>2015-10-02T23:05:48Z</updated>
    <published>2015-10-02T23:05:48Z</published>
    <title>Sparse Density Representations for Simultaneous Inference on Large
  Spatial Datasets</title>
    <summary>  Large spatial datasets often represent a number of spatial point processes
generated by distinct entities or classes of events. When crossed with
covariates, such as discrete time buckets, this can quickly result in a data
set with millions of individual density estimates. Applications that require
simultaneous access to a substantial subset of these estimates become resource
constrained when densities are stored in complex and incompatible formats. We
present a method for representing spatial densities along the nodes of sparsely
populated trees. Fast algorithms are provided for performing set operations and
queries on the resulting compact tree structures. The speed and simplicity of
the approach is demonstrated on both real and simulated spatial data.
</summary>
    <author>
      <name>Taylor Arnold</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 figures, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.00755v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.00755v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.00861v1</id>
    <updated>2015-10-03T20:05:58Z</updated>
    <published>2015-10-03T20:05:58Z</published>
    <title>A Geometric View of Posterior Approximation</title>
    <summary>  Although Bayesian methods are robust and principled, their application in
practice could be limited since they typically rely on computationally
intensive Markov Chain Monte Carlo algorithms for their implementation. One
possible solution is to find a fast approximation of posterior distribution and
use it for statistical inference. For commonly used approximation methods, such
as Laplace and variational free energy, the objective is mainly defined in
terms of computational convenience as opposed to a true distance measure
between the target and approximating distributions. In this paper, we provide a
geometric view of posterior approximation based on a valid distance measure
derived from ambient Fisher geometry. Our proposed framework is easily
generalizable and can inspire a new class of methods for approximate Bayesian
inference.
</summary>
    <author>
      <name>Tian Chen</name>
    </author>
    <author>
      <name>Jeffrey Streets</name>
    </author>
    <author>
      <name>Babak Shahbaba</name>
    </author>
    <link href="http://arxiv.org/abs/1510.00861v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.00861v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.02284v3</id>
    <updated>2016-10-02T12:06:46Z</updated>
    <published>2015-11-07T02:20:32Z</published>
    <title>A Derivative-Free Trust-Region Algorithm for Reliability-Based
  Optimization</title>
    <summary>  In this note, we present a derivative-free trust-region (TR) algorithm for
reliability based optimization (RBO) problems. The proposed algorithm consists
of solving a set of subproblems, in which simple surrogate models of the
reliability constraints are constructed and used in solving the subproblems.
Taking advantage of the special structure of the RBO problems, we employ a
sample reweighting method to evaluate the failure probabilities, which
constructs the surrogate for the reliability constraints by performing only a
single full reliability evaluation in each iteration. With numerical
experiments, we illustrate that the proposed algorithm is competitive against
existing methods.
</summary>
    <author>
      <name>Tian Gao</name>
    </author>
    <author>
      <name>Jinglai Li</name>
    </author>
    <link href="http://arxiv.org/abs/1511.02284v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.02284v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="90C15, 90C56, 65C50" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.03977v1</id>
    <updated>2015-11-12T17:26:47Z</updated>
    <published>2015-11-12T17:26:47Z</published>
    <title>Convergence of the risk for nonparametric IV quantile regression and
  nonparametric IV regression with full independence</title>
    <summary>  In econometrics some nonparametric instrumental regression models and
nonparametric demand models with endogeneity lead to nonlinear integral
equations with unknown integral kernels. We prove convergence rates of the risk
for the iteratively regularized Newton method applied to these problems.
Compared to related results we relay on a weaker non-linearity condition and
have stronger convergence results. We demonstrate by numerical simulations for
a nonparametric IV regression problem with continuous instrument and regressor
that the method produces better results than the standard method.
</summary>
    <author>
      <name>Fabian Dunker</name>
    </author>
    <link href="http://arxiv.org/abs/1511.03977v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.03977v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.01496v1</id>
    <updated>2015-12-04T17:56:41Z</updated>
    <published>2015-12-04T17:56:41Z</published>
    <title>Embarrassingly Parallel Sequential Markov-chain Monte Carlo for Large
  Sets of Time Series</title>
    <summary>  Bayesian computation crucially relies on Markov chain Monte Carlo (MCMC)
algorithms. In the case of massive data sets, running the Metropolis-Hastings
sampler to draw from the posterior distribution becomes prohibitive due to the
large number of likelihood terms that need to be calculated at each iteration.
In order to perform Bayesian inference for a large set of time series, we
consider an algorithm that combines 'divide and conquer" ideas previously used
to design MCMC algorithms for big data with a sequential MCMC strategy. The
performance of the method is illustrated using a large set of financial data.
</summary>
    <author>
      <name>Roberto Casarin</name>
    </author>
    <author>
      <name>Radu V. Craiu</name>
    </author>
    <author>
      <name>Fabrizio Leisen</name>
    </author>
    <link href="http://arxiv.org/abs/1512.01496v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.01496v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.02863v2</id>
    <updated>2016-03-18T14:01:17Z</updated>
    <published>2015-12-09T14:14:56Z</published>
    <title>On the eigenvalues of the spatial sign covariance matrix in more than
  two dimensions</title>
    <summary>  We gather several results on the eigenvalues of the spatial sign covariance
matrix of an elliptical distribution. It is shown that the eigenvalues are a
one-to-one function of the eigenvalues of the shape matrix and that they are
closer together than the latter. We further provide a one-dimensional integral
representation of the eigenvalues, which facilitates their numerical
computation.
</summary>
    <author>
      <name>Alexander Dürre</name>
    </author>
    <author>
      <name>David E. Tyler</name>
    </author>
    <author>
      <name>Daniel Vogel</name>
    </author>
    <link href="http://arxiv.org/abs/1512.02863v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.02863v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62H12, 62G20, 62H11" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.04468v1</id>
    <updated>2015-11-23T19:40:17Z</updated>
    <published>2015-11-23T19:40:17Z</published>
    <title>A Method to Calculate the Exit Time in Stochastic Simulations</title>
    <summary>  A novel method is presented to compute the exit time for the stochastic
simulation algorithm. The method is based on the addition of a series of random
variables and is derived using the convolution theorem. The final distribution
is derived and approximated in the frequency domain. The distribution for the
final time is transformed back to the real domain and can be sampled from in a
simulation. The result is an approximation of the classical stochastic
simulation algorithm that requires fewer random variates. An analysis of the
error and speedup compared to the stochastic simulation algorithm is presented.
</summary>
    <author>
      <name>Basil S. Bayati</name>
    </author>
    <link href="http://arxiv.org/abs/1512.04468v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.04468v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.04743v1</id>
    <updated>2015-12-15T11:52:14Z</updated>
    <published>2015-12-15T11:52:14Z</published>
    <title>Model comparison with missing data using MCMC and importance sampling</title>
    <summary>  Selecting between competing statistical models is a challenging problem
especially when the competing models are non-nested. In this paper we offer a
simple solution by devising an algorithm which combines MCMC and importance
sampling to obtain computationally efficient estimates of the marginal
likelihood which can then be used to compare the models. The algorithm is
successfully applied to longitudinal epidemic and time series data sets and
shown to outperform existing methods for computing the marginal likelihood.
</summary>
    <author>
      <name>Panayiota Touloupou</name>
    </author>
    <author>
      <name>Naif Alzahrani</name>
    </author>
    <author>
      <name>Peter Neal</name>
    </author>
    <author>
      <name>Simon E. F. Spencer</name>
    </author>
    <author>
      <name>Trevelyan J. McKinley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">34 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.04743v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.04743v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.05153v1</id>
    <updated>2015-12-16T12:50:30Z</updated>
    <published>2015-12-16T12:50:30Z</published>
    <title>An algorithm for the multivariate group lasso with covariance estimation</title>
    <summary>  We study a group lasso estimator for the multivariate linear regression model
that accounts for correlated error terms. A block coordinate descent algorithm
is used to compute this estimator. We perform a simulation study with
categorical data and multivariate time series data, typical settings with a
natural grouping among the predictor variables. Our simulation studies show the
good performance of the proposed group lasso estimator compared to alternative
estimators. We illustrate the method on a time series data set of gene
expressions.
</summary>
    <author>
      <name>Ines Wilms</name>
    </author>
    <author>
      <name>Christophe Croux</name>
    </author>
    <link href="http://arxiv.org/abs/1512.05153v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.05153v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.07246v3</id>
    <updated>2016-03-12T00:38:49Z</updated>
    <published>2015-12-22T20:51:41Z</published>
    <title>Efficient Thresholded Correlation using Truncated Singular Value
  Decomposition</title>
    <summary>  Efficiently computing a subset of a correlation matrix consisting of values
above a specified threshold is important to many practical applications.
Real-world problems in genomics, machine learning, finance other applications
can produce correlation matrices too large to explicitly form and tractably
compute. Often, only values corresponding to highly-correlated vectors are of
interest, and those values typically make up a small fraction of the overall
correlation matrix. We present a method based on the singular value
decomposition (SVD) and its relationship to the data covariance structure that
can efficiently compute thresholded subsets of very large correlation matrices.
</summary>
    <author>
      <name>James Baglama</name>
    </author>
    <author>
      <name>Michael Kane</name>
    </author>
    <author>
      <name>Bryan Lewis</name>
    </author>
    <author>
      <name>Alex Poliakov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.07246v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.07246v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.07678v3</id>
    <updated>2016-05-03T09:31:17Z</updated>
    <published>2015-12-24T01:28:30Z</published>
    <title>Composite Bayesian inference</title>
    <summary>  This paper revisits the concept of composite likelihood from the perspective
of probabilistic inference, and proposes a generalization called super
composite likelihood for sharper inference in multiclass problems. It is argued
that, beside providing a new interpretation and a general justification of
na\"ive Bayes procedures, super composite likelihood yields a much wider class
of discriminative models suitable for unsupervised and weakly supervised
learning.
</summary>
    <author>
      <name>Alexis Roche</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Working paper: extended results on pre-training and online training,
  comparision with restricted Boltzmann machines</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.07678v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.07678v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.04060v2</id>
    <updated>2017-02-17T08:42:26Z</updated>
    <published>2016-02-12T14:08:42Z</published>
    <title>Discrete approximation of a mixture distribution via restricted
  divergence</title>
    <summary>  Mixture distributions arise in many application areas, for example as
marginal distributions or convolutions of distributions. We present a method of
constructing an easily tractable discrete mixture distribution as an
approximation to a mixture distribution with a large to infinite number,
discrete or continuous, of components. The proposed DIRECT (Divergence
Restricting Conditional Tesselation) algorithm is set up such that a
pre-specified precision, defined in terms of Kullback-Leibler divergence
between true distribution and approximation, is guaranteed. Application of the
algorithm is demonstrated in two examples.
</summary>
    <author>
      <name>Christian Röver</name>
    </author>
    <author>
      <name>Tim Friede</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1080/10618600.2016.1276840</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1080/10618600.2016.1276840" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Computational and Graphical Statistics, 26(1):217-222
  (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1602.04060v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.04060v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.05238v1</id>
    <updated>2016-02-16T23:01:28Z</updated>
    <published>2016-02-16T23:01:28Z</published>
    <title>A Unified Monte-Carlo Jackknife for Small Area Estimation after Model
  Selection</title>
    <summary>  We consider estimation of measure of uncertainty in small area estimation
(SAE) when a procedure of model selection is involved prior to the estimation.
A unified Monte-Carlo jackknife method, called McJack, is proposed for
estimating the logarithm of the mean squared prediction error. We prove the
second-order unbiasedness of McJack, and demonstrate the performance of McJack
in assessing uncertainty in SAE after model selection through empirical
investigations that include simulation studies and real-data analyses.
</summary>
    <author>
      <name>Jiming Jiang</name>
    </author>
    <author>
      <name>P. Lahiri</name>
    </author>
    <author>
      <name>Thuan Nguyen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">33 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.05238v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.05238v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="Primary: 62D99, Secondary: 62F40" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.07527v1</id>
    <updated>2016-02-24T14:35:31Z</updated>
    <published>2016-02-24T14:35:31Z</published>
    <title>Differentiation of the Cholesky decomposition</title>
    <summary>  We review strategies for differentiating matrix-based computations, and
derive symbolic and algorithmic update rules for differentiating expressions
containing the Cholesky decomposition. We recommend new `blocked' algorithms,
based on differentiating the Cholesky algorithm DPOTRF in the LAPACK library,
which uses `Level 3' matrix-matrix operations from BLAS, and so is
cache-friendly and easy to parallelize. For large matrices, the resulting
algorithms are the fastest way to compute Cholesky derivatives, and are an
order of magnitude faster than the algorithms in common usage. In some
computing environments, symbolically-derived updates are faster for small
matrices than those based on differentiating Cholesky algorithms. The symbolic
and algorithmic approaches can be combined to get the best of both worlds.
</summary>
    <author>
      <name>Iain Murray</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, including 7 pages of code listings</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.07527v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.07527v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.00069v1</id>
    <updated>2016-02-29T21:58:17Z</updated>
    <published>2016-02-29T21:58:17Z</published>
    <title>Tukey depth: linear programming and applications</title>
    <summary>  Determining the representativeness of a point within a data cloud has
recently become a desirable task in multivariate analysis. The concept of
statistical depth function, which reflects centrality of an arbitrary point,
appears to be useful and has been studied intensively during the last decades.
Here the issue of exact computation of the classical Tukey data depth is
addressed. The paper suggests an algorithm that exploits connection between the
Tukey depth and linear separability and is based on iterative application of
linear programming. The algorithm further develops the idea of the cone
segmentation of the Euclidean space and allows for efficient implementation due
to the special search structure. The presentation is complemented by
relationship to similar concepts and examples of application.
</summary>
    <author>
      <name>Pavlo Mozharovskyi</name>
    </author>
    <link href="http://arxiv.org/abs/1603.00069v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.00069v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.00297v1</id>
    <updated>2016-02-27T09:11:24Z</updated>
    <published>2016-02-27T09:11:24Z</published>
    <title>Bayesian Quantile Regression for Ordinal Longitudinal Data</title>
    <summary>  Since the pioneering work by Koenker and Bassett (1978), quantile regression
models and its applications have become increasingly popular and important for
research in many areas. In this paper, a random effects ordinal quantile
regression model is proposed for analysis of longitudinal data with ordinal
outcome of interest. An efficient Gibbs sampling algorithm was derived for
fitting the model to the data based on a location scale mixture representation
of the skewed double exponential distribution. The proposed approach is
illustrated using simulated data and a real data example. This is the first
work to discuss quantile regression for analysis of longitudinal data with
ordinal outcome.
</summary>
    <author>
      <name>Rahim Alhamzawi</name>
    </author>
    <link href="http://arxiv.org/abs/1603.00297v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.00297v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.00351v1</id>
    <updated>2016-03-01T16:48:56Z</updated>
    <published>2016-03-01T16:48:56Z</published>
    <title>Analyzing Non-proportional Hazards: Use of the MRH Package</title>
    <summary>  In this manuscript we demonstrate the analysis of right-censored survival
outcomes using the MRH package in R. The MRH package implements the
multi-resolution hazard (MRH) model, which is a Polya-tree based, Bayesian
semi-parametric method for flexible estimation of the hazard rate and covariate
effects. The package allows for covariates to be included under the
proportional and non-proportional hazards assumption, and for robust estimation
of the hazard rate in periods of sparsely observed failures via a "pruning"
tool.
</summary>
    <author>
      <name>Yolanda Hagar</name>
    </author>
    <author>
      <name>Vanja Dukic</name>
    </author>
    <link href="http://arxiv.org/abs/1603.00351v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.00351v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.01136v1</id>
    <updated>2016-03-03T15:36:49Z</updated>
    <published>2016-03-03T15:36:49Z</published>
    <title>Multilevel Sequential Monte Carlo Samplers for Normalizing Constants</title>
    <summary>  This article considers the sequential Monte Carlo (SMC) approximation of
ratios of normalizing constants associated to posterior distributions which in
principle rely on continuum models. Therefore, the Monte Carlo estimation error
and the discrete approximation error must be balanced. A multilevel strategy is
utilized to substantially reduce the cost to obtain a given error level in the
approximation as compared to standard estimators. Two estimators are considered
and relative variance bounds are given. The theoretical results are numerically
illustrated for the example of identifying a parametrized permeability in an
elliptic equation given point-wise observations of the pressure.
</summary>
    <author>
      <name>Pierre Del Moral</name>
    </author>
    <author>
      <name>Ajay Jasra</name>
    </author>
    <author>
      <name>Kody Law</name>
    </author>
    <author>
      <name>Yan Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/1603.01136v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.01136v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.08815v1</id>
    <updated>2016-03-29T15:34:29Z</updated>
    <published>2016-03-29T15:34:29Z</published>
    <title>Spectral M-estimation with Applications to Hidden Markov Models</title>
    <summary>  Method of moment estimators exhibit appealing statistical properties, such as
asymptotic unbiasedness, for nonconvex problems. However, they typically
require a large number of samples and are extremely sensitive to model
misspecification. In this paper, we apply the framework of M-estimation to
develop both a generalized method of moments procedure and a principled method
for regularization. Our proposed M-estimator obtains optimal sample efficiency
rates (in the class of moment-based estimators) and the same well-known rates
on prediction accuracy as other spectral estimators. It also makes it
straightforward to incorporate regularization into the sample moment
conditions. We demonstrate empirically the gains in sample efficiency from our
approach on hidden Markov models.
</summary>
    <author>
      <name>Dustin Tran</name>
    </author>
    <author>
      <name>Minjae Kim</name>
    </author>
    <author>
      <name>Finale Doshi-Velez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Artificial Intelligence and Statistics, 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.08815v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.08815v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.09157v1</id>
    <updated>2016-03-30T12:37:27Z</updated>
    <published>2016-03-30T12:37:27Z</published>
    <title>Linear System Identification via EM with Latent Disturbances and
  Lagrangian Relaxation</title>
    <summary>  In the application of the Expectation Maximization algorithm to
identification of dynamical systems, internal states are typically chosen as
latent variables, for simplicity. In this work, we propose a different choice
of latent variables, namely, system disturbances. Such a formulation elegantly
handles the problematic case of singular state space models, and is shown,
under certain circumstances, to improve the fidelity of bounds on the
likelihood, leading to convergence in fewer iterations. To access these
benefits we develop a Lagrangian relaxation of the nonconvex optimization
problems that arise in the latent disturbances formulation, and proceed via
semidefinite programming.
</summary>
    <author>
      <name>Jack Umenberger</name>
    </author>
    <author>
      <name>Johan Wågberg</name>
    </author>
    <author>
      <name>Ian R. Manchester</name>
    </author>
    <author>
      <name>Thomas B. Schön</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.09157v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.09157v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.04582v1</id>
    <updated>2016-04-15T17:41:37Z</updated>
    <published>2016-04-15T17:41:37Z</published>
    <title>Simulations on the combinatorial structure of D-optimal designs</title>
    <summary>  In this work we present the results of several simulations on main-effect
factorial designs. The goal of such simulations is to investigate the
connections between the $D$-optimality of a design and its geometrical
structure. By means of a combinatorial object, namely the circuit basis of the
design matrix, we show that it is possible to define a simple index that
exhibits strong connections with the $D$-optimality.
</summary>
    <author>
      <name>Roberto Fontana</name>
    </author>
    <author>
      <name>Fabio Rapallo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages. Submitted for the Proceedings volume of the 8th IWS -
  International Workshop on Simulation</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.04582v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.04582v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62K15, 62K05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.05658v1</id>
    <updated>2016-04-19T17:28:07Z</updated>
    <published>2016-04-19T17:28:07Z</published>
    <title>Sequential Monte Carlo Smoothing with Parameter Estimation</title>
    <summary>  We propose two new Bayesian smoothing methods for general state-space models
with unknown parameters. The first approach is based on the particle learning
and smoothing algorithm, but with an adjustment in the backward resampling
weights. The second is a new method combining sequential parameter learning and
smoothing algorithms for general state-space models. This method is
straightforward but effective, and we find it is the best existing Sequential
Monte Carlo algorithm to solve the joint Bayesian smoothing problem. We first
illustrate the methods on three benchmark models using simulated data, and then
apply them to a stochastic volatility model for daily S&amp;P 500 index returns
during the financial crisis.
</summary>
    <author>
      <name>Biao Yang</name>
    </author>
    <author>
      <name>Jonathan R. Stroud</name>
    </author>
    <author>
      <name>Gabriel Huerta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 7 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.05658v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.05658v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.06383v3</id>
    <updated>2017-03-10T17:29:55Z</updated>
    <published>2016-04-21T16:52:55Z</published>
    <title>Piecewise Empirical Likelihood</title>
    <summary>  Non-parametric methods avoid the problem of having to specify a particular
data generating mechanism, but can be computationally intensive, reducing their
accessibility for large data problems. Empirical likelihood, a non-parametric
approach to the likelihood function, is also limited in application due to the
computational demands necessary. We propose a new approach that combines
multiple non-parametric likelihood-type components to build a data-driven
approximation of the true function. We will examine the theoretical properties
of this piecewise empirical likelihood and demonstrate the computational gains
of this methodology.
</summary>
    <author>
      <name>Adam Jaeger</name>
    </author>
    <author>
      <name>Nicole Lazar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Significant changes have been made to article, renamed split sample
  empirical likelihood. Replacement article is titled Split Sample Empirical
  Likelihood arXiv:1703.03312</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.06383v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.06383v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07177v1</id>
    <updated>2016-04-25T09:19:22Z</updated>
    <published>2016-04-25T09:19:22Z</published>
    <title>On the Use of Penalty MCMC for Differential Privacy</title>
    <summary>  We view the penalty algorithm of Ceperley and Dewing (1999), a Markov chain
Monte Carlo (MCMC) algorithm for Bayesian inference, in the context of data
privacy. Specifically, we study differential privacy of the penalty algorithm
and advocate its use for data privacy. We show that in the simple model of
independent observations the algorithm has desirable convergence and privacy
properties that scale with data size. Two special cases are also investigated
and privacy preserving schemes are proposed for those cases: (i) Data are
distributed among several data owners who are interested in the inference of a
common parameter while preserving their data privacy. (ii) The data likelihood
belongs to an exponential family.
</summary>
    <author>
      <name>Sinan Yıldırım</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 0 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.07177v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07177v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.05454v2</id>
    <updated>2016-09-27T05:00:34Z</updated>
    <published>2016-05-18T06:36:23Z</published>
    <title>Computing the variance of a conditional expectation via non-nested Monte
  Carlo</title>
    <summary>  Computing the variance of a conditional expectation has often been of
importance in uncertainty quantification. Sun et al. has introduced an unbiased
nested Monte Carlo estimator, which they call $1\frac{1}{2}$-level simulation
since the optimal inner-level sample size is bounded as the computational
budget increases. In this letter we construct unbiased non-nested Monte Carlo
estimators based on the so-called pick-freeze scheme due to Sobol'. An
extension of our approach to compute higher order moments of a conditional
expectation is also discussed.
</summary>
    <author>
      <name>Takashi Goda</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Operations Research Letters, Volume 45, Issue 1, 63-67, 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1605.05454v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.05454v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.06293v2</id>
    <updated>2016-06-01T12:05:41Z</updated>
    <published>2016-05-20T11:25:15Z</published>
    <title>The performance of univariate goodness-of-fit tests for normality based
  on the empirical characteristic function in large samples</title>
    <summary>  An empirical power comparison is made between two tests based on the
empirical characteristic function and some of the best performing tests for
normality. A simple normality test based on the empirical characteristic
function calculated in a single point is shown to outperform the more
complicated Epps-Pulley test and the frequentist tests included in the study in
large samples.
</summary>
    <author>
      <name>J. Martin van Zyl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 figures, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.06293v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.06293v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.06585v1</id>
    <updated>2016-05-21T04:42:44Z</updated>
    <published>2016-05-21T04:42:44Z</published>
    <title>Bayesian Analysis of Modified Weibull distribution under progressively
  censored competing risk model</title>
    <summary>  In this paper we study bayesian analysis of Modified Weibull distribution
under progressively censored competing risk model. This study is made for
progressively censored data. We use deterministic scan Gibbs sampling combined
with slice sampling to generate from the posterior distribution. Posterior
distribution is formed by taking prior distribution as reference prior. A real
life data analysis is shown for illustrative purpose.
</summary>
    <author>
      <name>Arabin Kumar Dey</name>
    </author>
    <author>
      <name>Abhilash Jha</name>
    </author>
    <author>
      <name>Sanku Dey</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 6 figures, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.06585v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.06585v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.00242v1</id>
    <updated>2016-06-01T11:51:49Z</updated>
    <published>2016-06-01T11:51:49Z</published>
    <title>ctsmr - Continuous Time Stochastic Modeling in R</title>
    <summary>  ctsmr is an R package providing a general framework for identifying and
estimating partially observed continuous-discrete time gray-box models. The
estimation is based on maximum likelihood principles and Kalman filtering
efficiently implemented in Fortran. This paper briefly demonstrates how to
construct a Continuous Time Stochastic Model using multivariate time series
data, and how to estimate the embedded parameters. The setup provides a unique
framework for statistical modeling of physical phenomena, and the approach is
often called grey box modeling. Finally three examples are provided to
demonstrate the capabilities of ctsmr.
</summary>
    <author>
      <name>Rune Juhl</name>
    </author>
    <author>
      <name>Jan Kloppenborg Møller</name>
    </author>
    <author>
      <name>Henrik Madsen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, including R code</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.00242v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.00242v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07620v1</id>
    <updated>2016-06-24T09:47:11Z</updated>
    <published>2016-06-24T09:47:11Z</published>
    <title>Computing the Oja Median in R: The Package OjaNP</title>
    <summary>  The Oja median is one of several extensions of the univariate median to the
multivariate case. It has many nice properties, but is computationally
demanding. In this paper, we first review the properties of the Oja median and
compare it to other multivariate medians. Afterwards we discuss four algorithms
to compute the Oja median, which are implemented in our R-package OjaNP.
Besides these algorithms, the package contains also functions to compute Oja
signs, Oja signed ranks, Oja ranks, and the related scatter concepts. To
illustrate their use, the corresponding multivariate one- and $C$-sample
location tests are implemented.
</summary>
    <author>
      <name>Daniel Fischer</name>
    </author>
    <author>
      <name>Karl Mosler</name>
    </author>
    <author>
      <name>Jyrki Möttönen</name>
    </author>
    <author>
      <name>Klaus Nordhausen</name>
    </author>
    <author>
      <name>Oleksii Pokotylo</name>
    </author>
    <author>
      <name>Daniel Vogel</name>
    </author>
    <link href="http://arxiv.org/abs/1606.07620v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07620v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.08160v1</id>
    <updated>2016-06-27T08:20:46Z</updated>
    <published>2016-06-27T08:20:46Z</published>
    <title>Geometric ergodicity of Rao and Teh's algorithm for Markov jump
  processes and CTBNs</title>
    <summary>  Rao and Teh (2012, 2013) introduced an efficient MCMC algorithm for sampling
from the posterior distribution of a hidden Markov jump process. The algorithm
is based on the idea of sampling virtual jumps. In the present paper we show
that the Markov chain generated by Rao and Teh's algorithm is geometrically
ergodic. To this end we establish a geometric drift condition towards a small
set. A similar result is also proved for a special version of the algorithm,
used for probabilistic inference in Continuous Time Bayesian Networks.
</summary>
    <author>
      <name>Błażej Miasojedow</name>
    </author>
    <author>
      <name>Wojcieh Niemiro</name>
    </author>
    <link href="http://arxiv.org/abs/1606.08160v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.08160v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.08350v2</id>
    <updated>2017-02-28T09:54:15Z</updated>
    <published>2016-06-27T16:24:03Z</published>
    <title>An Efficient Implementation of the Generalized Labeled Multi-Bernoulli
  Filter</title>
    <summary>  This paper proposes an efficient implementation of the generalized labeled
multi-Bernoulli (GLMB) filter by combining the prediction and update into a
single step. In contrast to an earlier implementation that involves separate
truncations in the prediction and update steps, the proposed implementation
requires only one truncation procedure for each iteration. Furthermore, we
propose an efficient algorithm for truncating the GLMB filtering density based
on Gibbs sampling. The resulting implementation has a linear complexity in the
number of measurements and quadratic in the number of hypothesized objects.
</summary>
    <author>
      <name>Ba Ngu Vo</name>
    </author>
    <author>
      <name>Ba Tuong Vo</name>
    </author>
    <author>
      <name>Hung Gia Hoang</name>
    </author>
    <link href="http://arxiv.org/abs/1606.08350v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.08350v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.08373v2</id>
    <updated>2016-07-23T19:03:41Z</updated>
    <published>2016-06-27T17:19:28Z</published>
    <title>Which ergodic averages have finite asymptotic variance?</title>
    <summary>  We show that the class of $L^2$ functions for which ergodic averages of a
reversible Markov chain have finite asymptotic variance is determined by the
class of $L^2$ functions for which ergodic averages of its associated jump
chain have finite asymptotic variance. This allows us to characterize
completely which ergodic averages have finite asymptotic variance when the
Markov chain is an independence sampler. In addition, we obtain a simple
sufficient condition for all ergodic averages of $L^2$ functions of the primary
variable in a pseudo-marginal Markov chain to have finite asymptotic variance.
</summary>
    <author>
      <name>George Deligiannidis</name>
    </author>
    <author>
      <name>Anthony Lee</name>
    </author>
    <link href="http://arxiv.org/abs/1606.08373v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.08373v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.01664v1</id>
    <updated>2016-07-06T15:16:44Z</updated>
    <published>2016-07-06T15:16:44Z</published>
    <title>Personalized Optimization for Computer Experiments with Environmental
  Inputs</title>
    <summary>  Optimization problems with both control variables and environmental variables
arise in many fields. This paper introduces a framework of personalized
optimization to han- dle such problems. Unlike traditional robust optimization,
personalized optimization devotes to finding a series of optimal control
variables for different values of environmental variables. Therefore, the
solution from personalized optimization consists of optimal surfaces defined on
the domain of the environmental variables. When the environmental variables can
be observed or measured, personalized optimization yields more reasonable and
better solution- s than robust optimization. The implementation of personalized
optimization for complex computer models is discussed. Based on statistical
modeling of computer experiments, we provide two algorithms to sequentially
design input values for approximating the optimal surfaces. Numerical examples
show the effectiveness of our algorithms.
</summary>
    <author>
      <name>Shifeng Xiong</name>
    </author>
    <link href="http://arxiv.org/abs/1607.01664v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.01664v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.04751v2</id>
    <updated>2017-02-17T23:19:32Z</updated>
    <published>2016-07-16T15:36:04Z</published>
    <title>Fast Simulation of Hyperplane-Truncated Multivariate Normal
  Distributions</title>
    <summary>  We introduce a fast and easy-to-implement simulation algorithm for a
multivariate normal distribution truncated on the intersection of a set of
hyperplanes, and further generalize it to efficiently simulate random variables
from a multivariate normal distribution whose covariance (precision) matrix can
be decomposed as a positive-definite matrix minus (plus) a low-rank symmetric
matrix. Example results illustrate the correctness and efficiency of the
proposed simulation algorithms.
</summary>
    <author>
      <name>Yulai Cong</name>
    </author>
    <author>
      <name>Bo Chen</name>
    </author>
    <author>
      <name>Mingyuan Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Bayesian Analysis</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.04751v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.04751v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.02199v3</id>
    <updated>2017-06-22T13:05:11Z</updated>
    <published>2016-08-07T09:39:13Z</published>
    <title>An EM algorithm for absolutely continuous Marshall-Olkin bivariate
  Pareto distribution with location and scale</title>
    <summary>  Recently \cite{AsimitFurmanVernic:2016} used EM algorithm to estimate
singular Marshall-Olkin bivariate Pareto distribution. We describe
\textbf{absolutely continuous} version of this distribution. We study
estimation of the parameters by EM algorithm both in presence and without
presence of location and scale parameters. Some innovative solutions are
provided for different problems arised during implementation of EM algorithm. A
real-life data analysis is also shown for illustrative purpose.
</summary>
    <author>
      <name>Arabin Kumar Dey</name>
    </author>
    <author>
      <name>Debasis Kundu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.02199v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.02199v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.05338v1</id>
    <updated>2016-08-18T17:19:49Z</updated>
    <published>2016-08-18T17:19:49Z</published>
    <title>The multi-level Monte Carlo method for simulations of turbulent flows</title>
    <summary>  In this paper the application of the multi-level Monte Carlo (MLMC) method on
numerical simulations of turbulent flows with uncertain parameters is
investigated. Several strategies for setting up the MLMC method are presented,
and the advantages and disadvantages of each strategy are also discussed. A
numerical experiment is carried out using the Antarctic Circumpolar Current
(ACC) with uncertain, small-scale bottom topographic features. It is
demonstrated that, unlike the pointwise solutions, the averaged volume
transports are correlated across grid resolutions, and the MLMC method could
increase simulation efficiency without losing accuracy in uncertainty
assessment.
</summary>
    <author>
      <name>Qingsha Chen</name>
    </author>
    <author>
      <name>Ju Ming</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.05338v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.05338v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.06189v1</id>
    <updated>2016-08-22T15:04:04Z</updated>
    <published>2016-08-22T15:04:04Z</published>
    <title>On penalized maximum likelihood estimation of approximate factor models</title>
    <summary>  In this paper, we mainly focus on the estimation of high-dimensional
approximate factor model. We rewrite the estimation of error covariance matrix
as a new form which shares similar properties as the penalized maximum
likelihood covariance estimator given by Bien and Tibshirani(2011). Based on
the lagrangian duality, we propose an APG algorithm to give a positive definite
estimate of the error covariance matrix. The new algorithm for the estimation
of approximate factor model has a desirable non-increasing property. By keeping
the error covariance matrix to be positive definite, the efficiency of the new
algorithm on estimation and forecasting is investigated via extensive
simulations and real data analysis.
</summary>
    <author>
      <name>Shaoxin Wang</name>
    </author>
    <author>
      <name>Hu Yang</name>
    </author>
    <author>
      <name>Chaoli Yao</name>
    </author>
    <link href="http://arxiv.org/abs/1608.06189v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.06189v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.02354v1</id>
    <updated>2016-09-08T09:40:44Z</updated>
    <published>2016-09-08T09:40:44Z</published>
    <title>Generalized Autoregressive Score Models in R: The GAS Package</title>
    <summary>  This paper presents the R package GAS for the analysis of time series under
the Generalized Autoregressive Score (GAS) framework of Creal et al. (2013) and
Harvey (2013). The distinctive feature of the GAS approach is the use of the
score function as the driver of time-variation in the parameters of nonlinear
models. The GAS package provides functions to simulate univariate and
multivariate GAS processes, estimate the GAS parameters and to make time series
forecasts. We illustrate the use of the GAS package with a detailed case study
on estimating the time-varying conditional densities of a set of financial
assets.
</summary>
    <author>
      <name>David Ardia</name>
    </author>
    <author>
      <name>Kris Boudt</name>
    </author>
    <author>
      <name>Leopoldo Catania</name>
    </author>
    <link href="http://arxiv.org/abs/1609.02354v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.02354v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.02106v1</id>
    <updated>2016-10-07T00:39:57Z</updated>
    <published>2016-10-07T00:39:57Z</published>
    <title>Numerical approximation of the Frobenius-Perron operator using the
  finite volume method</title>
    <summary>  We develop a finite-dimensional approximation of the Frobenius-Perron
operator using the finite volume method applied to the continuity equation for
the evolution of probability. A Courant-Friedrichs-Lewy condition ensures that
the approximation satisfies the Markov property, while existing convergence
theory for the finite volume method guarantees convergence of the discrete
operator to the continuous operator as mesh size tends to zero. Properties of
the approximation are demonstrated in a computed example of sequential
inference for the state of a low-dimensional mechanical system when
observations give rise to multi-modal distributions.
</summary>
    <author>
      <name>Richard A. Norton</name>
    </author>
    <author>
      <name>Colin Fox</name>
    </author>
    <author>
      <name>Malcolm E. Morrison</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.02106v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.02106v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65J22, 65M08 (primary), 62L12, 65C40, 65C60 (secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.02588v1</id>
    <updated>2016-10-08T22:06:58Z</updated>
    <published>2016-10-08T22:06:58Z</published>
    <title>Iterative proportional scaling revisited: a modern optimization
  perspective on big count data</title>
    <summary>  We revisit the classic iterative proportional scaling (IPS) for contingency
table analysis, from a modern optimization perspective. In contrast to the
criticisms made in the literature, we show that based on a coordinate descent
characterization, IPS can be slightly modified to deliver coefficient
estimates, and from a majorization-minimization standpoint, IPS can be extended
to handle log-affine models with general designs. The optimization techniques
help accelerate IPS to provide highly salable algorithms for big count data
applications, and can adapt IPS to shrinkage estimation to deal with a large
number of variables.
</summary>
    <author>
      <name>Yiyuan She</name>
    </author>
    <author>
      <name>Shao Tang</name>
    </author>
    <link href="http://arxiv.org/abs/1610.02588v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.02588v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.03182v1</id>
    <updated>2016-10-11T04:34:13Z</updated>
    <published>2016-10-11T04:34:13Z</published>
    <title>wtest: an R Package for Testing Main and Interaction Effect in Genotype
  Data with Binary Traits</title>
    <summary>  This R package evaluates main and pair-wise interaction effect of single
nucleotide polymorphisms (SNPs) via the W-test, scalable to whole genome-wide
data sets. The package provides fast and accurate p-value estimation of genetic
markers, as well as diagnostic checking on the probability distributions. It
allows flexible stage-wise or exhaustive association testing in a user-friendly
interface. Availability: The package is available in CRAN, or from website:
http://www2.ccrb.cuhk.edu.hk/wtest
</summary>
    <author>
      <name>Rui Sun</name>
    </author>
    <author>
      <name>Billy Chang</name>
    </author>
    <author>
      <name>Benny Chung-Ying Zee</name>
    </author>
    <author>
      <name>Maggie Haitian Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.03182v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.03182v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.03939v1</id>
    <updated>2016-10-13T04:58:38Z</updated>
    <published>2016-10-13T04:58:38Z</published>
    <title>Continuous-Time, Discrete-Event Simulation from Counting Processes</title>
    <summary>  This is a method for discrete event simulation specified by survival
analysis. It presents a sequence of steps. First, hazard rates from survival
analysis specify the rates of a set of counting processes. Second, those
counting processes define a transition kernel. Third, there are four different
ways to sample that transition kernel, including a first-principles derivation
of exact stochastic simulation algorithms (SSA) in continuous time. This
simulation allows time-dependent intensities which include both continuous and
atomic components. Separating the steps involved makes a clear correspondence
between mathematical formulation and algorithmic implementation.
</summary>
    <author>
      <name>Andrew J. Dolgert</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.03939v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.03939v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.6.1; I.6.8; G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.07894v1</id>
    <updated>2016-10-25T14:32:40Z</updated>
    <published>2016-10-25T14:32:40Z</published>
    <title>Counterfactual: An R Package for Counterfactual Analysis</title>
    <summary>  The Counterfactual package implements the estimation and inference methods of
Chernozhukov, Fern\'andez-Val and Melly (2013) for counterfactual analysis. The
counterfactual distributions considered are the result of changing either the
marginal distribution of covariates related to the outcome variable of
interest, or the conditional distribution of the outcome given the covariates.
They can be applied to estimate quantile treatment effects and wage
decompositions. This paper serves as an introduction to the package and
displays basic functionality of the commands contained within.
</summary>
    <author>
      <name>Mingli Chen</name>
    </author>
    <author>
      <name>Victor Chernozhukov</name>
    </author>
    <author>
      <name>Iván Fernández-Val</name>
    </author>
    <author>
      <name>Blaise Melly</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.07894v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.07894v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.08329v1</id>
    <updated>2016-10-26T13:48:39Z</updated>
    <published>2016-10-26T13:48:39Z</published>
    <title>quantreg.nonpar: An R Package for Performing Nonparametric Series
  Quantile Regression</title>
    <summary>  The R package quantreg.nonpar implements nonparametric quantile regression
methods to estimate and make inference on partially linear quantile models.
quantreg.nonpar obtains point estimates of the conditional quantile function
and its derivatives based on series approximations to the nonparametric part of
the model. It also provides pointwise and uniform confidence intervals over a
region of covariate values and/or quantile indices for the same functions using
analytical and resampling methods. This paper serves as an introduction to the
package and displays basic functionality of the functions contained within.
</summary>
    <author>
      <name>Michael Lipsitz</name>
    </author>
    <author>
      <name>Alexandre Belloni</name>
    </author>
    <author>
      <name>Victor Chernozhukov</name>
    </author>
    <author>
      <name>Iván Fernández-Val</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.08329v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.08329v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.08865v1</id>
    <updated>2016-10-19T04:39:53Z</updated>
    <published>2016-10-19T04:39:53Z</published>
    <title>Hit-and-Run for Sampling and Planning in Non-Convex Spaces</title>
    <summary>  We propose the Hit-and-Run algorithm for planning and sampling problems in
non-convex spaces. For sampling, we show the first analysis of the Hit-and-Run
algorithm in non-convex spaces and show that it mixes fast as long as certain
smoothness conditions are satisfied. In particular, our analysis reveals an
intriguing connection between fast mixing and the existence of smooth
measure-preserving mappings from a convex space to the non-convex space. For
planning, we show advantages of Hit-and-Run compared to state-of-the-art
planning methods such as Rapidly-Exploring Random Trees.
</summary>
    <author>
      <name>Yasin Abbasi-Yadkori</name>
    </author>
    <author>
      <name>Peter L. Bartlett</name>
    </author>
    <author>
      <name>Victor Gabillon</name>
    </author>
    <author>
      <name>Alan Malek</name>
    </author>
    <link href="http://arxiv.org/abs/1610.08865v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.08865v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.02754v1</id>
    <updated>2016-11-08T22:40:21Z</updated>
    <published>2016-11-08T22:40:21Z</published>
    <title>Reduced-dimensionality Legendre Chaos expansions via basis adaptation on
  1d active subspaces</title>
    <summary>  The recently introduced basis adaptation method for Homogeneous (Wiener)
Chaos expansions is explored in a new context where the rotation/projection
matrices are computed by discovering the active subspace where the random input
exhibits most of its variability. In the case of 1-dimensional active
subspaces, the methodology can be applicable to generalized Polynomial Chaos
expansions, thus enabling the efficient computation of the chaos coefficients
in expansions with arbitrary input distribution. Besides the significant
computational savings, additional attractive features such as high accuracy in
computing statistics of interest are also demonstrated.
</summary>
    <author>
      <name>Panagiotis A. Tsilifis</name>
    </author>
    <link href="http://arxiv.org/abs/1611.02754v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.02754v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.03177v1</id>
    <updated>2016-11-10T03:45:34Z</updated>
    <published>2016-11-10T03:45:34Z</published>
    <title>A Note on Random Walks with Absorbing barriers and Sequential Monte
  Carlo Methods</title>
    <summary>  In this article we consider importance sampling (IS) and sequential Monte
Carlo (SMC) methods in the context of 1-dimensional random walks with absorbing
barriers. In particular, we develop a very precise variance analysis for
several IS and SMC procedures. We take advantage of some explicit spectral
formulae available for these models to derive sharp and explicit estimates;
this provides stability properties of the associated normalized Feynman-Kac
semigroups. Our analysis allows one to compare the variance of SMC and IS
techniques for these models. The work in this article, is one of the few to
consider an in-depth analysis of an SMC method for a particular model-type as
well as variance comparison of SMC algorithms.
</summary>
    <author>
      <name>Pierre Del Moral</name>
    </author>
    <author>
      <name>Ajay Jasra</name>
    </author>
    <link href="http://arxiv.org/abs/1611.03177v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.03177v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.04416v1</id>
    <updated>2016-11-14T15:21:23Z</updated>
    <published>2016-11-14T15:21:23Z</published>
    <title>On numerical approximation schemes for expectation propagation</title>
    <summary>  Several numerical approximation strategies for the expectation-propagation
algorithm are studied in the context of large-scale learning: the Laplace
method, a faster variant of it, Gaussian quadrature, and a deterministic
version of variational sampling (i.e., combining quadrature with variational
approximation). Experiments in training linear binary classifiers show that the
expectation-propagation algorithm converges best using variational sampling,
while it also converges well using Laplace-style methods with smooth factors
but tends to be unstable with non-differentiable ones. Gaussian quadrature
yields unstable behavior or convergence to a sub-optimal solution in most
experiments.
</summary>
    <author>
      <name>Alexis Roche</name>
    </author>
    <link href="http://arxiv.org/abs/1611.04416v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.04416v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.07505v2</id>
    <updated>2016-12-16T14:59:14Z</updated>
    <published>2016-11-22T20:37:58Z</published>
    <title>Fitting log-linear models in sparse contingency tables using the
  eMLEloglin R package</title>
    <summary>  Log-linear modeling is a popular method for the analysis of contingency table
data. When the table is sparse, and the data falls on a proper face $F$ of the
convex support, there are consequences on model inference and model selection.
Knowledge of the cells determining $F$ is crucial to mitigating these effects.
We introduce the R package (R Core Team (2016)) eMLEloglin for determining $F$
and passing that information on to the glm package to fit the model properly.
</summary>
    <author>
      <name>Matthew Friedlander</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.07505v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.07505v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62H17 (Primary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.07537v1</id>
    <updated>2016-11-22T21:08:28Z</updated>
    <published>2016-11-22T21:08:28Z</published>
    <title>Analyzing Genome-wide Association Study Data with the R Package genMOSS</title>
    <summary>  The R package (R Core Team (2016)) genMOSS is specifically designed for the
Bayesian analysis of genome-wide association study data. The package implements
the mode oriented stochastic search (MOSS) procedure as well as a simple moving
window approach to identify combinations of single nucleotide polymorphisms
associated with a response. The prior used in Bayesian computations is the
generalized hyper Dirichlet.
</summary>
    <author>
      <name>Matthew Friedlander</name>
    </author>
    <author>
      <name>Adrian Dobra</name>
    </author>
    <author>
      <name>Helene Massam</name>
    </author>
    <author>
      <name>Laurent Briollais</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.07537v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.07537v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62-07 (Primary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.01872v2</id>
    <updated>2017-01-17T10:57:34Z</updated>
    <published>2016-12-06T15:40:13Z</published>
    <title>Simulation from quasi-stationary distributions on reducible state spaces</title>
    <summary>  Quasi-stationary distributions (QSDs)arise from stochastic processes that
exhibit transient equilibrium behaviour on the way to absorption QSDs are often
mathematically intractable and even drawing samples from them is not
straightforward. In this paper the framework of Sequential Monte Carlo samplers
is utilized to simulate QSDs and several novel resampling techniques are
proposed to accommodate models with reducible state spaces, with particular
focus on preserving particle diversity on discrete spaces. Finally an approach
is considered to estimate eigenvalues associated with QSDs, such as the decay
parameter.
</summary>
    <author>
      <name>Adam Griffin</name>
    </author>
    <author>
      <name>Paul A. Jenkins</name>
    </author>
    <author>
      <name>Gareth O. Roberts</name>
    </author>
    <author>
      <name>Simon E. F. Spencer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 9 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.01872v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.01872v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60J27, 62G09" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.01907v2</id>
    <updated>2017-03-24T14:47:30Z</updated>
    <published>2016-12-06T17:00:28Z</published>
    <title>KFAS: Exponential Family State Space Models in R</title>
    <summary>  State space modelling is an efficient and flexible method for statistical
inference of a broad class of time series and other data. This paper describes
an R package KFAS for state space modelling with the observations from an
exponential family, namely Gaussian, Poisson, binomial, negative binomial and
gamma distributions. After introducing the basic theory behind Gaussian and
non-Gaussian state space models, an illustrative example of Poisson time series
forecasting is provided. Finally, a comparison to alternative R packages
suitable for non-Gaussian time series modelling is presented.
</summary>
    <author>
      <name>Jouni Helske</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">39 pages, 7 figures. This is a preprint version of an article to
  appear in the Journal of Statistical Software. Change to previous version:
  Added grant number to acknowledgments</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.01907v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.01907v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.01930v2</id>
    <updated>2017-06-15T12:16:29Z</updated>
    <published>2016-12-06T18:04:09Z</published>
    <title>Nonparametric Bayesian label prediction on a graph</title>
    <summary>  An implementation of a nonparametric Bayesian approach to solving binary
classification problems on graphs is described. A hierarchical Bayesian
approach with a randomly scaled Gaussian prior is considered. The prior uses
the graph Laplacian to take into account the underlying geometry of the graph.
A method based on a theoretically optimal prior and a more flexible variant
using partial conjugacy are proposed. Two simulated data examples and two
examples using real data are used in order to illustrate the proposed methods.
</summary>
    <author>
      <name>Jarno Hartog</name>
    </author>
    <author>
      <name>Harry van Zanten</name>
    </author>
    <link href="http://arxiv.org/abs/1612.01930v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.01930v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.02357v1</id>
    <updated>2016-12-07T18:19:31Z</updated>
    <published>2016-12-07T18:19:31Z</published>
    <title>Methods and Tools for Bayesian Variable Selection and Model Averaging in
  Univariate Linear Regression</title>
    <summary>  In this paper we briefly review the main methodological aspects concerned
with the application of the Bayesian approach to model choice and model
averaging in the context of variable selection in regression models. This
includes prior elicitation, summaries of the posterior distribution and
computational strategies. We then examine and compare various publicly
available {\tt R}-packages for its practical implementation summarizing and
explaining the differences between packages and giving recommendations for
applied users. We find that all packages reviewed lead to very similar results,
but there are potentially important differences in flexibility and efficiency
of the packages.
</summary>
    <author>
      <name>Anabel Forte</name>
    </author>
    <author>
      <name>Gonzalo Garcia-Donato</name>
    </author>
    <author>
      <name>Mark Steel</name>
    </author>
    <link href="http://arxiv.org/abs/1612.02357v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.02357v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.04101v2</id>
    <updated>2017-08-14T10:16:15Z</updated>
    <published>2016-12-13T11:20:58Z</published>
    <title>Calculating probabilistic excursion sets and related quantities using
  excursions</title>
    <summary>  The R software package excursions contains methods for calculating
probabilistic excursion sets, contour credible regions, and simultaneous
confidence bands for latent Gaussian stochastic processes and fields. It also
contains methods for uncertainty quantification of contour maps and computation
of Gaussian integrals. This article describes the theoretical and computational
methods used in the package. The main functions of the package are introduced
and two examples illustrate how the package can be used.
</summary>
    <author>
      <name>David Bolin</name>
    </author>
    <author>
      <name>Finn Lindgren</name>
    </author>
    <link href="http://arxiv.org/abs/1612.04101v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.04101v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.05501v1</id>
    <updated>2016-12-16T15:10:12Z</updated>
    <published>2016-12-16T15:10:12Z</published>
    <title>The Bayesian analysis of contingency table data using the bayesloglin R
  package</title>
    <summary>  For log-linear analysis, the hyper Dirichlet conjugate prior is available to
work in the Bayesian paradigm. With this prior, the MC3 algorithm allows for
exploration of the space of models to try to find those with the highest
posterior probability. Once top models have been identified, a block Gibbs
sampler can be constructed to sample from the posterior distribution and to
estimate parameters of interest. Our aim in this paper, is to introduce the
bayesloglin R package \citep{R} which contains functions to carry out these
tasks.
</summary>
    <author>
      <name>Matthew Friedlander</name>
    </author>
    <link href="http://arxiv.org/abs/1612.05501v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.05501v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.09162v1</id>
    <updated>2016-12-29T14:58:55Z</updated>
    <published>2016-12-29T14:58:55Z</published>
    <title>High-dimensional Filtering using Nested Sequential Monte Carlo</title>
    <summary>  Sequential Monte Carlo (SMC) methods comprise one of the most successful
approaches to approximate Bayesian filtering. However, SMC without good
proposal distributions struggle in high dimensions. We propose nested
sequential Monte Carlo (NSMC), a methodology that generalises the SMC framework
by requiring only approximate, properly weighted, samples from the SMC proposal
distribution, while still resulting in a correct SMC algorithm. This way we can
exactly approximate the locally optimal proposal, and extend the class of
models for which we can perform efficient inference using SMC. We show improved
accuracy over other state-of-the-art methods on several spatio-temporal state
space models.
</summary>
    <author>
      <name>Christian A. Naesseth</name>
    </author>
    <author>
      <name>Fredrik Lindsten</name>
    </author>
    <author>
      <name>Thomas B. Schön</name>
    </author>
    <link href="http://arxiv.org/abs/1612.09162v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.09162v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.02201v3</id>
    <updated>2017-02-10T16:37:51Z</updated>
    <published>2017-01-09T14:55:12Z</published>
    <title>A fast algorithm for detecting maximal number of matched pairs under a
  given caliper</title>
    <summary>  We present a new algorithm which detects the maximal number of matched
disjoint pairs satisfying a given caliper when the matching is done with
respect to a scalar index (e.g., propensity score), and constructs a
corresponding matching. If each of the groups is ordered with respect to the
index then the number of operations needed is $O(N)$, where $N$ is the total
number of objects to be matched. The case of 1-to-$n$ matching is also
considered.
  Keywords: propensity score matching, matching with caliper
</summary>
    <author>
      <name>Pavel S. Ruzankin</name>
    </author>
    <link href="http://arxiv.org/abs/1701.02201v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.02201v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62P10" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3; G.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.03267v1</id>
    <updated>2017-01-12T08:44:14Z</updated>
    <published>2017-01-12T08:44:14Z</published>
    <title>Robust clustering for functional data based on trimming and constraints</title>
    <summary>  Many clustering algorithms when the data are curves or functions have been
recently proposed. However, the presence of contamination in the sample of
curves can influence the performance of most of them. In this work we propose a
robust, model-based clustering method based on an approximation to the "density
function" for functional data. The robustness results from the joint
application of trimming, for reducing the effect of contaminated observations,
and constraints on the variances, for avoiding spurious clusters in the
solution. The proposed method has been evaluated through a simulation study.
Finally, an application to a real data problem is given.
</summary>
    <author>
      <name>Diego Rivera-García</name>
    </author>
    <author>
      <name>Luis Angel García-Escudero</name>
    </author>
    <author>
      <name>Agustín Mayo-Iscar</name>
    </author>
    <author>
      <name>Joaquın Ortega</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 6 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.03267v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.03267v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.05609v1</id>
    <updated>2017-01-19T21:31:39Z</updated>
    <published>2017-01-19T21:31:39Z</published>
    <title>Confidence Intervals for Finite Difference Solutions</title>
    <summary>  Although applications of Bayesian analysis for numerical quadrature problems
have been considered before, it's only very recently that statisticians have
focused on the connections between statistics and numerical analysis of
differential equations. In line with this very recent trend, we show how
certain commonly used finite difference schemes for numerical solutions of
ordinary and partial differential equations can be considered in a regression
setting. Focusing on this regression framework, we apply a simple Bayesian
strategy to obtain confidence intervals for the finite difference solutions. We
apply this framework on several examples to show how the confidence intervals
are related to truncation error and illustrate the utility of the confidence
intervals for the examples considered.
</summary>
    <author>
      <name>Majnu John</name>
    </author>
    <author>
      <name>Yihren Wu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1080/03610918.2017.1335409</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1080/03610918.2017.1335409" rel="related"/>
    <link href="http://arxiv.org/abs/1701.05609v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.05609v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.00428v2</id>
    <updated>2017-02-27T17:17:15Z</updated>
    <published>2017-02-01T19:31:38Z</published>
    <title>Malliavin-based Multilevel Monte Carlo Estimators for Densities of
  Max-stable Processes</title>
    <summary>  We introduce a class of unbiased Monte Carlo estimators for the multivariate
density of max-stable fields generated by Gaussian processes. Our estimators
take advantage of recent results on exact simulation of max-stable fields
combined with identities studied in the Malliavin calculus literature and ideas
developed in the multilevel Monte Carlo literature. Our approach allows
estimating multivariate densities of max-stable fields with precision
$\varepsilon $ at a computational cost of order $O\left( \varepsilon ^{-2}\log
\log \log \left( 1/\varepsilon \right) \right) $.
</summary>
    <author>
      <name>Jose Blanchet</name>
    </author>
    <author>
      <name>Zhipeng Liu</name>
    </author>
    <link href="http://arxiv.org/abs/1702.00428v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.00428v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.02707v2</id>
    <updated>2017-02-14T04:22:00Z</updated>
    <published>2017-02-09T05:09:34Z</published>
    <title>A Fast Algorithm for the Coordinate-wise Minimum Distance Estimation</title>
    <summary>  Application of the minimum distance method to the linear regression model for
estimating regression parameters is a difficult and time-consuming process due
to the complexity of its distance function, and hence, it is computationally
expensive. To deal with the computational cost, this paper proposes a fast
algorithm which mainly uses technique of coordinate-wise minimization in order
to estimate the regression parameters. R package based on the proposed
algorithm and written in Rcpp is available online.
</summary>
    <author>
      <name>Jiwoong Kim</name>
    </author>
    <link href="http://arxiv.org/abs/1702.02707v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.02707v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.04391v1</id>
    <updated>2017-02-14T21:20:23Z</updated>
    <published>2017-02-14T21:20:23Z</published>
    <title>Bootstrap-based inferential improvements in beta autoregressive moving
  average model</title>
    <summary>  We consider the issue of performing accurate small sample inference in beta
autoregressive moving average model, which is useful for modeling and
forecasting continuous variables that assumes values in the interval $(0,1)$.
The inferences based on conditional maximum likelihood estimation have good
asymptotic properties, but their performances in small samples may be poor.
This way, we propose bootstrap bias corrections of the point estimators and
different bootstrap strategies for confidence interval improvements. Our Monte
Carlo simulations show that finite sample inference based on bootstrap
corrections is much more reliable than the usual inferences. We also presented
an empirical application.
</summary>
    <author>
      <name>Bruna Gregory Palm</name>
    </author>
    <author>
      <name>Fábio M. Bayer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted paper</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Communications in Statistics - Simulation and Computation, 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1702.04391v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.04391v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.08849v1</id>
    <updated>2017-02-28T16:31:21Z</updated>
    <published>2017-02-28T16:31:21Z</published>
    <title>Multi-Sensor Multi-object Tracking with the Generalized Labeled
  Multi-Bernoulli Filter</title>
    <summary>  This paper proposes an efficient implementation of the multi-sensor
generalized labeled multi-Bernoulli (GLMB) filter. The solution exploits the
GLMB joint prediction and update together with a new technique for truncating
the GLMB filtering density based on Gibbs sampling. The resulting algorithm has
quadratic complexity in the number of hypothesized object and linear in the
number of measurements of each individual sensors.
</summary>
    <author>
      <name>Ba Ngu Vo</name>
    </author>
    <author>
      <name>Ba Tuong Vo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1606.08350</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.08849v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.08849v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.03004v2</id>
    <updated>2017-03-10T21:00:26Z</updated>
    <published>2017-03-08T19:36:24Z</published>
    <title>New approximation for GARCH parameters estimate</title>
    <summary>  This paper presents a new approach for the optimization of GARCH parameters
estimation. Firstly, we propose a method for the localization of the maximum.
Thereafter, using the methods of least squares, we make a local approximation
for the projection of the likelihood function curve on two dimensional planes
by a polynomial of order two which will be used to calculate an estimation of
the maximum.
</summary>
    <author>
      <name>Yakoub Boularouk</name>
    </author>
    <author>
      <name>Nasr-eddine Hamri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 tables, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.03004v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.03004v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.07039v1</id>
    <updated>2017-03-21T03:27:22Z</updated>
    <published>2017-03-21T03:27:22Z</published>
    <title>A Simple Online Parameter Estimation Technique with Asymptotic
  Guarantees</title>
    <summary>  In many modern settings, data are acquired iteratively over time, rather than
all at once. Such settings are known as online, as opposed to offline or batch.
We introduce a simple technique for online parameter estimation, which can
operate in low memory settings, settings where data are correlated, and only
requires a single inspection of the available data at each time period. We show
that the estimators---constructed via the technique---are asymptotically normal
under generous assumptions, and present a technique for the online computation
of the covariance matrices for such estimators. A set of numerical studies
demonstrates that our estimators can be as efficient as their offline
counterparts, and that our technique generates estimates and confidence
intervals that match their offline counterparts in various parameter estimation
settings.
</summary>
    <author>
      <name>Hien D Nguyen</name>
    </author>
    <link href="http://arxiv.org/abs/1703.07039v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.07039v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.09658v2</id>
    <updated>2017-04-06T23:41:47Z</updated>
    <published>2017-03-28T16:24:42Z</published>
    <title>An orthogonal basis expansion method for solving path-independent
  stochastic differential equations</title>
    <summary>  In this article, we present an orthogonal basis expansion method for solving
stochastic differential equations with a path-independent solution of the form
$X_{t}=\phi(t,W_{t})$. For this purpose, we define a Hilbert space and
construct an orthogonal basis for this inner product space with the aid of
2D-Hermite polynomials. With considering $X_{t}$ as orthogonal basis expansion,
this method is implemented and the expansion coefficients are obtained by
solving a system of nonlinear integro-differential equations. The strength of
such a method is that expectation and variance of the solution is computed by
these coefficients directly. Eventually, numerical results demonstrate its
validity and efficiency in comparison with other numerical methods.
</summary>
    <author>
      <name>Rahman Farnoosh</name>
    </author>
    <author>
      <name>Amirhossein Sobhani</name>
    </author>
    <author>
      <name>Hamidreza Rezazadeh</name>
    </author>
    <link href="http://arxiv.org/abs/1703.09658v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.09658v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.09930v3</id>
    <updated>2017-08-08T02:09:04Z</updated>
    <published>2017-03-29T08:32:21Z</published>
    <title>Adaptive Gaussian process approximation for Bayesian inference with
  expensive likelihood functions</title>
    <summary>  We consider Bayesian inference problems with computationally intensive
likelihood functions. We propose a Gaussian process (GP) based method to
approximate the joint distribution of the unknown parameters and the data. In
particular, we write the joint density approximately as a product of an
approximate posterior density and an exponentiated GP surrogate. We then
provide an adaptive algorithm to construct such an approximation, where an
active learning method is used to choose the design points. With numerical
examples, we illustrate that the proposed method has competitive performance
against existing approaches for Bayesian computation.
</summary>
    <author>
      <name>Hongqiao Wang</name>
    </author>
    <author>
      <name>Jinglai Li</name>
    </author>
    <link href="http://arxiv.org/abs/1703.09930v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.09930v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.03239v1</id>
    <updated>2017-04-11T11:14:41Z</updated>
    <published>2017-04-11T11:14:41Z</published>
    <title>Sparse Bayesian vector autoregressions in huge dimensions</title>
    <summary>  We develop a Bayesian vector autoregressive (VAR) model that is capable of
handling vast dimensional information sets. Three features are introduced to
permit reliable estimation of the model. First, we assume that the reduced-form
errors in the VAR feature a factor stochastic volatility structure, allowing
for conditional equation-by-equation estimation. Second, we apply a
Dirichlet-Laplace prior to the VAR coefficients to cure the curse of
dimensionality. Finally, since simulation-based methods are needed to simulate
from the joint posterior distribution, we utilize recent innovations to
efficiently sample from high-dimensional multivariate Gaussian distributions
that improve upon recent algorithms by large margins. In the empirical exercise
we apply the model to US data and evaluate its forecasting capabilities.
</summary>
    <author>
      <name>Gregor Kastner</name>
    </author>
    <author>
      <name>Florian Huber</name>
    </author>
    <link href="http://arxiv.org/abs/1704.03239v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.03239v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.06374v1</id>
    <updated>2017-04-21T01:08:14Z</updated>
    <published>2017-04-21T01:08:14Z</published>
    <title>Recalibration: A post-processing method for approximate Bayesian
  computation</title>
    <summary>  A new recalibration post-processing method is presented to improve the
quality of the posterior approximation when using Approximate Bayesian
Computation (ABC) algorithms. Recalibration may be used in conjunction with
existing post-processing methods, such as regression-adjustments. In addition,
this work extends and strengthens the links between ABC and indirect inference
algorithms, allowing more extensive use of misspecified auxiliary models in the
ABC context. The method is illustrated using simulated examples to demonstrate
the effects of recalibration under various conditions, and through an
application to an analysis of stereological extremes both with and without the
use of auxiliary models. Code to implement recalibration post-processing is
available in the R package, abctools.
</summary>
    <author>
      <name>G. S. Rodrigues</name>
    </author>
    <author>
      <name>D. Prangle</name>
    </author>
    <author>
      <name>S. A. Sisson</name>
    </author>
    <link href="http://arxiv.org/abs/1704.06374v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.06374v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.07414v1</id>
    <updated>2017-04-24T18:48:32Z</updated>
    <published>2017-04-24T18:48:32Z</published>
    <title>BDSAR: a new package on Bregman divergence for Bayesian simultaneous
  autoregressive models</title>
    <summary>  BDSAR is an R package which estimates distances between probability
distributions and facilitates a dynamic and powerful analysis of diagnostics
for Bayesian models from the class of Simultaneous Autoregressive (SAR) spatial
models. The package offers a new and fine plot to compare models as well as it
works in an intuitive way to allow any analyst to easily build fine plots.
These are helpful to promote insights about influential observations in the
data.
</summary>
    <author>
      <name>Ian M Danilevicz</name>
    </author>
    <author>
      <name>Ricardo S Ehlers</name>
    </author>
    <link href="http://arxiv.org/abs/1704.07414v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.07414v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.07806v1</id>
    <updated>2017-04-25T17:34:52Z</updated>
    <published>2017-04-25T17:34:52Z</published>
    <title>A Note on Experiments and Software For Multidimensional Order Statistics</title>
    <summary>  In this note we describe experiments on an implementation of two methods
proposed in the literature for computing regions that correspond to a notion of
order statistics for multidimensional data. Our implementation, which works for
any dimension greater than one, is the only that we know of to be publicly
available. Experiments run using the software confirm that half-space peeling
generally gives better results than directly peeling convex hulls, but at a
computational cost.
</summary>
    <author>
      <name>David L. Woodruff</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of California Davis</arxiv:affiliation>
    </author>
    <author>
      <name>Stefan Zillmann</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Duisburg-Essen</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1704.07806v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.07806v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.00166v1</id>
    <updated>2017-04-29T10:49:58Z</updated>
    <published>2017-04-29T10:49:58Z</published>
    <title>On the convergence of Hamiltonian Monte Carlo</title>
    <summary>  This paper discusses the stability properties of the Hamiltonian Monte Carlo
(HMC) algorithm used to sample from a positive target density $\pi$ on
$\mathbb{R}^d$, with either a fixed or a random number of integration steps.
Under mild conditions on the potential $U$ associated with $\pi$, we show that
the Markov kernel associated to the HMC algorithm is irreducible and recurrent.
Under some additional conditions, the Markov kernel may be shown to be Harris
recurrent. Besides, verifiable conditions on $U$ are derived which imply
geometric convergence.
</summary>
    <author>
      <name>Alain Durmus</name>
    </author>
    <author>
      <name>Eric Moulines</name>
    </author>
    <author>
      <name>Eero Saksman</name>
    </author>
    <link href="http://arxiv.org/abs/1705.00166v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.00166v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.00946v1</id>
    <updated>2017-05-02T12:59:38Z</updated>
    <published>2017-05-02T12:59:38Z</published>
    <title>Variable selection in model-based clustering and discriminant analysis
  with a regularization approach</title>
    <summary>  Relevant methods of variable selection have been proposed in model-based
clustering and classification. These methods are making use of backward or
forward procedures to define the roles of the variables. Unfortunately, these
stepwise procedures are terribly slow and make these variable selection
algorithms inefficient to treat large data sets. In this paper, an alternative
regularization approach of variable selection is proposed for model-based
clustering and classification. In this approach, the variables are first ranked
with a lasso-like procedure in order to avoid painfully slow stepwise
algorithms. Thus, the variable selection methodology of Maugis et al (2009b)
can be efficiently applied on high-dimensional data sets.
</summary>
    <author>
      <name>Gilles Celeux</name>
    </author>
    <author>
      <name>Cathy Maugis-Rabusseau</name>
    </author>
    <author>
      <name>Mohammed Sedki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Advances in Data Analysis and Classification</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.00946v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.00946v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.03831v2</id>
    <updated>2017-05-11T12:57:17Z</updated>
    <published>2017-05-10T15:50:41Z</published>
    <title>Quasi-Reliable Estimates of Effective Sample Size</title>
    <summary>  The efficiency of a Markov chain Monte Carlo algorithm might be measured by
the cost of generating one independent sample, or equivalently, the total cost
divided by the effective sample size, defined in terms of the integrated
autocorrelation time. To ensure the reliability of such an estimate, it is
suggested that there be an adequate sampling of state space--- to the extent
that this can be determined from the available samples. A possible method for
doing this is derived and evaluated.
</summary>
    <author>
      <name>Youhan Fang</name>
    </author>
    <author>
      <name>Yudong Cao</name>
    </author>
    <author>
      <name>Robert D. Skeel</name>
    </author>
    <link href="http://arxiv.org/abs/1705.03831v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.03831v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.03944v1</id>
    <updated>2017-05-10T20:19:05Z</updated>
    <published>2017-05-10T20:19:05Z</published>
    <title>Efficient design of experiments for sensitivity analysis based on
  polynomial chaos expansions</title>
    <summary>  Global sensitivity analysis aims at quantifying respective effects of input
random variables (or combinations thereof) onto variance of a physical or
mathematical model response. Among the abundant literature on sensitivity
measures, Sobol' indices have received much attention since they provide
accurate information for most of models. We consider a problem of experimental
design points selection for Sobol' indices estimation. Based on the concept of
$D$-optimality, we propose a method for constructing an adaptive design of
experiments, effective for calculation of Sobol' indices based on Polynomial
Chaos Expansions. We provide a set of applications that demonstrate the
efficiency of the proposed approach.
</summary>
    <author>
      <name>E. Burnaev</name>
    </author>
    <author>
      <name>I. Panin</name>
    </author>
    <author>
      <name>B. Sudret</name>
    </author>
    <link href="http://arxiv.org/abs/1705.03944v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.03944v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.04219v1</id>
    <updated>2017-05-11T14:47:30Z</updated>
    <published>2017-05-11T14:47:30Z</published>
    <title>A critical analysis of resampling strategies for the regularized
  particle filter</title>
    <summary>  We analyze the performance of different resampling strategies for the
regularized particle filter regarding parameter estimation. We show in
particular, building on analytical insight obtained in the linear Gaussian
case, that resampling systematically can prevent the filtered density from
converging towards the true posterior distribution. We discuss several means to
overcome this limitation, including kernel bandwidth modulation, and provide
evidence that the resulting particle filter clearly outperforms traditional
bootstrap particle filters. Our results are supported by numerical simulations
on a linear textbook example, the logistic map and a non-linear plant growth
model.
</summary>
    <author>
      <name>Pierre Carmier</name>
    </author>
    <author>
      <name>Olexiy Kyrgyzov</name>
    </author>
    <author>
      <name>Paul-Henry Cournède</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.04219v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.04219v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.07598v1</id>
    <updated>2017-05-22T08:06:03Z</updated>
    <published>2017-05-22T08:06:03Z</published>
    <title>Rao-Blackwellized Particle Smoothing as Message Passing</title>
    <summary>  In this manuscript the fixed-lag smoothing problem for conditionally linear
Gaussian state-space models is investigated from a factor graph perspective.
More specifically, after formulating Bayesian smoothing for an arbitrary
state-space model as forward-backward message passing over a factor graph, we
focus on the above mentioned class of models and derive a novel
Rao-Blackwellized particle smoother for it. Then, we show how our technique can
be modified to estimate a point mass approximation of the so called joint
smoothing distribution. Finally, the estimation accuracy and the computational
requirements of our smoothing algorithms are analysed for a specific
state-space model.
</summary>
    <author>
      <name>Giorgio M. Vitetta</name>
    </author>
    <author>
      <name>Emilio Sirignano</name>
    </author>
    <author>
      <name>Francesco Montorsi</name>
    </author>
    <link href="http://arxiv.org/abs/1705.07598v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.07598v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.11123v1</id>
    <updated>2017-05-31T14:59:09Z</updated>
    <published>2017-05-31T14:59:09Z</published>
    <title>Bayesian Distributional Non-Linear Multilevel Modeling with the R
  Package brms</title>
    <summary>  The brms package allows R users to easily specify a wide range of Bayesian
multilevel models, which are fitted with the probabilistic programming language
Stan behind the scenes. A wide range of response distributions are supported in
combination with an intuitive and powerful multilevel formula syntax.
Non-linear relationships may be specified using non-linear predictor terms,
splines or Gaussian processes. Additionally, all parameters of the response
distribution can be predicted at the same time allowing for distributional
regression. Model fit can be investigated and compared using leave-one-out
cross-validation and graphical posterior-predictive checks. Many more
post-processing and plotting methods are implemented.
</summary>
    <author>
      <name>Paul-Christian Bürkner</name>
    </author>
    <link href="http://arxiv.org/abs/1705.11123v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.11123v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.02808v2</id>
    <updated>2017-06-22T22:04:44Z</updated>
    <published>2017-06-09T01:44:03Z</published>
    <title>A randomized Halton algorithm in R</title>
    <summary>  Randomized quasi-Monte Carlo (RQMC) sampling can bring orders of magnitude
reduction in variance compared to plain Monte Carlo (MC) sampling. The extent
of the efficiency gain varies from problem to problem and can be hard to
predict. This article presents an R function rhalton that produces scrambled
versions of Halton sequences. On some problems it brings efficiency gains of
several thousand fold. On other problems, the efficiency gain is minor. The
code is designed to make it easy to determine whether a given integrand will
benefit from RQMC sampling. An RQMC sample of n points in $[0,1]^d$ can be
extended later to a larger n and/or d.
</summary>
    <author>
      <name>Art B. Owen</name>
    </author>
    <link href="http://arxiv.org/abs/1706.02808v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.02808v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.04440v1</id>
    <updated>2017-06-13T16:12:08Z</updated>
    <published>2017-06-13T16:12:08Z</published>
    <title>trackr: A Framework for Enhancing Discoverability and Reproducibility of
  Data Visualizations and Other Artifacts in R</title>
    <summary>  Research is an incremental, iterative process, with new results relying and
building upon previous ones. Scientists need to find, retrieve, understand, and
verify results in order to confidently extend them, even when the results are
their own. We present the trackr framework for organizing, automatically
annotating, discovering, and retrieving results. We identify sources of
automatically extractable metadata for computational results, and we define an
extensible system for organizing, annotating, and searching for results based
on these and other metadata. We present an open-source implementation of these
concepts for plots, computational artifacts, and woven dynamic reports
generated in the R statistical computing language.
</summary>
    <author>
      <name>Gabriel Becker</name>
    </author>
    <author>
      <name>Sara E. Moore</name>
    </author>
    <author>
      <name>Michael Lawrence</name>
    </author>
    <link href="http://arxiv.org/abs/1706.04440v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.04440v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.04781v2</id>
    <updated>2017-06-18T21:26:21Z</updated>
    <published>2017-06-15T09:09:19Z</published>
    <title>Generalized Bouncy Particle Sampler</title>
    <summary>  As a special example of piecewise deterministic Markov process, bouncy
particle sampler is a rejection-free, irreversible Markov chain Monte Carlo
algorithm and can draw samples from target distribution efficiently. We
generalize bouncy particle sampler in terms of its transition dynamics. In BPS,
the transition dynamic at event time is deterministic, but in GBPS, it is
random. With the help of this randomness, GBPS can overcome the reducibility
problem in BPS without refreshement.
</summary>
    <author>
      <name>Changye Wu</name>
    </author>
    <author>
      <name>Christian P. Robert</name>
    </author>
    <link href="http://arxiv.org/abs/1706.04781v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.04781v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.06889v1</id>
    <updated>2017-06-21T13:23:30Z</updated>
    <published>2017-06-21T13:23:30Z</published>
    <title>gk: An R Package for the g-and-k and generalised g-and-h Distributions</title>
    <summary>  The g-and-k and (generalised) g-and-h distributions are flexible univariate
distributions which can model highly skewed or heavy tailed data through only
four parameters: location and scale, and two shape parameters influencing the
skewness and kurtosis. These distributions have the unusual property that they
are defined through their quantile function (inverse cumulative distribution
function) and their density is unavailable in closed form, which makes
parameter inference complicated. This paper presents the gk R package to work
with these distributions. It provides the usual distribution functions and
several algorithms for inference of independent identically distributed data,
including the finite difference stochastic approximation method, which has not
been used before for this problem.
</summary>
    <author>
      <name>Dennis Prangle</name>
    </author>
    <link href="http://arxiv.org/abs/1706.06889v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.06889v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.07797v1</id>
    <updated>2017-06-23T14:48:49Z</updated>
    <published>2017-06-23T14:48:49Z</published>
    <title>A computer algebra system for R: Macaulay2 and the m2r package</title>
    <summary>  Algebraic methods have a long history in statistics. The most prominent
manifestation of modern algebra in statistics can be seen in the field of
algebraic statistics, which brings tools from commutative algebra and algebraic
geometry to bear on statistical problems. Now over two decades old, algebraic
statistics has applications in a wide range of theoretical and applied
statistical domains. Nevertheless, algebraic statistical methods are still not
mainstream, mostly due to a lack of easy off-the-shelf implementations. In this
article we debut m2r, an R package that connects R to Macaulay2 through a
persistent back-end socket connection running locally or on a cloud server.
Topics range from basic use of m2r to applications and design philosophy.
</summary>
    <author>
      <name>David Kahle</name>
    </author>
    <author>
      <name>Christopher O'Neill</name>
    </author>
    <author>
      <name>Jeff Sommars</name>
    </author>
    <link href="http://arxiv.org/abs/1706.07797v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.07797v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.AG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.01660v1</id>
    <updated>2017-07-06T07:17:55Z</updated>
    <published>2017-07-06T07:17:55Z</published>
    <title>Parallel Particle MCMC with Poisson Resampling</title>
    <summary>  We introduce a new version of particle filter in which the number of
"children" of a particle at a given time has a Poisson distribution. As a
result, the number of particles is random and varies with time. An advantage of
this scheme is that decendants of different particles can evolve independently.
It makes easy to parallelize computations. We also show that the basic
techniques of particle MCMC, namely particle independent Metropolis-Hastings,
particle Gibbs Sampler and its version with backward sampling of ancestors,
work under our Poisson resampling scheme. We prove that versions of these
algorithms, suitably modified to our setup, preserve the target distribution on
the space of trajectories of hidden Markov process.
</summary>
    <author>
      <name>Błażej Miasojedow</name>
    </author>
    <author>
      <name>Wojciech Niemiro</name>
    </author>
    <author>
      <name>Michał Startek</name>
    </author>
    <link href="http://arxiv.org/abs/1707.01660v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.01660v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.02548v1</id>
    <updated>2017-07-09T09:32:52Z</updated>
    <published>2017-07-09T09:32:52Z</published>
    <title>Unified Method for Markov Chain Transition Model Estimation Using
  Incomplete Survey Data</title>
    <summary>  The Future Elderly Model and related microsimulations are modeled as Markov
chains. These simulations rely on longitudinal survey data to estimate their
transition models. The use of survey data presents several incomplete data
problems, including coarse and irregular spacing of interviews, data collection
from subsamples, and structural changes to surveys over time. The
Expectation-Maximization algorithm is adapted to create a method for maximum
likelihood estimation of Markov chain transition models using incomplete data.
The method is demonstrated on a simplified version of the Future Elderly Model.
</summary>
    <author>
      <name>Duncan Ermini Leaf</name>
    </author>
    <link href="http://arxiv.org/abs/1707.02548v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.02548v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65C60" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3; I.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.04112v1</id>
    <updated>2017-07-13T13:25:15Z</updated>
    <published>2017-07-13T13:25:15Z</published>
    <title>Small Sample Inference for the Common Coefficient of Variation</title>
    <summary>  This paper utilizes the modified signed log-likelihood ratio method for the
problem of inference about the common coefficient of variation in several
independent normal populations. This method is applicable for both the problem
of hypothesis testing and constructing a confidence interval for this
parameter. Simulation studies show that the coverage probability of this
proposed approach is close to the confidence coefficient. Also, its expected
length is smaller than expected lengths of other competing approaches. In fact,
the proposed approach is very satisfactory regardless of the number of
populations and the different values of the common coefficient of variation
even for very small sample size. Finally, we illustrate the proposed method
using two real data sets.
</summary>
    <author>
      <name>Mohmammad Reza Kazemi</name>
    </author>
    <author>
      <name>Ali Akbar Jafari</name>
    </author>
    <link href="http://arxiv.org/abs/1707.04112v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.04112v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.07149v3</id>
    <updated>2017-09-05T15:49:23Z</updated>
    <published>2017-07-22T12:03:59Z</published>
    <title>pre: An R Package for Fitting Prediction Rule Ensembles</title>
    <summary>  Prediction rule ensembles (PREs) are sparse collections of rules, offering
highly interpretable regression and classification models. This paper presents
the R package pre, which derives PREs through the methodology of Friedman and
Popescu (2008). The implementation and functionality of package pre is
described and illustrated through application on a dataset on the prediction of
depression. Furthermore, accuracy and sparsity of PREs is compared with that of
single trees, random forest and lasso regression in four benchmark datasets.
Results indicate that pre derives ensembles with predictive accuracy comparable
to that of random forests, while using a smaller number of variables for
prediction.
</summary>
    <author>
      <name>Marjolein Fokkema</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Working paper, 24 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.07149v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.07149v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.09705v2</id>
    <updated>2017-08-06T16:11:05Z</updated>
    <published>2017-07-31T03:07:00Z</published>
    <title>Mini-batch Tempered MCMC</title>
    <summary>  In this paper we propose a general framework of performing MCMC with only a
mini-batch of data. We show by estimating the Metropolis-Hasting ratio with
only a mini-batch of data, one is essentially sampling from the true posterior
raised to a known temperature. We show by experiments that our method,
Mini-batch Tempered MCMC (MINT-MCMC), can efficiently explore multiple modes of
a posterior distribution. As an application, we demonstrate one application of
MINT-MCMC as an inference tool for Bayesian neural networks. We also show an
cyclic version of our algorithm can be applied to build an ensemble of neural
networks with little additional training cost.
</summary>
    <author>
      <name>Dangna Li</name>
    </author>
    <author>
      <name>Wing H Wong</name>
    </author>
    <link href="http://arxiv.org/abs/1707.09705v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.09705v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.00347v1</id>
    <updated>2017-08-01T14:14:10Z</updated>
    <published>2017-08-01T14:14:10Z</published>
    <title>An Inverse Normal Transformation Solution for the comparison of two
  samples that contain both paired observations and independent observations</title>
    <summary>  Inverse normal transformations applied to the partially overlapping samples
t-tests by Derrick et.al. (2017) are considered for their Type I error
robustness and power. The inverse normal transformation solutions proposed in
this paper are shown to maintain Type I error robustness. For increasing
degrees of skewness they also offer improved power relative to the parametric
partially overlapping samples t-tests. The power when using inverse normal
transformation solutions are comparable to rank based non-parametric solutions.
</summary>
    <author>
      <name>Ben Derrick</name>
    </author>
    <author>
      <name>Paul White</name>
    </author>
    <author>
      <name>Deirdre Toher</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Inverse Normal Transformations Partially overlapping samples</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.00347v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.00347v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.00829v1</id>
    <updated>2017-08-02T17:06:28Z</updated>
    <published>2017-08-02T17:06:28Z</published>
    <title>Complexity Results for MCMC derived from Quantitative Bounds</title>
    <summary>  This paper considers whether MCMC quantitative convergence bounds can be
translated into complexity bounds. We prove that a certain realistic Gibbs
sampler algorithm converges in constant number of iterations. Our proof uses a
new general method of establishing a generalized geometric drift condition
defined in a subset of the state space. The subset is chosen to rule out some
"bad" states which have poor drift property when the dimension gets large.
Using the new general method, the obtained quantitative bounds for the Gibbs
sampler algorithm can be translated to tight complexity bounds in
high-dimensional setting. It is our hope that the new general approach can be
employed in many other specific examples to obtain complexity bounds for
high-dimensional Markov chains.
</summary>
    <author>
      <name>Jun Yang</name>
    </author>
    <author>
      <name>Jeffrey S. Rosenthal</name>
    </author>
    <link href="http://arxiv.org/abs/1708.00829v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.00829v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.02340v1</id>
    <updated>2017-08-08T00:45:18Z</updated>
    <published>2017-08-08T00:45:18Z</published>
    <title>Fast Approximate Data Assimilation for High-Dimensional Problems</title>
    <summary>  Currently, real-time data assimilation techniques are overwhelmed by data
volume , velocity and increasing complexity of computational models. In this
paper, we propose a novel data assimilation approach which only requires a
small number of samples and can be applied to high-dimensional systems. This
approach is based on linear latent variable models and leverages machinery to
achieve fast implementation. It does not require computing the high-dimensional
sample covariance matrix, which provides significant computational speed-up.
Since it is performed without calculating likelihood function, it can be
applied to data assimilation problems in which likelihood is intractable. In
addition, model error can be absorbed implicitly and reflected in the data
assimilation result. Two numerical experiments are conducted, and the proposed
approach shows promising results.
</summary>
    <author>
      <name>Xiao Lin</name>
    </author>
    <author>
      <name>Gabriel Terejanu</name>
    </author>
    <link href="http://arxiv.org/abs/1708.02340v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.02340v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.02632v1</id>
    <updated>2017-08-08T19:59:53Z</updated>
    <published>2017-08-08T19:59:53Z</published>
    <title>Using JAGS for Bayesian Cognitive Diagnosis Models: A Tutorial</title>
    <summary>  In this article, JAGS software was systematically introduced to fit common
Bayesian cognitive diagnosis models (CDMs), such as the deterministic inputs,
noisy "and" gate model, the deterministic inputs, noisy "or" gate model, the
linear logistic model, and the log-linear CDM. The unstructured structural
model and the higher-order structural model were both employed. We also showed
how to extend those models to consider the testlet-effect. Finally, an
empirical example was given as a tutorial to illustrate how to use our JAGS
code in R.
</summary>
    <author>
      <name>Peida Zhan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages,2 figures, 7 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.02632v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.02632v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.04870v1</id>
    <updated>2017-08-16T13:12:54Z</updated>
    <published>2017-08-16T13:12:54Z</published>
    <title>On residual and guided proposals for diffusion bridge simulation</title>
    <summary>  Recently Whitaker et al. (2017) considered Bayesian estimation of diffusion
driven mixed effects models using data-augmentation. The missing data,
diffusion bridges connecting discrete time observations, are drawn using a
"residual bridge construct". In this paper we compare this construct (which we
call residual proposal) with the guided proposals introduced in Schauer et al.
2017. It is shown that both approaches are related, but use a different
approximation to the intractable stochastic differential equation of the true
diffusion bridge. It reveals that the computational complexity of both
approaches is similar. Some examples are included to compare the ability of
both proposals to capture local nonlinearities in the dynamics of the true
bridge.
</summary>
    <author>
      <name>Frank van der Meulen</name>
    </author>
    <author>
      <name>Moritz Schauer</name>
    </author>
    <link href="http://arxiv.org/abs/1708.04870v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.04870v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62M05, 60J60 (Primary) 62F15, 65C05 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.08268v1</id>
    <updated>2017-08-28T10:57:26Z</updated>
    <published>2017-08-28T10:57:26Z</published>
    <title>Robust Monitoring of Many Time Series with Application to Fraud
  Detection</title>
    <summary>  Time series often contain outliers and level shifts or structural changes.
These unexpected events are of the utmost importance in fraud detection, as
they may pinpoint suspicious transactions. The presence of such unusual events
can easily mislead conventional time series analysis and yield erroneous
conclusions. In this paper we provide a unified framework for detecting
outliers and level shifts in short time series that may have a seasonal
pattern. The approach combines ideas from the FastLTS algorithm for robust
regression with alternating least squares. The double wedge plot is proposed, a
graphical display which indicates outliers and potential level shifts. The
methodology is illustrated on real and artificial time series.
</summary>
    <author>
      <name>Peter J. Rousseeuw</name>
    </author>
    <author>
      <name>Domenico Perrotta</name>
    </author>
    <author>
      <name>Marco Riani</name>
    </author>
    <author>
      <name>Mia Hubert</name>
    </author>
    <link href="http://arxiv.org/abs/1708.08268v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.08268v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.09028v1</id>
    <updated>2017-08-29T21:15:12Z</updated>
    <published>2017-08-29T21:15:12Z</published>
    <title>Tail approximations for sums of dependent regularly varying random
  variables under Archimedean copula models</title>
    <summary>  In this paper, we compare two numerical methods for approximating the
probability that the sum of dependent regularly varying random variables
exceeds a high threshold under Archimedean copula models. The first method is
based on conditional Monte Carlo. We present four estimators and show that most
of them have bounded relative errors. The second method is based on analytical
expressions of the multivariate survival or cumulative distribution functions
of the regularly varying random variables and provides sharp and deterministic
bounds of the probability of exceedance. We discuss implementation issues and
illustrate the accuracy of both procedures through numerical studies.
</summary>
    <author>
      <name>Hélène Cossette</name>
    </author>
    <author>
      <name>Etienne Marceau</name>
    </author>
    <author>
      <name>Quang Huy Nguyen</name>
    </author>
    <author>
      <name>Christian Robert</name>
    </author>
    <link href="http://arxiv.org/abs/1708.09028v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.09028v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.00151v1</id>
    <updated>2017-09-01T04:34:37Z</updated>
    <published>2017-09-01T04:34:37Z</published>
    <title>Determinantal Point Processes Stochastic Approximation for Combinatorial
  Optimization</title>
    <summary>  We study the problem of optimal subset selection from a set of correlated
random variables. In particular, we consider the associated combinatorial
optimization problem of maximizing the determinant of a symmetric positive
definite matrix that characterizes the chosen subset. This problem arises in
many domains, such as experimental designs, regression modeling, and
environmental statistics. We establish an efficient polynomial-time algorithm
using Determinantal Point Process for approximating the optimal solution to the
problem. We demonstrate the advantages of our methods by presenting
computational results for both synthetic and real data sets.
</summary>
    <author>
      <name>Yu Wang</name>
    </author>
    <author>
      <name>Nhu D. Le</name>
    </author>
    <author>
      <name>James V. Zidek</name>
    </author>
    <link href="http://arxiv.org/abs/1709.00151v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.00151v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.01303v1</id>
    <updated>2017-09-05T09:37:48Z</updated>
    <published>2017-09-05T09:37:48Z</published>
    <title>Hamiltonian Flow Simulation of Rare Events</title>
    <summary>  Hamiltonian Flow Monte Carlo(HFMC) methods have been implemented in
engineering, biology and chemistry. HFMC makes large gradient based steps to
rapidly explore the state space. The application of the Hamiltonian dynamics
allows to estimate rare events and sample from target distributions defined as
the change of measures. The estimates demonstrated a variance reduction of the
presented algorithm and its efficiency with respect to a standard Monte Carlo
and interacting particle based system(IPS). We tested the algorithm on the case
of the barrier option pricing.
</summary>
    <author>
      <name>Raphael Douady</name>
    </author>
    <author>
      <name>Shohruh Miryusupov</name>
    </author>
    <link href="http://arxiv.org/abs/1709.01303v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.01303v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.02317v1</id>
    <updated>2017-09-07T15:37:50Z</updated>
    <published>2017-09-07T15:37:50Z</published>
    <title>Computing optimal experimental designs with respect to a compound Bayes
  risk criterion</title>
    <summary>  We consider the problem of computing optimal experimental design on a finite
design space with respect to a compound Bayes risk criterion, which includes
the linear criterion for prediction in a random coefficient regression model.
We show that the problem can be restated as constrained A-optimality in an
artificial model. This permits using recently developed computational tools,
for instance the algorithms based on the second-order cone programming for
optimal approximate design, and mixed-integer second-order cone programming for
optimal exact designs. We demonstrate the use of the proposed method for the
problem of computing optimal designs of a random coefficient regression model
with respect to an integrated mean squared error criterion.
</summary>
    <author>
      <name>Radoslav Harman</name>
    </author>
    <author>
      <name>Maryna Prus</name>
    </author>
    <link href="http://arxiv.org/abs/1709.02317v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.02317v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62K05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.04743v1</id>
    <updated>2017-09-14T12:55:24Z</updated>
    <published>2017-09-14T12:55:24Z</published>
    <title>Evaluating probabilistic forecasts with the R package scoringRules</title>
    <summary>  Probabilistic forecasts in the form of probability distributions over future
events have become popular in several fields including meteorology, hydrology,
economics, and demography. In typical applications, many alternative
statistical models and data sources can be used to produce probabilistic
forecasts. Hence, evaluating and selecting among competing methods is an
important task. The scoringRules package for R provides functionality for
comparative evaluation of probabilistic models based on proper scoring rules,
covering a wide range of situations in applied work. This paper discusses
implementation and usage details, presents case studies from meteorology and
economics, and points to the relevant background literature.
</summary>
    <author>
      <name>Alexander Jordan</name>
    </author>
    <author>
      <name>Fabian Krüger</name>
    </author>
    <author>
      <name>Sebastian Lerch</name>
    </author>
    <link href="http://arxiv.org/abs/1709.04743v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.04743v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.06254v1</id>
    <updated>2017-09-19T04:55:10Z</updated>
    <published>2017-09-19T04:55:10Z</published>
    <title>BeSS: An R Package for Best Subset Selection in Linear, Logistic and
  CoxPH Models</title>
    <summary>  We introduce a new R package, BeSS, for solving the best subset selection
problem in linear, logistic and Cox's proportional hazard (CoxPH) models. It
utilizes a highly efficient active set algorithm based on primal and dual
variables, and supports sequential and golden search strategies for best subset
selection. We provide a C++ implementation of the algorithm using Rcpp
interface. We demonstrate through numerical experiments based on enormous
simulation and real datasets that the new BeSS package has competitive
performance compared to other R packages for best subset selection purpose.
</summary>
    <author>
      <name>Canhong Wen</name>
    </author>
    <author>
      <name>Aijun Zhang</name>
    </author>
    <author>
      <name>Shijie Quan</name>
    </author>
    <author>
      <name>Xueqin Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1709.06254v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.06254v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.06633v1</id>
    <updated>2017-09-19T20:25:00Z</updated>
    <published>2017-09-19T20:25:00Z</published>
    <title>Multilevel mixed effects parametric survival analysis</title>
    <summary>  With the release of Stata 14 came the mestreg command to fit multilevel mixed
effects parametric survival models, assuming normally distributed random
effects, estimated with maximum likelihood utilising Gaussian quadrature. In
this article, I present the user written stmixed command, which serves as both
an alternative and a complimentary program for the fitting of multilevel
parametric survival models, to mestreg. The key extensions include
incorporation of the flexible parametric Royston-Parmar survival model, and the
ability to fit multilevel relative survival models. The methods are illustrated
with a commonly used dataset of patients with kidney disease suffering
recurrent infections, and a simulated example, illustrating a simple approach
to simulating clustered survival data using survsim (Crowther and Lambert 2012,
2013).
</summary>
    <author>
      <name>Michael J. Crowther</name>
    </author>
    <link href="http://arxiv.org/abs/1709.06633v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.06633v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.06849v1</id>
    <updated>2017-09-19T14:38:20Z</updated>
    <published>2017-09-19T14:38:20Z</published>
    <title>Rbox: an integrated R package for ATOM Editor</title>
    <summary>  R is a programming language and environment that is a central tool in the
applied sciences for writing program. Its impact on the development of modern
statistics is inevitable. Current research, especially for big data may not be
done solely using R and will likely use different programming languages; hence,
having a modern integrated development environment (IDE) is very important.
Atom editor is modern IDE that is developed by GitHub, it is described as "A
hackable text editor for the 21st Century". This report is intended to present
a package deployed entitled Rbox that allows Atom Editor to write and run codes
professionally in R.
</summary>
    <author>
      <name>Saeid Amiri</name>
    </author>
    <link href="http://arxiv.org/abs/1709.06849v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.06849v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.07557v1</id>
    <updated>2017-09-22T01:05:44Z</updated>
    <published>2017-09-22T01:05:44Z</published>
    <title>A preconditioning approach for improved estimation of sparse polynomial
  chaos expansions</title>
    <summary>  Compressive sampling has been widely used for sparse polynomial chaos (PC)
approximation of stochastic functions. The recovery accuracy of compressive
sampling depends on the coherence properties of measurement matrix. In this
paper, we consider preconditioning the measurement matrix. Premultiplying a
linear equation system by a non-singular matrix results in an equivalent
equation system, but it can impact the coherence properties of preconditioned
measurement matrix and lead to a different recovery accuracy. In this work, we
propose a preconditioning scheme that significantly improves the coherence
properties of measurement matrix, and using theoretical motivations and
numerical examples highlight the promise of the proposed approach in improving
the accuracy of estimated polynomial chaos expansions.
</summary>
    <author>
      <name>Negin Alemazkoor</name>
    </author>
    <author>
      <name>Hadi Meidani</name>
    </author>
    <link href="http://arxiv.org/abs/1709.07557v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.07557v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.00873v1</id>
    <updated>2017-10-02T19:19:21Z</updated>
    <published>2017-10-02T19:19:21Z</published>
    <title>A note on perfect simulation for exponential random graph models</title>
    <summary>  In this paper we propose a perfect simulation algorithm for the Exponential
Random Graph Model, based on the Coupling From The Past method of Propp &amp;
Wilson (1996). We use a Glauber dynamics to construct the Markov Chain and we
prove the monotonicity of the ERGM for a subset of the parametric space. We
also obtain an upper bound on the running time of the algorithm that depends on
the mixing time of the Markov chain.
</summary>
    <author>
      <name>Andressa Cerqueira</name>
    </author>
    <author>
      <name>Aurélien Garivier</name>
    </author>
    <author>
      <name>Florencia Leonardi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.00873v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.00873v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.01057v1</id>
    <updated>2017-10-03T10:09:07Z</updated>
    <published>2017-10-03T10:09:07Z</published>
    <title>Improving approximate Bayesian computation via quasi Monte Carlo</title>
    <summary>  ABC (approximate Bayesian computation) is a general approach for dealing with
models with an intractable likelihood. In this work, we derive ABC algorithms
based on QMC (quasi- Monte Carlo) sequences. We show that the resulting ABC
estimates have a lower variance than their Monte Carlo counter-parts. We also
develop QMC variants of sequential ABC algorithms, which progressively adapt
the proposal distribution and the acceptance threshold. We illustrate our QMC
approach through several examples taken from the ABC literature. Keywords:
Approximate Bayesian computation, Likelihood-free inference, Quasi Monte Carlo,
Randomized Quasi Monte Carlo, Adaptive importance sampling
</summary>
    <author>
      <name>Alexander Buchholz</name>
    </author>
    <author>
      <name>Nicolas Chopin</name>
    </author>
    <link href="http://arxiv.org/abs/1710.01057v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.01057v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0709.2776v1</id>
    <updated>2007-09-18T09:10:25Z</updated>
    <published>2007-09-18T09:10:25Z</published>
    <title>A note on calculating autocovariances of periodic ARMA models</title>
    <summary>  An analytically simple and tractable formula for the start-up autocovariances
of periodic ARMA (PARMA) models is provided.
</summary>
    <author>
      <name>Abdelhakim Aknouche Hacène Belbachir Fayçal Hamdi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0709.2776v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0709.2776v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62M10 (Primary); 62F15 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0811.0174v1</id>
    <updated>2008-11-02T16:22:53Z</updated>
    <published>2008-11-02T16:22:53Z</published>
    <title>A Bit of Information Theory, and the Data Augmentation Algorithm
  Converges</title>
    <summary>  The data augmentation (DA) algorithm is a simple and powerful tool in
statistical computing. In this note basic information theory is used to prove a
nontrivial convergence theorem for the DA algorithm.
</summary>
    <author>
      <name>Yaming Yu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TIT.2008.929918</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TIT.2008.929918" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Information Theory 54 (2008) 5186--5188</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0811.0174v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0811.0174v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0910.4696v1</id>
    <updated>2009-10-26T18:52:32Z</updated>
    <published>2009-10-26T18:52:32Z</published>
    <title>Bayesian Core: The Complete Solution Manual</title>
    <summary>  This solution manual contains the unabridged and original solutions to all
the exercises proposed in Bayesian Core, along with R programs when necessary.
</summary>
    <author>
      <name>Christian P. Robert</name>
    </author>
    <author>
      <name>Jean-Michel Marin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">118+vii pages, 21 figures, 152 solutions</arxiv:comment>
    <link href="http://arxiv.org/abs/0910.4696v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0910.4696v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.2906v1</id>
    <updated>2010-01-17T17:43:52Z</updated>
    <published>2010-01-17T17:43:52Z</published>
    <title>Introducing Monte Carlo Methods with R Solutions to Odd-Numbered
  Exercises</title>
    <summary>  This is the solution manual to the odd-numbered exercises in our book
"Introducing Monte Carlo Methods with R", published by Springer Verlag on
December 10, 2009, and made freely available to everyone.
</summary>
    <author>
      <name>Christian P. Robert</name>
    </author>
    <author>
      <name>George Casella</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">87 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1001.2906v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.2906v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.1950v1</id>
    <updated>2010-03-09T18:55:18Z</updated>
    <published>2010-03-09T18:55:18Z</published>
    <title>Asymptotic optimality of the cross-entropy method for Markov chain
  problems</title>
    <summary>  The correspondence between the cross-entropy method and the zero-variance
approximation to simulate a rare event problem in Markov chains is shown. This
leads to a sufficient condition that the cross-entropy estimator is
asymptotically optimal.
</summary>
    <author>
      <name>Ad Ridder</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pager; 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1003.1950v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.1950v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60J22, 65C05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.3616v1</id>
    <updated>2010-04-21T04:39:45Z</updated>
    <published>2010-04-21T04:39:45Z</published>
    <title>Recursive Numerical Evaluation of the Cumulative Bivariate Normal
  Distribution</title>
    <summary>  We propose an algorithm for evaluation of the cumulative bivariate normal
distribution, building upon Marsaglia's ideas for evaluation of the cumulative
univariate normal distribution. The algorithm is mathematically transparent,
delivers competitive performance and can easily be extended to arbitrary
precision.
</summary>
    <author>
      <name>Christian Meyer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1004.3616v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.3616v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65D15" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.5723v1</id>
    <updated>2012-05-25T15:20:02Z</updated>
    <published>2012-05-25T15:20:02Z</published>
    <title>An asymptotic approximation for the permanent of a doubly stochastic
  matrix</title>
    <summary>  A determinantal approximation is obtained for the permanent of a doubly
stochastic matrix. For moderate-deviation matrix sequences, the asymptotic
relative error is of order $O(n^{-1})$.
</summary>
    <author>
      <name>Peter McCullagh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">One figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1205.5723v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.5723v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.6290v3</id>
    <updated>2014-02-10T18:43:25Z</updated>
    <published>2013-09-23T15:18:14Z</published>
    <title>Coefficient Matrices Computation of Structural Vector Autoregressive
  Model</title>
    <summary>  In this paper we present the Large Inverse Cholesky (LIC) method, an
efficient method for computing the coefficient matrices of a Structural Vector
Autoregressive (SVAR) model.
</summary>
    <author>
      <name>Aravindh Krishnamoorthy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2pp; pre-publication</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.6290v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.6290v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.00163v1</id>
    <updated>2017-04-29T09:53:59Z</updated>
    <published>2017-04-29T09:53:59Z</published>
    <title>A Proof of the Explicit Formula for Product Moments of Multivariate
  Gaussian Random Variables</title>
    <summary>  A detailed proof of a recent result on explicit formulae for the product
moments $E \left \{ X_1^{a_1} X_2^{a_2} \cdots X_n^{a_n}\right \}$ of
multivariate Gaussian random variables is provided in this note.
</summary>
    <author>
      <name>Iickho Song</name>
    </author>
    <link href="http://arxiv.org/abs/1705.00163v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.00163v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62H10, 62E15, 60G15" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0802.3455v5</id>
    <updated>2008-08-06T23:02:38Z</updated>
    <published>2008-02-23T19:13:10Z</published>
    <title>A Truncation Approach for Fast Computation of Distribution Functions</title>
    <summary>  In this paper, we propose a general approach for improving the efficiency of
computing distribution functions. The idea is to truncate the domain of
summation or integration.
</summary>
    <author>
      <name>Xinjia Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, no figure, revised Theorem 3</arxiv:comment>
    <link href="http://arxiv.org/abs/0802.3455v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0802.3455v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.2300v1</id>
    <updated>2008-09-12T22:49:05Z</updated>
    <published>2008-09-12T22:49:05Z</published>
    <title>Coupling Control Variates for Markov Chain Monte Carlo</title>
    <summary>  We show that Markov couplings can be used to improve the accuracy of Markov
chain Monte Carlo calculations in some situations where the steady-state
probability distribution is not explicitly known. The technique generalizes the
notion of control variates from classical Monte Carlo integration. We
illustrate it using two models of nonequilibrium transport.
</summary>
    <author>
      <name>Jonathan B. Goodman</name>
    </author>
    <author>
      <name>Kevin K. Lin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.jcp.2009.03.043</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.jcp.2009.03.043" rel="related"/>
    <link href="http://arxiv.org/abs/0809.2300v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.2300v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0810.4727v2</id>
    <updated>2008-11-11T02:49:24Z</updated>
    <published>2008-10-26T23:22:15Z</published>
    <title>Robust Estimation of Mean Values</title>
    <summary>  In this paper, we develop a computational approach for estimating the mean
value of a quantity in the presence of uncertainty. We demonstrate that, under
some mild assumptions, the upper and lower bounds of the mean value are
efficiently computable via a sample reuse technique, of which the computational
complexity is shown to posses a Poisson distribution.
</summary>
    <author>
      <name>Xinjia Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/0810.4727v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0810.4727v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0905.2181v1</id>
    <updated>2009-05-13T23:54:51Z</updated>
    <published>2009-05-13T23:54:51Z</published>
    <title>Non-Bayesian particle filters</title>
    <summary>  Particle filters for data assimilation in nonlinear problems use "particles"
(replicas of the underlying system) to generate a sequence of probability
density functions (pdfs) through a Bayesian process. This can be expensive
because a significant number of particles has to be used to maintain accuracy.
We offer here an alternative, in which the relevant pdfs are sampled directly
by an iteration. An example is discussed in detail.
</summary>
    <author>
      <name>Alexandre J. Chorin</name>
    </author>
    <author>
      <name>Xuemin Tu</name>
    </author>
    <link href="http://arxiv.org/abs/0905.2181v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0905.2181v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.3849v1</id>
    <updated>2009-06-21T19:11:49Z</updated>
    <published>2009-06-21T19:11:49Z</published>
    <title>Squeezing the Arimoto-Blahut algorithm for faster convergence</title>
    <summary>  The Arimoto--Blahut algorithm for computing the capacity of a discrete
memoryless channel is revisited. A so-called ``squeezing'' strategy is used to
design algorithms that preserve its simplicity and monotonic convergence
properties, but have provably better rates of convergence.
</summary>
    <author>
      <name>Yaming Yu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TIT.2010.2048452</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TIT.2010.2048452" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Trans. Inform. Theory 56 (2010) 3149-3157</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0906.3849v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.3849v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.3859v1</id>
    <updated>2010-01-21T17:58:19Z</updated>
    <published>2010-01-21T17:58:19Z</published>
    <title>Strict Monotonicity and Convergence Rate of Titterington's Algorithm for
  Computing D-optimal Designs</title>
    <summary>  We study a class of multiplicative algorithms introduced by Silvey et al.
(1978) for computing D-optimal designs. Strict monotonicity is established for
a variant considered by Titterington (1978). A formula for the rate of
convergence is also derived. This is used to explain why modifications
considered by Titterington (1978) and Dette et al. (2008) usually converge
faster.
</summary>
    <author>
      <name>Yaming Yu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.csda.2010.01.026</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.csda.2010.01.026" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computational Statistics and Data Analysis 54 (2010) 1419--1425.</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1001.3859v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.3859v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.4682v1</id>
    <updated>2010-02-25T02:52:35Z</updated>
    <published>2010-02-25T02:52:35Z</published>
    <title>Non-Central Limit Theorem Statistical Analysis for the "Long-tailed"
  Internet Society</title>
    <summary>  This article presents a statistical analysis method and introduces the
corresponding software package "tailstat," which is believed to be widely
applicable to today's internet society. The proposed method facilitates
statistical analyses with small sample sets from given populations, which
render the central limit theorem inapplicable. A large-scale case study
demonstrates the effectiveness of the method and provides implications for
applying similar analyses to other cases.
</summary>
    <author>
      <name>Kazutaka Kurihara</name>
    </author>
    <author>
      <name>Yohei Tutiya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1002.4682v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.4682v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.5930v2</id>
    <updated>2011-03-02T21:12:32Z</updated>
    <published>2010-03-30T21:35:26Z</published>
    <title>Stochastic Stepwise Ensembles for Variable Selection</title>
    <summary>  In this article, we advocate the ensemble approach for variable selection. We
point out that the stochastic mechanism used to generate the variable-selection
ensemble (VSE) must be picked with care. We construct a VSE using a stochastic
stepwise algorithm, and compare its performance with numerous state-of-the-art
algorithms.
</summary>
    <author>
      <name>Lu Xin</name>
    </author>
    <author>
      <name>Mu Zhu</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Computational and Graphical Statistics, June 2012, Vol.
  21, No. 2, Pages 275 - 294</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1003.5930v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.5930v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1101.1264v1</id>
    <updated>2011-01-06T17:40:36Z</updated>
    <published>2011-01-06T17:40:36Z</published>
    <title>Bayesian Analysis of Loss Ratios Using the Reversible Jump Algorithm</title>
    <summary>  In this paper we consider the problem of model choice for a set of insurance
loss ratios. We use a reversible jump algorithm for our model discrimination
and show how the vanilla reversible jump algorithm can be improved on using
recent methodological advances in reversible jump computation.
</summary>
    <author>
      <name>Garfield Brown</name>
    </author>
    <author>
      <name>Steve Brooks</name>
    </author>
    <link href="http://arxiv.org/abs/1101.1264v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1101.1264v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.1486v1</id>
    <updated>2011-05-08T01:46:29Z</updated>
    <published>2011-05-08T01:46:29Z</published>
    <title>Estimating Bernoulli trial probability from a small sample</title>
    <summary>  The standard textbook method for estimating the probability of a biased coin
from finite tosses implicitly assumes the sample sizes are large and gives
incorrect results for small samples. We describe the exact solution, which is
correct for any sample size.
</summary>
    <author>
      <name>Norman D. Megill</name>
    </author>
    <author>
      <name>Mladen Pavicic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1105.1486v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.1486v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1107.5342v2</id>
    <updated>2011-07-28T16:54:13Z</updated>
    <published>2011-07-20T16:05:37Z</published>
    <title>Esparsidade, Estrutura, Escalamento e Estabilidade em Algebra Linear
  Computacional</title>
    <summary>  Sparsity, Structure, Scaling and Stability in Computational Linear Algebra -
Textbook from the IX School of Computer Science, held on July 24-31 of 1994 at
Recife, Brazil.
  Esparsidade, Estrutura, Escalamento e Estabilidade em Algebra Linear
Computacional - Livro texto da IX Escola de Computacao, realizada nos dias 24 a
31 de Julho de 1994 em Recife, Brasil.
  This textbook is written in Portuguese Language.
</summary>
    <author>
      <name>Julio M. Stern</name>
    </author>
    <link href="http://arxiv.org/abs/1107.5342v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.5342v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.HO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.HO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.5364v2</id>
    <updated>2014-03-03T14:42:00Z</updated>
    <published>2011-08-26T18:19:23Z</published>
    <title>Phylogenetic Ornstein-Uhlenbeck regression curves</title>
    <summary>  Regression curves for studying trait relationships are developed herein. The
adaptive evolution model is considered an Ornstein-Uhlenbeck system whose
parameters are estimated by a novel engagement of generalized least-squares and
optimization. Our algorithm is implemented to ecological data.
</summary>
    <author>
      <name>Dwueng-Chwuan Jhwueng</name>
    </author>
    <author>
      <name>Vasileios Maroulas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1108.5364v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.5364v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60H30, 62J12, 62P10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.0433v1</id>
    <updated>2011-11-02T09:28:16Z</updated>
    <published>2011-11-02T09:28:16Z</published>
    <title>A closed-form approximation for the median of the beta distribution</title>
    <summary>  A simple closed-form approximation for the median of the beta distribution
Beta(a, b) is introduced: (a-1/3)/(a+b-2/3) for (a,b) both larger than 1 has a
relative error of less than 4%, rapidly decreasing to zero as both shape
parameters increase.
</summary>
    <author>
      <name>Jouni Kerman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1111.0433v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.0433v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.0798v1</id>
    <updated>2012-04-03T20:21:40Z</updated>
    <published>2012-04-03T20:21:40Z</published>
    <title>Fast Calculation of Calendar Time-, Age- and Duration Dependent Time at
  Risk in the Lexis Space</title>
    <summary>  In epidemiology, the person-years method is broadly used to estimate the
incidence rates of health related events. This needs determination of time at
risk stratified by period, age and sometimes by duration of disease or
exposition. The article describes a fast method for calculating the time at
risk in two- or three-dimensional Lexis diagrams based on Siddon's algorithm.
</summary>
    <author>
      <name>Ralph Brinks</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.0798v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.0798v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.6516v1</id>
    <updated>2012-04-29T20:35:21Z</updated>
    <published>2012-04-29T20:35:21Z</published>
    <title>Detection of additive outliers in Poisson INteger-valued AutoRegressive
  time series</title>
    <summary>  Outlying observations are commonly encountered in the analysis of time
series. In this paper the problem of detecting additive outliers in
integer-valued time series is considered. We show how Gibbs sampling can be
used to detect outlying observations in INAR(1) processes. The methodology
proposed is illustrated using examples as well as an observed data set.
</summary>
    <author>
      <name>Maria Eduarda Silva</name>
    </author>
    <author>
      <name>Isabel Pereira</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.6516v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.6516v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62M, 62M05, 62M010" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.6812v1</id>
    <updated>2012-06-28T19:58:18Z</updated>
    <published>2012-06-28T19:58:18Z</published>
    <title>Stirling's approximations for exchangeable Gibbs weights</title>
    <summary>  We obtain some approximation results for the weights appearing in the
exchangeable partition probability function identifying Gibbs partition models
of parameter $\alpha \in (0,1)$, as introduced in Gnedin and Pitman (2006). We
rely on approximation results for central and non-central generalized Stirling
numbers and on known results for conditional and unconditional $\alpha$
diversity. We provide an application to an approximate Bayesian nonparametric
estimation of discovery probability in species sampling problems under
normalized inverse Gaussian priors.
</summary>
    <author>
      <name>Annalisa Cerquetti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.6812v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.6812v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60G58, 60G09" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.2048v2</id>
    <updated>2013-06-11T05:34:51Z</updated>
    <published>2013-04-07T19:27:56Z</published>
    <title>Bayesian Computational Tools</title>
    <summary>  This chapter surveys advances in the field of Bayesian computation over the
past twenty years, with missing data. It also contains some novel computational
entries on the double-exponential model that may be of interest per se.
</summary>
    <author>
      <name>Christian P. Robert</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Universite Paris-Dauphine</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 10 figures, revision of a paper written as a chapter for
  the Annual Review of Statistics and Its Applications</arxiv:comment>
    <link href="http://arxiv.org/abs/1304.2048v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.2048v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.2129v4</id>
    <updated>2013-10-16T16:47:02Z</updated>
    <published>2013-04-08T08:07:19Z</published>
    <title>A gentle introduction to the discrete Laplace method for estimating
  Y-STR haplotype frequencies</title>
    <summary>  Y-STR data simulated under a Fisher-Wright model of evolution with a
single-step mutation model turns out to be well predicted by a method using
discrete Laplace distributions.
</summary>
    <author>
      <name>Mikkel Meyer Andersen</name>
    </author>
    <author>
      <name>Poul Svante Eriksen</name>
    </author>
    <author>
      <name>Niels Morling</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1304.2129v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.2129v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.1835v1</id>
    <updated>2013-11-06T09:41:27Z</updated>
    <published>2013-11-06T09:41:27Z</published>
    <title>Linear Regression without computing pseudo-inverse matrix</title>
    <summary>  We are presenting a method of linear regression based on Gram-Schmidt
orthogonal projection that does not compute a pseudo-inverse matrix. This is
useful when we want to make several regressions with random data vectors for
simulation purposes.
</summary>
    <author>
      <name>Demetris T. Christopoulos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.1835v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.1835v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="Primary 97K80, Secondary 62J05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.3830v1</id>
    <updated>2013-11-15T12:44:43Z</updated>
    <published>2013-11-15T12:44:43Z</published>
    <title>Applications of geometric discrepancy in numerical analysis and
  statistics</title>
    <summary>  In this paper we discuss various connections between geometric discrepancy
measures, such as discrepancy with respect to convex sets (and convex sets with
smooth boundary in particular), and applications to numerical analysis and
statistics, like point distributions on the sphere, the acceptance-rejection
algorithm and certain Markov chain Monte Carlo algorithms.
</summary>
    <author>
      <name>Josef Dick</name>
    </author>
    <link href="http://arxiv.org/abs/1311.3830v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.3830v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65D30, 65D32" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.7525v1</id>
    <updated>2013-11-29T11:34:37Z</updated>
    <published>2013-11-29T11:34:37Z</published>
    <title>Polynomial regression using trapezoidal rule for computing Legendre
  coefficients</title>
    <summary>  We are presenting a method for computing the Fourier coefficients of a given
polynomial regression by using the trapezoidal rule for numerical integration.
As function basis we use the orthogonal Legendre polynomials. The results are
accurate and stable compared to Forsythe's method.
</summary>
    <author>
      <name>Demetris T. Christopoulos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 2 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.7525v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.7525v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="Primary 62J05, Secondary 65D99" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.5496v1</id>
    <updated>2013-12-19T12:01:18Z</updated>
    <published>2013-12-19T12:01:18Z</published>
    <title>On idiosyncratic stochasticity of financial leverage effects</title>
    <summary>  We model leverage as stochastic but independent of return shocks and of
volatility and perform likelihood-based inference via the recently developed
iterated filtering algorithm using S&amp;P500 data, contributing new evidence to
the still slim empirical support for random leverage variation.
</summary>
    <author>
      <name>Carles Bretó</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Statistics &amp; Probability Letters 91 (2014) 20-26</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1312.5496v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.5496v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.1562v1</id>
    <updated>2014-03-06T20:24:38Z</updated>
    <published>2014-03-06T20:24:38Z</published>
    <title>The Laplace Motion in Phylogenetic Comparative Methods</title>
    <summary>  The majority of current phylogenetic comparative methods assume that the
stochastic evolutionary process is homogeneous over the phylogeny or offer
relaxations of this in rather limited and usually parameter expensive ways.
Here we make a preliminary investigation, by means of a numerical experiment,
whether the Laplace motion process can offer an alternative approach.
</summary>
    <author>
      <name>Krzysztof Bartoszek</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">http://kkzmbm.mimuw.edu.pl/?pageId=4&amp;sprawId=18</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Eighteenth National Conference on Applications
  of Mathematics in Biology and Medicine, 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1403.1562v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.1562v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.3807v1</id>
    <updated>2014-08-17T09:52:06Z</updated>
    <published>2014-08-17T09:52:06Z</published>
    <title>On solving Ordinary Differential Equations using Gaussian Processes</title>
    <summary>  We describe a set of Gaussian Process based approaches that can be used to
solve non-linear Ordinary Differential Equations. We suggest an explicit
probabilistic solver and two implicit methods, one analogous to Picard
iteration and the other to gradient matching. All methods have greater accuracy
than previously suggested Gaussian Process approaches. We also suggest a
general approach that can yield error estimates from any standard ODE solver.
</summary>
    <author>
      <name>David Barber</name>
    </author>
    <link href="http://arxiv.org/abs/1408.3807v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.3807v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.3934v1</id>
    <updated>2014-09-13T11:13:16Z</updated>
    <published>2014-09-13T11:13:16Z</published>
    <title>Efficient subgraph-based sampling of Ising-type models with frustration</title>
    <summary>  Here is proposed a general subgraph-based method for efficiently sampling
certain graphical models, typically using subgraphs of a fixed treewidth, and
also a related method for finding minimum energy (ground) states. In the case
of models with frustration, such as the spin glass, evidence is presented that
this method can be more efficient than traditional single-site update methods.
</summary>
    <author>
      <name>Alex Selby</name>
    </author>
    <link href="http://arxiv.org/abs/1409.3934v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.3934v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="82-04 (Primary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.07039v1</id>
    <updated>2015-01-28T09:20:45Z</updated>
    <published>2015-01-28T09:20:45Z</published>
    <title>Chaotic Boltzmann machines with two elements</title>
    <summary>  In this brief note, we show that chaotic Boltzmann machines truly yield
samples from the probabilistic distribution of the corresponding Boltzmann
machines if they are composed of only two elements. This note is an English
translation (with slight modifications) of the article originally written in
Japanese [H. Suzuki, Seisan Kenkyu 66 (2014), 315-316].
</summary>
    <author>
      <name>Hideyuki Suzuki</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.11188/seisankenkyu.66.315</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.11188/seisankenkyu.66.315" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Seisan Kenkyu 66 (2014), 315-316 (in Japanese)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1501.07039v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.07039v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.01527v1</id>
    <updated>2015-02-05T13:10:43Z</updated>
    <published>2015-02-05T13:10:43Z</published>
    <title>Some comments about A. Ronald Gallant's "Reflections on the Probability
  Space Induced by Moment Conditions with Implications for Bayesian Inference"</title>
    <summary>  This note is commenting on Ronald Gallant's (2015) reflections on the
construction of Bayesian prior distributions from moment conditions. The main
conclusion is that the paper does not deliver a working principle that could
justify inference based on such priors.
</summary>
    <author>
      <name>Christian P. Robert</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Université Paris-Dauphine, University of Warwick, and CREST</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, submitted to the Journal of Financial Econometrics for the
  discussion of Gallant (2015)</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.01527v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.01527v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.04662v1</id>
    <updated>2015-03-16T14:19:21Z</updated>
    <published>2015-03-16T14:19:21Z</published>
    <title>Bayesian Essentials with R: The Complete Solution Manual</title>
    <summary>  This is the collection of solutions for all the exercises proposed in
Bayesian Essentials with R (2014).
</summary>
    <author>
      <name>Christian P. Robert</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Universite Paris-Dauphine and University of Warwick</arxiv:affiliation>
    </author>
    <author>
      <name>Jean-Michel Marin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Universite de Montpellier</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">117 pages, 124 exercises, 22 figures. arXiv admin note: substantial
  text overlap with arXiv:0910.4696</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.04662v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.04662v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.06410v1</id>
    <updated>2015-03-22T11:32:34Z</updated>
    <published>2015-03-22T11:32:34Z</published>
    <title>What the F-measure doesn't measure: Features, Flaws, Fallacies and Fixes</title>
    <summary>  The F-measure or F-score is one of the most commonly used single number
measures in Information Retrieval, Natural Language Processing and Machine
Learning, but it is based on a mistake, and the flawed assumptions render it
unsuitable for use in most contexts! Fortunately, there are better
alternatives.
</summary>
    <author>
      <name>David M. W. Powers</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.06410v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.06410v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.03030v1</id>
    <updated>2015-05-12T14:41:08Z</updated>
    <published>2015-05-12T14:41:08Z</published>
    <title>On the Exact Simulation of (Jump) Diffusion Bridges</title>
    <summary>  In this paper we outline methodology to efficiently simulate (jump) diffusion
bridge sample paths without discretisation error. We achieve this by
considering the simulation of conditioned (jump) diffusion bridge sample paths
in light of recent work developing a mathematical framework for simulating
finite dimensional sample path skeletons (which flexibly characterise the
entirety of sample paths).
</summary>
    <author>
      <name>Murray Pollock</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.03030v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.03030v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.CP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.03251v1</id>
    <updated>2015-12-10T13:42:35Z</updated>
    <published>2015-12-10T13:42:35Z</published>
    <title>Histogram Arithmetic under Uncertainty of Probability Density Function</title>
    <summary>  In this article we propose a method of performing arithmetic operations on
varia-bles with unknown distribution. The approach to the evaluation results of
arithme-tic operations can select probability intervals of the algebraic
equations and their systems solutions, of differential equations and their
systems in case of histogram evaluation of the empirical density distributions
of random parameters.
</summary>
    <author>
      <name>V. N. Petrushin</name>
    </author>
    <author>
      <name>E. V. Nikulchev</name>
    </author>
    <author>
      <name>D. A. Korolev</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.12988/ams.2015.510644</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.12988/ams.2015.510644" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Applied Mathematical Sciences 9(2015) 7043-7052</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1512.03251v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.03251v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.08732v1</id>
    <updated>2016-05-27T18:07:10Z</updated>
    <published>2016-05-27T18:07:10Z</published>
    <title>Computing the Bergsma Dassios sign-covariance</title>
    <summary>  Bergsma and Dassios (2014) introduced an independence measure which is zero
if and only if two random variables are independent. This measure can be
naively calculated in $O(n^4)$. Weihs et al. (2015) showed that it can be
calculated in $O(n^2 \log n)$. In this note we will show that using the methods
described in Heller et al. (2016), the measure can easily be calculated in only
$O(n^2)$.
</summary>
    <author>
      <name>Yair Heller</name>
    </author>
    <author>
      <name>Ruth Heller</name>
    </author>
    <link href="http://arxiv.org/abs/1605.08732v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.08732v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.02391v1</id>
    <updated>2016-07-08T14:47:24Z</updated>
    <published>2016-07-08T14:47:24Z</published>
    <title>Estimation of the global regularity of a multifractional Brownian motion</title>
    <summary>  This paper presents a new estimator of the global regularity index of a
multifractional Brownian motion. Our estimation method is based upon a ratio
statistic, which compares the realized global quadratic variation of a
multifractional Brownian motion at two different frequencies. We show that a
logarithmic transformation of this statistic converges in probability to the
minimum of the Hurst function, which is, under weak assumptions, identical to
the global regularity index of the path.
</summary>
    <author>
      <name>Joachim Lebovits</name>
    </author>
    <author>
      <name>Mark Podolskij</name>
    </author>
    <link href="http://arxiv.org/abs/1607.02391v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.02391v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.03054v1</id>
    <updated>2016-12-09T15:21:46Z</updated>
    <published>2016-12-09T15:21:46Z</published>
    <title>DERGMs: Degeneracy-restricted exponential random graph models</title>
    <summary>  We propose a new exponential family of models for random graphs. Starting
from the standard exponential random graph model (ERGM) framework, we propose
an extension that addresses some of the well-known issues with ERGMs.
Specifically, we solve the problem of computational intractability and
`degenerate' model behavior by an interpretable support restriction.
</summary>
    <author>
      <name>Vishesh Karwa</name>
    </author>
    <author>
      <name>Sonja Petrović</name>
    </author>
    <author>
      <name>Denis Bajić</name>
    </author>
    <link href="http://arxiv.org/abs/1612.03054v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.03054v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.02522v1</id>
    <updated>2017-01-10T11:33:41Z</updated>
    <published>2017-01-10T11:33:41Z</published>
    <title>Magnus expansions and pseudospectra of Master Equations</title>
    <summary>  New directions in research on master equations are showcased by example.
Magnus expansions, time-varying rates, and pseudospectra are highlighted. Exact
eigenvalues are found and contrasted with the large errors produced by standard
numerical methods in some cases. Isomerisation provides a running example and
an illustrative application to chemical kinetics. We also give a brief example
of the totally asymmetric exclusion process.
</summary>
    <author>
      <name>Arieh Iserles</name>
    </author>
    <author>
      <name>Shev MacNamara</name>
    </author>
    <link href="http://arxiv.org/abs/1701.02522v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.02522v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.09570v1</id>
    <updated>2017-03-27T02:18:36Z</updated>
    <published>2017-03-27T02:18:36Z</published>
    <title>A Tidy Data Model for Natural Language Processing using cleanNLP</title>
    <summary>  The package cleanNLP provides a set of fast tools for converting a textual
corpus into a set of normalized tables. The underlying natural language
processing pipeline utilizes Stanford's CoreNLP library, exposing a number of
annotation tasks for text written in English, French, German, and Spanish.
Annotators include tokenization, part of speech tagging, named entity
recognition, entity linking, sentiment analysis, dependency parsing,
coreference resolution, and information extraction.
</summary>
    <author>
      <name>Taylor Arnold</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages; 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.09570v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.09570v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.01795v1</id>
    <updated>2017-05-04T11:01:50Z</updated>
    <published>2017-05-04T11:01:50Z</published>
    <title>Análisis econométrico de series temporales en Gretl: La Ley de Okun</title>
    <summary>  Gretl is an econometrics package, including a shared library, a command-line
client program and a graphical user interface which offers an intuitive user
interface. This paper explains the Okun's Law, which is an empirically observed
relationship between unemployment and losses in a country's production, with
spanish data processed in Gretl.
</summary>
    <author>
      <name>Eduardo Calvo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in French. JEL Classification: C22, C51, E24</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.01795v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.01795v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.02940v1</id>
    <updated>2017-06-09T13:12:27Z</updated>
    <published>2017-06-09T13:12:27Z</published>
    <title>Bayesian nonparametrics for stochastic epidemic models</title>
    <summary>  The vast majority of models for the spread of communicable diseases are
parametric in nature and involve underlying assumptions about how the disease
spreads through a population. In this article we consider the use of Bayesian
nonparametric approaches to analysing data from disease outbreaks. Specifically
we focus on methods for estimating the infection process in simple models under
the assumption that this process has an explicit time-dependence.
</summary>
    <author>
      <name>Theodore Kypraios</name>
    </author>
    <author>
      <name>Philip D. O'Neill</name>
    </author>
    <link href="http://arxiv.org/abs/1706.02940v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.02940v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.09974v1</id>
    <updated>2017-07-31T17:40:40Z</updated>
    <published>2017-07-31T17:40:40Z</published>
    <title>Some variations of EM algorithms for Marshall-Olkin bivariate Pareto
  distribution with location and scale</title>
    <summary>  Recently Asimit et. al used an EM algorithm to estimate Marshall-Olkin
bivariate Pareto distribution. The distribution has seven parameters. We
describe few alternative approaches of EM algorithm. A numerical simulation is
performed to verify the performance of different proposed algorithms. A
real-life data analysis is also shown for illustrative purposes.
</summary>
    <author>
      <name>Arabin Kumar Dey</name>
    </author>
    <author>
      <name>Biplab Paul</name>
    </author>
    <link href="http://arxiv.org/abs/1707.09974v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.09974v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.05515v1</id>
    <updated>2017-09-16T14:12:20Z</updated>
    <published>2017-09-16T14:12:20Z</published>
    <title>Some variations on Random Survival Forest with application to Cancer
  Research</title>
    <summary>  Random survival forest can be extremely time consuming for large data set. In
this paper we propose few computationally efficient algorithms in prediction of
survival function. We explore the behavior of the algorithms for different
cancer data sets. Our construction includes right censoring data too. We have
also applied the same for competing risk survival function.
</summary>
    <author>
      <name>Arabin Kumar Dey</name>
    </author>
    <author>
      <name>Anshul Juneja</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages; 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.05515v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.05515v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0706.0787v2</id>
    <updated>2009-01-16T15:47:31Z</updated>
    <published>2007-06-06T09:08:43Z</published>
    <title>Construction of Bayesian Deformable Models via Stochastic Approximation
  Algorithm: A Convergence Study</title>
    <summary>  The problem of the definition and the estimation of generative models based
on deformable templates from raw data is of particular importance for modelling
non aligned data affected by various types of geometrical variability. This is
especially true in shape modelling in the computer vision community or in
probabilistic atlas building for Computational Anatomy (CA). A first coherent
statistical framework modelling the geometrical variability as hidden variables
has been given by Allassonni\`ere, Amit and Trouv\'e (JRSS 2006). Setting the
problem in a Bayesian context they proved the consistency of the MAP estimator
and provided a simple iterative deterministic algorithm with an EM flavour
leading to some reasonable approximations of the MAP estimator under low noise
conditions. In this paper we present a stochastic algorithm for approximating
the MAP estimator in the spirit of the SAEM algorithm. We prove its convergence
to a critical point of the observed likelihood with an illustration on images
of handwritten digits.
</summary>
    <author>
      <name>Stéphanie Allassonnière</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CMAP</arxiv:affiliation>
    </author>
    <author>
      <name>Estelle Kuhn</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAGA</arxiv:affiliation>
    </author>
    <author>
      <name>Alain Trouvé</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CMLA</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/0706.0787v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0706.0787v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60J22, 62F10, 62F15, 62M40." scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0706.3443v1</id>
    <updated>2007-06-23T09:45:48Z</updated>
    <published>2007-06-23T09:45:48Z</published>
    <title>The SSM Toolbox for Matlab</title>
    <summary>  State Space Models (SSM) is a MATLAB 7.0 software toolbox for doing time
series analysis by state space methods. The software features fully interactive
construction and combination of models, with support for univariate and
multivariate models, complex time-varying (dynamic) models, non-Gaussian
models, and various standard models such as ARIMA and structural time-series
models. The software includes standard functions for Kalman filtering and
smoothing, simulation smoothing, likelihood evaluation, parameter estimation,
signal extraction and forecasting, with incorporation of exact initialization
for filters and smoothers, and support for missing observations and multiple
time series input with common analysis structure. The software also includes
implementations of TRAMO model selection and Hillmer-Tiao decomposition for
ARIMA models. The software will provide a general toolbox for doing time series
analysis on the MATLAB platform, allowing users to take advantage of its
readily available graph plotting and general matrix computation capabilities.
</summary>
    <author>
      <name>Jyh-Ying Peng</name>
    </author>
    <author>
      <name>John A. D. Aston</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Software available from authors</arxiv:comment>
    <link href="http://arxiv.org/abs/0706.3443v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0706.3443v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0707.0167v1</id>
    <updated>2007-07-02T08:39:27Z</updated>
    <published>2007-07-02T08:39:27Z</published>
    <title>The random Tukey depth</title>
    <summary>  The computation of the Tukey depth, also called halfspace depth, is very
demanding, even in low dimensional spaces, because it requires the
consideration of all possible one-dimensional projections. In this paper we
propose a random depth which approximates the Tukey depth. It only takes into
account a finite number of one-dimensional projections which are chosen at
random. Thus, this random depth requires a very small computation time even in
high dimensional spaces. Moreover, it is easily extended to cover the
functional framework.
  We present some simulations indicating how many projections should be
considered depending on the sample size and on the dimension of the sample
space. We also compare this depth with some others proposed in the literature.
It is noteworthy that the random depth, based on a very low number of
projections, obtains results very similar to those obtained with other depths.
</summary>
    <author>
      <name>J. A. Cuesta-Albertos</name>
    </author>
    <author>
      <name>A. Nieto-Reyes</name>
    </author>
    <link href="http://arxiv.org/abs/0707.0167v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0707.0167v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0708.1485v2</id>
    <updated>2007-12-14T07:44:22Z</updated>
    <published>2007-08-10T16:54:30Z</published>
    <title>Pathwise coordinate optimization</title>
    <summary>  We consider ``one-at-a-time'' coordinate-wise descent algorithms for a class
of convex optimization problems. An algorithm of this kind has been proposed
for the $L_1$-penalized regression (lasso) in the literature, but it seems to
have been largely ignored. Indeed, it seems that coordinate-wise algorithms are
not often used in convex optimization. We show that this algorithm is very
competitive with the well-known LARS (or homotopy) procedure in large lasso
problems, and that it can be applied to related methods such as the garotte and
elastic net. It turns out that coordinate-wise descent does not work in the
``fused lasso,'' however, so we derive a generalized algorithm that yields the
solution in much less time that a standard convex optimizer. Finally, we
generalize the procedure to the two-dimensional fused lasso, and demonstrate
its performance on some image smoothing problems.
</summary>
    <author>
      <name>Jerome Friedman</name>
    </author>
    <author>
      <name>Trevor Hastie</name>
    </author>
    <author>
      <name>Holger Höfling</name>
    </author>
    <author>
      <name>Robert Tibshirani</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1214/07-AOAS131</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1214/07-AOAS131" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in at http://dx.doi.org/10.1214/07-AOAS131 the Annals of
  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Annals of Applied Statistics 2007, Vol. 1, No. 2, 302-332</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0708.1485v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0708.1485v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0708.4152v1</id>
    <updated>2007-08-30T13:06:33Z</updated>
    <published>2007-08-30T13:06:33Z</published>
    <title>Estimation in hidden Markov models via efficient importance sampling</title>
    <summary>  Given a sequence of observations from a discrete-time, finite-state hidden
Markov model, we would like to estimate the sampling distribution of a
statistic. The bootstrap method is employed to approximate the confidence
regions of a multi-dimensional parameter. We propose an importance sampling
formula for efficient simulation in this context. Our approach consists of
constructing a locally asymptotically normal (LAN) family of probability
distributions around the default resampling rule and then minimizing the
asymptotic variance within the LAN family. The solution of this minimization
problem characterizes the asymptotically optimal resampling scheme, which is
given by a tilting formula. The implementation of the tilting formula is
facilitated by solving a Poisson equation. A few numerical examples are given
to demonstrate the efficiency of the proposed importance sampling scheme.
</summary>
    <author>
      <name>Cheng-Der Fuh</name>
    </author>
    <author>
      <name>Inchi Hu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3150/07--BEJ5163</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3150/07--BEJ5163" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at http://dx.doi.org/10.3150/07--BEJ5163 in the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Bernoulli 2007, Vol. 13, No. 2, 492-513</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0708.4152v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0708.4152v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0709.1309v1</id>
    <updated>2007-09-10T04:31:05Z</updated>
    <published>2007-09-10T04:31:05Z</published>
    <title>Bayes and empirical Bayes changepoint problems</title>
    <summary>  We generalize the approach of Liu and Lawrence (1999) for multiple
changepoint problems where the number of changepoints is unknown. The approach
is based on dynamic programming recursion for efficient calculation of the
marginal probability of the data with the hidden parameters integrated out. For
the estimation of the hyperparameters, we propose to use Monte Carlo EM when
training data are available. We argue that there is some advantages of using
samples from the posterior which takes into account the uncertainty of the
changepoints, compared to the traditional MAP estimator, which is also more
expensive to compute in this context. The samples from the posterior obtained
by our algorithm are independent, getting rid of the convergence issue
associated with the MCMC approach. We illustrate our approach on limited
simulations and some real data set.
</summary>
    <author>
      <name>Heng Lian</name>
    </author>
    <link href="http://arxiv.org/abs/0709.1309v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0709.1309v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4242v4</id>
    <updated>2008-05-30T08:38:00Z</updated>
    <published>2007-10-23T11:23:46Z</published>
    <title>Adaptive Importance Sampling in General Mixture Classes</title>
    <summary>  In this paper, we propose an adaptive algorithm that iteratively updates both
the weights and component parameters of a mixture importance sampling density
so as to optimise the importance sampling performances, as measured by an
entropy criterion. The method is shown to be applicable to a wide class of
importance sampling densities, which includes in particular mixtures of
multivariate Student t distributions. The performances of the proposed scheme
are studied on both artificial and real examples, highlighting in particular
the benefit of a novel Rao-Blackwellisation device which can be easily
incorporated in the updating scheme.
</summary>
    <author>
      <name>Olivier Cappé</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LTCI</arxiv:affiliation>
    </author>
    <author>
      <name>Randal Douc</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CMAP</arxiv:affiliation>
    </author>
    <author>
      <name>Arnaud Guillin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LATP</arxiv:affiliation>
    </author>
    <author>
      <name>Jean-Michel Marin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Futurs</arxiv:affiliation>
    </author>
    <author>
      <name>Christian P. Robert</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CEREMADE</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11222-008-9059-x</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11222-008-9059-x" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Removed misleading comment in Section 2</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Statistics and Computing 18, 4 (2008) 447-459</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4242v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4242v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0711.0186v1</id>
    <updated>2007-11-01T18:57:06Z</updated>
    <published>2007-11-01T18:57:06Z</published>
    <title>Population-Based Reversible Jump Markov Chain Monte Carlo</title>
    <summary>  In this paper we present an extension of population-based Markov chain Monte
Carlo (MCMC) to the trans-dimensional case. One of the main challenges in
MCMC-based inference is that of simulating from high and trans-dimensional
target measures. In such cases, MCMC methods may not adequately traverse the
support of the target; the simulation results will be unreliable. We develop
population methods to deal with such problems, and give a result proving the
uniform ergodicity of these population algorithms, under mild assumptions. This
result is used to demonstrate the superiority, in terms of convergence rate, of
a population transition kernel over a reversible jump sampler for a Bayesian
variable selection problem. We also give an example of a population algorithm
for a Bayesian multivariate mixture model with an unknown number of components.
This is applied to gene expression data of 1000 data points in six dimensions
and it is demonstrated that our algorithm out performs some competing Markov
chain samplers.
</summary>
    <author>
      <name>Ajay Jasra</name>
    </author>
    <author>
      <name>David A. Stephens</name>
    </author>
    <author>
      <name>Chris C. Holmes</name>
    </author>
    <link href="http://arxiv.org/abs/0711.0186v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0711.0186v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0712.3744v4</id>
    <updated>2010-06-13T07:14:37Z</updated>
    <published>2007-12-21T16:39:42Z</published>
    <title>Convergence properties of the expected improvement algorithm</title>
    <summary>  This paper has been withdrawn from the arXiv. It is now published by Elsevier
in the Journal of Statistical Planning and Inference, under the modified title
"Convergence properties of the expected improvement algorithm with fixed mean
and covariance functions". See http://dx.doi.org/10.1016/j.jspi.2010.04.018
  An author-generated post-print version is available from the HAL repository
of SUPELEC at http://hal-supelec.archives-ouvertes.fr/hal-00217562
  Abstract : "This paper deals with the convergence of the expected improvement
algorithm, a popular global optimization algorithm based on a Gaussian process
model of the function to be optimized. The first result is that under some mild
hypotheses on the covariance function k of the Gaussian process, the expected
improvement algorithm produces a dense sequence of evaluation points in the
search domain, when the function to be optimized is in the reproducing kernel
Hilbert space generated by k. The second result states that the density
property also holds for P-almost all continuous functions, where P is the
(prior) probability distribution induced by the Gaussian process."
</summary>
    <author>
      <name>Emmanuel Vazquez</name>
    </author>
    <author>
      <name>Julien Bect</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn</arxiv:comment>
    <link href="http://arxiv.org/abs/0712.3744v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0712.3744v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0712.4273v4</id>
    <updated>2017-03-01T13:40:32Z</updated>
    <published>2007-12-27T19:44:34Z</published>
    <title>Online EM Algorithm for Latent Data Models</title>
    <summary>  In this contribution, we propose a generic online (also sometimes called
adaptive or recursive) version of the Expectation-Maximisation (EM) algorithm
applicable to latent variable models of independent observations. Compared to
the algorithm of Titterington (1984), this approach is more directly connected
to the usual EM algorithm and does not rely on integration with respect to the
complete data distribution. The resulting algorithm is usually simpler and is
shown to achieve convergence to the stationary points of the Kullback-Leibler
divergence between the marginal distribution of the observation and the model
distribution at the optimal rate, i.e., that of the maximum likelihood
estimator. In addition, the proposed approach is also suitable for conditional
(or regression) models, as illustrated in the case of the mixture of linear
regressions model.
</summary>
    <author>
      <name>Olivier Cappé</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LTCI</arxiv:affiliation>
    </author>
    <author>
      <name>Eric Moulines</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LTCI</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1111/j.1467-9868.2009.00698.x</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1111/j.1467-9868.2009.00698.x" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Version that includes the corrigendum published in volume 73, part 5
  (2011), of the Journal of the Royal Statistical Society, Series B + the
  correction of a typo in Eqs. (32-33)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of the Royal Statistical Society: Series B, Royal
  Statistical Society, 2009, 71 (3), pp.593-613</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0712.4273v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0712.4273v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0801.0499v7</id>
    <updated>2011-09-15T03:51:07Z</updated>
    <published>2008-01-03T10:29:09Z</published>
    <title>Adjusted Bayesian inference for selected parameters</title>
    <summary>  We address the problem of providing inference from a Bayesian perspective for
parameters selected after viewing the data. We present a Bayesian framework for
providing inference for selected parameters, based on the observation that
providing Bayesian inference for selected parameters is a truncated data
problem. We show that if the prior for the parameter is non-informative, or if
the parameter is a "fixed" unknown constant, then it is necessary to adjust the
Bayesian inference for selection. Our second contribution is the introduction
of Bayesian False Discovery Rate controlling methodology,which generalizes
existing Bayesian FDR methods that are only defined in the two-group mixture
model.We illustrate our results by applying them to simulated data and data
froma microarray experiment.
</summary>
    <author>
      <name>Daniel Yekutieli</name>
    </author>
    <link href="http://arxiv.org/abs/0801.0499v7" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.0499v7" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0801.2748v1</id>
    <updated>2008-01-17T18:53:07Z</updated>
    <published>2008-01-17T18:53:07Z</published>
    <title>A greedy approach to sparse canonical correlation analysis</title>
    <summary>  We consider the problem of sparse canonical correlation analysis (CCA), i.e.,
the search for two linear combinations, one for each multivariate, that yield
maximum correlation using a specified number of variables. We propose an
efficient numerical approximation based on a direct greedy approach which
bounds the correlation at each stage. The method is specifically designed to
cope with large data sets and its computational complexity depends only on the
sparsity levels. We analyze the algorithm's performance through the tradeoff
between correlation and parsimony. The results of numerical simulation suggest
that a significant portion of the correlation may be captured using a
relatively small number of variables. In addition, we examine the use of sparse
CCA as a regularization method when the number of available samples is small
compared to the dimensions of the multivariates.
</summary>
    <author>
      <name>Ami Wiesel</name>
    </author>
    <author>
      <name>Mark Kliger</name>
    </author>
    <author>
      <name>Alfred O. Hero III</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0801.2748v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.2748v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0801.3513v5</id>
    <updated>2008-06-05T07:04:06Z</updated>
    <published>2008-01-23T07:26:00Z</published>
    <title>On some difficulties with a posterior probability approximation
  technique</title>
    <summary>  In Scott (2002) and Congdon (2006), a new method is advanced to compute
posterior probabilities of models under consideration. It is based solely on
MCMC outputs restricted to single models, i.e., it is bypassing reversible jump
and other model exploration techniques. While it is indeed possible to
approximate posterior probabilities based solely on MCMC outputs from single
models, as demonstrated by Gelfand and Dey (1994) and Bartolucci et al. (2006),
we show that the proposals of Scott (2002) and Congdon (2006) are biased and
advance several arguments towards this thesis, the primary one being the
confusion between model-based posteriors and joint pseudo-posteriors. From a
practical point of view, the bias in Scott's (2002) approximation appears to be
much more severe than the one in Congdon's (2006), the later being often of the
same magnitude as the posterior probability it approximates, although we also
exhibit an example where the divergence from the true posterior probability is
extreme.
</summary>
    <author>
      <name>Christian Robert</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CEREMADE</arxiv:affiliation>
    </author>
    <author>
      <name>Jean-Michel Marin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Futurs</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1214/08-BA316</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1214/08-BA316" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Second version, resubmitted</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Bayesian Analysis(2008), 3(2), 427-442</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0801.3513v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.3513v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0801.3552v3</id>
    <updated>2010-11-07T14:19:40Z</updated>
    <published>2008-01-23T12:55:29Z</published>
    <title>A statistical analysis of probabilistic counting algorithms</title>
    <summary>  This paper considers the problem of cardinality estimation in data stream
applications. We present a statistical analysis of probabilistic counting
algorithms, focusing on two techniques that use pseudo-random variates to form
low-dimensional data sketches. We apply conventional statistical methods to
compare probabilistic algorithms based on storing either selected order
statistics, or random projections. We derive estimators of the cardinality in
both cases, and show that the maximal-term estimator is recursively computable
and has exponentially decreasing error bounds. Furthermore, we show that the
estimators have comparable asymptotic efficiency, and explain this result by
demonstrating an unexpected connection between the two approaches.
</summary>
    <author>
      <name>Peter Clifford</name>
    </author>
    <author>
      <name>Ioana A. Cosma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 0 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Scandinavian Journal of Statistics, 39, 1, 1-14, 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0801.3552v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.3552v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0801.3559v1</id>
    <updated>2008-01-23T12:00:10Z</updated>
    <published>2008-01-23T12:00:10Z</published>
    <title>Efficient l_{alpha} Distance Approximation for High Dimensional Data
  Using alpha-Stable Projection</title>
    <summary>  In recent years, large high-dimensional data sets have become commonplace in
a wide range of applications in science and commerce. Techniques for dimension
reduction are of primary concern in statistical analysis. Projection methods
play an important role. We investigate the use of projection algorithms that
exploit properties of the alpha-stable distributions. We show that l_{alpha}
distances and quasi-distances can be recovered from random projections with
full statistical efficiency by L-estimation. The computational requirements of
our algorithm are modest; after a once-and-for-all calculation to determine an
array of length k, the algorithm runs in O(k) time for each distance, where k
is the reduced dimension of the projection.
</summary>
    <author>
      <name>Peter Clifford</name>
    </author>
    <author>
      <name>Ioana A. Cosma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures, submitted to COMPSTAT2008</arxiv:comment>
    <link href="http://arxiv.org/abs/0801.3559v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.3559v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0801.3887v4</id>
    <updated>2009-07-10T20:40:26Z</updated>
    <published>2008-01-25T07:25:25Z</published>
    <title>Properties of Nested Sampling</title>
    <summary>  Nested sampling is a simulation method for approximating marginal likelihoods
proposed by Skilling (2006). We establish that nested sampling has an
approximation error that vanishes at the standard Monte Carlo rate and that
this error is asymptotically Gaussian. We show that the asymptotic variance of
the nested sampling approximation typically grows linearly with the dimension
of the parameter. We discuss the applicability and efficiency of nested
sampling in realistic problems, and we compare it with two current methods for
computing marginal likelihood. We propose an extension that avoids resorting to
Markov chain Monte Carlo to obtain the simulated points.
</summary>
    <author>
      <name>Nicolas Chopin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CREST</arxiv:affiliation>
    </author>
    <author>
      <name>Christian Robert</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CREST, Ceremade</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1093/biomet/asq021</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1093/biomet/asq021" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Revision submitted to Biometrika</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Biometrika 97(3):741-755, 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0801.3887v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.3887v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0802.1357v1</id>
    <updated>2008-02-10T23:28:34Z</updated>
    <published>2008-02-10T23:28:34Z</published>
    <title>A Bayesian reassessment of nearest-neighbour classification</title>
    <summary>  The k-nearest-neighbour procedure is a well-known deterministic method used
in supervised classification. This paper proposes a reassessment of this
approach as a statistical technique derived from a proper probabilistic model;
in particular, we modify the assessment made in a previous analysis of this
method undertaken by Holmes and Adams (2002,2003), and evaluated by Manocha and
Girolami (2007), where the underlying probabilistic model is not completely
well-defined. Once a clear probabilistic basis for the k-nearest-neighbour
procedure is established, we derive computational tools for conducting Bayesian
inference on the parameters of the corresponding model. In particular, we
assess the difficulties inherent to pseudo-likelihood and to path sampling
approximations of an intractable normalising constant, and propose a perfect
sampling strategy to implement a correct MCMC sampler associated with our
model. If perfect sampling is not available, we suggest using a Gibbs sampling
approximation. Illustrations of the performance of the corresponding Bayesian
classifier are provided for several benchmark datasets, demonstrating in
particular the limitations of the pseudo-likelihood approximation in this
set-up.
</summary>
    <author>
      <name>Lionel Cucala</name>
    </author>
    <author>
      <name>Jean-Michel Marin</name>
    </author>
    <author>
      <name>Christian Robert</name>
    </author>
    <author>
      <name>Mike Titterington</name>
    </author>
    <link href="http://arxiv.org/abs/0802.1357v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0802.1357v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0802.1521v2</id>
    <updated>2009-01-16T15:48:38Z</updated>
    <published>2008-02-11T20:08:27Z</published>
    <title>Stochastic Algorithm For Parameter Estimation For Dense Deformable
  Template Mixture Model</title>
    <summary>  Estimating probabilistic deformable template models is a new approach in the
fields of computer vision and probabilistic atlases in computational anatomy. A
first coherent statistical framework modelling the variability as a hidden
random variable has been given by Allassonni\`ere, Amit and Trouv\'e in [1] in
simple and mixture of deformable template models. A consistent stochastic
algorithm has been introduced in [2] to face the problem encountered in [1] for
the convergence of the estimation algorithm for the one component model in the
presence of noise. We propose here to go on in this direction of using some
"SAEM-like" algorithm to approximate the MAP estimator in the general Bayesian
setting of mixture of deformable template model. We also prove the convergence
of this algorithm toward a critical point of the penalised likelihood of the
observations and illustrate this with handwritten digit images.
</summary>
    <author>
      <name>Stéphanie Allassonnière</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CMAP</arxiv:affiliation>
    </author>
    <author>
      <name>Estelle Kuhn</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAGA</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/0802.1521v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0802.1521v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0803.0054v2</id>
    <updated>2008-08-23T06:46:35Z</updated>
    <published>2008-03-01T08:33:24Z</published>
    <title>Adaptive methods for sequential importance sampling with application to
  state space models</title>
    <summary>  In this paper we discuss new adaptive proposal strategies for sequential
Monte Carlo algorithms--also known as particle filters--relying on criteria
evaluating the quality of the proposed particles. The choice of the proposal
distribution is a major concern and can dramatically influence the quality of
the estimates. Thus, we show how the long-used coefficient of variation of the
weights can be used for estimating the chi-square distance between the target
and instrumental distributions of the auxiliary particle filter. As a
by-product of this analysis we obtain an auxiliary adjustment multiplier weight
type for which this chi-square distance is minimal. Moreover, we establish an
empirical estimate of linear complexity of the Kullback-Leibler divergence
between the involved distributions. Guided by these results, we discuss
adaptive designing of the particle filter proposal distribution and illustrate
the methods on a numerical example.
</summary>
    <author>
      <name>Julien Cornebise</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LTCI</arxiv:affiliation>
    </author>
    <author>
      <name>Eric Moulines</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LTCI</arxiv:affiliation>
    </author>
    <author>
      <name>Jimmy Olsson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint of the article to be published in Statistics and Comptuing.
  36 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0803.0054v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0803.0054v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0803.0525v1</id>
    <updated>2008-03-04T19:31:23Z</updated>
    <published>2008-03-04T19:31:23Z</published>
    <title>An EM algorithm for estimation in the Mixture Transition Distribution
  model</title>
    <summary>  The Mixture Transition Distribution (MTD) model was introduced by Raftery to
face the need for parsimony in the modeling of high-order Markov chains in
discrete time. The particularity of this model comes from the fact that the
effect of each lag upon the present is considered separately and additively, so
that the number of parameters required is drastically reduced. However, the
efficiency for the MTD parameter estimations proposed up to date still remains
problematic on account of the large number of constraints on the parameters. In
this paper, an iterative procedure, commonly known as Expectation-Maximization
(EM) algorithm, is developed cooperating with the principle of Maximum
Likelihood Estimation (MLE) to estimate the MTD parameters. Some applications
of modeling MTD show the proposed EM algorithm is easier to be used than the
algorithm developed by Berchtold. Moreover, the EM Estimations of parameters
for high-order MTD models led on DNA sequences outperform the corresponding
fully parametrized Markov chain in terms of Bayesian Information Criterion. A
software implementation of our algorithm is available in the library seq++ at
http://stat.genopole.cnrs.fr/seqpp
</summary>
    <author>
      <name>Sophie Lèbre</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">SG</arxiv:affiliation>
    </author>
    <author>
      <name>Pierre-Yves Bourguinon</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">SG</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Statistical Computation and Simulation (2008) ?</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0803.0525v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0803.0525v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0804.3152v1</id>
    <updated>2008-04-21T11:07:23Z</updated>
    <published>2008-04-21T11:07:23Z</published>
    <title>Bayesian computation for statistical models with intractable normalizing
  constants</title>
    <summary>  This paper deals with some computational aspects in the Bayesian analysis of
statistical models with intractable normalizing constants. In the presence of
intractable normalizing constants in the likelihood function, traditional MCMC
methods cannot be applied. We propose an approach to sample from such posterior
distributions. The method can be thought as a Bayesian version of the MCMC-MLE
approach of Geyer and Thompson (1992). To the best of our knowledge, this is
the first general and asymptotically consistent Monte Carlo method for such
problems. We illustrate the method with examples from image segmentation and
social network modeling. We study as well the asymptotic behavior of the
algorithm and obtain a strong law of large numbers for empirical averages.
</summary>
    <author>
      <name>Yves Atchade</name>
    </author>
    <author>
      <name>Nicolas Lartillot</name>
    </author>
    <author>
      <name>Christian P. Robert</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 4 figures, submitted for publication</arxiv:comment>
    <link href="http://arxiv.org/abs/0804.3152v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0804.3152v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0805.1971v2</id>
    <updated>2008-12-12T09:15:21Z</updated>
    <published>2008-05-14T06:30:55Z</published>
    <title>Confidence regions for the multinomial parameter with small sample size</title>
    <summary>  Consider the observation of n iid realizations of an experiment with d&gt;1
possible outcomes, which corresponds to a single observation of a multinomial
distribution M(n,p) where p is an unknown discrete distribution on {1,...,d}.
In many applications, the construction of a confidence region for p when n is
small is crucial. This concrete challenging problem has a long history. It is
well known that the confidence regions built from asymptotic statistics do not
have good coverage when n is small. On the other hand, most available methods
providing non-asymptotic regions with controlled coverage are limited to the
binomial case d=2. In the present work, we propose a new method valid for any
d&gt;1. This method provides confidence regions with controlled coverage and small
volume, and consists of the inversion of the "covering collection"' associated
with level-sets of the likelihood. The behavior when d/n tends to infinity
remains an interesting open problem beyond the scope of this work.
</summary>
    <author>
      <name>Djalil Chafai</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UPTE, IMT</arxiv:affiliation>
    </author>
    <author>
      <name>Didier Concordet</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UPTE, IMT</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1198/jasa.2009.tm08152</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1198/jasa.2009.tm08152" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in Journal of the American Statistical
  Association (JASA)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of the American Statistical Association 104, 1071-1079
  (2009)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0805.1971v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0805.1971v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0805.2256v9</id>
    <updated>2009-03-28T14:03:51Z</updated>
    <published>2008-05-15T12:34:17Z</published>
    <title>Adaptive approximate Bayesian computation</title>
    <summary>  Sequential techniques can enhance the efficiency of the approximate Bayesian
computation algorithm, as in Sisson et al.'s (2007) partial rejection control
version. While this method is based upon the theoretical works of Del Moral et
al. (2006), the application to approximate Bayesian computation results in a
bias in the approximation to the posterior. An alternative version based on
genuine importance sampling arguments bypasses this difficulty, in connection
with the population Monte Carlo method of Cappe et al. (2004), and it includes
an automatic scaling of the forward kernel. When applied to a population
genetics example, it compares favourably with two other versions of the
approximate algorithm.
</summary>
    <author>
      <name>Mark A. Beaumont</name>
    </author>
    <author>
      <name>Jean-Marie Cornuet</name>
    </author>
    <author>
      <name>Jean-Michel Marin</name>
    </author>
    <author>
      <name>Christian P. Robert</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1093/biomet/asp052</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1093/biomet/asp052" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures, one algorithm, third revised resubmission to
  Biometrika</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Biometrika 96(4), 983-990, 2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0805.2256v9" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0805.2256v9" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0807.0725v2</id>
    <updated>2008-09-17T06:35:28Z</updated>
    <published>2008-07-04T11:36:38Z</published>
    <title>Case-deletion importance sampling estimators: Central limit theorems and
  related results</title>
    <summary>  Case-deleted analysis is a popular method for evaluating the influence of a
subset of cases on inference. The use of Monte Carlo estimation strategies in
complicated Bayesian settings leads naturally to the use of importance sampling
techniques to assess the divergence between full-data and case-deleted
posteriors and to provide estimates under the case-deleted posteriors. However,
the dependability of the importance sampling estimators depends critically on
the variability of the case-deleted weights. We provide theoretical results
concerning the assessment of the dependability of case-deleted importance
sampling estimators in several Bayesian models. In particular, these results
allow us to establish whether or not the estimators satisfy a central limit
theorem. Because the conditions we derive are of a simple analytical nature,
the assessment of the dependability of the estimators can be verified routinely
before estimation is performed. We illustrate the use of the results in several
examples.
</summary>
    <author>
      <name>Ilenia Epifani</name>
    </author>
    <author>
      <name>Steven N. MacEachern</name>
    </author>
    <author>
      <name>Mario Peruggia</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1214/08-EJS259</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1214/08-EJS259" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in at http://dx.doi.org/10.1214/08-EJS259 the Electronic
  Journal of Statistics (http://www.i-journals.org/ejs/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Electronic Journal of Statistics 2008, Vol. 2, 774-806</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0807.0725v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0807.0725v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62F15, 62J20 (Primary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0807.3151v1</id>
    <updated>2008-07-20T11:18:29Z</updated>
    <published>2008-07-20T11:18:29Z</published>
    <title>Principle of detailed balance and convergence assessment of Markov Chain
  Monte Carlo methods and simulated annealing</title>
    <summary>  Markov Chain Monte Carlo (MCMC) methods are employed to sample from a given
distribution of interest, whenever either the distribution does not exist in
closed form, or, if it does, no efficient method to simulate an independent
sample from it is available. Although a wealth of diagnostic tools for
convergence assessment of MCMC methods have been proposed in the last two
decades, the search for a dependable and easy to implement tool is ongoing. We
present in this article a criterion based on the principle of detailed balance
which provides a qualitative assessment of the convergence of a given chain.
The criterion is based on the behaviour of a one-dimensional statistic, whose
asymptotic distribution under the assumption of stationarity is derived; our
results apply under weak conditions and have the advantage of being completely
intuitive. We implement this criterion as a stopping rule for simulated
annealing in the problem of finding maximum likelihood estimators for
parameters of a 20-component mixture model. We also apply it to the problem of
sampling from a 10-dimensional funnel distribution via slice sampling and the
Metropolis-Hastings algorithm. Furthermore, based on this convergence criterion
we define a measure of efficiency of one algorithm versus another.
</summary>
    <author>
      <name>Ioana A. Cosma</name>
    </author>
    <author>
      <name>Masoud Asgharian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0807.3151v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0807.3151v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0807.4858v1</id>
    <updated>2008-07-30T13:17:11Z</updated>
    <published>2008-07-30T13:17:11Z</published>
    <title>Construction of weakly CUD sequences for MCMC sampling</title>
    <summary>  In Markov chain Monte Carlo (MCMC) sampling considerable thought goes into
constructing random transitions. But those transitions are almost always driven
by a simulated IID sequence. Recently it has been shown that replacing an IID
sequence by a weakly completely uniformly distributed (WCUD) sequence leads to
consistent estimation in finite state spaces. Unfortunately, few WCUD sequences
are known. This paper gives general methods for proving that a sequence is
WCUD, shows that some specific sequences are WCUD, and shows that certain
operations on WCUD sequences yield new WCUD sequences. A numerical example on a
42 dimensional continuous Gibbs sampler found that some WCUD inputs sequences
produced variance reductions ranging from tens to hundreds for posterior means
of the parameters, compared to IID inputs.
</summary>
    <author>
      <name>Seth D. Tribble</name>
    </author>
    <author>
      <name>Art B. Owen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1214/07-EJS162</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1214/07-EJS162" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in at http://dx.doi.org/10.1214/07-EJS162 the Electronic
  Journal of Statistics (http://www.i-journals.org/ejs/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Electronic Journal of Statistics 2008, Vol. 2, 634-660</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0807.4858v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0807.4858v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62F15 (Primary) 11K45, 11K41 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0808.2902v7</id>
    <updated>2012-01-09T08:05:22Z</updated>
    <published>2008-08-21T10:43:14Z</published>
    <title>A Short History of Markov Chain Monte Carlo: Subjective Recollections
  from Incomplete Data</title>
    <summary>  We attempt to trace the history and development of Markov chain Monte Carlo
(MCMC) from its early inception in the late 1940s through its use today. We see
how the earlier stages of Monte Carlo (MC, not MCMC) research have led to the
algorithms currently in use. More importantly, we see how the development of
this methodology has not only changed our solutions to problems, but has
changed the way we think about problems.
</summary>
    <author>
      <name>Christian Robert</name>
    </author>
    <author>
      <name>George Casella</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1214/10-STS351</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1214/10-STS351" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in at http://dx.doi.org/10.1214/10-STS351 the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Statistical Science 2011, Vol. 26, No. 1, 102-115</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0808.2902v7" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0808.2902v7" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0808.3416v1</id>
    <updated>2008-08-25T23:10:09Z</updated>
    <published>2008-08-25T23:10:09Z</published>
    <title>Uncertainty quantification in complex systems using approximate solvers</title>
    <summary>  This paper proposes a novel uncertainty quantification framework for
computationally demanding systems characterized by a large vector of
non-Gaussian uncertainties. It combines state-of-the-art techniques in advanced
Monte Carlo sampling with Bayesian formulations. The key departure from
existing works is the use of inexpensive, approximate computational models in a
rigorous manner. Such models can readily be derived by coarsening the
discretization size in the solution of the governing PDEs, increasing the time
step when integration of ODEs is performed, using fewer iterations if a
non-linear solver is employed or making use of lower order models. It is shown
that even in cases where the inexact models provide very poor approximations of
the exact response, statistics of the latter can be quantified accurately with
significant reductions in the computational effort. Multiple approximate models
can be used and rigorous confidence bounds of the estimates produced are
provided at all stages.
</summary>
    <author>
      <name>Phaedon-Stelios Koutsourelakis</name>
    </author>
    <link href="http://arxiv.org/abs/0808.3416v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0808.3416v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65C05, 62F15, 62G08" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.0974v3</id>
    <updated>2009-01-26T18:39:14Z</updated>
    <published>2008-09-05T09:38:00Z</published>
    <title>Least Squares and Shrinkage Estimation under Bimonotonicity Constraints</title>
    <summary>  In this paper we describe active set type algorithms for minimization of a
smooth function under general order constraints, an important case being
functions on the set of bimonotone r-by-s matrices. These algorithms can be
used, for instance, to estimate a bimonotone regression function via least
squares or (a smooth approximation of) least absolute deviations. Another
application is shrinkage estimation in image denoising or, more generally,
regression problems with two ordinal factors after representing the data in a
suitable basis which is indexed by pairs (i,j) in {1,...,r}x{1,...,s}. Various
numerical examples illustrate our methods.
</summary>
    <author>
      <name>Rudolf Beran</name>
    </author>
    <author>
      <name>Lutz Duembgen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11222-009-9124-0</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11222-009-9124-0" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Statistics and Computing, Volume 20, Number 2 (2010), pp. 177-189</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0809.0974v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.0974v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.2274v4</id>
    <updated>2009-07-05T21:27:06Z</updated>
    <published>2008-09-12T19:38:02Z</published>
    <title>A randomized algorithm for principal component analysis</title>
    <summary>  Principal component analysis (PCA) requires the computation of a low-rank
approximation to a matrix containing the data being analyzed. In many
applications of PCA, the best possible accuracy of any rank-deficient
approximation is at most a few digits (measured in the spectral norm, relative
to the spectral norm of the matrix being approximated). In such circumstances,
efficient algorithms have not come with guarantees of good accuracy, unless one
or both dimensions of the matrix being approximated are small. We describe an
efficient algorithm for the low-rank approximation of matrices that produces
accuracy very close to the best possible, for matrices of arbitrary sizes. We
illustrate our theoretical results via several numerical examples.
</summary>
    <author>
      <name>Vladimir Rokhlin</name>
    </author>
    <author>
      <name>Arthur Szlam</name>
    </author>
    <author>
      <name>Mark Tygert</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 6 tables, 1 figure; to appear in the SIAM Journal on Matrix
  Analysis and Applications</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">A randomized algorithm for principal component analysis, SIAM
  Journal on Matrix Analysis and Applications, 31 (3): 1100-1124, 2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0809.2274v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.2274v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.4178v2</id>
    <updated>2009-02-23T09:51:42Z</updated>
    <published>2008-09-24T13:09:50Z</published>
    <title>Non-linear regression models for Approximate Bayesian Computation</title>
    <summary>  Approximate Bayesian inference on the basis of summary statistics is
well-suited to complex problems for which the likelihood is either
mathematically or computationally intractable. However the methods that use
rejection suffer from the curse of dimensionality when the number of summary
statistics is increased. Here we propose a machine-learning approach to the
estimation of the posterior density by introducing two innovations. The new
method fits a nonlinear conditional heteroscedastic regression of the parameter
on the summary statistics, and then adaptively improves estimation using
importance sampling. The new algorithm is compared to the state-of-the-art
approximate Bayesian methods, and achieves considerable reduction of the
computational burden in two examples of inference in statistical genetics and
in a queueing model.
</summary>
    <author>
      <name>M. G. B. Blum</name>
    </author>
    <author>
      <name>O. Francois</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11222-009-9116-0</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11222-009-9116-0" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 figures; version 3 minor changes; to appear in Statistics and
  Computing</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Statistics and Computing, 20: 63-73 (2010)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0809.4178v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.4178v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.4654v1</id>
    <updated>2008-09-26T15:12:29Z</updated>
    <published>2008-09-26T15:12:29Z</published>
    <title>Smooth supersaturated models</title>
    <summary>  In areas such as kernel smoothing and non-parametric regression there is
emphasis on smooth interpolation and smooth statistical models. Splines are
known to have optimal smoothness properties in one and higher dimensions. It is
shown, with special attention to polynomial models, that smooth interpolators
can be constructed by first extending the monomial basis and then minimising a
measure of smoothness with respect to the free parameters in the extended
basis. Algebraic methods are a help in choosing the extended basis which can
also be found as a saturated basis for an extended experimental design with
dummy design points. One can get arbitrarily close to optimal smoothing for any
dimension and over any region, giving a simple alternative models of spline
type. The relationship to splines is shown in one and two dimensions. A case
study is given which includes benchmarking against kriging methods.
</summary>
    <author>
      <name>Ron A. Bates</name>
    </author>
    <author>
      <name>Hugo Maruri-Aguilar</name>
    </author>
    <author>
      <name>Henry P. Wynn</name>
    </author>
    <link href="http://arxiv.org/abs/0809.4654v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.4654v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0810.1163v1</id>
    <updated>2008-10-07T11:28:00Z</updated>
    <published>2008-10-07T11:28:00Z</published>
    <title>Generalised linear mixed model analysis via sequential Monte Carlo
  sampling</title>
    <summary>  We present a sequential Monte Carlo sampler algorithm for the Bayesian
analysis of generalised linear mixed models (GLMMs). These models support a
variety of interesting regression-type analyses, but performing inference is
often extremely difficult, even when using the Bayesian approach combined with
Markov chain Monte Carlo (MCMC). The Sequential Monte Carlo sampler (SMC) is a
new and general method for producing samples from posterior distributions. In
this article we demonstrate use of the SMC method for performing inference for
GLMMs. We demonstrate the effectiveness of the method on both simulated and
real data, and find that sequential Monte Carlo is a competitive alternative to
the available MCMC techniques.
</summary>
    <author>
      <name>Y. Fan</name>
    </author>
    <author>
      <name>D. S. Leslie</name>
    </author>
    <author>
      <name>M. P. Wand</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1214/07-EJS158</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1214/07-EJS158" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in at http://dx.doi.org/10.1214/07-EJS158 the Electronic
  Journal of Statistics (http://www.i-journals.org/ejs/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Electronic Journal of Statistics 2008, Vol. 2, 916-938</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0810.1163v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0810.1163v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0810.2643v1</id>
    <updated>2008-10-15T09:36:23Z</updated>
    <published>2008-10-15T09:36:23Z</published>
    <title>Bayesian evidence for finite element model updating</title>
    <summary>  This paper considers the problem of model selection within the context of
finite element model updating. Given that a number of FEM updating models, with
different updating parameters, can be designed, this paper proposes using the
Bayesian evidence statistic to assess the probability of each updating model.
This makes it possible then to evaluate the need for alternative updating
parameters in the updating of the initial FE model. The model evidences are
compared using the Bayes factor, which is the ratio of evidences. The Jeffrey
scale is used to determine the differences in the models. The Bayesian evidence
is calculated by integrating the likelihood of the data given the model and its
parameters over the a priori model parameter space using the new nested
sampling algorithm. The nested algorithm samples this likelihood distribution
by using a hard likelihood-value constraint on the sampling region while
providing the posterior samples of the updating model parameters as a
by-product. This method is used to calculate the evidence of a number of
plausible finite element models.
</summary>
    <author>
      <name>Linda Mthembu</name>
    </author>
    <author>
      <name>Tshilidzi Marwala</name>
    </author>
    <author>
      <name>Michael I. Friswell</name>
    </author>
    <author>
      <name>Sondipon Adhikari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, to appear in the IMAC 2009</arxiv:comment>
    <link href="http://arxiv.org/abs/0810.2643v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0810.2643v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0810.5474v1</id>
    <updated>2008-10-30T12:05:12Z</updated>
    <published>2008-10-30T12:05:12Z</published>
    <title>Approximating the marginal likelihood using copula</title>
    <summary>  Model selection is an important activity in modern data analysis and the
conventional Bayesian approach to this problem involves calculation of marginal
likelihoods for different models, together with diagnostics which examine
specific aspects of model fit. Calculating the marginal likelihood is a
difficult computational problem. Our article proposes some extensions of the
Laplace approximation for this task that are related to copula models and which
are easy to apply. Variations which can be used both with and without
simulation from the posterior distribution are considered, as well as use of
the approximations with bridge sampling and in random effects models with a
large number of latent variables. The use of a t-copula to obtain higher
accuracy when multivariate dependence is not well captured by a Gaussian copula
is also discussed.
</summary>
    <author>
      <name>David J. Nott</name>
    </author>
    <author>
      <name>Robert J. Kohn</name>
    </author>
    <author>
      <name>Mark Fielding</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0810.5474v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0810.5474v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0811.0528v1</id>
    <updated>2008-11-04T14:42:34Z</updated>
    <published>2008-11-04T14:42:34Z</published>
    <title>Local antithetic sampling with scrambled nets</title>
    <summary>  We consider the problem of computing an approximation to the integral
$I=\int_{[0,1]^d}f(x) dx$. Monte Carlo (MC) sampling typically attains a root
mean squared error (RMSE) of $O(n^{-1/2})$ from $n$ independent random function
evaluations. By contrast, quasi-Monte Carlo (QMC) sampling using carefully
equispaced evaluation points can attain the rate $O(n^{-1+\varepsilon})$ for
any $\varepsilon&gt;0$ and randomized QMC (RQMC) can attain the RMSE
$O(n^{-3/2+\varepsilon})$, both under mild conditions on $f$. Classical
variance reduction methods for MC can be adapted to QMC. Published results
combining QMC with importance sampling and with control variates have found
worthwhile improvements, but no change in the error rate. This paper extends
the classical variance reduction method of antithetic sampling and combines it
with RQMC. One such method is shown to bring a modest improvement in the RMSE
rate, attaining $O(n^{-3/2-1/d+\varepsilon})$ for any $\varepsilon&gt;0$, for
smooth enough $f$.
</summary>
    <author>
      <name>Art B. Owen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1214/07-AOS548</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1214/07-AOS548" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in at http://dx.doi.org/10.1214/07-AOS548 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Annals of Statistics 2008, Vol. 36, No. 5, 2319-2343</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0811.0528v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0811.0528v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65C05 (Primary); 68U20, 65D32 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0811.2843v1</id>
    <updated>2008-11-18T08:57:00Z</updated>
    <published>2008-11-18T08:57:00Z</published>
    <title>An Algorithm for Unconstrained Quadratically Penalized Convex
  Optimization</title>
    <summary>  A descent algorithm, "Quasi-Quadratic Minimization with Memory" (QQMM), is
proposed for unconstrained minimization of the sum, $F$, of a non-negative
convex function, $V$, and a quadratic form. Such problems come up in
regularized estimation in machine learning and statistics. In addition to
values of $F$, QQMM requires the (sub)gradient of $V$. Two features of QQMM
help keep low the number of evaluations of the objective function it needs.
First, QQMM provides good control over stopping the iterative search. This
feature makes QQMM well adapted to statistical problems because in such
problems the objective function is based on random data and therefore stopping
early is sensible. Secondly, QQMM uses a complex method for determining trial
minimizers of $F$. After a description of the problem and algorithm a
simulation study comparing QQMM to the popular BFGS optimization algorithm is
described. The simulation study and other experiments suggest that QQMM is
generally substantially faster than BFGS in the problem domain for which it was
designed. A QQMM-BFGS hybrid is also generally substantially faster than BFGS
but does better than QQMM when QQMM is very slow.
</summary>
    <author>
      <name>Steven P. Ellis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to the Electronic Journal of Statistics
  (http://www.i-journals.org/ejs/) by the Institute of Mathematical Statistics
  (http://www.imstat.org)</arxiv:comment>
    <link href="http://arxiv.org/abs/0811.2843v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0811.2843v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65K10 (Primary) 62-04 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
