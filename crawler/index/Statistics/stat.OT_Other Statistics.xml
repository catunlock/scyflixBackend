<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Astat.OT%26id_list%3D%26start%3D0%26max_results%3D500" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:stat.OT&amp;id_list=&amp;start=0&amp;max_results=500</title>
  <id>http://arxiv.org/api/EtyhQYGwe4EkxVEmZ1Fqt3wImLU</id>
  <updated>2017-10-08T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">435</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">500</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/1007.1787v1</id>
    <updated>2010-07-11T16:53:03Z</updated>
    <published>2010-07-11T16:53:03Z</published>
    <title>The two sample problem: Exact distributions, numerical solutions,
  simulations</title>
    <summary>  The work presented in this article suggests a solution to the two sample
problem. Keywords: Two sample problem, Welch-Aspin solution, Fisher-Behrens
problem, nuisance parameter, similarity, the Linnik phenomenon.
</summary>
    <author>
      <name>D. E. Chambers</name>
    </author>
    <link href="http://arxiv.org/abs/1007.1787v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1007.1787v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.2590v1</id>
    <updated>2012-01-12T15:24:29Z</updated>
    <published>2012-01-12T15:24:29Z</published>
    <title>It is Time to Stop Teaching Frequentism to Non-statisticians</title>
    <summary>  We should cease teaching frequentist statistics to undergraduates and switch
to Bayes. Doing so will reduce the amount of confusion and over-certainty rife
among users of statistics.
</summary>
    <author>
      <name>William M. Briggs</name>
    </author>
    <link href="http://arxiv.org/abs/1201.2590v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.2590v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.6339v1</id>
    <updated>2012-12-27T10:33:34Z</updated>
    <published>2012-12-27T10:33:34Z</published>
    <title>A paradox on the spectral representation of stationary random processes</title>
    <summary>  In this note our aim is to show a paradox in the spectral representation of
stationary random processes.
</summary>
    <author>
      <name>Mohammad Mohammadi</name>
    </author>
    <author>
      <name>Adel Mohammadpour</name>
    </author>
    <author>
      <name>Afshin Parvardeh</name>
    </author>
    <link href="http://arxiv.org/abs/1212.6339v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.6339v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.06292v1</id>
    <updated>2017-04-19T14:54:43Z</updated>
    <published>2017-04-19T14:54:43Z</published>
    <title>Remark On Variance Bounds</title>
    <summary>  It is shown that the formula for the variance of combined series yields
surprisingly simple proofs of some well known variance bounds.
</summary>
    <author>
      <name>R. Sharma</name>
    </author>
    <link href="http://arxiv.org/abs/1704.06292v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.06292v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60E15" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.6249v1</id>
    <updated>2012-03-28T12:46:41Z</updated>
    <published>2012-03-28T12:46:41Z</published>
    <title>Reading Theorie Analytique des Probabilites</title>
    <summary>  This note is an extended read of my read of Laplace's book Theorie Analytique
des Probabilites, when considered from a Bayesian viewpoint but without
historical nor comparative pretentions. A deeper analysis is provided in Dale
(1999).
</summary>
    <author>
      <name>Christian P. Robert</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UNiversite Paris-Dauphine, IUF, and CREST</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1203.6249v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.6249v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.HO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.01183v1</id>
    <updated>2016-06-03T16:52:22Z</updated>
    <published>2016-06-03T16:52:22Z</published>
    <title>Peter Hall's work on high-dimensional data and classification</title>
    <summary>  In this article, I summarise Peter Hall's contributions to high-dimensional
data, including their geometric representations and variable selection methods
based on ranking. I also discuss his work on classification problems,
concluding with some personal reflections on my own interactions with him.
</summary>
    <author>
      <name>Richard J. Samworth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.01183v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.01183v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.06168v1</id>
    <updated>2016-11-18T17:32:40Z</updated>
    <published>2016-11-18T17:32:40Z</published>
    <title>On $p$-values</title>
    <summary>  Models are consistently treated as approximations and all procedures are
consistent with this. They do not treat the model as being true. In this
context $p$-values are one measure of approximation, a small $p$-value
indicating a poor approximation. Approximation regions are defined and
distinguished from confidence regions.
</summary>
    <author>
      <name>Laurie Davies</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.06168v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.06168v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62A01" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.04583v1</id>
    <updated>2017-01-17T09:14:38Z</updated>
    <published>2017-01-17T09:14:38Z</published>
    <title>PUMA criterion = MODE criterion</title>
    <summary>  We show that the recently proposed (enhanced) PUMA estimator for array
processing minimizes the same criterion function as the well-established MODE
estimator. (PUMA = principal-singular-vector utilization for modal analysis,
MODE = method of direction estimation.)
</summary>
    <author>
      <name>Dave Zachariah</name>
    </author>
    <author>
      <name>Petre Stoica</name>
    </author>
    <author>
      <name>Magnus Jansson</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TSP.2017.2742982</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TSP.2017.2742982" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Signal Processing, vol. 65, no. 22, 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1701.04583v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.04583v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.5690v1</id>
    <updated>2010-06-24T23:14:14Z</updated>
    <published>2010-06-24T23:14:14Z</published>
    <title>Sweave Documentation for "Implementing Markov chain Monte Carlo:
  Estimating with confidence"</title>
    <summary>  This file is the Sweave documentation for the examples provided in Flegal, J.
M. and Jones, G. L. (2010), "Implementing Markov chain Monte Carlo: Estimating
with confidence", in Handbook of Markov Chain Monte Carlo, edited by Brooks,
S., Gelman, A., Jones, G., and Meng, X. published by Chapman &amp; Hall/CRC Press.
</summary>
    <author>
      <name>James M. Flegal</name>
    </author>
    <author>
      <name>Galin L. Jones</name>
    </author>
    <link href="http://arxiv.org/abs/1006.5690v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.5690v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.5598v1</id>
    <updated>2011-06-28T09:03:13Z</updated>
    <published>2011-06-28T09:03:13Z</published>
    <title>Some notes on biasedness and unbiasedness of two-sample
  Kolmogorov-Smirnov test</title>
    <summary>  This paper deals with two-sample Kolmogorov-Smirnov test and its biasedness.
This test is not unbiased in general in case of different sample sizes. We
found out most biased distribution for some values of significance level
$\alpha$. Moreover we discovered that there exists number of observation and
significance level $\alpha$ such that this test is unbiased at level $\alpha$.
</summary>
    <author>
      <name>Peter Bubelíny</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1106.5598v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.5598v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.4669v3</id>
    <updated>2012-08-24T00:47:07Z</updated>
    <published>2012-02-18T00:06:02Z</published>
    <title>A Concise Resolution to the Two Envelope Paradox</title>
    <summary>  In this paper, I will demonstrate a new perspective on the Two Envelope
Problem. I hope to show with convincing clarity how the paradox results from an
inherent problem pertaining to the interpretation of Bayesian probability.
Specifically, a subjective probability that is inconsistent with reality can
mislead reasoning based on Bayesian decision theory.
</summary>
    <author>
      <name>Eric Bliss</name>
    </author>
    <link href="http://arxiv.org/abs/1202.4669v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.4669v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.4746v1</id>
    <updated>2012-02-21T20:48:23Z</updated>
    <published>2012-02-21T20:48:23Z</published>
    <title>A Proof on Asymptotics of Wavelet Variance of a Long Memory Process by
  Using Taylor Expansion</title>
    <summary>  A long memory process has self-similarity or scale-invariant properties in
low frequencies. We prove that the log of the scale-dependent wavelet variance
for a long memory process is asymptotically proportional to scales by using the
Taylor expansion of wavelet variances.
</summary>
    <author>
      <name>Wonsang You</name>
    </author>
    <author>
      <name>Wojciech Kordecki</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">LIN Technical Reports 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1202.4746v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.4746v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.3559v1</id>
    <updated>2012-03-15T20:51:14Z</updated>
    <published>2012-03-15T20:51:14Z</published>
    <title>A divergence formula for regularization methods with an L2 constraint</title>
    <summary>  We derive a divergence formula for a group of regularization methods with an
L2 constraint. The formula is useful for regularization parameter selection,
because it provides an unbiased estimate for the number of degrees of freedom.
We begin with deriving the formula for smoothing splines and then extend it to
other settings such as penalized splines, ridge regression, and functional
linear regression.
</summary>
    <author>
      <name>Yixin Fang</name>
    </author>
    <author>
      <name>Yuanjia Wang</name>
    </author>
    <author>
      <name>Xin Huang</name>
    </author>
    <link href="http://arxiv.org/abs/1203.3559v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.3559v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.3747v1</id>
    <updated>2012-03-16T15:54:50Z</updated>
    <published>2012-03-16T15:54:50Z</published>
    <title>Note on the closed-form MLEs of k-component load-sharing systems</title>
    <summary>  Recently Kim and Kvam (2004) and Singh, Sharma, Kumar (2008) proposed
different load-sharing models and developed parametric inference for the these
models. However, their parametric estimates are calculated using iterative
numerical methods. In this note, we provide the general closed-form MLEs for
the two load-sharing models provided by them.
</summary>
    <author>
      <name>Chanseok Park</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 Pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1203.3747v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.3747v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62F10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.5334v1</id>
    <updated>2012-04-24T11:07:36Z</updated>
    <published>2012-04-24T11:07:36Z</published>
    <title>On individual neutrality and collective decision making</title>
    <summary>  We derive a simple mathematical "theory" to show that two decision-making
entities can work better together only if at least one of them is occasionally
willing to stay neutral. This provides a mathematical "justification" for an
age-old cliche among marriage counselors.
</summary>
    <author>
      <name>Mu Zhu</name>
    </author>
    <author>
      <name>Shangsi Wang</name>
    </author>
    <author>
      <name>Lu Xin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The Mathematical Scientist, Vol. 37, No. 2, accepted and to appear in
  December 2012</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The Mathematical Scientist, December 2012, Vol. 37, No. 2, Pages
  141 - 146</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1204.5334v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.5334v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.3588v1</id>
    <updated>2012-05-16T08:13:01Z</updated>
    <published>2012-05-16T08:13:01Z</published>
    <title>Uncertainties and Ambiguities in Percentiles and how to Avoid Them</title>
    <summary>  The recently proposed fractional scoring scheme is used to attribute
publications to percentile rank classes. It is shown that in this way
uncertainties and ambiguities in the evaluation of percentile ranks do not
occur. Using the fractional scoring the total score of all papers exactly
reproduces the theoretical value.
</summary>
    <author>
      <name>Michael Schreiber</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures, submitted to Journal of the American Society for
  Information Science and Technology</arxiv:comment>
    <link href="http://arxiv.org/abs/1205.3588v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.3588v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.7008v1</id>
    <updated>2012-11-24T14:15:56Z</updated>
    <published>2012-11-24T14:15:56Z</published>
    <title>Benford's law: A theoretical explanation for base 2</title>
    <summary>  In this paper, we present a possible theoretical explanation for benford's
law. We develop a recursive relation between the probabilities, using simple
intuitive ideas. We first use numerical solutions of this recursion and verify
that the solutions converge to the benford's law. Finally we solve the
recursion analytically to yeild the benford's law for base 2.
</summary>
    <author>
      <name>H. M. Bharath</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1211.7008v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.7008v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.7920v1</id>
    <updated>2013-04-30T08:58:51Z</updated>
    <published>2013-04-30T08:58:51Z</published>
    <title>From Ordinary Differential Equations to Structural Causal Models: the
  deterministic case</title>
    <summary>  We show how, and under which conditions, the equilibrium states of a
first-order Ordinary Differential Equation (ODE) system can be described with a
deterministic Structural Causal Model (SCM). Our exposition sheds more light on
the concept of causality as expressed within the framework of Structural Causal
Models, especially for cyclic models.
</summary>
    <author>
      <name>Joris M. Mooij</name>
    </author>
    <author>
      <name>Dominik Janzing</name>
    </author>
    <author>
      <name>Bernhard Schölkopf</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to UAI 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1304.7920v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.7920v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.6995v1</id>
    <updated>2013-05-30T03:54:24Z</updated>
    <published>2013-05-30T03:54:24Z</published>
    <title>Review of: Mixed Effects Models and Extensions in Ecology with R</title>
    <summary>  This is a review of the book "Mixed Effects Models and Extensions in Ecology
with R" by Zuur, Ieno, Walker, Saveliev and Smith (2009, Springer). I was asked
to review this book for The American Statistician in 2010. After I wrote the
review, the invitation was revoked. This is the review.
</summary>
    <author>
      <name>J. Andrew Royle</name>
    </author>
    <link href="http://arxiv.org/abs/1305.6995v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.6995v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.4731v1</id>
    <updated>2013-12-17T11:51:36Z</updated>
    <published>2013-12-17T11:51:36Z</published>
    <title>Statistical inference for exponential functionals of Lévy processes</title>
    <summary>  In this paper, we consider the exponential functional
\(A_{\infty}=\int_0^\infty e^{-\xi_s}ds\) of a L{\'e}vy process \(\xi_s\) and
aim to estimate the characteristics of \(\xi_{s}\) from the distribution of
\(A_{\infty}\). We present a new approach, which allows to statistically infer
on the L{\'e}vy triplet of \(\xi_{t}\), and study the theoretical properties of
the proposed estimators. The suggested algorithms are illustrated with
numerical simulations.
</summary>
    <author>
      <name>Denis Belomestny</name>
    </author>
    <author>
      <name>Vladimir Panov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.4731v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.4731v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.8115v1</id>
    <updated>2014-01-31T10:25:44Z</updated>
    <published>2014-01-31T10:25:44Z</published>
    <title>Inhomogeneous K-function for germ-grain models</title>
    <summary>  In this paper, we propose a generalization to germ-grain models of the
inhomogeneous K-function of Point Processes. We apply them to a sample of
images of peripheral blood smears obtained from patients with Sickle Cell
Disease, in order to decide whether the sample belongs to the thin, thick or
morphological region.
</summary>
    <author>
      <name>M. Ángeles Gallego</name>
    </author>
    <author>
      <name>M. Victoria Ibáñez</name>
    </author>
    <author>
      <name>Amelia Simó</name>
    </author>
    <link href="http://arxiv.org/abs/1401.8115v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.8115v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62M30, 62M40" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.0947v4</id>
    <updated>2014-04-22T19:08:07Z</updated>
    <published>2014-02-05T06:24:02Z</published>
    <title>On Renyi entropy convergence of the max domain of attraction</title>
    <summary>  In this paper, we prove that the Renyi entropy of linearly normalized partial
maxima of independent and identically distributed random variables is
convergent to the corresponding limit Renyi entropy when the linearly
normalized partial maxima converges to some nondegenerate random variable.
</summary>
    <author>
      <name>Ali Saeb</name>
    </author>
    <link href="http://arxiv.org/abs/1402.0947v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.0947v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.5557v1</id>
    <updated>2014-03-21T18:50:06Z</updated>
    <published>2014-03-21T18:50:06Z</published>
    <title>Claude Bouchu, intendant de Bourgogne au 17ème siècle, a-t-il
  inventé le mot "statistique"</title>
    <summary>  The objective of this paper is to examine the assertion that the word
"statistics" would have been used for the first time in the 17th century, in a
report written by Claude Bouchu, administrator of Bourgogne. A historical and
bibliographical analysis is carried out to judge the credibility of this
thesis. The physical inspection of the report then makes it possible to bring a
final answer.
</summary>
    <author>
      <name>Dominique Pepin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CRIEF</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in French</arxiv:comment>
    <link href="http://arxiv.org/abs/1403.5557v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.5557v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.0913v1</id>
    <updated>2014-06-04T00:24:25Z</updated>
    <published>2014-06-04T00:24:25Z</published>
    <title>Generalized probabilities in statistical theories</title>
    <summary>  In this review article we present different formal frameworks for the
description of generalized probabilities in statistical theories. We discuss
the particular cases of probabilities appearing in classical and quantum
mechanics, possible generalizations of the approaches of A. N. Kolmogorov and
R. T. Cox to non-commutative models, and the approach to generalized
probabilities based on convex sets.
</summary>
    <author>
      <name>F. Holik</name>
    </author>
    <author>
      <name>C. Massri</name>
    </author>
    <author>
      <name>A. Plastino</name>
    </author>
    <author>
      <name>M. Sáenz</name>
    </author>
    <link href="http://arxiv.org/abs/1406.0913v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.0913v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.4515v1</id>
    <updated>2014-10-15T13:15:50Z</updated>
    <published>2014-10-15T13:15:50Z</published>
    <title>A Conversation with Howell Tong</title>
    <summary>  The following conversation is partly based on an interview that took place in
the Hong Kong University of Science and Technology in July 2013.
</summary>
    <author>
      <name>Kung-Sik Chan</name>
    </author>
    <author>
      <name>Qiwei Yao</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1214/13-STS464</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1214/13-STS464" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in at http://dx.doi.org/10.1214/13-STS464 the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Statistical Science 2014, Vol. 29, No. 3, 425-438</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1410.4515v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.4515v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.6002v2</id>
    <updated>2014-10-29T10:17:21Z</updated>
    <published>2014-10-22T11:20:45Z</published>
    <title>Estimating the Tail Index by using Model Averaging</title>
    <summary>  The ideas of model averaging are used to find weights in peak-over-threshold
problems using a possible range of thresholds. A range of the largest
observations are chosen and considered as possible thresholds, each time
performing estimation. Weights based on an information criterion for each
threshold are calculated. A weighted estimate of the threshold and shape
parameter can be calculated.
</summary>
    <author>
      <name>J. Martin van Zyl</name>
    </author>
    <link href="http://arxiv.org/abs/1410.6002v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.6002v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.8868v1</id>
    <updated>2014-10-29T17:58:15Z</updated>
    <published>2014-10-29T17:58:15Z</published>
    <title>Precinct Size Matters - The Large Precinct Bias in US Presidential
  Elections</title>
    <summary>  Examination of precinct level data in US presidential elections reveals a
correlation of large precincts and increased fraction of Republican votes. The
large precinct bias is analyzed with respect to voter heterogeneity and voter
inconvenience as precinct size increases. The analysis shows that voter
inconvenience is a significant factor in election outcomes in certain states,
and may significantly disadvantage Democratic candidates.
</summary>
    <author>
      <name>Glenn Webb</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 12 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.8868v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.8868v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="91F10 (primary), 91D10 (secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.0539v1</id>
    <updated>2014-11-03T15:59:29Z</updated>
    <published>2014-11-03T15:59:29Z</published>
    <title>A note on Bayesian logistic regression for spatial exponential family
  Gibbs point processes</title>
    <summary>  Recently, a very attractive logistic regression inference method for
exponential family Gibbs spatial point processes was introduced. We combined it
with the technique of quadratic tangential variational approximation and
derived a new Bayesian technique for analysing spatial point patterns. The
technique is described in detail, and demonstrated on numerical examples.
</summary>
    <author>
      <name>Tuomas Rajala</name>
    </author>
    <link href="http://arxiv.org/abs/1411.0539v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.0539v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.4824v1</id>
    <updated>2014-11-18T12:28:16Z</updated>
    <published>2014-11-18T12:28:16Z</published>
    <title>Quantile of a Mixture</title>
    <summary>  In this note, we give an explicit expression for the quantile of a mixture of
two random variables. We carefully examine all possible cases of discrete and
continuous variables with possibly unbounded support. The result is useful for
finding bounds on the Value-at-Risk of risky portfolios when only partial
information is available (Bernard and Vanduffel (2014)).
</summary>
    <author>
      <name>Carole Bernard</name>
    </author>
    <author>
      <name>Steven Vanduffel</name>
    </author>
    <link href="http://arxiv.org/abs/1411.4824v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.4824v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.05057v1</id>
    <updated>2015-06-20T09:57:33Z</updated>
    <published>2015-06-20T09:57:33Z</published>
    <title>Visualizing Probabilistic Proof</title>
    <summary>  The author revisits the Blue Bus Problem, a famous thought-experiment in law
involving probabilistic proof, and presents simple Bayesian solutions to
different versions of the blue bus hypothetical. In addition, the author
expresses his solutions in standard and visual formats, i.e. in terms of
probabilities and natural frequencies.
</summary>
    <author>
      <name>Enrique Guerra-Pujol</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">37 pages, 7 diagrams</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Washington University Jurisprudence Review, vol. 7, no. 1 (2014),
  pp. 39-75</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1507.05057v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.05057v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.03346v1</id>
    <updated>2015-09-10T21:50:12Z</updated>
    <published>2015-09-10T21:50:12Z</published>
    <title>A multi-dimensional stream and its signature representation</title>
    <summary>  The signature of a path is an essential object in the theory of rough paths.
The signature representation of the data stream can recover standard
statistics, e.g. the moments of the data stream. The classification of random
walks indicates the advantages of using the signature of a stream as the
feature set for machine learning.
</summary>
    <author>
      <name>Hao Ni</name>
    </author>
    <link href="http://arxiv.org/abs/1509.03346v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.03346v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.08307v1</id>
    <updated>2015-09-28T13:27:45Z</updated>
    <published>2015-09-28T13:27:45Z</published>
    <title>An Outline of the Bayesian Decision Theory</title>
    <summary>  In this paper we give an outline on the Bayesian Decision Theory.
</summary>
    <author>
      <name>H. R. N. van Erp</name>
    </author>
    <author>
      <name>R. O. Linger</name>
    </author>
    <author>
      <name>P. H. A. J. M. van Gelder</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1409.8269</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.08307v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.08307v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.09081v1</id>
    <updated>2016-05-30T00:50:39Z</updated>
    <published>2016-05-30T00:50:39Z</published>
    <title>Understanding Convolutional Neural Networks</title>
    <summary>  Convoulutional Neural Networks (CNNs) exhibit extraordinary performance on a
variety of machine learning tasks. However, their mathematical properties and
behavior are quite poorly understood. There is some work, in the form of a
framework, for analyzing the operations that they perform. The goal of this
project is to present key results from this theory, and provide intuition for
why CNNs work.
</summary>
    <author>
      <name>Jayanth Koushik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Statistical Machine Learning Course Project at Carnegie Mellon
  University</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.09081v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.09081v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02352v1</id>
    <updated>2016-06-07T23:36:00Z</updated>
    <published>2016-06-07T23:36:00Z</published>
    <title>A statistical inference course based on p-values</title>
    <summary>  Introductory statistical inference texts and courses treat the point
estimation, hypothesis testing, and interval estimation problems separately,
with primary emphasis on large-sample approximations. Here I present an
alternative approach to teaching this course, built around p-values,
emphasizing provably valid inference for all sample sizes. Details about
computation and marginalization are also provided, with several illustrative
examples, along with a course outline.
</summary>
    <author>
      <name>Ryan Martin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1080/00031305.2016.1208629</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1080/00031305.2016.1208629" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 2 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The American Statistician, 2017, volume 71, number 2, pages
  128--136</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1606.02352v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02352v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.00609v1</id>
    <updated>2017-03-31T09:07:41Z</updated>
    <published>2017-03-31T09:07:41Z</published>
    <title>What is the best fractional derivative to fit data?</title>
    <summary>  The aim of this work is to show, based on concrete data observation, that the
choice of the fractional derivative when modelling a problem is relevant for
the accuracy of a method. Using the least squares fitting technique, we
determine the order of the fractional differential equation that better
describes the experimental data, for different types of fractional derivatives.
</summary>
    <author>
      <name>Ricardo Almeida</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is a preprint of a paper whose final and definite form is with
  "Applicable Analysis and Discrete Mathematics"</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.00609v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.00609v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.01732v1</id>
    <updated>2017-04-06T07:42:17Z</updated>
    <published>2017-04-06T07:42:17Z</published>
    <title>A Mathematically Sensible Explanation of the Concept of Statistical
  Population</title>
    <summary>  In statistics education, the concept of population is widely felt hard to
grasp, as a result of vague explanations in textbooks. Some textbook authors
therefore chose not to mention it. This paper offers a new explanation by
proposing a new theoretical framework of population and sampling, which aims to
achieve high mathematical sensibleness. In the explanation, the term population
is given clear definition, and the relationship between simple random sampling
and iid random variables are examined mathematically.
</summary>
    <author>
      <name>Yiping Cheng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.01732v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.01732v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.07578v1</id>
    <updated>2017-05-22T06:53:19Z</updated>
    <published>2017-05-22T06:53:19Z</published>
    <title>Semiparametric estimation in the normal variance-mean mixture model</title>
    <summary>  In this paper we study the problem of statistical inference on the parameters
of the semiparametric variance-mean mixtures. This class of mixtures has
recently become rather popular in statistical and financial modelling. We
design a semiparametric estimation procedure that first estimates the mean of
the underlying normal distribution and then recovers nonparametrically the
density of the corresponding mixing distribution. We illustrate the performance
of our procedure on simulated and real data.
</summary>
    <author>
      <name>Denis Belomestny</name>
    </author>
    <author>
      <name>Vladimir Panov</name>
    </author>
    <link href="http://arxiv.org/abs/1705.07578v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.07578v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.03805v1</id>
    <updated>2017-06-12T18:43:19Z</updated>
    <published>2017-06-12T18:43:19Z</published>
    <title>Fiducial on a string</title>
    <summary>  The fiducial argument of Fisher (1973) has been described as his biggest
blunder, but the recent review of Hannig et al. (2016) demonstrates the current
and increasing interest in this brilliant idea. This short note analyses an
example introduced by Seidenfeld (1992) where the fiducial distribution is
restricted to a string.
  Keywords and phrases: Bayesian and fiducial inference, Restrictions on
parameters, Uncertainty quantification, Epistemic probability, Statistics on a
manifold.
</summary>
    <author>
      <name>Gunnar Taraldsen</name>
    </author>
    <author>
      <name>Bo Henry Lindqvist</name>
    </author>
    <link href="http://arxiv.org/abs/1706.03805v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.03805v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.07059v1</id>
    <updated>2017-07-30T11:19:25Z</updated>
    <published>2017-07-30T11:19:25Z</published>
    <title>Redundancy schemes for engineering coherent systems via a
  signature-based approach</title>
    <summary>  This paper proposes a signature-based approach for solving redundancy
allocation problems when component lifetimes are not only heterogeneous but
also dependent. The two common schemes for allocations, that is active and
standby redundancies, are considered. If the component lifetimes are
independent, the proposed approach leads to simple manipulations. Various
illustrative examples are also analysed. This method can be implemented for
practical complex engineering systems.
</summary>
    <author>
      <name>Mahdi Doostparast</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 Pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.07059v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.07059v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62N05 94A17" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.4148v1</id>
    <updated>2012-04-18T17:46:32Z</updated>
    <published>2012-04-18T17:46:32Z</published>
    <title>Algorithm for multivariate data standardization up to third moment</title>
    <summary>  An algorithm for transforming multivariate data to a form with normalized
first, second and third moments is presented.
</summary>
    <author>
      <name>Vadim Asnin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.4148v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.4148v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.1670v1</id>
    <updated>2012-09-07T23:17:52Z</updated>
    <published>2012-09-07T23:17:52Z</published>
    <title>Calculation of Some Expected Values for Parameterized Mean Model with
  Gaussian Noise</title>
    <summary>  This document derives several expected values related to the parameterized
mean model with Gaussian noise and their simplified forms.
</summary>
    <author>
      <name>Umut Orguner</name>
    </author>
    <link href="http://arxiv.org/abs/1209.1670v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.1670v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.2248v1</id>
    <updated>2013-12-08T19:36:39Z</updated>
    <published>2013-12-08T19:36:39Z</published>
    <title>Basic univariate and bivariate statistics for symbolic data: a critical
  review</title>
    <summary>  Some proofs of the problems of the basic statistics proposed for numeric
symbolic data.
</summary>
    <author>
      <name>Antonio Irpino</name>
    </author>
    <link href="http://arxiv.org/abs/1312.2248v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.2248v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.1732v1</id>
    <updated>2010-09-09T10:39:36Z</updated>
    <published>2010-09-09T10:39:36Z</published>
    <title>Extreme shock models: an alternative perspective</title>
    <summary>  Extreme shock models have been introduced in Gut and H\"usler (1999) to study
systems that at random times are subject to shock of random magnitude. These
systems break down when some shock overcomes a given resistance level. In this
paper we propose an alternative approach to extreme shock models using
reinforced urn processes. As a consequence of this we are able to look at the
same problem under a Bayesian nonparametric perspective, providing the
predictive distribution of systems' defaults.
</summary>
    <author>
      <name>Pasquale Cirillo</name>
    </author>
    <author>
      <name>Jürg Hüsler</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.spl.2010.09.014</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.spl.2010.09.014" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Statistics and Probability Letters, 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1009.1732v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.1732v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.2582v1</id>
    <updated>2011-05-12T21:24:22Z</updated>
    <published>2011-05-12T21:24:22Z</published>
    <title>Baby Morse Theory in Data Analysis</title>
    <summary>  A methodology is proposed for inferring the topology underlying point cloud
data. The approach employs basic elements of Morse Theory, and is capable of
producing not only a point estimate of various topological quantities (e.g.,
genus), but it can also assess their sampling uncertainty in a probabilistic
fashion. Several examples of point cloud data in three dimensions are utilized
to demonstrate how the method yields interval estimates for the topology of the
data as a 2-dimensional surface embedded in R^3.
</summary>
    <author>
      <name>Caren Marzban</name>
    </author>
    <author>
      <name>Ulvi Yurtsever</name>
    </author>
    <link href="http://arxiv.org/abs/1105.2582v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.2582v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.2895v2</id>
    <updated>2011-06-22T09:07:34Z</updated>
    <published>2011-06-15T07:47:09Z</published>
    <title>Statistical Inference: The Big Picture</title>
    <summary>  Statistics has moved beyond the frequentist-Bayesian controversies of the
past. Where does this leave our ability to interpret results? I suggest that a
philosophy compatible with statistical practice, labeled here statistical
pragmatism, serves as a foundation for inference. Statistical pragmatism is
inclusive and emphasizes the assumptions that connect statistical models with
observed data. I argue that introductory courses often mischaracterize the
process of statistical inference and I propose an alternative "big picture"
depiction.
</summary>
    <author>
      <name>Robert E. Kass</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1214/10-STS337</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1214/10-STS337" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in at http://dx.doi.org/10.1214/10-STS337 the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Statistical Science 2011, Vol. 26, No. 1, 1-9</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1106.2895v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.2895v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.3940v1</id>
    <updated>2011-06-20T15:24:01Z</updated>
    <published>2011-06-20T15:24:01Z</published>
    <title>Cooperative spectrum sensing over unreliable reporting channel</title>
    <summary>  This article aims to analyze a cooperative spectrum sensing scheme using a
centralized approach with unreliable reporting channel. The spectrum sensing is
applied to a cognitive radio system, where each cognitive radio performs a
simple energy detection and send the decision to a fusion center through a
reporting channel. When the decisions are available at the fusion center, a
n-out-of-K rule is applied. The impact of the choice of the parameter n in the
cognitive radio system performance is analyzed in the case where the reporting
channel introduces errors.
</summary>
    <author>
      <name>Amanda de Paula</name>
    </author>
    <author>
      <name>Cristiano Panazio</name>
    </author>
    <link href="http://arxiv.org/abs/1106.3940v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.3940v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1107.0202v1</id>
    <updated>2011-07-01T11:47:51Z</updated>
    <published>2011-07-01T11:47:51Z</published>
    <title>Revealing Sub-Optimality Conditions of Strategic Decisions</title>
    <summary>  Conceptual view of fitness and fitness measurement of strategic decisions on
information systems, technological systems and innovation are becoming more
important in recent years. This paper determines some dynamics of fitness
landscape which are lead to termination of decision makers' research before
reaching the global maximum in strategic decisions. These dynamics are
specified according to management decision making models and supported with
simulation results. This article determines simulation results by means of
"Fitness Value" and "Probability of Optimality". Correlation between these two
concepts may be remarkable according to revealing optimal values in innovative
and research-based decision making approaches beside sub-optimal results of
traditional decision making approaches.
</summary>
    <author>
      <name>H. Kemal Ilter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1107.0202v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.0202v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.1879v1</id>
    <updated>2011-08-09T08:31:58Z</updated>
    <published>2011-08-09T08:31:58Z</published>
    <title>Boundary detection in disease mapping studies</title>
    <summary>  In disease mapping, the aim is to estimate the spatial pattern in disease
risk over an extended geographical region, so that areas with elevated risks
can be identified. A Bayesian hierarchical approach is typically used to
produce such maps, which models the risk surface with a set of spatially smooth
random effects. However, in complex urban settings there are likely to be
boundaries in the risk surface, which separate populations that are
geographically adjacent but have very different risk profiles. Therefore this
paper proposes an approach for detecting such risk boundaries, and tests its
effectiveness by simulation. Finally, the model is applied to lung cancer
incidence data in Greater Glasgow, Scotland, between 2001 and 2005.
</summary>
    <author>
      <name>Duncan Lee</name>
    </author>
    <author>
      <name>Richard Mitchell</name>
    </author>
    <link href="http://arxiv.org/abs/1108.1879v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.1879v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.6531v1</id>
    <updated>2011-09-29T14:11:50Z</updated>
    <published>2011-09-29T14:11:50Z</published>
    <title>Technical Report: Energy Evaluation of preamble Sampling MAC Protocols
  for Wireless Sensor Networks</title>
    <summary>  The paper presents a simple probabilistic analysis of the energy consumption
in preamble sampling MAC protocols. We validate the analytical results with
simulations. We compare the classical MAC protocols (B-MAC and X-MAC) with
LAMAC, a method proposed in a companion paper. Our analysis highlights the
energy savings achievable with LA-MAC with respect to B-MAC and X-MAC. It also
shows that LA-MAC provides the best performance in the considered case of high
density networks under traffic congestion.
</summary>
    <author>
      <name>Giorgio Corbellini</name>
    </author>
    <author>
      <name>Cedric Abgrall</name>
    </author>
    <author>
      <name>Emilio Calvanese Strinati</name>
    </author>
    <author>
      <name>Andrzej Duda</name>
    </author>
    <link href="http://arxiv.org/abs/1109.6531v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.6531v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.0349v1</id>
    <updated>2011-10-03T13:37:18Z</updated>
    <published>2011-10-03T13:37:18Z</published>
    <title>Modern Portfolio Theory using SAS\textregistered OR</title>
    <summary>  Investment approaches in financial instruments have been varied and often
produce unpredictable results. Many investors in the earlier days of investment
banking suffered catastrophical losses due to poor strategy and lack of
understanding of the financial market. With the development of investment
banking, many innovative investment strategies have been proposed to make
portfolio returns higher than the overall market. One of the most famous
theories of portfolio creation and management is the modern portfolio theory
proposed by Harry Markowitz. In this paper, we shall apply the theory in
creating a portfolio of stocks as well as managing it.
</summary>
    <author>
      <name>Murphy Choy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SAS Global Forum 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1110.0349v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.0349v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.0559v1</id>
    <updated>2011-11-02T16:40:40Z</updated>
    <published>2011-11-02T16:40:40Z</published>
    <title>Model Selection in Undirected Graphical Models with the Elastic Net</title>
    <summary>  Structure learning in random fields has attracted considerable attention due
to its difficulty and importance in areas such as remote sensing, computational
biology, natural language processing, protein networks, and social network
analysis. We consider the problem of estimating the probabilistic graph
structure associated with a Gaussian Markov Random Field (GMRF), the Ising
model and the Potts model, by extending previous work on $l_1$ regularized
neighborhood estimation to include the elastic net $l_1+l_2$ penalty.
Additionally, we show numerical evidence that the edge density plays a role in
the graph recovery process. Finally, we introduce a novel method for augmenting
neighborhood estimation by leveraging pair-wise neighborhood union estimates.
</summary>
    <author>
      <name>Mihai Cucuringu</name>
    </author>
    <author>
      <name>Jesus Puente</name>
    </author>
    <author>
      <name>David Shue</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1111.0559v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.0559v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.5402v2</id>
    <updated>2012-04-10T03:02:12Z</updated>
    <published>2012-03-24T10:41:23Z</published>
    <title>Sparse solution of overdetermined linear systems when the columns of $A$
  are orthogonal</title>
    <summary>  In this paper, we consider the problem of obtaining the best $k$-sparse
solution of $Ax=y$ subject to the constraint that the columns of $A$ are
orthogonal. The naive approach for obtaining a solution to this problem has
exponential complexity and there exist $l_1$ regularization methods such as
Lasso to obtain approximate solutions. In this paper, we show that we can
obtain an exact solution to the problem, with much less computational effort
compared to the brute force search when the columns of $A$ are orthogonal.
</summary>
    <author>
      <name>Phanindra V. Jampana</name>
    </author>
    <author>
      <name>Sastry S. Challa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1203.5402v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.5402v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.4304v1</id>
    <updated>2012-05-19T08:33:59Z</updated>
    <published>2012-05-19T08:33:59Z</published>
    <title>In praise of the referee</title>
    <summary>  There has been a lively debate in many fields, including statistics and
related applied fields such as psychology and biomedical research, on possible
reforms of the scholarly publishing system. Currently, referees contribute so
much to improve scientific papers, both directly through constructive criticism
and indirectly through the threat of rejection. We discuss ways in which new
approaches to journal publication could continue to make use of the valuable
efforts of peer reviewers.
</summary>
    <author>
      <name>Nicolas Chopin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ENSAE, CREST</arxiv:affiliation>
    </author>
    <author>
      <name>Andrew Gelman</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Columbia University</arxiv:affiliation>
    </author>
    <author>
      <name>Kerrie L. Mengersen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Queensland University of Technology</arxiv:affiliation>
    </author>
    <author>
      <name>Christian P. Robert</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Universite Paris-Dauphine, IUF, and CREST</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1205.4304v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.4304v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.4811v1</id>
    <updated>2012-05-22T06:06:06Z</updated>
    <published>2012-05-22T06:06:06Z</published>
    <title>Networks with time structure from time series</title>
    <summary>  We propose a method of constructing a network, in which its time structure is
directly incorporated, based on a deterministic model from a time series. To
construct such a network, we transform a linear model containing terms with
different time delays into network topology. The terms in the model are
translated into temporal nodes of the network. On each link connecting these
nodes, we assign a positive real number representing the strength of
relationship, or the "distance," between nodes specified by the parameters of
the model. The method is demonstrated by a known system and applied to two
actual time series.
</summary>
    <author>
      <name>Tomomichi Nakamura</name>
    </author>
    <author>
      <name>Toshihiro Tanizawa</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.physa.2012.05.039</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.physa.2012.05.039" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 5 figures, accepted to be published in Physica A</arxiv:comment>
    <link href="http://arxiv.org/abs/1205.4811v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.4811v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.0364v1</id>
    <updated>2013-06-03T11:37:35Z</updated>
    <published>2013-06-03T11:37:35Z</published>
    <title>On moment indeterminacy of the Benini income distribution</title>
    <summary>  The Benini distribution is a lognormal-like distribution generalizing the
Pareto distribution. Like the Pareto and the lognormal distributions it was
originally proposed for modeling economic size distributions, notably the size
distribution of personal income. This paper explores a probabilistic property
of the Benini distribution, showing that it is not determined by the sequence
of its moments although all the moments are finite. It also provides explicit
examples of distributions possessing the same set of moments. Related
distributions are briefly explored.
</summary>
    <author>
      <name>Christian Kleiber</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s00362-013-0535-9</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s00362-013-0535-9" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 1 figure</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Statistical Papers, 2013, Vol. 54, No. 4, 1121-1130</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1306.0364v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.0364v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="Primary 60E05, Secondary 62E10, 44A60" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.4101v2</id>
    <updated>2013-09-26T03:47:47Z</updated>
    <published>2013-07-15T20:53:57Z</published>
    <title>Decision Making for Inconsistent Expert Judgments Using Negative
  Probabilities</title>
    <summary>  In this paper we provide a simple random-variable example of inconsistent
information, and analyze it using three different approaches: Bayesian,
quantum-like, and negative probabilities. We then show that, at least for this
particular example, both the Bayesian and the quantum-like approaches have less
normative power than the negative probabilities one.
</summary>
    <author>
      <name>J. Acacio de Barros</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, revised version to appear in the Proceedings of the QI2013
  (Quantum Interactions) conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.4101v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.4101v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.1023v3</id>
    <updated>2013-09-15T21:42:38Z</updated>
    <published>2013-08-05T15:55:25Z</published>
    <title>On the asymptotics of Ajtai-Komlós-Tusnády statistics</title>
    <summary>  In our days there is a widespread analysis of Wasserstein distances between
theoretical and empirical measures. One of the first investigation of the topic
is given in the paper written by Ajtai, Koml\'os and Tusn\'ady in $1984.$
  Interestingly, all the neighboring questions posed by that paper were settled
already without the original one. In this paper we are going to delineate the
limit behavior of the original statistics with the help of computer
simulations. At the same time we kept an eye on theoretical grasping of the
problem. Based on our computer simulations our opinion is that the limit
distribution is Gaussian.
</summary>
    <author>
      <name>L. Rejtő</name>
    </author>
    <author>
      <name>G. Tusnády</name>
    </author>
    <link href="http://arxiv.org/abs/1308.1023v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.1023v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62H10, 81T80" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.2442v1</id>
    <updated>2013-10-09T11:46:37Z</updated>
    <published>2013-10-09T11:46:37Z</published>
    <title>A Conversation with Stephen E. Fienberg</title>
    <summary>  The following conversation is based in part on a transcript of a 2009
interview funded by Pfizer Global Research-Connecticut, the American
Statistical Association and the Department of Statistics at the University of
Connecticut-Storrs as part of the "Conversations with Distinguished
Statisticians in Memory of Professor Harry O. Posten".
</summary>
    <author>
      <name>Miron L. Straf</name>
    </author>
    <author>
      <name>Judith M. Tanur</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1214/12-STS411</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1214/12-STS411" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in at http://dx.doi.org/10.1214/12-STS411 the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Statistical Science 2013, Vol. 28, No. 3, 447-463</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1310.2442v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.2442v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.7141v1</id>
    <updated>2013-10-26T18:40:08Z</updated>
    <published>2013-10-26T18:40:08Z</published>
    <title>Meeting Student Needs for Multivariate Data Analysis: A Case Study in
  Teaching a Multivariate Data Analysis Course with No Pre-requisites</title>
    <summary>  Modern students encounter big, messy data sets long before setting foot in
our classrooms. Many of our students need to develop skills in exploratory data
analysis and multivariate analysis techniques for their jobs after college, but
these topics are not covered in introductory statistics courses. This case
study describes my experience in designing and teaching a course on
multivariate data analysis with no pre-requisites, using real data, active
learning, and other activities to help students tackle the material.
</summary>
    <author>
      <name>Amy S. Wagaman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Article is 20 pages. Appendix is 20 pages of example material from
  the course</arxiv:comment>
    <link href="http://arxiv.org/abs/1310.7141v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.7141v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ed-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="97K80" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.6791v3</id>
    <updated>2014-03-15T05:49:50Z</updated>
    <published>2014-02-27T05:16:13Z</published>
    <title>A method for comparing chess openings</title>
    <summary>  A quantitative method is described for comparing chess openings. Test
openings and baseline openings are run through chess engines under controlled
conditions and compared to evaluate the effectiveness of the test openings. The
results are intuitively appealing and in some cases they agree with expert
opinion. The specific contribution of this work is the development of an
objective measure that may be used for the evaluation and refutation of chess
openings, a process that had been left to thought experiments and subjective
conjectures and thereby to a large variety of opinion and a great deal of
debate.
</summary>
    <author>
      <name>Jamal Munshi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Updated in March, 2014 to correct a data entry error in the Caro-Kann
  opening</arxiv:comment>
    <link href="http://arxiv.org/abs/1402.6791v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.6791v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.1789v3</id>
    <updated>2014-10-21T05:35:33Z</updated>
    <published>2014-04-07T13:54:00Z</published>
    <title>A Conversation with Donald B. Rubin</title>
    <summary>  Donald Bruce Rubin is John L. Loeb Professor of Statistics at Harvard
University. He has made fundamental contributions to statistical methods for
missing data, causal inference, survey sampling, Bayesian inference, computing
and applications to a wide range of disciplines, including psychology,
education, policy, law, economics, epidemiology, public health and other social
and biomedical sciences.
</summary>
    <author>
      <name>Fan Li</name>
    </author>
    <author>
      <name>Fabrizia Mealli</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1214/14-STS489</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1214/14-STS489" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in at http://dx.doi.org/10.1214/14-STS489 the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Statistical Science 2014, Vol. 29, No. 3, 439-457</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1404.1789v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.1789v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.6676v2</id>
    <updated>2014-10-05T06:28:45Z</updated>
    <published>2014-05-26T18:44:11Z</published>
    <title>Statistique et Big Data Analytics; Volumétrie, L'Attaque des Clones</title>
    <summary>  This article assumes acquired the skills and expertise of a statistician in
unsupervised (NMF, k-means, SVD) and supervised learning (regression, CART,
random forest). What skills and knowledge do a statistician must acquire to
reach the "Volume" scale of big data? After a quick overview of the different
strategies available and especially of those imposed by Hadoop, the algorithms
of some available learning methods are outlined in order to understand how they
are adapted to the strong stresses of the Map-Reduce functionalities
</summary>
    <author>
      <name>Philippe Besse</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IMT</arxiv:affiliation>
    </author>
    <author>
      <name>Nathalie Villa-Vialaneix</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MIAT INRA</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in French</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.6676v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.6676v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.3803v1</id>
    <updated>2014-09-10T12:32:26Z</updated>
    <published>2014-09-10T12:32:26Z</published>
    <title>Revealing the Beauty behind the Sleeping Beauty Problem</title>
    <summary>  A large number of essays address the Sleeping Beauty problem, which
undermines the validity of Bayesian inference and Bas Van Fraassen's
'Reflection Principle'. In this study a straightforward analysis of the problem
based on probability theory is presented. The key difference from previous
works is that apart from the random experiment imposed by the problem's
description, a different one is also considered, in order to negate the
confusion on the involved conditional probabilities. The results of the
analysis indicate that no inconsistency takes place, whereas both Bayesian
inference and 'Reflection Principle' are valid.
</summary>
    <author>
      <name>Ioannis Mariolis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.3803v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.3803v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.1107v1</id>
    <updated>2014-10-05T01:49:22Z</updated>
    <published>2014-10-05T01:49:22Z</published>
    <title>Using Board Games and Mathematica to Teach the Fundamentals of Finite
  Stationary Markov Chains</title>
    <summary>  Markov chains are an important example for a course on stochastic processes
because simple board games can be used to illustrate the fundamental concepts.
For example, a looping board game (like Monopoly) consists of all recurrent
states, and a game where players win by reaching a final square (like Chutes
and Ladders) consists of all transient states except for the last one. With the
availability of computer algebra packages, these games can be analyzed. For
example, the mean times in transient states and the stationary probabilities
for recurrent states are easily computed. This article analyzes some simple
board games with Mathematica, and indicates how this can be extended to more
complex situations.
</summary>
    <author>
      <name>Roger Bilisoly</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A proceedings article based on a Joint Statistical Meetings talk in
  2008</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.1107v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.1107v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.7261v1</id>
    <updated>2014-12-23T06:15:39Z</updated>
    <published>2014-12-23T06:15:39Z</published>
    <title>From Curriculum Guidelines to Learning Objectives: A Survey of Five
  Statistics Programs</title>
    <summary>  The 2000 ASA Guidelines for Undergraduate Statistics majors aimed to provide
guidance to programs with undergraduate degrees in statistics as to the content
and skills that statistics majors should be learning. With new guidelines
forthcoming, it is important to help programs develop an assessment cycle of
evaluation. How do we know the students are learning what we want them to
learn? How do we improve the program over time? The first step in this process
is to translate the broader Guidelines into institution-specific measurable
learning outcomes. This paper provides examples of how five programs did so for
the 2000 Guidelines. We hope they serve as illustrative examples for programs
moving forward with the new guidelines.
</summary>
    <author>
      <name>Beth Chance</name>
    </author>
    <author>
      <name>Roxy Peck</name>
    </author>
    <link href="http://arxiv.org/abs/1412.7261v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.7261v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.01908v1</id>
    <updated>2015-01-08T17:26:58Z</updated>
    <published>2015-01-08T17:26:58Z</published>
    <title>The Compass for Statistical Researchers</title>
    <summary>  We have hiked many miles alongside several professors as we traversed our
statistical path -- a regime switching trail which changed direction following
a class on the foundations of our discipline. As we play the game of research
in that limbo between student and academic, one thing among Prof. Bernardi's
teachings has never been more clear: to draw a route in the research map you
not only need to know your destination, but you must also understand where you
are and how you arrived there.
</summary>
    <author>
      <name>Daniele Durante</name>
    </author>
    <author>
      <name>Davide Vidotto</name>
    </author>
    <author>
      <name>Sabrina Vettori</name>
    </author>
    <link href="http://arxiv.org/abs/1501.01908v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.01908v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.03811v1</id>
    <updated>2015-01-14T22:07:23Z</updated>
    <published>2015-01-14T22:07:23Z</published>
    <title>The Problem Of Grue Isn't</title>
    <summary>  The so-called problem of grue was introduced by Nelson Goodman in 1954 as a
"riddle" about induction, a riddle which has been widely thought to cast doubt
on the validity and rationality of induction. That unnecessary doubt in turn is
partly responsible for the reluctance to adopt the view that probability is
part of logic. Several authors have pointed out deficiencies in grue;
nevertheless, the "problem" still excites. Here, adapted from Groarke, is
presented the basis of grue, along with another simple demonstration that the
"problem" makes no sense and is brought about by a misunderstanding of
causation.
</summary>
    <author>
      <name>William M. Briggs</name>
    </author>
    <link href="http://arxiv.org/abs/1501.03811v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.03811v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.hist-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.08653v1</id>
    <updated>2015-03-30T12:27:28Z</updated>
    <published>2015-03-30T12:27:28Z</published>
    <title>Exponentiated Extended Weibull-Power Series Class of Distributions</title>
    <summary>  In this paper, we introduce a new class of distributions by compounding the
exponentiated extended Weibull family and power series family. This
distribution contains several lifetime models such as the complementary
extended Weibull-power series, generalized exponential-power series,
generalized linear failure rate-power series, exponentiated Weibull-power
series, generalized modified Weibull-power series, generalized Gompertz-power
series and exponentiated extended Weibull distributions as special cases. We
obtain several properties of this new class of distributions such as Shannon
entropy, mean residual life, hazard rate function, quantiles and moments. The
maximum likelihood estimation procedure via a EM-algorithm is presented.
</summary>
    <author>
      <name>Saeid Tahmasebi</name>
    </author>
    <author>
      <name>Ali Akbar Jafari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication Ciencia e Natura Journal</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.08653v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.08653v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.05982v1</id>
    <updated>2015-07-21T20:39:39Z</updated>
    <published>2015-07-21T20:39:39Z</published>
    <title>On locating statistics in the world of finding out</title>
    <summary>  This paper attempts to situate statistics in relation to qualitative research
methods and other means of "finding out". It compares and contrasts aspects of
qualitative research methods and statistical inquiry and attempts to answer the
question of whether and how elements of qualitative research methods should be
included in statistics teaching.
</summary>
    <author>
      <name>Chris J. Wild</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages. The paper is an extended discussion of "Enhancing statistics
  education by including qualitative research" by Irena Ograjen\v{s}ek and Iddo
  Gal that will appear, in some form, in the International Statistical Review
  together with the parent paper. It can however be read as a stand-alone paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.05982v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.05982v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.07905v2</id>
    <updated>2015-09-24T19:53:13Z</updated>
    <published>2015-07-28T19:19:54Z</published>
    <title>The XL-mHG Test For Enrichment: A Technical Report</title>
    <summary>  The minimum hypergeometric test (mHG) is a powerful nonparametric hypothesis
test to detect enrichment in ranked binary lists. Here, I provide a detailed
review of its definition, as well as the algorithms used in its implementation,
which enable the efficient computation of an exact p-value. I then introduce a
generalization of the mHG, termed XL-mHG, which provides additional control
over the type of enrichment tested, and describe the precise algorithmic
modifications necessary to compute its test statistic and p-value. The XL-mHG
algorithm is a building block of GO-PCA, a recently proposed method for the
exploratory analysis of gene expression data using prior knowledge.
</summary>
    <author>
      <name>Florian Wagner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">corrected terminology ("threshold" =&gt; "cutoff"); added section 5,
  "Quantifying the strength of enrichment"</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.07905v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.07905v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.08934v1</id>
    <updated>2015-07-31T16:28:30Z</updated>
    <published>2015-07-31T16:28:30Z</published>
    <title>A Framework for Infusing Authentic Data Experiences Within Statistics
  Courses</title>
    <summary>  Working with complex data is one of the important updates to the 2014 ASA
Curriculum Guidelines for Undergraduate Programs in Statistical Science.
Infusing 'authentic data experiences' within courses allow students
opportunities to learn and practice data skills as they prepare a dataset for
analysis. While more modest in scope than a senior-level culminating
experience, authentic data experiences provide an opportunity to demonstrate
connections between data skills and statistical skills. The result is more
practice of data skills for undergraduate statisticians.
</summary>
    <author>
      <name>Scott D. Grimshaw</name>
    </author>
    <link href="http://arxiv.org/abs/1507.08934v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.08934v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.04858v3</id>
    <updated>2016-11-04T12:44:06Z</updated>
    <published>2015-12-15T17:06:23Z</published>
    <title>Solution for the Indefinite Integral of the Standard Normal Probability
  Density Function</title>
    <summary>  Conventional wisdom assumes that the indefinite integral of the probability
density function for the standard normal distribution cannot be expressed in
finite elementary terms. While this is true, there is an expression for this
anti-derivative in infinite elementary terms that, when being differentiated,
directly yields the standard normal density function. We derive this function
using infinite partial integration and review its relation to the cumulative
distribution function for the standard normal distribution and the error
function.
</summary>
    <author>
      <name>Joram Soch</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 figure (caption corrected)</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.04858v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.04858v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.00665v1</id>
    <updated>2015-12-30T18:49:51Z</updated>
    <published>2015-12-30T18:49:51Z</published>
    <title>Impugning Randomness, Convincingly</title>
    <summary>  John organized a state lottery and his wife won the main prize. You may feel
that the event of her winning wasn't particularly random, but how would you
argue that in a fair court of law? Traditional probability theory does not even
have the notion of random events. Algorithmic information theory does, but it
is not applicable to real-world scenarios like the lottery one. We attempt to
rectify that.
</summary>
    <author>
      <name>Yuri Gurevich</name>
    </author>
    <author>
      <name>Grant Olney Passmore</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Bulletin of Euro. Assoc. for Theor. Computer Science 104, June
  2011. Studia Logica 82 (2012)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1601.00665v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.00665v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.02221v3</id>
    <updated>2017-03-08T01:13:13Z</updated>
    <published>2016-04-08T04:00:49Z</published>
    <title>Box-Cox symmetric distributions and applications to nutritional data</title>
    <summary>  We introduce the Box-Cox symmetric class of distributions, which is useful
for modeling positively skewed, possibly heavy-tailed, data. The new class of
distributions includes the Box-Cox t, Box-Cox Cole-Gree, Box-Cox power
exponential distributions, and the class of the log-symmetric distributions as
special cases. It provides easy parameter interpretation, which makes it
convenient for regression modeling purposes. Additionally, it provides enough
flexibility to handle outliers. The usefulness of the Box-Cox symmetric models
is illustrated in applications to nutritional data.
</summary>
    <author>
      <name>Silvia L. P. Ferrari</name>
    </author>
    <author>
      <name>Giovana Fumes</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10182-017-0291-6</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10182-017-0291-6" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The final publication is available at Springer (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1604.02221v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.02221v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03189v1</id>
    <updated>2016-06-10T05:47:56Z</updated>
    <published>2016-06-10T05:47:56Z</published>
    <title>Bringing Order to the Chaos in the Brickyard</title>
    <summary>  An allegory published in 1963 titled Chaos in the Brickyard spoke to the
decline in the quality of research. In the intervening time greater awareness
of the issues and actions to improve research endeavors have emerged. Still,
problems persist. This paper is intended to clarify some of the challenges,
particularly with respect to quantitative research, then suggest ways to
improve the quality of published research. The paper highlights where feasible
refinements in analytical techniques can be made and provides a guide to
fundamental principles related to data analysis in research.
</summary>
    <author>
      <name>Bethany Shifflett</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages (including references), 1 figure, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.03189v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03189v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.00858v1</id>
    <updated>2016-07-04T12:40:15Z</updated>
    <published>2016-07-04T12:40:15Z</published>
    <title>Embracing Data Science</title>
    <summary>  Statistics is running the risk of appearing irrelevant to today's
undergraduate students. Today's undergraduate students are familiar with data
science projects and they judge statistics against what they have seen.
Statistics, especially at the introductory level, should take inspiration from
data science so that the discipline is not seen as somehow lesser than data
science. This article provides a brief overview of data science, outlines ideas
for how introductory courses could take inspiration from data science, and
provides a reference to materials for developing stand-alone data science
courses.
</summary>
    <author>
      <name>Adam Loy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 1 figure</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The UMAP Journal 36 (2015) 285-292</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1607.00858v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.00858v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.04807v2</id>
    <updated>2016-11-05T04:39:25Z</updated>
    <published>2016-07-16T23:18:24Z</published>
    <title>Progress on a Conjecture Regarding the Triangular Distribution</title>
    <summary>  Triangular distributions are a well-known class of distributions that are
often used as an elementary example of a probability model. Maximum likelihood
estimation of the mode parameter of the triangular distribution over the unit
interval can be performed via an order statistics-based method. It had been
conjectured that such a method can be conducted using only a constant number of
likelihood function evaluations, on average, as the sample size becomes large.
We prove two theorems that validate this conjecture. Graphical and numerical
results are presented to supplement our proofs.
</summary>
    <author>
      <name>Hien D Nguyen</name>
    </author>
    <author>
      <name>Geoffrey J McLachlan</name>
    </author>
    <link href="http://arxiv.org/abs/1607.04807v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.04807v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.00048v1</id>
    <updated>2016-09-30T22:30:37Z</updated>
    <published>2016-09-30T22:30:37Z</published>
    <title>Spatial Temporal Exponential-Family Point Process Models for the
  Evolution of Social Systems</title>
    <summary>  We develop a class of exponential-family point processes based on a latent
social space to model the coevolution of social structure and behavior over
time. Temporal dynamics are modeled as a discrete Markov process specified
through individual transition distributions for each actor in the system at a
given time. We prove that these distributions have an analytic closed form
under certain conditions and use the result to develop likelihood-based
inference. We provide a computational framework to enable both simulation and
inference in practice. Finally, we demonstrate the value of these models by
analyzing alcohol and drug use over time in the context of adolescent
friendship networks.
</summary>
    <author>
      <name>Joshua D. EmBree</name>
    </author>
    <author>
      <name>Mark S. Handcock</name>
    </author>
    <link href="http://arxiv.org/abs/1610.00048v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.00048v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.00290v1</id>
    <updated>2016-10-02T15:47:24Z</updated>
    <published>2016-10-02T15:47:24Z</published>
    <title>Conditional Visualization for Statistical Models: An Introduction to the
  condvis Package in R</title>
    <summary>  The condvis package is for interactive visualization of sections in data
space, showing fitted models on the section, and observed data near the
section. The primary goal is the interpretation of complex models, and showing
how the observed data support the fitted model. There is a video accompaniment
to this paper available at https://www.youtube.com/watch?v=rKFq7xwgdX0. This is
a preprint version of an article to appear in the Journal of Statistical
Software.
</summary>
    <author>
      <name>Mark O'Connell</name>
    </author>
    <author>
      <name>Catherine B. Hurley</name>
    </author>
    <author>
      <name>Katarina Domijan</name>
    </author>
    <link href="http://arxiv.org/abs/1610.00290v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.00290v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.03073v1</id>
    <updated>2016-10-25T11:41:42Z</updated>
    <published>2016-10-25T11:41:42Z</published>
    <title>Causal influence in linear response models</title>
    <summary>  The intuition of causation is so fundamental that almost every research study
in life sciences refers to this concept. However a widely accepted formal
definition of causal influence between observables is still missing. In the
framework of linear Langevin networks without feedbacks (linear response
models) we developed a measure of causal influence based on a decomposition of
information flows over time. We discuss its main properties and compare it with
other information measures like the Transfer Entropy. Finally we outline some
difficulties of the extension to a general definition of causal influence for
complex systems.
</summary>
    <author>
      <name>Andrea Auconi</name>
    </author>
    <author>
      <name>Andrea Giansanti</name>
    </author>
    <author>
      <name>Edda Klipp</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.95.042315</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.95.042315" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 9 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 95, 042315 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1611.03073v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.03073v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.03974v2</id>
    <updated>2016-12-28T22:29:54Z</updated>
    <published>2016-11-12T09:13:12Z</published>
    <title>On approximations via convolution-defined mixture models</title>
    <summary>  An often-cited fact regarding mixing distributions is that their densities
can approximate the densities of any unknown distribution to arbitrary degrees
of accuracy provided that the mixing distribution is sufficiently complex. This
fact is often not made concrete. We investigate theorems that provide
approximation bounds for mixing distributions. Novel connections are drawn
between the approximation bounds of mixing distributions and estimation bounds
for the maximum likelihood estimator of finite mixtures of location-scale
distributions. New approximation and estimation bounds are obtained in the
context of finite mixtures of truncated location-scale distributions.
</summary>
    <author>
      <name>Hien D. Nguyen</name>
    </author>
    <author>
      <name>Geoffrey J. McLachlan</name>
    </author>
    <link href="http://arxiv.org/abs/1611.03974v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.03974v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.00823v1</id>
    <updated>2017-02-02T20:46:28Z</updated>
    <published>2017-02-02T20:46:28Z</published>
    <title>Nonparametric Spherical Regression Using Diffeomorphic Mappings</title>
    <summary>  Spherical regression explores relationships between variables on spherical
domains. We develop a nonparametric model that uses a diffeomorphic map from a
sphere to itself. The restriction of this mapping to diffeomorphisms is natural
in several settings. The model is estimated in a penalized maximum-likelihood
framework using gradient-based optimization. Towards that goal, we specify a
first-order roughness penalty using the Jacobian of diffeomorphisms. We compare
the prediction performance of the proposed model with state-of-the-art methods
using simulated and real data involving cloud deformations, wind directions,
and vector-cardiograms. This model is found to outperform others in capturing
relationships between spherical variables.
</summary>
    <author>
      <name>Michael Rosenthal</name>
    </author>
    <author>
      <name>Wei Wu</name>
    </author>
    <author>
      <name>Eric Klassen</name>
    </author>
    <author>
      <name>Anuj Srivastava</name>
    </author>
    <link href="http://arxiv.org/abs/1702.00823v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.00823v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.00352v1</id>
    <updated>2017-02-28T11:31:57Z</updated>
    <published>2017-02-28T11:31:57Z</published>
    <title>Do Reichenbachian Common Cause Systems of Arbitrary Finite Size Exist?</title>
    <summary>  The principle of common cause asserts that positive correlations between
causally unrelated events ought to be explained through the action of some
shared causal factors. Reichenbachian common cause systems are probabilistic
structures aimed at accounting for cases where correlations of the aforesaid
sort cannot be explained through the action of a single common cause. The
existence of Reichenbachian common cause systems of arbitrary finite size for
each pair of non-causally correlated events was allegedly demonstrated by
Hofer-Szab\'o and R\'edei in 2006. This paper shows that their proof is
logically deficient, and we propose an improved proof.
</summary>
    <author>
      <name>Claudio Mazzola</name>
    </author>
    <author>
      <name>Peter Evans</name>
    </author>
    <link href="http://arxiv.org/abs/1703.00352v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.00352v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.hist-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.04185v1</id>
    <updated>2017-03-12T22:11:45Z</updated>
    <published>2017-03-12T22:11:45Z</published>
    <title>Estimating the Probability that a Function Observed with Noise is Convex</title>
    <summary>  Consider a real-valued function that can only be observed with stochastic
simulation noise at a finite set of design points within a Euclidean space. We
wish to determine whether there exists a convex function that goes through the
true function values at the design points. We develop an asymptotically
consistent Bayesian sequential sampling procedure that estimates the posterior
probability of this being true. In each iteration, the posterior probability is
estimated using Monte Carlo simulation. We offer three variance reduction
methods -- change of measure, acceptance-rejection, and conditional Monte
Carlo. Numerical experiments suggest that the conditional Monte Carlo method
should be preferred.
</summary>
    <author>
      <name>Nanjing Jian</name>
    </author>
    <author>
      <name>Shane G. Henderson</name>
    </author>
    <link href="http://arxiv.org/abs/1703.04185v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.04185v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.04467v4</id>
    <updated>2017-09-15T15:30:29Z</updated>
    <published>2017-03-13T16:19:01Z</published>
    <title>spmoran: An R package for Moran's eigenvector-based spatial regression
  analysis</title>
    <summary>  The objective of this study is illustrating how to use "spmoran," which is an
R package for Moran's eigenvector-based spatial regression analysis. spmoran
estimates regression models in the presence of spatial dependence, including
eigenvector spatial filtering (ESF), random effects ESF (RE-ESF) models, and
their extensions. These models are estimated in a computationally efficient
manner. For the illustration, this study applies ESF and RE-ESF models for a
land price analysis.
</summary>
    <author>
      <name>Daisuke Murakami</name>
    </author>
    <link href="http://arxiv.org/abs/1703.04467v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.04467v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.10117v1</id>
    <updated>2017-03-28T06:14:11Z</updated>
    <published>2017-03-28T06:14:11Z</published>
    <title>Data-Mining Research in Education</title>
    <summary>  As an interdisciplinary discipline, data mining (DM) is popular in education
area especially when examining students' learning performances. It focuses on
analyzing educational related data to develop models for improving learners'
learning experiences and enhancing institutional effectiveness. Therefore, DM
does help education institutions provide high-quality education for its
learners. Applying data mining in education also known as educational data
mining (EDM), which enables to better understand how students learn and
identify how improve educational outcomes. Present paper is designed to justify
the capabilities of data mining approaches in the filed of education. The
latest trends on EDM research are introduced in this review. Several specific
algorithms, methods, applications and gaps in the current literature and future
insights are discussed here.
</summary>
    <author>
      <name>Jiechao Cheng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.10117v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.10117v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.07890v1</id>
    <updated>2017-05-20T14:38:36Z</updated>
    <published>2017-05-20T14:38:36Z</published>
    <title>Explanation and exact formula of Zipfs law evaluated from rank-share
  combinatorics</title>
    <summary>  This work proves that ranks and shares are statistically dependent on one
another, based on simple combinatorics. It presents a formula for rank-share
distribution and illustrates that Zipfs law, is descended from expected values
of various ranks in the new distribution. All conclusions, formulas and charts
presented here were tested against publicly available statistical data in
different areas. The correlation coefficient between the calculated values and
statistical numbers provided by Bureau of Labor Statistics was 0.99899.
Monte-Carlo simulations were performed as additional evidence.
</summary>
    <author>
      <name>A Shyklo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.07890v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.07890v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.06537v1</id>
    <updated>2017-07-20T14:26:39Z</updated>
    <published>2017-07-20T14:26:39Z</published>
    <title>Cognitive Transfer Outcomes for a Simulation-Based Introductory
  Statistics Curriculum</title>
    <summary>  Cognitive transfer is the ability to apply learned skills and knowledge to
new applications and contexts. This investigation evaluates cognitive transfer
outcomes for a tertiary-level introductory statistics course using the CATALST
curriculum, which exclusively used simulation-based methods to develop
foundations of statistical inference. A common assessment instrument
administered at the end of each course measured learning outcomes for students.
CATALST students showed evidence of both near and far transfer outcomes while
scoring as high, or higher on the assessed learning objectives, when compared
with peers enrolled in similar courses that emphasized parametric inferential
methods (e.g. the t-test).
</summary>
    <author>
      <name>Matthew D. Beckman</name>
    </author>
    <author>
      <name>Robert C. delMas</name>
    </author>
    <author>
      <name>Joan Garfield</name>
    </author>
    <link href="http://arxiv.org/abs/1707.06537v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.06537v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.07607v1</id>
    <updated>2017-07-24T15:30:05Z</updated>
    <published>2017-07-24T15:30:05Z</published>
    <title>We are not alone ! (at least, most of us). Homonymy in large scale
  social groups</title>
    <summary>  This article brings forward an estimation of the proportion of homonyms in
large scale groups based on the distribution of first names and last names in a
subset of these groups. The estimation is based on the generalization of the
"birthday paradox problem". The main results is that, in societies such as
France or the United States, identity collisions (based on first + last names)
are frequent. The large majority of the population has at least one homonym.
But in smaller settings, it is much less frequent : even if small groups of a
few thousand people have at least one couple of homonyms, only a few
individuals have an homonym.
</summary>
    <author>
      <name>Arthur Charpentier</name>
    </author>
    <author>
      <name>Baptiste Coulmont</name>
    </author>
    <link href="http://arxiv.org/abs/1707.07607v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.07607v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.07625v1</id>
    <updated>2017-07-21T15:24:06Z</updated>
    <published>2017-07-21T15:24:06Z</published>
    <title>Restoring a smooth function from its noisy integrals</title>
    <summary>  Numerical (and experimental) data analysis often requires the restoration of
a smooth function from a set of sampled integrals over finite bins. We present
the bin hierarchy method that efficiently computes the maximally smooth
function from the sampled integrals using essentially all the information
contained in the data. We perform extensive tests with different classes of
functions and levels of data quality, including Monte Carlo data suffering from
a severe sign problem.
</summary>
    <author>
      <name>Olga Goulko</name>
    </author>
    <author>
      <name>Nikolay Prokof'ev</name>
    </author>
    <author>
      <name>Boris Svistunov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.07625v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.07625v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.other" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.09319v1</id>
    <updated>2017-07-26T20:24:41Z</updated>
    <published>2017-07-26T20:24:41Z</published>
    <title>A Fourier-invariant method for locating point-masses and computing their
  attributes</title>
    <summary>  Motivated by the interest of observing the growth of cancer cells among
normal living cells and exploring how galaxies and stars are truly formed, the
objective of this paper is to introduce a rigorous and effective method for
counting point-masses, determining their spatial locations, and computing their
attributes. Based on computation of Hermite moments that are Fourier-invariant,
our approach facilitates the processing of both spatial and Fourier data in any
dimension.
</summary>
    <author>
      <name>Charles K. Chui</name>
    </author>
    <author>
      <name>Hrushikesh N. Mhaskar</name>
    </author>
    <link href="http://arxiv.org/abs/1707.09319v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.09319v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.04098v1</id>
    <updated>2017-08-14T12:39:44Z</updated>
    <published>2017-08-14T12:39:44Z</published>
    <title>Statistics Educational Challenge in the 21st Century</title>
    <summary>  What do we teach and what should we teach? An honest answer to this question
is painful, very painful--what we teach lags decades behind what we practice.
How can we reduce this `gap' to prepare a data science workforce of trained
next-generation statisticians? This is a challenging open problem that requires
many well-thought-out experiments before finding the secret sauce. My goal in
this article is to lay out some basic principles and guidelines (rather than
creating a pseudo-curriculum based on cherry-picked topics) to expedite this
process for finding an `objective' solution.
</summary>
    <author>
      <name>Subhadeep Mukhopadhyay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Invited Opinion Article</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.04098v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.04098v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.03981v1</id>
    <updated>2017-09-12T17:38:42Z</updated>
    <published>2017-09-12T17:38:42Z</published>
    <title>Aggregating incoherent agents who disagree</title>
    <summary>  In this paper, we explore how we should aggregate the degrees of belief of of
a group of agents to give a single coherent set of degrees of belief, when at
least some of those agents might be probabilistically incoherent. There are a
number of way of aggregating degrees of belief, and there are a number of ways
of fixing incoherent degrees of belief. When we have picked one of each, should
we aggregate first and then fix, or fix first and then aggregate? Or should we
try to do both at once? And when do these different procedures agree with one
another? In this paper, we focus particularly on the final question.
</summary>
    <author>
      <name>Richard Pettigrew</name>
    </author>
    <link href="http://arxiv.org/abs/1709.03981v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.03981v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.01298v1</id>
    <updated>2017-09-30T17:30:32Z</updated>
    <published>2017-09-30T17:30:32Z</published>
    <title>A Coin-Tossing Conundrum</title>
    <summary>  It is shown that an equiprobability hypothesis leads to a scenario in which
it is possible to predict the outcome of a single toss of a fair coin with a
success probability greater than 50%. We discuss whether this hypothesis might
be independent of the usual hypotheses governing probability, as well as
whether this hypothesis might be assumed as a result of the Principle of
Indifference. Also discussed are ways to implement or circumvent the
hypothesis.
</summary>
    <author>
      <name>James Stein</name>
    </author>
    <author>
      <name>Leonard M. Wapner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.01298v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.01298v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60G50" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.01598v1</id>
    <updated>2017-10-03T04:03:19Z</updated>
    <published>2017-10-03T04:03:19Z</published>
    <title>A geometer's view of the the Cramér-Rao bound on estimator variance</title>
    <summary>  The classical Cram\'er-Rao inequality gives a lower bound for the variance of
a unbiased estimator of an unknown parameter, in some statistical model of a
random process. In this note we rewrite the statment and proof of the bound
using contemporary geometric language. Our intended audience is geometrically
inclined mathematicians with some interest in connections to statistics, with
which they need not be too familiar.
</summary>
    <author>
      <name>Anthony D. Blaom</name>
    </author>
    <link href="http://arxiv.org/abs/1710.01598v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.01598v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.3585v1</id>
    <updated>2011-08-17T21:59:07Z</updated>
    <published>2011-08-17T21:59:07Z</published>
    <title>Some properties of the moment estimator of shape parameter for the gamma
  distribution</title>
    <summary>  Exact distribution of the moment estimator of shape parameter for the gamma
distribution for small samples is derived. Order preserving properties of this
estimator are presented.
</summary>
    <author>
      <name>Piotr Nowak</name>
    </author>
    <link href="http://arxiv.org/abs/1108.3585v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.3585v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.5939v1</id>
    <updated>2013-11-23T00:46:49Z</updated>
    <published>2013-11-23T00:46:49Z</published>
    <title>Hypergeometric tail inequalities: ending the insanity</title>
    <summary>  The hypergeometric distribution is briefly and informally surveyed, including
popular notation, symmetries, and the tail inequalities $Pr[i \ge E[i]+tn] \le
e^{-2t^2n}$ and $Pr[i \le E[i]-tn] \le e^{-2t^2n}$.
</summary>
    <author>
      <name>Matthew Skala</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.5939v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.5939v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60-01" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.5903v1</id>
    <updated>2013-12-20T11:54:46Z</updated>
    <published>2013-12-20T11:54:46Z</published>
    <title>Co-jumps and Markov counting systems in random environments</title>
    <summary>  We provide transition rates for Markov counting systems subject to correlated
environmental noises motivated by multi-strain disease models. Such noises
induce simultaneous counts, which can help model infinitesimal count
correlation (regardless of whether such correlation is due to correlated
noises).
</summary>
    <author>
      <name>Carles Bretó</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.5903v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.5903v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.2220v1</id>
    <updated>2014-02-10T17:31:27Z</updated>
    <published>2014-02-10T17:31:27Z</published>
    <title>Donald Arthur Preece: A life in statistics, mathematics and music</title>
    <summary>  Biography and publications list for Donald Arthur Preece, who died on 6
January 2014, who made many contributions in statistics (experimental design)
and in combinatorics.
</summary>
    <author>
      <name>R. A. Bailey</name>
    </author>
    <link href="http://arxiv.org/abs/1402.2220v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.2220v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.HO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.HO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="01A70, 05B99, 62K99" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.08137v2</id>
    <updated>2017-07-09T18:21:33Z</updated>
    <published>2017-06-25T16:37:38Z</published>
    <title>A Contemporary Overview of Probabilistic Latent Variable Models</title>
    <summary>  In this paper we provide a conceptual overview of latent variable models
within a probabilistic modeling framework, an overview that emphasizes the
compositional nature and the interconnectedness of the seemingly disparate
models commonly encountered in statistical practice.
</summary>
    <author>
      <name>Rick Farouni</name>
    </author>
    <link href="http://arxiv.org/abs/1706.08137v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.08137v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62H99" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1008.0756v1</id>
    <updated>2010-08-04T12:00:33Z</updated>
    <published>2010-08-04T12:00:33Z</published>
    <title>Phasetype distributions, autoregressive processes and overshoot</title>
    <summary>  Autoregressive processes are intensively studied in statistics and other
fields of applied stochastics. For many applications the overshoot and the
threshold-time are of special interest. When the upward innovations are in the
class of phasetype distributions we determine the joint distribution of this
two quantities and apply this result to problems of optimal stopping. Using a
principle of continuous fit this leads to explicit solutions.
</summary>
    <author>
      <name>Sören Christensen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1239/jap/1331216832</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1239/jap/1331216832" rel="related"/>
    <link href="http://arxiv.org/abs/1008.0756v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1008.0756v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60G40, 62L15, 60G99" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.2368v1</id>
    <updated>2011-11-09T23:43:54Z</updated>
    <published>2011-11-09T23:43:54Z</published>
    <title>On a connection between Stein characterizations and Fisher information</title>
    <summary>  We generalize the so-called density approach to Stein characterizations of
probability distributions. We prove an elementary factorization property of the
resulting Stein operator in terms of a generalized (standardized) score
function. We use this result to connect Stein characterizations with
information distances such as the generalized (standardized) Fisher
information.
</summary>
    <author>
      <name>Christophe Ley</name>
    </author>
    <author>
      <name>Yvik Swan</name>
    </author>
    <link href="http://arxiv.org/abs/1111.2368v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.2368v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.4340v2</id>
    <updated>2014-07-15T14:41:58Z</updated>
    <published>2012-09-19T19:31:58Z</published>
    <title>Moments and Absolute Moments of the Normal Distribution</title>
    <summary>  We present formulas for the (raw and central) moments and absolute moments of
the normal distribution. We note that these results are not new, yet many
textbooks miss out on at least some of them. Hence, we believe that it is
worthwhile to collect these formulas and their derivations in these notes.
</summary>
    <author>
      <name>Andreas Winkelbauer</name>
    </author>
    <link href="http://arxiv.org/abs/1209.4340v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.4340v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.0718v1</id>
    <updated>2013-04-02T18:07:18Z</updated>
    <published>2013-04-02T18:07:18Z</published>
    <title>A Peer-based Model of Fat-tailed Outcomes</title>
    <summary>  It is well known that the distribution of returns from various financial
instruments are leptokurtic, meaning that the distributions have "fatter tails"
than a Normal distribution, and have skew toward zero. This paper presents a
graceful micro-level explanation for such fat-tailed outcomes, using agents
whose private valuations have Normally-distributed errors, but whose utility
function includes a term for the percentage of others who also buy.
</summary>
    <author>
      <name>Ben Klemens</name>
    </author>
    <link href="http://arxiv.org/abs/1304.0718v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.0718v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.TR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.TR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.5901v1</id>
    <updated>2013-12-20T11:45:31Z</updated>
    <published>2013-12-20T11:45:31Z</published>
    <title>Trajectory composition of Poisson time changes and Markov counting
  systems</title>
    <summary>  Changing time of simple continuous-time Markov counting processes by
independent unit-rate Poisson processes results in Markov counting processes
for which we provide closed-form transition rates via composition of
trajectories and with which we construct novel, simpler infinitesimally
over-dispersed processes.
</summary>
    <author>
      <name>Carles Bretó</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.spl.2014.01.032</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.spl.2014.01.032" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 1 figure</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Statistics and Probability Letters 2014, Vol. 88, 91-98</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1312.5901v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.5901v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.1649v1</id>
    <updated>2014-12-04T12:53:28Z</updated>
    <published>2014-12-04T12:53:28Z</published>
    <title>A Class of Conjugate Priors Defined on the Unit Simplex</title>
    <summary>  Dirichlet distribution and Dirichlet process as its infinite dimensional
generalization are primarily used conjugate prior of categorical and
multinomial distributions in Bayesian statistics. Extensions have been proposed
to broaden applications for different purposes. In this article, we explore a
class of prior distributions closely related to Dirichlet distribution
incorporating additional information on the data generating mechanism. Examples
are given to show potential use of the models.
</summary>
    <author>
      <name>Xuenan Feng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 1 figure, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.1649v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.1649v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.5471v6</id>
    <updated>2014-02-10T12:28:51Z</updated>
    <published>2010-06-28T21:15:07Z</published>
    <title>Cognitive Constructivism and the Epistemic Significance of Sharp
  Statistical Hypotheses in Natural Sciences</title>
    <summary>  This book presents our case in defense of a constructivist epistemological
framework and the use of compatible statistical theory and inference tools. The
basic metaphor of decision theory is the maximization of a gambler's expected
fortune, according to his own subjective utility, prior beliefs an learned
experiences. This metaphor has proven to be very useful, leading the
development of Bayesian statistics since its XX-th century revival, rooted on
the work of de Finetti, Savage and others. The basic metaphor presented in this
text, as a foundation for cognitive constructivism, is that of an
eigen-solution, and the verification of its objective epistemic status. The
FBST - Full Bayesian Significance Test - is the cornerstone of a set of
statistical tolls conceived to assess the epistemic value of such
eigen-solutions, according to their four essential attributes, namely,
sharpness, stability, separability and composability. We believe that this
alternative perspective, complementary to the one ofered by decision theory,
can provide powerful insights and make pertinent contributions in the context
of scientific research.
</summary>
    <author>
      <name>J. M. Stern</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">426 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1006.5471v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.5471v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1007.1094v1</id>
    <updated>2010-07-07T10:12:20Z</updated>
    <published>2010-07-07T10:12:20Z</published>
    <title>Hotelling's test for highly correlated data</title>
    <summary>  This paper is motivated by the analysis of gene expression sets, especially
by finding differentially expressed gene sets between two phenotypes. Gene
$\log_2$ expression levels are highly correlated and, very likely, have
approximately normal distribution. Therefore, it seems reasonable to use
two-sample Hotelling's test for such data. We discover some unexpected
properties of the test making it different from the majority of tests
previously used for such data. It appears that the Hotelling's test does not
always reach maximal power when all marginal distributions are differentially
expressed. For highly correlated data its maximal power is attained when about
a half of marginal distributions are essentially different. For the case when
the correlation coefficient is greater than 0.5 this test is more powerful if
only one marginal distribution is shifted, omparing to the case when all
marginal distributions are equally shifted. Moreover, when the correlation
coefficient increases the power of Hotelling's test increases as well.
</summary>
    <author>
      <name>Peter Bubeliny</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1007.1094v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1007.1094v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.2723v1</id>
    <updated>2010-09-14T17:38:36Z</updated>
    <published>2010-09-14T17:38:36Z</published>
    <title>Tests of Non-Equivalence among Absolutely Nonsingular Tensors through
  Geometric Invariants</title>
    <summary>  4x4x3 absolutely nonsingular tensors are characterized by their determinant
polynomial. Non-quivalence among absolutely nonsingular tensors with respect to
a class of linear transformations, which do not chage the tensor rank,is
studied. It is shown theoretically that affine geometric invariants of the
constant surface of a determinant polynomial is useful to discriminate
non-equivalence among absolutely nonsingular tensors. Also numerical
caluculations are presented and these invariants are shown to be useful indeed.
For the caluculation of invarinats by 20-spherical design is also commented. We
showed that an algebraic problem in tensor data analysis can be attacked by an
affine geometric method.
</summary>
    <author>
      <name>Toshio Sakata</name>
    </author>
    <author>
      <name>Kazumitsu Maehra</name>
    </author>
    <author>
      <name>Takeshi Sasaki</name>
    </author>
    <author>
      <name>Toshio Sumi</name>
    </author>
    <author>
      <name>Mitsuhiro Miyazaki</name>
    </author>
    <author>
      <name>Yoshitaka Watanabe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 3 figures, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1009.2723v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.2723v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="15A69, 15A72, 53A15 (Primary) 43A90 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1101.0145v1</id>
    <updated>2010-12-30T19:52:48Z</updated>
    <published>2010-12-30T19:52:48Z</published>
    <title>Squaring the Circle and Cubing the Sphere: Circular and Spherical
  Copulas</title>
    <summary>  Do there exist circular and spherical copulas in $R^d$? That is, do there
exist circularly symmetric distributions on the unit disk in $R^2$ and
spherically symmetric distributions on the unit ball in $R^d$, $d\ge3$, whose
one-dimensional marginal distributions are uniform? The answer is yes for $d=2$
and 3, where the circular and spherical copulas are unique and can be
determined explicitly, but no for $d\ge4$. A one-parameter family of elliptical
bivariate copulas is obtained from the unique circular copula in $R^2$ by
oblique coordinate transformations. Copulas obtained by a non-linear
transformation of a uniform distribution on the unit ball in $R^d$ are also
described, and determined explicitly for $d=2$.
</summary>
    <author>
      <name>Michael D. Perlman</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Washington, Seattle</arxiv:affiliation>
    </author>
    <author>
      <name>Jon A. Wellner</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Washington, Seattle</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages; 15 figures submitted to: Symmetry</arxiv:comment>
    <link href="http://arxiv.org/abs/1101.0145v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1101.0145v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62H05, 62E10 (primary), 62H11, 60E05 (secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.6145v4</id>
    <updated>2013-06-18T12:01:41Z</updated>
    <published>2011-05-31T01:19:14Z</published>
    <title>Maximum lilkelihood estimation in the $β$-model</title>
    <summary>  We study maximum likelihood estimation for the statistical model for
undirected random graphs, known as the $\beta$-model, in which the degree
sequences are minimal sufficient statistics. We derive necessary and sufficient
conditions, based on the polytope of degree sequences, for the existence of the
maximum likelihood estimator (MLE) of the model parameters. We characterize in
a combinatorial fashion sample points leading to a nonexistent MLE, and
nonestimability of the probability parameters under a nonexistent MLE. We
formulate conditions that guarantee that the MLE exists with probability
tending to one as the number of nodes increases.
</summary>
    <author>
      <name>Alessandro Rinaldo</name>
    </author>
    <author>
      <name>Sonja Petrović</name>
    </author>
    <author>
      <name>Stephen E. Fienberg</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1214/12-AOS1078</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1214/12-AOS1078" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in at http://dx.doi.org/10.1214/12-AOS1078 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Annals of Statistics 2013, Vol. 41, No. 3, 1085-1110</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1105.6145v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.6145v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.3259v2</id>
    <updated>2011-07-12T23:54:52Z</updated>
    <published>2011-06-16T15:55:15Z</published>
    <title>A flexible observed factor model with separate dynamics for the factor
  volatilities and their correlation matrix</title>
    <summary>  Our article considers a regression model with observed factors. The observed
factors have a flexible stochastic volatility structure that has separate
dynamics for the volatilities and the correlation matrix. The correlation
matrix of the factors is time-varying and its evolution is described by an
inverse Wishart process. The model specifies the evolution of the observed
volatilities flexibly and is particularly attractive when the dimension of the
observations is high. A Markov chain Monte Carlo algorithm is developed to
estimate the model. It is straightforward to use this algorithm to obtain the
predictive distributions of future observations and to carry out model
selection. The model is illustrated and compared to other Wishart-type factor
multivariate stochastic volatility models using various empirical data
including monthly stock returns and portfolio weighted returns. The evidence
suggests that our model has better predictive performance. The paper also
allows the idiosyncratic errors to follow individual stochastic volatility
processes in order to deal with more volatile data such as daily or weekly
stock returns.
</summary>
    <author>
      <name>Yu-Cheng Ku</name>
    </author>
    <author>
      <name>Peter Bloomfield</name>
    </author>
    <author>
      <name>Robert Kohn</name>
    </author>
    <link href="http://arxiv.org/abs/1106.3259v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.3259v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.5640v1</id>
    <updated>2011-09-26T17:05:47Z</updated>
    <published>2011-09-26T17:05:47Z</published>
    <title>Removing Gaussian Noise by Optimization of Weights in Non-Local Means</title>
    <summary>  A new image denoising algorithm to deal with the additive Gaussian white
noise model is given. Like the non-local means method, the filter is based on
the weighted average of the observations in a neighborhood, with weights
depending on the similarity of local patches. But in contrast to the non-local
means filter, instead of using a fixed Gaussian kernel, we propose to choose
the weights by minimizing a tight upper bound of mean square error. This
approach makes it possible to define the weights adapted to the function at
hand, mimicking the weights of the oracle filter. Under some regularity
conditions on the target image, we show that the obtained estimator converges
at the usual optimal rate. The proposed algorithm is parameter free in the
sense that it automatically calculates the bandwidth of the smoothing kernel;
it is fast and its implementation is straightforward. The performance of the
new filter is illustrated by numerical simulations.
</summary>
    <author>
      <name>Qiyu Jin</name>
    </author>
    <author>
      <name>Ion Grama</name>
    </author>
    <author>
      <name>Quansheng Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages, 6 figures and 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1109.5640v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.5640v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.5909v5</id>
    <updated>2014-03-12T06:59:12Z</updated>
    <published>2011-09-27T14:26:46Z</published>
    <title>Markov properties for mixed graphs</title>
    <summary>  In this paper, we unify the Markov theory of a variety of different types of
graphs used in graphical Markov models by introducing the class of loopless
mixed graphs, and show that all independence models induced by $m$-separation
on such graphs are compositional graphoids. We focus in particular on the
subclass of ribbonless graphs which as special cases include undirected graphs,
bidirected graphs, and directed acyclic graphs, as well as ancestral graphs and
summary graphs. We define maximality of such graphs as well as a pairwise and a
global Markov property. We prove that the global and pairwise Markov properties
of a maximal ribbonless graph are equivalent for any independence model that is
a compositional graphoid.
</summary>
    <author>
      <name>Kayvan Sadeghi</name>
    </author>
    <author>
      <name>Steffen Lauritzen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3150/12-BEJ502</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3150/12-BEJ502" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in at http://dx.doi.org/10.3150/12-BEJ502 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Bernoulli 2014, Vol. 20, No. 2, 676-696</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1109.5909v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.5909v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.6565v1</id>
    <updated>2011-09-29T15:42:07Z</updated>
    <published>2011-09-29T15:42:07Z</published>
    <title>A Statistical Significance Simulation Study for the General Scientist</title>
    <summary>  When a scientist performs an experiment they normally acquire a set of
measurements and are expected to demonstrate that their results are
"statistically significant" thus confirming whatever hypothesis they are
testing. The main method for establishing statistical significance involves
demonstrating that there is a low probability that the observed experimental
results were the product of random chance. This is typically defined as p &lt;
0.05, which indicates there is less than a 5% chance that the observed results
occurred randomly. This research study visually demonstrates that the commonly
used definition for "statistical significance" can erroneously imply a
significant finding. This is demonstrated by generating random Gaussian noise
data and analyzing that data using statistical testing based on the established
two-sample t-test. This study demonstrates that insignificant yet
"statistically significant" findings are possible at moderately large sample
sizes which are very common in many fields of modern science.
</summary>
    <author>
      <name>Jacob Levman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 3 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1109.6565v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.6565v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.4168v3</id>
    <updated>2013-12-17T08:25:13Z</updated>
    <published>2011-10-19T03:18:24Z</published>
    <title>Stable mixed graphs</title>
    <summary>  In this paper, we study classes of graphs with three types of edges that
capture the modified independence structure of a directed acyclic graph (DAG)
after marginalisation over unobserved variables and conditioning on selection
variables using the $m$-separation criterion. These include MC, summary, and
ancestral graphs. As a modification of MC graphs, we define the class of
ribbonless graphs (RGs) that permits the use of the $m$-separation criterion.
RGs contain summary and ancestral graphs as subclasses, and each RG can be
generated by a DAG after marginalisation and conditioning. We derive simple
algorithms to generate RGs, from given DAGs or RGs, and also to generate
summary and ancestral graphs in a simple way by further extension of the
RG-generating algorithm. This enables us to develop a parallel theory on these
three classes and to study the relationships between them as well as the use of
each class.
</summary>
    <author>
      <name>Kayvan Sadeghi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3150/12-BEJ454</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3150/12-BEJ454" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in at http://dx.doi.org/10.3150/12-BEJ454 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Bernoulli 2013, Vol. 19, No. 5B, 2330-2358</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1110.4168v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.4168v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.4539v1</id>
    <updated>2011-10-20T14:30:15Z</updated>
    <published>2011-10-20T14:30:15Z</published>
    <title>Markov Equivalences for Subclasses of Loopless Mixed Graphs</title>
    <summary>  In this paper we discuss four problems regarding Markov equivalences for
subclasses of loopless mixed graphs. We classify these four problems as finding
conditions for internal Markov equivalence, which is Markov equivalence within
a subclass, for external Markov equivalence, which is Markov equivalence
between subclasses, for representational Markov equivalence, which is the
possibility of a graph from a subclass being Markov equivalent to a graph from
another subclass, and finding algorithms to generate a graph from a certain
subclass that is Markov equivalent to a given graph. We particularly focus on
the class of maximal ancestral graphs and its subclasses, namely regression
graphs, bidirected graphs, undirected graphs, and directed acyclic graphs, and
present novel results for representational Markov equivalence and algorithms.
</summary>
    <author>
      <name>Kayvan Sadeghi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 8 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1110.4539v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.4539v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.5110v3</id>
    <updated>2012-06-08T10:46:55Z</updated>
    <published>2011-11-22T06:12:16Z</published>
    <title>Grouped sparse paired comparisons in the Bradley-Terry model</title>
    <summary>  In a wide class of paired comparisons, especially in the sports games, in
which all subjects are divided into several groups, the intragroup comparisons
are dense and the intergroup comparisons are sparse. Typical examples include
the NFL regular season. Motivated by these situations, we propose group
sparsity for paired comparisons and show the consistency and asymptotical
normality of the maximum likelihood estimate in the Bradley-Terry model when
the number of parameters goes to infinity in this paper. Simulations are
carried out to illustrate the group sparsity and asymptotical results.
</summary>
    <author>
      <name>Ting Yan</name>
    </author>
    <author>
      <name>Jinfeng Xu</name>
    </author>
    <author>
      <name>Yaning Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by the author due to that it needs to
  be revised greatly</arxiv:comment>
    <link href="http://arxiv.org/abs/1111.5110v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.5110v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.0846v1</id>
    <updated>2012-01-04T08:53:42Z</updated>
    <published>2012-01-04T08:53:42Z</published>
    <title>Using complex surveys to estimate the $L_1$-median of a functional
  variable: application to electricity load curves</title>
    <summary>  Mean profiles are widely used as indicators of the electricity consumption
habits of customers. Currently, in \'Electricit\'e De France (EDF), class load
profiles are estimated using point-wise mean function. Unfortunately, it is
well known that the mean is highly sensitive to the presence of outliers, such
as one or more consumers with unusually high-levels of consumption. In this
paper, we propose an alternative to the mean profile: the $L_1$-median profile
which is more robust. When dealing with large datasets of functional data (load
curves for example), survey sampling approaches are useful for estimating the
median profile avoiding storing the whole data. We propose here estimators of
the median trajectory using several sampling strategies and estimators. A
comparison between them is illustrated by means of a test population. We
develop a stratification based on the linearized variable which substantially
improves the accuracy of the estimator compared to simple random sampling
without replacement. We suggest also an improved estimator that takes into
account auxiliary information. Some potential areas for future research are
also highlighted.
</summary>
    <author>
      <name>Mohamed Chaouch</name>
    </author>
    <author>
      <name>Camelia Goga</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1111/j.1751-5823.2011.00172.x</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1111/j.1751-5823.2011.00172.x" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear in International Statistical Review</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Statistical Review, 2012, Vol. 80, pages 40-59</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1201.0846v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.0846v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.6450v1</id>
    <updated>2012-01-31T05:54:46Z</updated>
    <published>2012-01-31T05:54:46Z</published>
    <title>A Brief History of the Statistics Department of the University of
  California at Berkeley</title>
    <summary>  The early history of our department was dominated by Jerzy Neyman
(1894-1981), while the next phase was largely in the hands of Neyman's
students, with Erich Lehmann (1917-2009) being a central, long-lived and
much-loved member of this group. We are very fortunate in having Constance
Reid's biography "Neyman -- From Life" and Erich's "Reminiscences of a
Statistician: The Company I Kept" and other historical material documenting the
founding and growth of the department, and the people in it. In what follows,
we will draw heavily from these sources, describing what seems to us to be a
remarkable success story: one person starting "a cell of statistical research
and teaching ... not being hampered by any existing traditions and routines"
and seeing that cell grow rapidly into a major force in academic statistics
worldwide. That it has remained so for (at least) the half-century after its
founding is a testament to the strength of Neyman's model for a department of
statistics.
</summary>
    <author>
      <name>Terry Speed</name>
    </author>
    <author>
      <name>Jim Pitman</name>
    </author>
    <author>
      <name>John Rice</name>
    </author>
    <link href="http://arxiv.org/abs/1201.6450v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.6450v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.HO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62-03 01Axx" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.4208v1</id>
    <updated>2012-06-19T13:50:41Z</updated>
    <published>2012-06-19T13:50:41Z</published>
    <title>A Fast Non-Gaussian Bayesian Matching Pursuit Method for Sparse
  Reconstruction</title>
    <summary>  A fast matching pursuit method using a Bayesian approach is introduced for
sparse signal recovery. This method, referred to as nGpFBMP, performs Bayesian
estimates of sparse signals even when the signal prior is non-Gaussian or
unknown. It is agnostic on signal statistics and utilizes a priori statistics
of additive noise and the sparsity rate of the signal, which are shown to be
easily estimated from data if not available. nGpFBMP utilizes a greedy approach
and order-recursive updates of its metrics to find the most dominant sparse
supports to determine the approximate minimum mean square error (MMSE) estimate
of the sparse signal. Simulation results demonstrate the power and robustness
of our proposed estimator.
</summary>
    <author>
      <name>Mudassir Masood</name>
    </author>
    <author>
      <name>Tareq Al-Naffouri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures, 3 tables, submitted to IEEE Transactions on
  Signal Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.4208v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.4208v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.0700v1</id>
    <updated>2012-07-03T14:39:42Z</updated>
    <published>2012-07-03T14:39:42Z</published>
    <title>A statistical view on team handball results: home advantage, team
  fitness and prediction of match outcomes</title>
    <summary>  We analyze the results of the German Team Handball Bundesliga for ten seasons
in a model-free statistical time series approach. We will show that the home
advantage is nearly negligible compared to the total sum of goals. Specific
interest has been spent on the time evolution of the team fitness expressed in
terms of the goal difference. In contrast to soccer, our results indicate a
decay of the team fitness values over a season while the long time correlation
behavior over years is nearly comparable. We are able to explain the dominance
of a few teams by the large value for the total number of goals in a match. A
method for the prediction of match winners is presented in good accuracy with
the real results. We analyze the properties of promoted teams and indicate
drastic level changes between the Bundesliga and the second league. Our
findings reflect in good agreement recent discussions on modern successful
attack strategies.
</summary>
    <author>
      <name>Jens Smiatek</name>
    </author>
    <author>
      <name>Andreas Heuer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1207.0700v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.0700v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.4309v2</id>
    <updated>2012-09-20T08:16:24Z</updated>
    <published>2012-07-18T09:23:12Z</published>
    <title>Vine Constructions of Levy Copulas</title>
    <summary>  Levy copulas are the most general concept to capture jump dependence in
multivariate Levy processes. They translate the intuition and many features of
the copula concept into a time series setting. A challenge faced by both,
distributional and Levy copulas, is to find flexible but still applicable
models for higher dimensions. To overcome this problem, the concept of pair
copula constructions has been successfully applied to distributional copulas.
In this paper, we develop the pair construction for Levy copulas (PLCC).
Similar to pair constructions of distributional copulas, the pair construction
of a d-dimensional Levy copula consists of d(d-1)/2 bivariate dependence
functions. We show that only d-1 of these bivariate functions are Levy copulas,
whereas the remaining functions are distributional copulas. Since there are no
restrictions concerning the choice of the copulas, the proposed pair
construction adds the desired flexibility to Levy copula models. We discuss
estimation and simulation in detail and apply the pair construction in a
simulation study.
</summary>
    <author>
      <name>Oliver Grothe</name>
    </author>
    <author>
      <name>Stephan Nicklas</name>
    </author>
    <link href="http://arxiv.org/abs/1207.4309v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.4309v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.RM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60G51, 62H99" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.1487v3</id>
    <updated>2016-11-23T16:47:54Z</updated>
    <published>2012-08-07T18:24:36Z</published>
    <title>Demmartingales and the functionnal Hill process for small parameters</title>
    <summary>  Association of random variables and Demimartingales are recent fields for
handling asymptotic behaviors of sums of dependent random variables. We apply
their techniques to establish the asymptotic law of a demimartingale
  We next apply the results to find the asymptotic behavior the functional Hill
process for small parameters within the Extreme Value Theory (EVT) field. Such
a result would have been very hard to find whithout demimartingales techniques.
</summary>
    <author>
      <name>Adja Mbarka Fall</name>
    </author>
    <author>
      <name>Gane Samb Lo</name>
    </author>
    <author>
      <name>Cheikhna Hamallah Ndiaye</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by the author due to a crucial
  representation error of slowly varying function. We will update and correct
  it for a new submission</arxiv:comment>
    <link href="http://arxiv.org/abs/1208.1487v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.1487v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.4019v3</id>
    <updated>2017-08-30T20:45:30Z</updated>
    <published>2012-09-18T16:28:44Z</published>
    <title>Experimental design for Partially Observed Markov Decision Processes</title>
    <summary>  This paper deals with the question of how to most effectively conduct
experiments in Partially Observed Markov Decision Processes so as to provide
data that is most informative about a parameter of interest. Methods from
Markov decision processes, especially dynamic programming, are introduced and
then used in an algorithm to maximize a relevant Fisher Information. The
algorithm is then applied to two POMDP examples. The methods developed can also
be applied to stochastic dynamical systems, by suitable discretization, and we
consequently show what control policies look like in the Morris-Lecar Neuron
model, and simulation results are presented. We discuss how parameter
dependence within these methods can be dealt with by the use of priors, and
develop tools to update control policies online. This is demonstrated in
another stochastic dynamical system describing growth dynamics of DNA template
in a PCR model.
</summary>
    <author>
      <name>Leifur Thorbergsson</name>
    </author>
    <author>
      <name>Giles Hooker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">39 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.4019v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.4019v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.5272v2</id>
    <updated>2012-11-16T08:19:31Z</updated>
    <published>2012-09-24T13:59:13Z</published>
    <title>Does the specification of uncertainty hurt the progress of
  scientometrics?</title>
    <summary>  In "Caveats for using statistical significance tests in research
assessments,"--Journal of Informetrics 7(1)(2013) 50-62, available at
arXiv:1112.2516 -- Schneider (2013) focuses on Opthof &amp; Leydesdorff (2010) as
an example of the misuse of statistics in the social sciences. However, our
conclusions are theoretical since they are not dependent on the use of one
statistics or another. We agree with Schneider insofar as he proposes to
develop further statistical instruments (such as effect sizes). Schneider
(2013), however, argues on meta-theoretical grounds against the specification
of uncertainty because, in his opinion, the presence of statistics would
legitimate decision-making. We disagree: uncertainty can also be used for
opening a debate. Scientometric results in which error bars are suppressed for
meta-theoretical reasons should not be trusted.
</summary>
    <author>
      <name>Loet Leydesdorff</name>
    </author>
    <link href="http://arxiv.org/abs/1209.5272v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.5272v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.0962v1</id>
    <updated>2012-10-03T02:28:34Z</updated>
    <published>2012-10-03T02:28:34Z</published>
    <title>Breaking Monotony with Meaning: Motivation in Crowdsourcing Markets</title>
    <summary>  We conduct the first natural field experiment to explore the relationship
between the "meaningfulness" of a task and worker effort. We employed about
2,500 workers from Amazon's Mechanical Turk (MTurk), an online labor market, to
label medical images. Although given an identical task, we experimentally
manipulated how the task was framed. Subjects in the meaningful treatment were
told that they were labeling tumor cells in order to assist medical
researchers, subjects in the zero-context condition (the control group) were
not told the purpose of the task, and, in stark contrast, subjects in the
shredded treatment were not given context and were additionally told that their
work would be discarded. We found that when a task was framed more
meaningfully, workers were more likely to participate. We also found that the
meaningful treatment increased the quantity of output (with an insignificant
change in quality) while the shredded treatment decreased the quality of output
(with no change in quantity). We believe these results will generalize to other
short-term labor markets. Our study also discusses MTurk as an exciting
platform for running natural field experiments in economics.
</summary>
    <author>
      <name>Dana Chandler</name>
    </author>
    <author>
      <name>Adam Kapelner</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.jebo.2013.03.003</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.jebo.2013.03.003" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 15 figures, 4 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Economic Behavior &amp; Organization, 90, 2013, pp 123-133</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1210.0962v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.0962v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3; J.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.7225v1</id>
    <updated>2012-10-26T19:33:43Z</updated>
    <published>2012-10-26T19:33:43Z</published>
    <title>The anti-Bayesian moment and its passing</title>
    <summary>  The present article is the reply to the discussion of our earlier "Not only
defended but also applied" (arXiv:1006.5366, to appear in The American
Statistician) that arose from our memory of a particularly intemperate
anti-Bayesian statement in Feller's beautiful and classic book on probability
theory. We felt that it was worth exploring the very extremeness of Feller's
words, along with similar anti-Bayesian remarks by others, in order to better
understand the background underlying controversies that still exist regarding
the foundations of statistics. We thank the four discussants of our article for
their contributions to our understanding of these controversies as they have
existed in the past and persist today.
</summary>
    <author>
      <name>Andrew Gelman</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Columbia University</arxiv:affiliation>
    </author>
    <author>
      <name>Christian P. Robert</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University Paris-Dauphine and CREST</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, to appear in The American Statistician as a reply to the
  discussion</arxiv:comment>
    <link href="http://arxiv.org/abs/1210.7225v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.7225v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.HO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.0765v2</id>
    <updated>2013-01-14T13:11:09Z</updated>
    <published>2013-01-04T16:28:23Z</published>
    <title>Perceptive Statistical Variability Indicators</title>
    <summary>  The concepts of variability and uncertainty, both epistemic and alleatory,
came from experience and coexist with different connotations. Therefore this
article attempts to express their relation by analytic means firstly setting
sights on their differences and then on their common characteristics. Inspired
with the definition of average number of equally probable events based on
entropy concept in probability theory, the article introduced two related
perceptive statistical measures which indicate the same variability as the
basic probability distribution. First is the equivalent number of a
hypothetical distribution with one sure and all the other impossible outcomes
which indicates variability. Second is the appropriate equivalent number of a
hypothetical distribution with all equal probabilities which indicates
invariability. The article interprets the common properties of variability and
uncertainty on theoretical distributions and on ocean-wide wind wave
directional properties by using the long term observations compiled in the
Global Wave Statistics.
</summary>
    <author>
      <name>Kalman Ziha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 7 pictures, 2 tables Originally written in MS Word .doc
  forat</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.0765v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.0765v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.3518v1</id>
    <updated>2013-03-14T17:13:44Z</updated>
    <published>2013-03-14T17:13:44Z</published>
    <title>Propagation of initial errors on the parameters for linear and Gaussian
  state space models</title>
    <summary>  For linear and Gaussian state space models parametrized by $\theta_0 \in
\Theta \subset \mathbb{R}^r, r \geq 1$ corresponding to the vector of
parameters of the model, the Kalman filter gives exactly the solution for the
optimal filtering under weak assumptions. This result supposes that $\theta_0$
is perfectly known. In most real applications, this assumption is not realistic
since $\theta_0$ is unknown and has to be estimated. In this paper, we analysis
the Kalman filter for a biased estimator of $\theta_0$. We show the propagation
of this bias on the estimation of the hidden state. We give an expression of
this propagation for linear and Gaussian state space models and we extend this
result for almost linear models estimated by the Extended Kalman filter. An
illustration is given for the autoregressive process with measurement noises
widely studied in econometrics to model economic and financial data.
</summary>
    <author>
      <name>Salima El Kolei</name>
    </author>
    <link href="http://arxiv.org/abs/1303.3518v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.3518v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.7445v1</id>
    <updated>2013-09-28T10:19:10Z</updated>
    <published>2013-09-28T10:19:10Z</published>
    <title>I hear, I forget. I do, I understand: a modified Moore-method
  mathematical statistics course</title>
    <summary>  Moore introduced a method for graduate mathematics instruction that consisted
primarily of individual student work on challenging proofs (Jones, 1977). Cohen
(1982) described an adaptation with less explicit competition suitable for
undergraduate students at a liberal arts college. This paper details an
adaptation of this modified Moore-method to teach mathematical statistics, and
describes ways that such an approach helps engage students and foster the
teaching of statistics.
  Groups of students worked a set of 3 difficult problems (some theoretical,
some applied) every two weeks. Class time was devoted to coaching sessions with
the instructor, group meeting time, and class presentations. R was used to
estimate solutions empirically where analytic results were intractable, as well
as to provide an environment to undertake simulation studies with the aim of
deepening understanding and complementing analytic solutions. Each group
presented comprehensive solutions to complement oral presentations. Development
of parallel techniques for empirical and analytic problem solving was an
explicit goal of the course, which also attempted to communicate ways that
statistics can be used to tackle interesting problems. The group problem
solving component and use of technology allowed students to attempt much more
challenging questions than they could otherwise solve.
</summary>
    <author>
      <name>Nicholas Jon Horton</name>
    </author>
    <link href="http://arxiv.org/abs/1309.7445v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.7445v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62-01" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.7851v3</id>
    <updated>2014-07-14T02:25:47Z</updated>
    <published>2013-12-30T20:21:15Z</published>
    <title>Effective Degrees of Freedom: A Flawed Metaphor</title>
    <summary>  To most applied statisticians, a fitting procedure's degrees of freedom is
synonymous with its model complexity, or its capacity for overfitting to data.
In particular, it is often used to parameterize the bias-variance tradeoff in
model selection. We argue that, contrary to folk intuition, model complexity
and degrees of freedom are not synonymous and may correspond very poorly. We
exhibit and theoretically explore various examples of fitting procedures for
which degrees of freedom is not monotonic in the model complexity parameter,
and can exceed the total dimension of the response space. Even in very simple
settings, the degrees of freedom can exceed the dimension of the ambient space
by an arbitrarily large amount. We show the degrees of freedom for any
non-convex projection method can be unbounded.
</summary>
    <author>
      <name>Lucas Janson</name>
    </author>
    <author>
      <name>William Fithian</name>
    </author>
    <author>
      <name>Trevor Hastie</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1093/biomet/asv019</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1093/biomet/asv019" rel="related"/>
    <link href="http://arxiv.org/abs/1312.7851v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.7851v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.1089v1</id>
    <updated>2014-02-04T19:02:27Z</updated>
    <published>2014-02-04T19:02:27Z</published>
    <title>Null hypothesis significance tests: A mix-up of two different theories,
  the basis for widespread confusion and numerous misinterpretations</title>
    <summary>  Null hypothesis statistical significance tests (NHST) are widely used in
quantitative research in the empirical sciences including scientometrics.
Nevertheless, since their introduction nearly a century ago significance tests
have been controversial. Many researchers are not aware of the numerous
criticisms raised against NHST. As practiced, NHST has been characterized as a
null ritual that is overused and too often misapplied and misinterpreted. NHST
is in fact a patchwork of two fundamentally different classical statistical
testing models, often blended with some wishful quasi-Bayesian interpretations.
This is undoubtedly a major reason why NHST is very often misunderstood. But
NHST also has intrinsic logical problems and the epistemic range of the
information provided by such tests is much more limited than most researchers
recognize. In this article we introduce to the scientometric community the
theoretical origins of NHST, which is mostly absent from standard statistical
textbooks, and we discuss some of the most prevalent problems relating to the
practice of NHST and trace these problems back to the mixup of the two
different theoretical origins. Finally, we illustrate some of the
misunderstandings with examples from the scientometric literature and bring
forward some modest recommendations for a more sound practice in quantitative
data analysis.
</summary>
    <author>
      <name>Jesper W. Schneider</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in the journal Scientometrics</arxiv:comment>
    <link href="http://arxiv.org/abs/1402.1089v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.1089v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.1894v1</id>
    <updated>2014-02-08T22:08:04Z</updated>
    <published>2014-02-08T22:08:04Z</published>
    <title>R Markdown: Integrating A Reproducible Analysis Tool into Introductory
  Statistics</title>
    <summary>  Nolan and Temple Lang argue that "the ability to express statistical
computations is an essential skill." A key related capacity is the ability to
conduct and present data analysis in a way that another person can understand
and replicate. The copy-and-paste workflow that is an artifact of antiquated
user-interface design makes reproducibility of statistical analysis more
difficult, especially as data become increasingly complex and statistical
methods become increasingly sophisticated. R Markdown is a new technology that
makes creating fully-reproducible statistical analysis simple and painless. It
provides a solution suitable not only for cutting edge research, but also for
use in an introductory statistics course. We present evidence that R Markdown
can be used effectively in introductory statistics courses, and discuss its
role in the rapidly-changing world of statistical computation.
</summary>
    <author>
      <name>Ben Baumer</name>
    </author>
    <author>
      <name>Mine Cetinkaya-Rundel</name>
    </author>
    <author>
      <name>Andrew Bray</name>
    </author>
    <author>
      <name>Linda Loi</name>
    </author>
    <author>
      <name>Nicholas J. Horton</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, plus a 10 page appendix</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Technology Innovations in Statistics Education, 8(1), 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1402.1894v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.1894v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62-01" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.3371v2</id>
    <updated>2014-04-09T16:25:31Z</updated>
    <published>2014-03-13T19:01:28Z</published>
    <title>Spectral Correlation Hub Screening of Multivariate Time Series</title>
    <summary>  This chapter discusses correlation analysis of stationary multivariate
Gaussian time series in the spectral or Fourier domain. The goal is to identify
the hub time series, i.e., those that are highly correlated with a specified
number of other time series. We show that Fourier components of the time series
at different frequencies are asymptotically statistically independent. This
property permits independent correlation analysis at each frequency,
alleviating the computational and statistical challenges of high-dimensional
time series. To detect correlation hubs at each frequency, an existing
correlation screening method is extended to the complex numbers to accommodate
complex-valued Fourier components. We characterize the number of hub
discoveries at specified correlation and degree thresholds in the regime of
increasing dimension and fixed sample size. The theory specifies appropriate
thresholds to apply to sample correlation matrices to detect hubs and also
allows statistical significance to be attributed to hub discoveries. Numerical
results illustrate the accuracy of the theory and the usefulness of the
proposed spectral framework.
</summary>
    <author>
      <name>Hamed Firouzi</name>
    </author>
    <author>
      <name>Dennis Wei</name>
    </author>
    <author>
      <name>Alfred O. Hero III</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages, To appear in Excursions in Harmonic Analysis: The February
  Fourier Talks at the Norbert Wiener Center</arxiv:comment>
    <link href="http://arxiv.org/abs/1403.3371v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.3371v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.2781v1</id>
    <updated>2014-05-12T14:36:45Z</updated>
    <published>2014-05-12T14:36:45Z</published>
    <title>Conditional quantile estimation through optimal quantization</title>
    <summary>  In this paper, we use quantization to construct a nonparametric estimator of
conditional quantiles of a scalar response $Y$ given a d-dimensional vector of
covariates $X$. First we focus on the population level and show how optimal
quantization of $X$, which consists in discretizing $X$ by projecting it on an
appropriate grid of $N$ points, allows to approximate conditional quantiles of
$Y$ given $X$. We show that this is approximation is arbitrarily good as $N$
goes to infinity and provide a rate of convergence for the approximation error.
Then we turn to the sample case and define an estimator of conditional
quantiles based on quantization ideas. We prove that this estimator is
consistent for its fixed-$N$ population counterpart. The results are
illustrated on a numerical example. Dominance of our estimators over local
constant/linear ones and nearest neighbor ones is demonstrated through
extensive simulations in the companion paper Charlier et al.(2014b).
</summary>
    <author>
      <name>Isabelle Charlier</name>
    </author>
    <author>
      <name>Davy Paindaveine</name>
    </author>
    <author>
      <name>Jérôme Saracco</name>
    </author>
    <link href="http://arxiv.org/abs/1405.2781v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.2781v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.4136v1</id>
    <updated>2014-05-16T11:47:51Z</updated>
    <published>2014-05-16T11:47:51Z</published>
    <title>The Sociotype, a New Conceptual Construct on Human Social Networks:
  Application in Mental Health and Quality of Life</title>
    <summary>  The present work discusses the pertinence of a 'sociotype' construct, both
theoretically and empirically oriented. The term, based on the conceptual chain
genotype-phenotype-sociotype, suggests an evolutionary preference in the human
species for some determined averages of social relationships. This core pattern
or 'sociotype' has been explored herein for the networking relationships of
young people--165 university students filling in a 20-items questionnaire on
their social interactions. In spite that this is a preliminary study,
interesting results have been obtained on gender conversation time, mental
health, sociability level, and satisfaction with personal relationships. This
sociotype hypothesis could be a timely enterprise for mental health and quality
of life policies.
</summary>
    <author>
      <name>R. del Moral</name>
    </author>
    <author>
      <name>J. Navarro</name>
    </author>
    <author>
      <name>Y. Lopez-del Hoyo</name>
    </author>
    <author>
      <name>J. D. Gomez-Quintero</name>
    </author>
    <author>
      <name>J. Garcia-Campayo</name>
    </author>
    <author>
      <name>P. C. Marijuan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 4 tables, 1 annex</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.4136v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.4136v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.6416v1</id>
    <updated>2014-05-25T18:36:29Z</updated>
    <published>2014-05-25T18:36:29Z</published>
    <title>Discussion of "Single and Two-Stage Cross-Sectional and Time Series
  Benchmarking Procedures for SAE"</title>
    <summary>  We congratulate the authors for a stimulating and valuable manuscript,
providing a careful review of the state-of the-art in cross-sectional and
time-series benchmarking procedures for small area estimation. They develop a
novel two-stage benchmarking method for hierarchical time series models, where
they evaluate their procedure by estimating monthly total unemployment using
data from the U.S. Census Bureau. We discuss three topics: linearity and model
misspecification, computational complexity and model comparisons, and, some
aspects on small area estimation in practice. More specifically, we pose the
following questions to the authors, that they may wish to answer: How robust is
their model to misspecification? Is it time to perhaps move away from linear
models of the type considered by (Battese et al. 1988; Fay and Herriot 1979)?
What is the asymptotic computational complexity and what comparisons can be
made to other models? Should the benchmarking constraints be inherently fixed
or should they be random?
</summary>
    <author>
      <name>Rebecca C. Steorts</name>
    </author>
    <author>
      <name>M. Delores Ugarte</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.6416v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.6416v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.7129v4</id>
    <updated>2016-08-28T20:00:42Z</updated>
    <published>2014-05-28T06:29:35Z</published>
    <title>Marginalization and Conditioning for LWF Chain Graphs</title>
    <summary>  In this paper, we deal with the problem of marginalization over and
conditioning on two disjoint subsets of the node set of chain graphs (CGs) with
the LWF Markov property. For this purpose, we define the class of chain mixed
graphs (CMGs) with three types of edges and, for this class, provide a
separation criterion under which the class of CMGs is stable under
marginalization and conditioning and contains the class of LWF CGs as its
subclass. We provide a method for generating such graphs after marginalization
and conditioning for a given CMG or a given LWF CG. We then define and study
the class of anterial graphs, which is also stable under marginalization and
conditioning and contains LWF CGs, but has a simpler structure than CMGs.
</summary>
    <author>
      <name>Kayvan Sadeghi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1214/16-AOS1451</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1214/16-AOS1451" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">48 pages, 9 figures, 2 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Annals of Statistics, 44 (4), 1792--1816, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1405.7129v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.7129v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.5004v2</id>
    <updated>2014-12-09T09:15:43Z</updated>
    <published>2014-06-19T10:53:39Z</published>
    <title>A mobile web for enhancing statistics and mathematics education</title>
    <summary>  A freely available educational application (a mobile website) is presented.
This provides access to educational material and drilling on selected topics
within mathematics and statistics with an emphasis on tablets and mobile
phones. The application adapts to the student's performance, selecting from
easy to difficult questions, or older material etc. These adaptations are based
on statistical models and analyses of data from testing precursors of the
system within several courses, from calculus and introductory statistics
through multiple linear regression. The application can be used in both on-line
and off-line modes. The behavior of the application is determined by
parameters, the effects of which can be estimated statistically. Results
presented include analyses of how the internal algorithms relate to passing a
course and general incremental improvement in knowledge during a semester.
</summary>
    <author>
      <name>Jamie Lentin</name>
    </author>
    <author>
      <name>Anna H. Jonsdottir</name>
    </author>
    <author>
      <name>David Stern</name>
    </author>
    <author>
      <name>Victoria Mokua</name>
    </author>
    <author>
      <name>Gunnar Stefansson</name>
    </author>
    <link href="http://arxiv.org/abs/1406.5004v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.5004v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ed-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62-07, 62K99, 97A99" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.5241v1</id>
    <updated>2014-06-19T23:38:33Z</updated>
    <published>2014-06-19T23:38:33Z</published>
    <title>Using Google Scholar to predict self citation: A case study in Health
  Economics</title>
    <summary>  Metrics designed to quantify the influence of academics are increasingly used
and easily estimable, and perhaps the most popular is the h index. Metrics such
as this are however potentially impacted through excessive self citation. This
work explores the issue using a group of researchers working in a well defined
sub field of economics, namely Health Economics. It then employs self citation
identification software, and identifies the characteristics that best predict
self citation. This provides evidence regarding the scale of self citation in
the field, and the degree to which self citation impacts on inferences about
the relative influence of individual Health Economists. Using data from 545
Health Economists, it suggests self citation to be associated with the
geographical region and longevity of the Health Economist, with early career
researchers and researchers from mainland Europe and Australasia self citing
most frequently.
</summary>
    <author>
      <name>Richard Norman</name>
    </author>
    <author>
      <name>Francisco M Couto</name>
    </author>
    <link href="http://arxiv.org/abs/1406.5241v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.5241v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.6018v3</id>
    <updated>2016-02-28T16:03:33Z</updated>
    <published>2014-06-23T18:48:26Z</published>
    <title>A brief history of long memory: Hurst, Mandelbrot and the road to ARFIMA</title>
    <summary>  Long memory plays an important role in many fields by determining the
behaviour and predictability of systems; for instance, climate, hydrology,
finance, networks and DNA sequencing. In particular, it is important to test if
a process is exhibiting long memory since that impacts the accuracy and
confidence with which one may predict future events on the basis of a small
amount of historical data. A major force in the development and study of long
memory was the late Benoit B. Mandelbrot. Here we discuss the original
motivation of the development of long memory and Mandelbrot's influence on this
fascinating field. We will also elucidate the sometimes contrasting approaches
to long memory in different scientific communities
</summary>
    <author>
      <name>Timothy Graves</name>
    </author>
    <author>
      <name>Robert B. Gramacy</name>
    </author>
    <author>
      <name>Nicholas Watkins</name>
    </author>
    <author>
      <name>Christian Franzke</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3390/e19090437</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3390/e19090437" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">40 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Entropy, 19(9), (2017) 437</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1406.6018v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.6018v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.0308v1</id>
    <updated>2014-07-01T16:26:21Z</updated>
    <published>2014-07-01T16:26:21Z</published>
    <title>Using an Online Learning Environment to Teach an Undergraduate
  Statistics Course: the tutor-web</title>
    <summary>  A learning environment, the tutor-web (http://tutor-web.net), has been
developed and used for educational research. The system is accessible and free
to use for anyone having access to the Web. It is based on open source software
and the teaching material is licensed under the Creative Commons
Attribution-ShareAlike License. The system has been used for computer-assisted
education in statistics and mathematics. It offers a unique way to structure
and link together teaching material and includes interactive quizzes with the
primary purpose of increasing learning rather than mere evaluation.
  The system was used in a course on basic statistics in the University of
Iceland, spring 2013. A randomized trial was conducted to investigate the
difference in learning between students doing regular homework and students
using the system. The difference between the groups was not found to be
significant.
</summary>
    <author>
      <name>Anna Helga Jonsdottir</name>
    </author>
    <author>
      <name>Gunnar Stefansson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at Edulearn 2013, Barcelona</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.0308v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.0308v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.0963v1</id>
    <updated>2014-08-05T13:34:26Z</updated>
    <published>2014-08-05T13:34:26Z</published>
    <title>The Final Solutions of Monty Hall Problem and Three Prisoners Problem</title>
    <summary>  Recently we proposed the linguistic interpretation of quantum mechanics
(called quantum and classical measurement theory, or quantum language), which
was characterized as a kind of metaphysical and linguistic turn of the
Copenhagen interpretation. This turn from physics to language does not only
extend quantum theory to classical systems but also yield the quantum
mechanical world view (i.e., the philosophy of quantum mechanics, in other
words, quantum philosophy).And we believe that this quantum language is the
most powerful language to describe science. The purpose of this paper is to
describe the Monty-Hall problem and the three prisoners problem in quantum
language. We of course believe that our proposal is the final solutions of the
two problems. Thus in this paper, we can answer the question: "Why have
philosophers continued to stick to these problems?" And the readers will find
that these problems are never elementary, and they can not be solved without
the deep understanding of "probability" and "dualism".
  KEY WORDS: Philosophy of probability, Fisher Maximum Likelihood Method,
Bayes' Method,The Principle of Equal (a priori) Probabilities
</summary>
    <author>
      <name>Shiro Ishikawa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1207.0407, arXiv:1204.3892</arxiv:comment>
    <link href="http://arxiv.org/abs/1408.0963v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.0963v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.4916v4</id>
    <updated>2014-09-15T14:35:22Z</updated>
    <published>2014-08-21T08:39:45Z</published>
    <title>The two envelopes paradox in non-Bayesian and Bayesian statistics</title>
    <summary>  The purpose of this paper is to clarify the (non-Bayesian and Bayesian)
two-envelope problems in terms of quantum language (or, measurement theory),
which was recently proposed as a linguistic turn of quantum mechanics (with the
Copenhagen interpretation). The two envelopes paradox is only a kind of high
school student's probability puzzle, and it may be exaggerated to say that this
is an unsolved problem. However, since we are convinced that quantum language
is just statistics of the future, we believe that there is no clear answer
without the description by quantum language. In this sense, the readers are to
find that quantum language provides the final answer (i.e., the easiest and
deepest understanding) to the two envelope-problems in both non-Bayesian and
Bayesian statistics. Also, we add the discussion about St. Petersburg
two-envelope paradox.
</summary>
    <author>
      <name>Shiro Ishikawa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1408.4916v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.4916v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.4696v2</id>
    <updated>2015-04-15T04:10:44Z</updated>
    <published>2014-09-16T16:47:47Z</published>
    <title>Differentially Private Exponential Random Graphs</title>
    <summary>  We propose methods to release and analyze synthetic graphs in order to
protect privacy of individual relationships captured by the social network.
Proposed techniques aim at fitting and estimating a wide class of exponential
random graph models (ERGMs) in a differentially private manner, and thus offer
rigorous privacy guarantees. More specifically, we use the randomized response
mechanism to release networks under $\epsilon$-edge differential privacy. To
maintain utility for statistical inference, treating the original graph as
missing, we propose a way to use likelihood based inference and Markov chain
Monte Carlo (MCMC) techniques to fit ERGMs to the produced synthetic networks.
We demonstrate the usefulness of the proposed techniques on a real data
example.
</summary>
    <author>
      <name>Vishesh Karwa</name>
    </author>
    <author>
      <name>Aleksandra B. Slavković</name>
    </author>
    <author>
      <name>Pavel Krivitsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">minor edits</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.4696v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.4696v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.5196v3</id>
    <updated>2014-11-18T15:35:33Z</updated>
    <published>2014-09-18T05:40:20Z</published>
    <title>How to read probability distributions as statements about process</title>
    <summary>  Probability distributions can be read as simple expressions of information.
Each continuous probability distribution describes how information changes with
magnitude. Once one learns to read a probability distribution as a measurement
scale of information, opportunities arise to understand the processes that
generate the commonly observed patterns. Probability expressions may be parsed
into four components: the dissipation of all information, except the
preservation of average values, taken over the measurement scale that relates
changes in observed values to changes in information, and the transformation
from the underlying scale on which information dissipates to alternative scales
on which probability pattern may be expressed. Information invariances set the
commonly observed measurement scales and the relations between them. In
particular, a measurement scale for information is defined by its invariance to
specific transformations of underlying values into measurable outputs.
Essentially all common distributions can be understood within this simple
framework of information invariance and measurement scale.
</summary>
    <author>
      <name>Steven A. Frank</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3390/e16116059</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3390/e16116059" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">v2: added table of contents, adjusted section numbers v3: minor
  editing, updated reference</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Entropy 16:6059-6098 (2014)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1409.5196v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.5196v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.2759v3</id>
    <updated>2015-08-04T20:25:38Z</updated>
    <published>2014-10-10T12:14:46Z</published>
    <title>Network Analysis with the Enron Email Corpus</title>
    <summary>  We use the Enron email corpus to study relationships in a network by applying
six different measures of centrality. Our results came out of an in-semester
undergraduate research seminar. The Enron corpus is well suited to statistical
analyses at all levels of undergraduate education. Through this note's focus on
centrality, students can explore the dependence of statistical models on
initial assumptions and the interplay between centrality measures and
hierarchical ranking, and they can use completed studies as springboards for
future research. The Enron corpus also presents opportunities for research into
many other areas of analysis, including social networks, clustering, and
natural language processing.
</summary>
    <author>
      <name>Johanna Hardin</name>
    </author>
    <author>
      <name>Ghassan Sarkis</name>
    </author>
    <author>
      <name>P. C. Urc</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in Journal of Statistics Education, Volume 23, Number 2, 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.2759v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.2759v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.3127v3</id>
    <updated>2015-08-04T20:16:03Z</updated>
    <published>2014-10-12T18:17:04Z</published>
    <title>Data Science in Statistics Curricula: Preparing Students to "Think with
  Data"</title>
    <summary>  A growing number of students are completing undergraduate degrees in
statistics and entering the workforce as data analysts. In these positions,
they are expected to understand how to utilize databases and other data
warehouses, scrape data from Internet sources, program solutions to complex
problems in multiple languages, and think algorithmically as well as
statistically. These data science topics have not traditionally been a major
component of undergraduate programs in statistics. Consequently, a curricular
shift is needed to address additional learning outcomes. The goal of this paper
is to motivate the importance of data science proficiency and to provide
examples and resources for instructors to implement data science in their own
statistics curricula. We provide case studies from seven institutions. These
varied approaches to teaching data science demonstrate curricular innovations
to address new needs. Also included here are examples of assignments designed
for courses that foster engagement of undergraduates with data and data
science.
</summary>
    <author>
      <name>Johanna Hardin</name>
    </author>
    <author>
      <name>Roger Hoerl</name>
    </author>
    <author>
      <name>Nicholas J. Horton</name>
    </author>
    <author>
      <name>Deborah Nolan</name>
    </author>
    <link href="http://arxiv.org/abs/1410.3127v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.3127v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.3929v1</id>
    <updated>2014-10-15T04:47:12Z</updated>
    <published>2014-10-15T04:47:12Z</published>
    <title>Distributed Detection of a Random Process over a Multiple Access Channel
  under Energy and Bandwidth Constraints</title>
    <summary>  We analyze a binary hypothesis testing problem built on a wireless sensor
network (WSN) for detecting a stationary random process distributed both in
space and time with circularly-symmetric complex Gaussian distribution under
the Neyman-Pearson framework. Using an analog scheme, the sensors transmit
different linear combinations of their measurements through a multiple access
channel (MAC) to reach the fusion center (FC), whose task is to decide whether
the process is present or not. Considering an energy constraint on each node
transmission and a limited amount of channel uses, we compute the miss error
exponent of the proposed scheme using Large Deviation Theory (LDT) and show
that the proposed strategy is asymptotically optimal (when the number of
sensors approaches to infinity) among linear orthogonal schemes. We also show
that the proposed scheme obtains significant energy saving in the low
signal-to-noise ratio regime, which is the typical scenario of WSNs. Finally, a
Monte Carlo simulation of a 2-dimensional process in space validates the
analytical results.
</summary>
    <author>
      <name>Juan Augusto Maya</name>
    </author>
    <author>
      <name>Leonardo Rey Vega</name>
    </author>
    <author>
      <name>Cecilia G. Galarza</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper was submitted to IEEE Transactions on Signal Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.3929v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.3929v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.5279v1</id>
    <updated>2014-11-19T16:22:18Z</updated>
    <published>2014-11-19T16:22:18Z</published>
    <title>What Teachers Should Know about the Bootstrap: Resampling in the
  Undergraduate Statistics Curriculum</title>
    <summary>  I have three goals in this article: (1) To show the enormous potential of
bootstrapping and permutation tests to help students understand statistical
concepts including sampling distributions, standard errors, bias, confidence
intervals, null distributions, and P-values. (2) To dig deeper, understand why
these methods work and when they don't, things to watch out for, and how to
deal with these issues when teaching. (3) To change statistical practice---by
comparing these methods to common t tests and intervals, we see how inaccurate
the latter are; we confirm this with asymptotics. n &gt;= 30 isn't enough---think
n &gt;= 5000. Resampling provides diagnostics, and more accurate alternatives.
Sadly, the common bootstrap percentile interval badly under-covers in small
samples; there are better alternatives. The tone is informal, with a few
stories and jokes.
</summary>
    <author>
      <name>Tim Hesterberg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">83 pages, 23 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.5279v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.5279v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62-01 (Primary) 62G09 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3; I.6.8; K.3.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.5294v3</id>
    <updated>2015-07-06T05:40:07Z</updated>
    <published>2014-12-17T09:12:47Z</published>
    <title>Generalized Labeled Multi-Bernoulli Approximation of Multi-Object
  Densities</title>
    <summary>  In multi-object inference, the multi-object probability density captures the
uncertainty in the number and the states of the objects as well as the
statistical dependence between the objects. Exact computation of the
multi-object density is generally intractable and tractable implementations
usually require statistical independence assumptions between objects. In this
paper we propose a tractable multi-object density approximation that can
capture statistical dependence between objects. In particular, we derive a
tractable Generalized Labeled Multi-Bernoulli (GLMB) density that matches the
cardinality distribution and the first moment of the labeled multi-object
distribution of interest. It is also shown that the proposed approximation
minimizes the Kullback-Leibler divergence over a special tractable class of
GLMB densities. Based on the proposed GLMB approximation we further demonstrate
a tractable multi-object tracking algorithm for generic measurement models.
Simulation results for a multi-object Track-Before-Detect example using radar
measurements in low signal-to-noise ratio (SNR) scenarios verify the
applicability of the proposed approach.
</summary>
    <author>
      <name>Francesco Papi</name>
    </author>
    <author>
      <name>Ba-Ngu Vo</name>
    </author>
    <author>
      <name>Ba-Tuong Vo</name>
    </author>
    <author>
      <name>Claudio Fantacci</name>
    </author>
    <author>
      <name>Michael Beard</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TSP.2015.2454478</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TSP.2015.2454478" rel="related"/>
    <link href="http://arxiv.org/abs/1412.5294v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.5294v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.03215v1</id>
    <updated>2015-01-14T00:16:25Z</updated>
    <published>2015-01-14T00:16:25Z</published>
    <title>Creating, Automating, and Assessing Online Homework in Introductory
  Statistics and Mathematics Classes</title>
    <summary>  Although textbook publishers offer course management systems, they do so to
promote brand loyalty, and while an open source tool such as WeBWorK is
promising, it requires administrative and IT buy-in. So supported in part by a
College Access Challenge Grant from the Department of Education, we
collaborated with other instructors to create online homework sets for three
classes: Elementary Algebra, Intermediate Algebra, and Statistics for
Behavioral Sciences I. After experimentation, some of these question pools are
now created by Mathematica programs that can generate data sets from specified
distributions, generate random polynomials that factor in a given way, create
image files of histograms, scatterplots, and so forth. These programs produce
files that can be read by the software package, Respondus, which then uploads
the questions into Blackboard Learn, the course management system used by the
Connecticut State University system. Finally, we summarize five classes worth
of student performance data along with lessons learned while working on this
project.
</summary>
    <author>
      <name>Karen Santoro</name>
    </author>
    <author>
      <name>Roger Bilisoly</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 9 figures. Based on a presentation at the Joint Statistical
  Meetings, Section on Statistical Education, August, 2014, in Boston,
  Massachusetts, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/1501.03215v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.03215v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.HO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.05019v3</id>
    <updated>2016-01-24T23:44:53Z</updated>
    <published>2015-01-20T23:30:37Z</published>
    <title>Robust Hypothesis Testing with $α$-Divergence</title>
    <summary>  A robust minimax test for two composite hypotheses, which are determined by
the neighborhoods of two nominal distributions with respect to a set of
distances - called $\alpha-$divergence distances, is proposed. Sion's minimax
theorem is adopted to characterize the saddle value condition. Least favorable
distributions, the robust decision rule and the robust likelihood ratio test
are derived. If the nominal probability distributions satisfy a symmetry
condition, the design procedure is shown to be simplified considerably. The
parameters controlling the degree of robustness are bounded from above and the
bounds are shown to be resulting from a solution of a set of equations. The
simulations performed evaluate and exemplify the theoretical derivations.
</summary>
    <author>
      <name>Gökhan Gül</name>
    </author>
    <author>
      <name>Abdelhak M. Zoubir</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TSP.2016.2569405</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TSP.2016.2569405" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to IEEE Trans. on Signal Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/1501.05019v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.05019v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.02555v1</id>
    <updated>2015-02-09T16:44:37Z</updated>
    <published>2015-02-09T16:44:37Z</published>
    <title>What are the true clusters?</title>
    <summary>  Constructivist philosophy and Hasok Chang's active scientific realism are
used to argue that the idea of "truth" in cluster analysis depends on the
context and the clustering aims. Different characteristics of clusterings are
required in different situations. Researchers should be explicit about on what
requirements and what idea of "true clusters" their research is based, because
clustering becomes scientific not through uniqueness but through transparent
and open communication. The idea of "natural kinds" is a human construct, but
it highlights the human experience that the reality outside the observer's
control seems to make certain distinctions between categories inevitable.
Various desirable characteristics of clusterings and various approaches to
define a context-dependent truth are listed, and I discuss what impact these
ideas can have on the comparison of clustering methods, and the choice of a
clustering methods and related decisions in practice.
</summary>
    <author>
      <name>Christian Hennig</name>
    </author>
    <link href="http://arxiv.org/abs/1502.02555v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.02555v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="03A05, 62H30, 91C20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.00781v1</id>
    <updated>2015-03-02T23:11:28Z</updated>
    <published>2015-03-02T23:11:28Z</published>
    <title>Teaching and Learning Data Visualization: Ideas and Assignments</title>
    <summary>  This article discusses how to make statistical graphics a more prominent
element of the undergraduate statistics curricula. The focus is on several
different types of assignments that exemplify how to incorporate graphics into
a course in a pedagogically meaningful way. These assignments include having
students deconstruct and reconstruct plots, copy masterful graphs, create
one-minute visual revelations, convert tables into `pictures', and develop
interactive visualizations with, e.g., the virtual earth as a plotting canvas.
In addition to describing the goals and details of each assignment, we also
discuss the broader topic of graphics and key concepts that we think warrant
inclusion in the statistics curricula. We advocate that more attention needs to
be paid to this fundamental field of statistics at all levels, from
introductory undergraduate through graduate level courses. With the rapid rise
of tools to visualize data, e.g., Google trends, GapMinder, ManyEyes, and
Tableau, and the increased use of graphics in the media, understanding the
principles of good statistical graphics, and having the ability to create
informative visualizations is an ever more important aspect of statistics
education.
</summary>
    <author>
      <name>Deborah Nolan</name>
    </author>
    <author>
      <name>Jamis Perrett</name>
    </author>
    <link href="http://arxiv.org/abs/1503.00781v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.00781v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.02188v3</id>
    <updated>2015-04-28T14:51:44Z</updated>
    <published>2015-03-07T16:46:59Z</published>
    <title>Challenges and opportunities for statistics and statistical education:
  looking back, looking forward</title>
    <summary>  The 175th anniversary of the ASA provides an opportunity to look back into
the past and peer into the future. What led our forebears to found the
association? What commonalities do we still see? What insights might we glean
from their experiences and observations? I will use the anniversary as a chance
to reflect on where we are now and where we are headed in terms of statistical
education amidst the growth of data science. Statistics is the science of
learning from data. By fostering more multivariable thinking, building
data-related skills, and developing simulation-based problem solving, we can
help to ensure that statisticians are fully engaged in data science and the
analysis of the abundance of data now available to us.
</summary>
    <author>
      <name>Nicholas Jon Horton</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In press: The American Statistician</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.02188v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.02188v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.05570v1</id>
    <updated>2015-03-18T20:05:24Z</updated>
    <published>2015-03-18T20:05:24Z</published>
    <title>A Data Science Course for Undergraduates: Thinking with Data</title>
    <summary>  Data science is an emerging interdisciplinary field that combines elements of
mathematics, statistics, computer science, and knowledge in a particular
application domain for the purpose of extracting meaningful information from
the increasingly sophisticated array of data available in many settings. These
data tend to be non-traditional, in the sense that they are often live, large,
complex, and/or messy. A first course in statistics at the undergraduate level
typically introduces students with a variety of techniques to analyze small,
neat, and clean data sets. However, whether they pursue more formal training in
statistics or not, many of these students will end up working with data that is
considerably more complex, and will need facility with statistical computing
techniques. More importantly, these students require a framework for thinking
structurally about data. We describe an undergraduate course in a liberal arts
environment that provides students with the tools necessary to apply data
science. The course emphasizes modern, practical, and useful skills that cover
the full data analysis spectrum, from asking an interesting question to
acquiring, managing, manipulating, processing, querying, analyzing, and
visualizing data, as well communicating findings in written, graphical, and
oral forms.
</summary>
    <author>
      <name>Ben Baumer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages total including supplementary materials</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.05570v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.05570v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62-01" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.09072v1</id>
    <updated>2015-02-03T12:41:45Z</updated>
    <published>2015-02-03T12:41:45Z</published>
    <title>Failure and Uses of Jaynes' Principle of Transformation Groups</title>
    <summary>  Bertand's paradox is a fundamental problem in probability that casts doubt on
the applicability of the indifference principle by showing that it may yield
contradictory results, depending on the meaning assigned to "randomness".
Jaynes claimed that symmetry requirements (the principle of transformation
groups) solve the paradox by selecting a unique solution to the problem. I show
that this is not the case and that every variant obtained from the principle of
indifference can also be obtained from Jaynes' principle of transformation
groups. This is because the same symmetries can be mathematically implemented
in different ways, depending on the procedure of random selection that one
uses. I describe a simple experiment that supports a result from symmetry
arguments, but the solution is different from Jaynes'. Jaynes' method is thus
best seen as a tool to obtain probability distributions when the principle of
indifference is inconvenient, but it cannot resolve ambiguities inherent in the
use of that principle and still depends on explicitly defining the selection
procedure.
</summary>
    <author>
      <name>Alon Drory</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10701-015-9876-7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10701-015-9876-7" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 figures. Accepted for publication by Foundations of Physics</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.09072v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.09072v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.07087v1</id>
    <updated>2015-05-26T19:39:38Z</updated>
    <published>2015-05-26T19:39:38Z</published>
    <title>Propagation of Uncertainty in Risk Analysis and Safety Integrity Level
  Composition</title>
    <summary>  In many risk analyses the results are only given as mean values and often the
input data are also mean values. However the required accuracy of the result is
often an interval of values e. g. for the derivation of a Safety Integrity
Level (SIL). In this paper we reason what should be the accuracy of the input
data of risk analyses if a particular certainty of the result is demanded. Also
the backside of the coin, the SIL composition is discussed. The results show
that common methods for risk analysis are faulty and that SIL allocation by a
kind of SIL calculus seems infeasible without additional requirements on the
composed components. A justification of a common practice for parameter scaling
in well-constructed semi-quantitative risk analysis is also provided.
</summary>
    <author>
      <name>Jens Braband</name>
    </author>
    <author>
      <name>Hendrik Schäbe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.07087v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.07087v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.05346v1</id>
    <updated>2015-07-19T21:54:44Z</updated>
    <published>2015-07-19T21:54:44Z</published>
    <title>Mere Renovation is Too Little Too Late: We Need to Rethink Our
  Undergraduate Curriculum from the Ground Up</title>
    <summary>  The last half-dozen years have seen The American Statistician publish
well-argued and provocative calls to change our thinking about statistics and
how we teach it, among them Brown and Kass (2009), Nolan and Temple-Lang
(2010), and Legler et al. (2010). Within this past year, the ASA has issued a
new and comprehensive set of guidelines for undergraduate programs (ASA 2014).
Accepting (and applauding) all this as background, the current article argues
the need to rethink our curriculum from the ground up, and offers five
principles and two caveats intended to help us along the path toward a new
synthesis. These principles and caveats rest on my sense of three parallel
evolutions: the convergence of trends in the roles of mathematics, computation,
and context within statistics education. These ongoing changes, together with
the articles cited above and the seminal provocation by Leo Breiman (2001) call
for a deep rethinking of what we teach to undergraduates. In particular,
following Brown and Kass, we should put priority on two goals, to make
fundamental concepts accessible and to minimize prerequisites to research.
</summary>
    <author>
      <name>George W. Cobb</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Mount Holyoke College</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">33 pages. To be published in The American Statistician</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.05346v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.05346v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.06411v1</id>
    <updated>2015-07-23T08:39:34Z</updated>
    <published>2015-07-23T08:39:34Z</published>
    <title>Arbitrariness of peer review: A Bayesian analysis of the NIPS experiment</title>
    <summary>  The principle of peer review is central to the evaluation of research, by
ensuring that only high-quality items are funded or published. But peer review
has also received criticism, as the selection of reviewers may introduce biases
in the system. In 2014, the organizers of the ``Neural Information Processing
Systems\rq\rq{} conference conducted an experiment in which $10\%$ of submitted
manuscripts (166 items) went through the review process twice. Arbitrariness
was measured as the conditional probability for an accepted submission to get
rejected if examined by the second committee. This number was equal to $60\%$,
for a total acceptance rate equal to $22.5\%$. Here we present a Bayesian
analysis of those two numbers, by introducing a hidden parameter which measures
the probability that a submission meets basic quality criteria. The standard
quality criteria usually include novelty, clarity, reproducibility, correctness
and no form of misconduct, and are met by a large proportions of submitted
items. The Bayesian estimate for the hidden parameter was equal to $56\%$
($95\%$CI: $ I = (0.34, 0.83)$), and had a clear interpretation. The result
suggested the total acceptance rate should be increased in order to decrease
arbitrariness estimates in future review processes.
</summary>
    <author>
      <name>Olivier Francois</name>
    </author>
    <link href="http://arxiv.org/abs/1507.06411v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.06411v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.07244v2</id>
    <updated>2015-07-29T16:42:15Z</updated>
    <published>2015-07-26T20:11:24Z</published>
    <title>The Crisis Of Evidence: Why Probability And Statistics Cannot Discover
  Cause</title>
    <summary>  Probability models are only useful at explaining the uncertainty of what we
do not know, and should never be used to say what we already know. Probability
and statistical models are useless at discerning cause. Classical statistical
procedures, in both their frequentist and Bayesian implementations are, falsely
imply they can speak about cause. No hypothesis test, or Bayes factor, should
ever be used again. Even assuming we know the cause or partial cause for some
set of observations, reporting via relative risk exagerates the certainty we
have in the future, often by a lot. This over-certainty is made much worse when
parametetric and not predictive methods are used. Unfortunately, predictive
methods are rarely used; and even when they are, cause must still be an
assumption, meaning (again) certainty in our scientific pronouncements is too
high.
</summary>
    <author>
      <name>William M. Briggs</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Corrected typos; 22 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.07244v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.07244v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.02083v1</id>
    <updated>2015-08-09T20:52:30Z</updated>
    <published>2015-08-09T20:52:30Z</published>
    <title>Studies on properties and estimation problems for modified extension of
  exponential distribution</title>
    <summary>  The present paper considers modified extension of the exponential
distribution with three parameters. We study the main properties of this new
distribution, with special emphasis on its median, mode and moments function
and some characteristics related to reliability studies. For Modified-
extension exponential distribution (MEXED) we have obtained the Bayes
Estimators of scale and shape parameters using Lindley's approximation
(L-approximation) under squared error loss function. But, through this
approximation technique it is not possible to compute the interval estimates of
the parameters. Therefore, we also propose Gibbs sampling method to generate
sample from the posterior distribution. On the basis of generated posterior
sample we computed the Bayes estimates of the unknown parameters and
constructed 95 % highest posterior density credible intervals. A Monte Carlo
simulation study is carried out to compare the performance of Bayes estimators
with the corresponding classical estimators in terms of their simulated risk. A
real data set has been considered for illustrative purpose of the study.
</summary>
    <author>
      <name>M. A. El-Damcese</name>
    </author>
    <author>
      <name>Dina. A. Ramadan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5120/ijca2015905891</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5120/ijca2015905891" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22,3</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.02083v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.02083v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.02384v1</id>
    <updated>2015-08-09T20:57:55Z</updated>
    <published>2015-08-09T20:57:55Z</published>
    <title>The Third Way Of Probability &amp; Statistics: Beyond Testing and Estimation
  To Importance, Relevance, and Skill</title>
    <summary>  There is a third way of implementing probability models and practicing. This
is to answer questions put in terms of observables. This eliminates frequentist
hypothesis testing and Bayes factors and it also eliminates parameter
estimation. The Third Way is the logical probability approach, which is to make
statements $\Pr(Y \in y | X,D,M)$ about observables of interest $Y$ taking
values $y$, given probative data $X$, past observations (when present) $D$ and
some model (possibly deduced) $M$. Significance and the false idea that
probability models show causality are no more, and in their place are
importance and relevance. Models are built keeping on information that is
relevant and important to a decision maker (and not a statistician). All models
are stated in publicly verifiable fashion, as predictions. All models must
undergo a verification process before any trust is put into them.
</summary>
    <author>
      <name>William M. Briggs</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.02384v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.02384v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.05453v1</id>
    <updated>2015-08-22T01:58:18Z</updated>
    <published>2015-08-22T01:58:18Z</published>
    <title>Beyond subjective and objective in statistics</title>
    <summary>  We argue that the words "objectivity" and "subjectivity" in statistics
discourse are used in a mostly unhelpful way, and we propose to replace each of
them with broader collections of attributes, with objectivity replaced by
transparency, consensus, impartiality, and correspondence to observable
reality, and subjectivity replaced by awareness of multiple perspectives and
context dependence. The advantage of these reformulations is that the
replacement terms do not oppose each other. Instead of debating over whether a
given statistical method is subjective or objective (or normatively debating
the relative merits of subjectivity and objectivity in statistical practice),
we can recognize desirable attributes such as transparency and acknowledgment
of multiple perspectives as complementary goals. We demonstrate the
implications of our proposal with recent applied examples from pharmacology,
election polling, and socioeconomic stratification.
</summary>
    <author>
      <name>Andrew Gelman</name>
    </author>
    <author>
      <name>Christian Hennig</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.05453v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.05453v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.05541v1</id>
    <updated>2015-08-22T19:02:52Z</updated>
    <published>2015-08-22T19:02:52Z</published>
    <title>Explorations in Statistics Research: An Approach to Expose
  Undergraduates to Authentic Data Analysis</title>
    <summary>  The Explorations in Statistics Research workshop is a one-week NSF-funded
summer program that introduces undergraduate students to current research
problems in applied statistics. The goal of the workshop is to expose students
to exciting, modern applied statistical research and practice, with the
ultimate aim of interesting them in seeking more training in statistics at the
undergraduate and graduate levels. The program is explicitly designed to engage
students in the connections between authentic domain problems and the
statistical ideas and approaches needed to address these problems, which is an
important aspect of statistical thinking that is difficult to teach and
sometimes lacking in our methodological courses and programs. Over the past
nine years, we ran the workshop six times and a similar program in the sciences
two times. We describe the program, summarize feedback from participants, and
identify the key features to its success. We abstract these features and
provide a set of recommendations for how faculty can incorporate important
elements into their regular courses.
</summary>
    <author>
      <name>Deborah Nolan</name>
    </author>
    <author>
      <name>Duncan Temple Lang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1080/00031305.2015.1073624</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1080/00031305.2015.1073624" rel="related"/>
    <link href="http://arxiv.org/abs/1508.05541v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.05541v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.05914v1</id>
    <updated>2015-08-24T18:59:10Z</updated>
    <published>2015-08-24T18:59:10Z</published>
    <title>Extended Dynamic Generalized Linear Models: the two-parameter
  exponential family</title>
    <summary>  We develop a Bayesian framework for estimation and prediction of dynamic
models for observations from the two-parameter exponential family. Different
link functions are introduced to model both the mean and the precision in the
exponential family allowing the introduction of covariates and time series
components. We explore conjugacy and analytical approximations under the class
of partial specified models to keep the computation fast. The algorithm of
West, Harrison and Migon (1985) is extended to cope with the two-parameter
exponential family models. The methodological novelties are illustrated with
two applications to real data. The first, considers unemployment rates in
Brazil and the second some macroeconomic variables for the United Kingdom.
</summary>
    <author>
      <name>Mariana Albi de Oliveira Souza</name>
    </author>
    <author>
      <name>Helio dos Santos Migon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 7 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.05914v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.05914v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.07804v3</id>
    <updated>2015-12-22T09:53:34Z</updated>
    <published>2015-09-25T17:40:16Z</published>
    <title>From Statistician to Data Scientist</title>
    <summary>  According to a recent report from the European Commission, the world
generates every minute 1.7 million of billions of data bytes, the equivalent of
360,000 DVDs, and companies that build their decision-making processes by
exploiting these data increase their productivity. The treatment and
valorization of massive data has consequences on the employment of graduate
students in statistics. Which additional skills do students trained in
statistics need to acquire to become data scientists ? How to evolve training
so that future graduates can adapt to rapid changes in this area, without
neglecting traditional jobs and the fundamental and lasting foundation for the
training? After considering the notion of big data and questioning the
emergence of a "new" science: Data Science, we present the current developments
in the training of engineers in Mathematical and Modeling at INSA Toulouse.
</summary>
    <author>
      <name>Philippe Besse</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IMT, INSA Toulouse</arxiv:affiliation>
    </author>
    <author>
      <name>Beatrice Laurent</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IMT</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in French</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.07804v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.07804v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.06013v1</id>
    <updated>2015-11-18T22:50:37Z</updated>
    <published>2015-11-18T22:50:37Z</published>
    <title>Statistical Engineering: An Idea Whose Time Has Come?</title>
    <summary>  Several authors, including the American Statistician (ASA), have noted the
challenges facing statisticians when attacking large, complex, unstructured
problems, as opposed to well-defined textbook problems. Clearly, the standard
paradigm of selecting the one "correct" statistical method for such problems is
not sufficient; a new paradigm is needed. Statistical engineering has been
proposed as a discipline that can provide a viable paradigm to attack such
problems, used in conjunction with sound statistical science. Of course, in
order to develop as a true discipline, statistical engineering needs a
well-developed theory, not just a formal definition and successful case
studies. This article documents and disseminates the current state of the
underlying theory of statistical engineering. Our purpose is to provide a
vehicle for applied statisticians to further enhance the practice of
statistics, and for academics so interested to continue development of the
underlying theory of statistical engineering.
</summary>
    <author>
      <name>Roger W. Hoerl</name>
    </author>
    <author>
      <name>Ronald D. Snee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.06013v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.06013v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.08180v4</id>
    <updated>2016-11-25T00:05:39Z</updated>
    <published>2015-11-25T19:51:41Z</published>
    <title>J. B. S. Haldane's Contribution to the Bayes Factor Hypothesis Test</title>
    <summary>  This article brings attention to some historical developments that gave rise
to the Bayes factor for testing a point null hypothesis against a composite
alternative. In line with current thinking, we find that the conceptual
innovation - to assign prior mass to a general law - is due to a series of
three articles by Dorothy Wrinch and Sir Harold Jeffreys (1919, 1921, 1923).
However, our historical investigation also suggests that in 1932 J. B. S.
Haldane made an important contribution to the development of the Bayes factor
by proposing the use of a mixture prior comprising a point mass and a
continuous probability density. Jeffreys was aware of Haldane's work and it may
have inspired him to pursue a more concrete statistical implementation for his
conceptual ideas. It thus appears that Haldane may have played a much bigger
role in the statistical development of the Bayes factor than has hitherto been
assumed.
</summary>
    <author>
      <name>Alexander Etz</name>
    </author>
    <author>
      <name>Eric-Jan Wagenmakers</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 3 photographs</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.08180v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.08180v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="Primary 62F03, secondary 62-03" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.01347v1</id>
    <updated>2016-02-03T15:52:24Z</updated>
    <published>2016-02-03T15:52:24Z</published>
    <title>Determining optimal factors for chemical synthesis of pharmaceutical
  products using experimental data</title>
    <summary>  In a chemical synthesis process to manufacture a pharmaceutical product, an
initial set of substances evolve according to chemical reactions, under certain
process conditions, into a series of new substances. One of these substances is
a target pharmaceutical product and two are unwanted by-products. The aim is to
determine the factors (process conditions and amounts of initial substances)
that maximise the probability of the amounts of pharmaceutical product and
by-products being greater and less than certain levels, respectively. The
relationship between the factors and amounts of substances of interest is
theoretically described by the intractable solution to a system of ordinary
differential equations incorporating temperature dependence. This relationship
is refined using observations from an experiment by the fitting of a
statistical model. Predictions from this model are used to evaluate the
probability of satisfying the constraints and this probability is maximised
over the set of factors to provide optimal manufacturing conditions for the
pharmaceutical product. To accelerate the model-fitting and the evaluation of
the probability of satisfying the constraints, the numerical solution to the
differential equations is approximated using statistical emulators.
</summary>
    <author>
      <name>Antony Overstall</name>
    </author>
    <author>
      <name>David Woods</name>
    </author>
    <author>
      <name>Kieran Martin</name>
    </author>
    <link href="http://arxiv.org/abs/1602.01347v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.01347v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.04091v1</id>
    <updated>2016-02-12T15:57:27Z</updated>
    <published>2016-02-12T15:57:27Z</published>
    <title>Interactive graphics for functional data analyses</title>
    <summary>  Although there are established graphics that accompany the most common
functional data analyses, generating these graphics for each dataset and
analysis can be cumbersome and time consuming. Often, the barriers to
visualization inhibit useful exploratory data analyses and prevent the
development of intuition for a method and its application to a particular
dataset. The refund.shiny package was developed to address these issues for
several of the most common functional data analyses. After conducting an
analysis, the plot_shiny() function is used to generate an interactive
visualization environment that contains several distinct graphics, many of
which are updated in response to user input. These visualizations reduce the
burden of exploratory analyses and can serve as a useful tool for the
communication of results to non-statisticians.
</summary>
    <author>
      <name>Julia Wrobel</name>
    </author>
    <author>
      <name>So Young Park</name>
    </author>
    <author>
      <name>Ana Maria Staicu</name>
    </author>
    <author>
      <name>Jeff Goldsmith</name>
    </author>
    <link href="http://arxiv.org/abs/1602.04091v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.04091v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.00738v1</id>
    <updated>2016-02-29T14:52:56Z</updated>
    <published>2016-02-29T14:52:56Z</published>
    <title>Mandelbrot's 1/f fractional renewal models of 1963-67: The non-ergodic
  missing link between change points and long range dependence</title>
    <summary>  The problem of 1/f noise has been with us for about a century. Because it is
so often framed in Fourier spectral language, the most famous solutions have
tended to be the stationary long range dependent (LRD) models such as
Mandelbrot's fractional Gaussian noise. In view of the increasing importance to
physics of non-ergodic fractional renewal models, I present preliminary results
of my research into the history of Mandelbrot's very little known work in that
area from 1963-67. I speculate about how the lack of awareness of this work in
the physics and statistics communities may have affected the development of
complexity science, and I discuss the differences between the Hurst effect, 1/f
noise and LRD, concepts which are often treated as equivalent.
</summary>
    <author>
      <name>Nicholas Wynn Watkins</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-55789-2_14</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-55789-2_14" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages. Corrected and improved version of a manuscript submitted to
  ITISE 2016 meeting in Granada, Spain</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Advances in Time Series Analysis and Forecasting. ITISE 2016.
  Contributions to Statistics. Springer, Cham</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1603.00738v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.00738v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.04912v3</id>
    <updated>2017-09-02T03:33:40Z</updated>
    <published>2016-03-15T22:39:10Z</published>
    <title>Dynamic Data in the Statistics Classroom</title>
    <summary>  The call for using real data in the classroom has long meant using datasets
which are culled, cleaned, and wrangled prior to any student working with the
observations. However, an important part of teaching statistics should include
actually retrieving data from the Internet. Nowadays, there are many different
sources of data that are continually updated by the organization hosting the
data website. The R tools to download such dynamic data have improved in such a
way to make accessing the data possible even in an introductory statistics
class. We provide five full analyses on dynamic data as well as an additional
nine sources of dynamic data that can be brought into the classroom. The goal
of our work is to demonstrate that using dynamic data can have a short learning
curve, even for introductory students or faculty unfamiliar with the landscape.
The examples provided are unlikely to create expert data scrapers, but they
should help motivate students and faculty toward more engaged use of online
data sources.
</summary>
    <author>
      <name>Johanna Hardin</name>
    </author>
    <link href="http://arxiv.org/abs/1603.04912v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.04912v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.07408v1</id>
    <updated>2016-03-24T01:40:52Z</updated>
    <published>2016-03-24T01:40:52Z</published>
    <title>Fisher, Neyman-Pearson or NHST? A Tutorial for Teaching Data Testing</title>
    <summary>  Despite frequent calls for the overhaul of null hypothesis significance
testing (NHST), this controversial procedure remains ubiquitous in behavioral,
social and biomedical teaching and research. Little change seems possible once
the procedure becomes well ingrained in the minds and current practice of
researchers; thus, the optimal opportunity for such change is at the time the
procedure is taught, be this at undergraduate or at postgraduate levels. This
paper presents a tutorial for the teaching of data testing procedures, often
referred to as hypothesis testing theories. The first procedure introduced is
the approach to data testing followed by Fisher (tests of significance); the
second is the approach followed by Neyman and Pearson (tests of acceptance);
the final procedure is the incongruent combination of the previous two theories
into the current approach (NSHT). For those researchers sticking with the
latter, two compromise solutions on how to improve NHST conclude the tutorial.
</summary>
    <author>
      <name>Jose D. Perezgonzalez</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3389/fpsyg.2015.00223</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3389/fpsyg.2015.00223" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 5 figures, published article. Frontiers in Psychology,
  6:223</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.07408v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.07408v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.09453v1</id>
    <updated>2016-03-31T04:48:41Z</updated>
    <published>2016-03-31T04:48:41Z</published>
    <title>An overview and perspective on social network monitoring</title>
    <summary>  In this expository paper we give an overview of some statistical methods for
the monitoring of social networks. We discuss the advantages and limitations of
various methods as well as some relevant issues. One of our primary
contributions is to give the relationships between network monitoring methods
and monitoring methods in engineering statistics and public health
surveillance. We encourage researchers in the industrial process monitoring
area to work on developing and comparing the performance of social network
monitoring methods. We also discuss some of the issues in social network
monitoring and give a number of research ideas.
</summary>
    <author>
      <name>William H. Woodall</name>
    </author>
    <author>
      <name>Meng J. Zhao</name>
    </author>
    <author>
      <name>Kamran Paynabar</name>
    </author>
    <author>
      <name>Ross Sparks</name>
    </author>
    <author>
      <name>James D. Wilson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear, IIE Transactions</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.09453v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.09453v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.05224v2</id>
    <updated>2017-02-03T16:47:31Z</updated>
    <published>2016-04-18T16:06:22Z</published>
    <title>BFDA: A Matlab Toolbox for Bayesian Functional Data Analysis</title>
    <summary>  We provide a MATLAB toolbox, BFDA, that implements a Bayesian hierarchical
model to smooth multiple functional data with the assumptions of the same
underlying Gaussian process distribution, a Gaussian process prior for the mean
function, and an Inverse-Wishart process prior for the covariance function.
This model-based approach can borrow strength from all functional data to
increase the smoothing accuracy, as well as estimate the mean-covariance
functions simultaneously. An option of approximating the Bayesian inference
process using cubic B-spline basis functions is integrated in BFDA, which
allows for efficiently dealing with high-dimensional functional data. Examples
of using BFDA in various scenarios and conducting follow-up functional
regression are provided. The advantages of BFDA include: (1) Simultaneously
smooths multiple functional data and estimates the mean-covariance functions in
a nonparametric way; (2) flexibly deals with sparse and high-dimensional
functional data with stationary and nonstationary covariance functions, and
without the requirement of common observation grids; (3) provides accurately
smoothed functional data for follow-up analysis.
</summary>
    <author>
      <name>Jingjing Yang</name>
    </author>
    <author>
      <name>Peng Ren</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A tool paper submitted to the Journal of Statistical Software</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.05224v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.05224v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.05418v1</id>
    <updated>2016-04-19T03:33:28Z</updated>
    <published>2016-04-19T03:33:28Z</published>
    <title>Its All on the Square- The Importance of the Sum of Squares and Making
  the General Linear Model Simple</title>
    <summary>  Statistics is one of the most valuable of disciplines. Science is based on
proof and it alone produces results, other approaches are not, and do not.
Statistics is the only acceptable language of proof in science. Yet statistics
is difficult to understand for a large percentage of those who will be
evaluating and even doing research. Reasons for this difficulty may be that
statistics operates counter to the way people think, as well as the widespread
phobia of numeracy. Adding to the difficulty is that undergraduate textbooks
tend to make statistical tests seem to be an unorganized conglomeration of
unrelated procedures, and this leads to a failure of students to understand
that all of the parametric procedures they are studying in an introductory
course are ultimately doing the same thing and stem from common sources. In
statistics, precisely because the material is complex, the presentation must be
simple! This article endeavors to do just that.
</summary>
    <author>
      <name>Alexander Nussbaum</name>
    </author>
    <author>
      <name>Richard Seides</name>
    </author>
    <link href="http://arxiv.org/abs/1604.05418v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.05418v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07397v1</id>
    <updated>2016-04-25T18:26:51Z</updated>
    <published>2016-04-25T18:26:51Z</published>
    <title>Teaching Data Science</title>
    <summary>  We describe an introductory data science course, entitled Introduction to
Data Science, offered at the University of Illinois at Urbana-Champaign. The
course introduced general programming concepts by using the Python programming
language with an emphasis on data preparation, processing, and presentation.
The course had no prerequisites, and students were not expected to have any
programming experience. This introductory course was designed to cover a wide
range of topics, from the nature of data, to storage, to visualization, to
probability and statistical analysis, to cloud and high performance computing,
without becoming overly focused on any one subject. We conclude this article
with a discussion of lessons learned and our plans to develop new data science
courses.
</summary>
    <author>
      <name>Robert J. Brunner</name>
    </author>
    <author>
      <name>Edward J. Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 figures, International Conference on Computational
  Science (ICCS 2016)</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.07397v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07397v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ed-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.02533v2</id>
    <updated>2016-10-20T21:21:17Z</updated>
    <published>2016-08-08T17:53:46Z</published>
    <title>Designing Modular Software: A Case Study in Introductory Statistics</title>
    <summary>  Modular programming is a development paradigm that emphasizes self-contained,
flexible, and independent pieces of functionality. This practice allows new
features to be seamlessly added when desired, and unwanted features to be
removed, thus simplifying the user-facing view of the software. The recent rise
of web-based software applications has presented new challenges for designing
an extensible, modular software system. In this paper, we outline a framework
for designing such a system, with a focus on reproducibility of the results. We
present as a case study a Shiny-based web application called intRo, that allows
the user to perform basic data analyses and statistical routines. Finally, we
highlight some challenges we encountered, and how to address them, when
combining modular programming concepts with reactive programming as used by
Shiny.
</summary>
    <author>
      <name>Eric Hare</name>
    </author>
    <author>
      <name>Andee Kaplan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.02533v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.02533v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.04843v1</id>
    <updated>2016-08-17T03:26:35Z</updated>
    <published>2016-08-17T03:26:35Z</published>
    <title>Putting Down Roots: A Graphical Exploration of Community Attachment</title>
    <summary>  In this paper, we explore the relationships that individuals have with their
communities. This work was prepared as part of the ASA Data Expo '13 sponsored
by the Graphics Section and the Computing Section, using data provided by the
Knight Foundation Soul of the Community survey. The Knight Foundation in
cooperation with Gallup surveyed 43,000 people over three years in 26
communities across the United States with the intention of understanding the
association between community attributes and the degree of attachment people
feel towards their community. These include the different facets of both urban
and rural communities, the impact of quality education, and the trend in the
perceived economic conditions of a community over time. The goal of our work is
to facilitate understanding of why people feel attachment to their communities
through the use of an interactive and web-based visualization. We will explain
the development and use of web-based interactive graphics, including an
overview of the R package Shiny and the JavaScript library D3, focusing on the
choices made in producing the visualizations and technical aspects of how they
were created. Then we describe the stories about community attachment that
unfolded from our analysis.
</summary>
    <author>
      <name>Andee Kaplan</name>
    </author>
    <author>
      <name>Eric Hare</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.04843v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.04843v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.06330v4</id>
    <updated>2017-07-25T20:54:59Z</updated>
    <published>2016-08-22T22:10:05Z</published>
    <title>Revisiting nested group testing procedures: new results, comparisons,
  and robustness</title>
    <summary>  Group testing has its origin in the identification of syphilis in the US army
during World War II. Much of the theoretical framework of group testing was
developed starting in the late 1950s, with continued work into the 1990s.
Recently, with the advent of new laboratory and genetic technologies, there has
been an increasing interest in group testing designs for cost saving purposes.
In this paper, we compare different nested designs, including Dorfman, Sterrett
and an optimal nested procedure obtained through dynamic programming. To
elucidate these comparisons, we develop closed-form expressions for the optimal
Sterrett procedure and provide a concise review of the prior literature for
other commonly used procedures. We consider designs where the prevalence of
disease is known as well as investigate the robustness of these procedures when
it is incorrectly assumed. This article provides a technical presentation that
will be of interest to researchers as well as from a pedagogical perspective.
Supplementary material for this article is available online.
</summary>
    <author>
      <name>Yaakov Malinovsky</name>
    </author>
    <author>
      <name>Paul S. Albert</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted for publication on May 3, 2016. Revised version</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.06330v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.06330v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.01537v2</id>
    <updated>2016-10-06T17:44:02Z</updated>
    <published>2016-10-05T17:23:14Z</published>
    <title>Scale and curvature effects in principal geodesic analysis</title>
    <summary>  There is growing interest in using the close connection between differential
geometry and statistics to model smooth manifold-valued data. In particular,
much work has been done recently to generalize principal component analysis
(PCA), the method of dimension reduction in linear spaces, to Riemannian
manifolds. One such generalization is known as principal geodesic analysis
(PGA). This paper, in a novel fashion, obtains Taylor expansions in scaling
parameters introduced in the domain of objective functions in PGA. It is shown
this technique not only leads to better closed-form approximations of PGA but
also reveals the effects that scale, curvature and the distribution of data
have on solutions to PGA and on their differences to first-order tangent space
approximations. This approach should be able to be applied not only to PGA but
also to other generalizations of PCA and more generally to other intrinsic
statistics on Riemannian manifolds.
</summary>
    <author>
      <name>Drew Lazar</name>
    </author>
    <author>
      <name>Lizhen Lin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.jmva.2016.09.009</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.jmva.2016.09.009" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 8 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Multivariate Analysis 153 (2017) 64-82</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1610.01537v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.01537v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60D05, 62H11, 53C22" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.05733v1</id>
    <updated>2016-10-17T17:06:46Z</updated>
    <published>2016-10-17T17:06:46Z</published>
    <title>A Devastating Example for the Halfer Rule</title>
    <summary>  How should we update de dicto beliefs in the face of de se evidence? The
Sleeping Beauty problem divides philosophers into two camps, halfers and
thirders. But there is some disagreement among halfers about how their position
should generalize to other examples. A full generalization is not always given;
one notable exception is the Halfer Rule, under which the agent updates her
uncentered beliefs based on only the uncentered part of her evidence. In this
brief article, I provide a simple example for which the Halfer Rule prescribes
credences that, I argue, cannot be reasonably held by anyone. In particular,
these credences constitute an egregious violation of the Reflection Principle.
I then discuss the consequences for halfing in general.
</summary>
    <author>
      <name>Vincent Conitzer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11098-014-0384-y</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11098-014-0384-y" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The final publication is available at Springer via
  http://dx.doi.org/10.1007/s11098-014-0384-y</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Philosophical Studies, Volume 172, Issue 8, pp, 1985-1992, August
  2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1610.05733v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.05733v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.03072v1</id>
    <updated>2016-11-01T13:22:12Z</updated>
    <published>2016-11-01T13:22:12Z</published>
    <title>Apocalypse Now? Reviving the Doomsday Argument</title>
    <summary>  Whether the fate of our species can be forecast from its past has been the
topic of considerable controversy. One refutation of the so-called Doomsday
Argument is based on the premise that we are more likely to exist in a universe
containing a greater number of observers. Here we present a Bayesian
reformulation of the Doomsday Argument which is immune to this effect. By
marginalising over the spatial configuration of observers, we find that any
preference for a larger total number of observers has no impact on the inferred
local number. Our results remain unchanged when we adopt either the
Self-Indexing Assumption (SIA) or the Self-Sampling Assumption (SSA).
Furthermore the median value of our posterior distribution is found to be in
agreement with the frequentist forecast. Humanity's prognosis for the coming
century is well approximated by a global catastrophic risk of 0.2% per year.
</summary>
    <author>
      <name>Fergus Simpson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.03072v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.03072v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.pop-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.06545v1</id>
    <updated>2016-11-20T16:58:34Z</updated>
    <published>2016-11-20T16:58:34Z</published>
    <title>Stop the tests: Opinion bias and statistical tests</title>
    <summary>  When statisticians quarrel about hypothesis testing, the debate usually focus
on which method is the correct one. The fundamental question of whether we
should test hypothesis at all tends to be forgotten. This lack of debate has
its roots on our desire to have ideas we believe and defend. But cognitive
experiments have been showing that, when we do choose ideas, we become prey to
a large number of biases. Several of our biases can be grouped together in a
single description, an opinion bias. This opinion bias is nothing more than our
desire to believe in something and to defend it. Also, despite our feelings,
believing has no solid logical or philosophical grounds. In this paper, I will
show that if we combine the fact that even logic can never prove an idea right
or wrong and the problems our brains cause when we pick ideas, hypothesis
testing and its terminology are a recipe for disaster. Testing should have no
place when we are thinking about hypothesis.
</summary>
    <author>
      <name>André C. R. Martins</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.06545v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.06545v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.08118v1</id>
    <updated>2016-11-24T09:46:13Z</updated>
    <published>2016-11-24T09:46:13Z</published>
    <title>BayesVarSel: Bayesian Testing, Variable Selection and model averaging in
  Linear Models using R</title>
    <summary>  This paper introduces the R package BayesVarSel which implements objective
Bayesian methodology for hypothesis testing and variable selection in linear
models. The package computes posterior probabilities of the competing
hypotheses/models and provides a suite of tools, specifically proposed in the
literature, to properly summarize the results. Additionally, \ourpack\ is armed
with functions to compute several types of model averaging estimations and
predictions with weights given by the posterior probabilities. BayesVarSel
contains exact algorithms to perform fast computations in problems of small to
moderate size and heuristic sampling methods to solve large problems. The
software is intended to appeal to a broad spectrum of users, so the interface
has been carefully designed to be highly intuititive and is inspired by the
well-known lm function. The issue of prior inputs is carefully addressed. In
the default usage (fully automatic for the user)BayesVarSel implements the
criteria-based priors proposed by Bayarri et al (2012), but the advanced user
has the possibility of using several other popular priors in the literature.
The package is available through the Comprehensive R Archive Network, CRAN. We
illustrate the use of BayesVarSel with several data examples.
</summary>
    <author>
      <name>Gonzalo Garcia-Donato</name>
    </author>
    <author>
      <name>Anabel Forte</name>
    </author>
    <link href="http://arxiv.org/abs/1611.08118v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.08118v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.01619v1</id>
    <updated>2016-12-06T01:24:34Z</updated>
    <published>2016-12-06T01:24:34Z</published>
    <title>High-dimensional nonparametric monotone function estimation using BART</title>
    <summary>  For the estimation of a regression relationship between Y and a large set of
po- tential predictors x 1 , . . . , x p , the flexible nature of a
nonparametric approach such as BART (Bayesian Additive Regression Trees) allows
for a much richer set of possibilities than a more restrictive parametric
approach. However, it may often occur that subject matter considerations
suggest the relationship will be monotone in one or more of the predictors. For
such situations, we propose monotone BART, a constrained version of BART that
uses the monotonicity information to improve function estimation with- out the
need of using a parametric form. Imposing monotonicity, when appropriate,
results in (i) function estimates that are smoother and more interpretable,
(ii) better out-of-sample predictive performance, (iii) less uncertainty, and
(iv) less sensitivity to prior choice. While some of the key aspects of the
unconstrained BART model carry over directly to monotone BART, the imposition
of the monotonicity constraints ne- cessitates a fundamental rethinking of how
the model is implemented. In particular, in the original BART algorithm, the
Markov Chain Monte Carlo algorithm relied on a conditional conjugacy that is no
longer available in a high-dimensional, constrained space.
</summary>
    <author>
      <name>Hugh A. Chipman</name>
    </author>
    <author>
      <name>Edward I. George</name>
    </author>
    <author>
      <name>Robert E. McCulloch</name>
    </author>
    <author>
      <name>Thomas S. Shively</name>
    </author>
    <link href="http://arxiv.org/abs/1612.01619v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.01619v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.02252v1</id>
    <updated>2016-12-07T14:04:11Z</updated>
    <published>2016-12-07T14:04:11Z</published>
    <title>Building Communication Skills in a Theoretical Statistics Course</title>
    <summary>  The traditional theoretical statistics course which develops the theoretical
underpinnings of the discipline (usually following a probability course) is
undergoing near-continuous revision in the statistics community. In particular,
recent versions of this course have incorporated more and more computation. We
take a look at a different aspect of the revision - building student
communication skills in the course, in both written and verbal forms, to allow
students to demonstrate their ability to explain statistical concepts. Two
separate projects are discussed, both of which were engaged in by a class of
size 17 in Spring 2015. The first project had a computational aspect (performed
using R), a statistical theory component, and a writing component, and was
based on the historical German tank problem. The second project involved a
class presentation and written report summarizing, critiquing, and/or
explaining an article selected from The American Statistician.
</summary>
    <author>
      <name>Amy Wagaman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">slightly modified version of JSM 2016 proceedings paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.02252v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.02252v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.07542v1</id>
    <updated>2016-12-22T10:59:59Z</updated>
    <published>2016-12-22T10:59:59Z</published>
    <title>A Graph Downsampling Technique Based On Graph Fourier Transform</title>
    <summary>  In this paper, we provide a Graph Fourier Transform based approach to
downsample signals on graphs. For bandlimited signals on a graph, a test is
provided to identify whether signal reconstruction is possible from the given
downsampled signal. Moreover, if the signal is not bandlimited, we provide a
quality measure for comparing different downsampling schemes. Using this
quality measure, we propose a greedy downsampling algorithm. Most of the
prevailing approaches consider undirected graphs, and exploit the topological
properties of the graph in order to downsample the grid, while the proposed
method exploits spectral properties of graph signals, and is applicable to
directed graphs, undirected graphs, and graphs with negative edge-weights. We
provide several experiments demonstrating our downsampling scheme, and compare
our quality measure with measures like normalized cuts.
</summary>
    <author>
      <name>Nileshkumar Vaishnav</name>
    </author>
    <author>
      <name>Aditya Tatu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Written in journal paper format, inlcudes 8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.07542v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.07542v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.08452v1</id>
    <updated>2017-01-29T23:52:51Z</updated>
    <published>2017-01-29T23:52:51Z</published>
    <title>Enriching students' conceptual understanding of confidence intervals: An
  interactive trivia-based classroom activity</title>
    <summary>  Confidence intervals provide a way to determine plausible values for a
population parameter. They are omnipresent in research articles involving
statistical analyses. Appropriately, a key statistical literacy learning
objective is the ability to interpret and understand confidence intervals in a
wide range of settings. As instructors, we devote a considerable amount of time
and effort to ensure that students master this topic in introductory courses
and beyond. Yet, studies continue to find that confidence intervals are
commonly misinterpreted and that even experts have trouble calibrating their
individual confidence levels. In this article, we present a ten-minute trivia
game-based activity that addresses these misconceptions by exposing students to
confidence intervals from a personal perspective. We describe how the activity
can be integrated into a statistics course as a one-time activity or with
repetition at intervals throughout a course, discuss results of using the
activity in class, and present possible extensions.
</summary>
    <author>
      <name>Xiaofei Wang</name>
    </author>
    <author>
      <name>Nicholas G. Reich</name>
    </author>
    <author>
      <name>Nicholas J. Horton</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages; in press at the American Statistician</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.08452v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.08452v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62-01" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.02432v1</id>
    <updated>2017-02-07T07:58:35Z</updated>
    <published>2017-02-07T07:58:35Z</published>
    <title>The Use of Burst Frequency Offsets in the Search for MH370</title>
    <summary>  Malaysian Airlines flight MH370 veered off course unexpectedly during a
scheduled trip from Kuala Lumpur to Beijing on the 7th of March 2014. MH370 was
tracked via military radar into the Malacca Straits and after disappearing from
radar was believed to have turned south towards the southern Indian Ocean
before crashing approximately six hours later. This article presents details of
the Burst Frequency Offsets (BFOs) associated with automated satellite
communications messages received from MH370 and what information they give
about the end of the flight scenario. The article presents:
  1. A review of the statistical analysis of BFOs for several previous flights
of this aircraft.
  2. A method for presenting the difference between the measured BFO and the
predicted BFO as a function of the aircraft's track angle.
  3. An analysis of the behavior of the reference frequency oscillator in the
plane's satellite data unit after power outage events.
  4. How the results of the oscillator analysis, combined with direct Doppler
analysis suggest that the final two BFOs recorded show that the aircraft was
rapidly descending at the time of the last satellite communications
transmissions.
</summary>
    <author>
      <name>Ian D. Holland</name>
    </author>
    <link href="http://arxiv.org/abs/1702.02432v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.02432v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.08261v1</id>
    <updated>2017-02-27T13:02:58Z</updated>
    <published>2017-02-27T13:02:58Z</published>
    <title>J.B.S. Haldane Could Have Done Better</title>
    <summary>  In a review on the contribution of J.B.S. Haldane to the development of the
Bayes factor hypothesis test (arXiv:1511.08180), Etz and Wagenmakers focus on
Haldane's proposition of a mixture prior in a genetic example (Haldane 1932, A
note on inverse probability. Mathematical Proceedings of the Cambridge
Philosophical Society, 28, 55-61.). As Haldane never followed up on these
ideas, it is difficult to gauge his motivation and intentions. I argue that
contrary to Haldane's stated intention of replacing flat priors with more
reasonable assumptions, he actually chose in this example an unreasonable flat
prior. Considering the information available to Haldane, I derive a superior
prior and compare to Haldane's flat prior. Haldane's main intent with his
article seems to have been to explore the different parameter regions of the
binomial and the conjugate beta. Furthermore, I agree with Etz and Wagenmakers
that Haldane serendipitously adopted a mixture prior comprising a point mass
and smooth distribution in his genetic example.
</summary>
    <author>
      <name>Claus Vogl</name>
    </author>
    <link href="http://arxiv.org/abs/1702.08261v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.08261v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.06109v1</id>
    <updated>2017-03-16T13:07:54Z</updated>
    <published>2017-03-16T13:07:54Z</published>
    <title>Generalised Reichenbachian Common Cause Systems</title>
    <summary>  The principle of the common cause claims that if an improbable coincidence
has occurred, there must exist a common cause. This is generally taken to mean
that positive correlations between non-causally related events should disappear
when conditioning on the action of some underlying common cause. The extended
interpretation of the principle, by contrast, urges that common causes should
be called for in order to explain positive deviations between the estimated
correlation of two events and the expected value of their correlation. The aim
of this paper is to provide the extended reading of the principle with a
general probabilistic model, capturing the simultaneous action of a system of
multiple common causes. To this end, two distinct models are elaborated, and
the necessary and sufficient conditions for their existence are determined.
</summary>
    <author>
      <name>Claudio Mazzola</name>
    </author>
    <link href="http://arxiv.org/abs/1703.06109v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.06109v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.08648v2</id>
    <updated>2017-05-01T03:14:19Z</updated>
    <published>2017-03-25T04:44:47Z</published>
    <title>The new concepts of measurement error's regularities and effect
  characteristics</title>
    <summary>  In several literatures, the authors give a new thinking of measurement theory
system based on error non-classification philosophy, which completely
overthrows the existing measurement concepts system of precision, trueness and
accuracy. In this paper, aiming at the issues of error's regularities and
effect characteristics, the authors will do a thematic explanation, and prove
that the error's regularities actually come from different cognitive
perspectives, is also unable to be used for classifying errors, and that the
error's effect characteristics actually depend on artificial condition rules of
repeated measurement, and is still unable to be used for classifying errors.
Thus, from the perspectives of error's regularities and effect characteristics,
the existing error classification philosophy is still incorrect; and an
uncertainty concept system, which must be interpreted by the error
non-classification philosophy, naturally become the only way out of measurement
theory.
</summary>
    <author>
      <name>Xiaoming Ye</name>
    </author>
    <author>
      <name>Haibo Liu</name>
    </author>
    <author>
      <name>Mo Ling</name>
    </author>
    <author>
      <name>Xuebin Xiao</name>
    </author>
    <link href="http://arxiv.org/abs/1703.08648v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.08648v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.00674v1</id>
    <updated>2017-04-03T16:43:24Z</updated>
    <published>2017-04-03T16:43:24Z</published>
    <title>Nonparametric estimation of the conditional distribution at regression
  boundary points</title>
    <summary>  Nonparametric regression is a standard statistical tool with increased
importance in the Big Data era. Boundary points pose additional difficulties
but local polynomial regression can be used to alleviate them. Local linear
regression, for example, is easy to implement and performs quite well both at
interior as well as boundary points. Estimating the conditional distribution
function and/or the quantile function at a given regressor point is immediate
via standard kernel methods but problems ensue if local linear methods are to
be used. In particular, the distribution function estimator is not guaranteed
to be monotone increasing, and the quantile curves can "cross". In the paper at
hand, a simple method of correcting the local linear distribution estimator for
monotonicity is proposed, and its good performance is demonstrated via
simulations and real data examples.
</summary>
    <author>
      <name>Srinjoy Das</name>
    </author>
    <author>
      <name>Dimitris N. Politis</name>
    </author>
    <link href="http://arxiv.org/abs/1704.00674v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.00674v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.03812v8</id>
    <updated>2017-06-27T09:47:59Z</updated>
    <published>2017-04-12T16:09:00Z</published>
    <title>The reinterpretation of standard deviation concept</title>
    <summary>  Existing mathematical theory interprets the concept of standard deviation as
the dispersion degree. Therefore, in measurement theory, both uncertainty
concept and precision concept, which are expressed with standard deviation or
times standard deviation, are also defined as the dispersion of measurement
result, so that the concept logic is tangled. Through comparative analysis of
the standard deviation concept and re-interpreting the measurement error
evaluation principle, this paper points out that the concept of standard
deviation is actually single error's probability interval value instead of
dispersion degree, and that the error with any regularity can be evaluated by
standard deviation, corrected this mathematical concept, and gave the
correction direction of measurement concept logic. These will bring a global
change to measurement theory system.
</summary>
    <author>
      <name>Xiaoming Ye</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.03812v8" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.03812v8" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60A05, 60A10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.07512v1</id>
    <updated>2017-04-25T01:55:24Z</updated>
    <published>2017-04-25T01:55:24Z</published>
    <title>Information vs. Uncertainty as the Foundation for a Science of
  Environmental Modeling</title>
    <summary>  Information accounting provides a better foundation for hypothesis testing
than does uncertainty quantification. A quantitative account of science is
derived under this perspective that alleviates the need for epistemic bridge
principles, solves the problem of ad hoc falsification criteria, and deals with
verisimilitude by facilitating a general approach to process-level diagnostics.
Our argument is that the well-known inconsistencies of both Bayesian and
classical statistical hypothesis tests are due to the fact that probability
theory is an insufficient logic of science. Information theory, as an extension
of probability theory, is required to provide a complete logic on which to base
quantitative theories of empirical learning. The organizing question in this
case becomes not whether our theories or models are more or less true, or about
how much uncertainty is associated with a particular model, but instead whether
there is any information available from experimental data that might allow us
to improve the model. This becomes a formal hypothesis test, provides a theory
of model diagnostics, and suggests a new approach to building dynamical systems
models.
</summary>
    <author>
      <name>Grey Nearing</name>
    </author>
    <author>
      <name>Hoshin Gupta</name>
    </author>
    <link href="http://arxiv.org/abs/1704.07512v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.07512v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.01451v2</id>
    <updated>2017-06-13T02:00:31Z</updated>
    <published>2017-05-03T14:38:31Z</published>
    <title>Feasibility study on the least square method for fitting non-Gaussian
  noise data</title>
    <summary>  This study is to investigate the feasibility of least square method in
fitting non-Gaussian noise data. We add different levels of the two typical
non-Gaussian noises, L\'evy and stretched Gaussian noises, to exact value of
the selected functions including linear equations, polynomial and exponential
equations, and the maximum absolute and the mean square errors are calculated
for the different cases. L\'evy and stretched Gaussian distributions have many
applications in fractional and fractal calculus. It is observed that the
non-Gaussian noises are less accurately fitted than the Gaussian noise, but the
stretched Gaussian cases appear to perform better than the L\'evy noise cases.
It is stressed that the least-squares method is inapplicable to the
non-Gaussian noise cases when the noise level is larger than 5%.
</summary>
    <author>
      <name>Wei Xu</name>
    </author>
    <author>
      <name>Wen Chen</name>
    </author>
    <author>
      <name>Yingjie Liang</name>
    </author>
    <link href="http://arxiv.org/abs/1705.01451v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.01451v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.03560v1</id>
    <updated>2017-05-08T17:13:11Z</updated>
    <published>2017-05-08T17:13:11Z</published>
    <title>A Dutch Book against Sleeping Beauties Who Are Evidential Decision
  Theorists</title>
    <summary>  In the context of the Sleeping Beauty problem, it has been argued that
so-called "halfers" can avoid Dutch book arguments by adopting evidential
decision theory. I introduce a Dutch book for a variant of the Sleeping Beauty
problem and argue that evidential decision theorists fall prey to it, whether
they are halfers or thirders. The argument crucially requires that an action
can provide evidence for what the agent would do not only at other decision
points where she has exactly the same information, but also at decision points
where she has different but "symmetric" information.
</summary>
    <author>
      <name>Vincent Conitzer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11229-015-0691-7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11229-015-0691-7" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper appears in {\em Synthese}, Volume 192, Issue 9, pp.
  2887-2899, October 2015. The final publication is available at Springer via
  http://dx.doi.org/10.1007/s11229-015-0691-7</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Synthese, Volume 192, Issue 9, pp. 2887-2899, October 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1705.03560v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.03560v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.08544v1</id>
    <updated>2017-05-23T21:35:04Z</updated>
    <published>2017-05-23T21:35:04Z</published>
    <title>Data Visualization on Day One: Bringing Big Ideas into Intro Stats Early
  and Often</title>
    <summary>  In a world awash with data, the ability to think and compute with data has
become an important skill for students in many fields. For that reason,
inclusion of some level of statistical computing in many introductory-level
courses has grown more common in recent years. Existing literature has
documented multiple success stories of teaching statistics with R, bolstered by
the capabilities of R Markdown. In this article, we present an in-class data
visualization activity intended to expose students to R and R Markdown during
the first week of an introductory statistics class. The activity begins with a
brief lecture on exploratory data analysis in R. Students are then placed in
small groups tasked with exploring a new dataset to produce three
visualizations that describe particular insights that are not immediately
obvious from the data. Upon completion, students will have produced a series of
univariate and multivariate visualizations on a real dataset and practiced
describing them.
</summary>
    <author>
      <name>Xiaofei Wang</name>
    </author>
    <author>
      <name>Cynthia Rush</name>
    </author>
    <author>
      <name>Nicholas Jon Horton</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in Technology Innovations in Statistics Education</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.08544v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.08544v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.09530v1</id>
    <updated>2017-05-26T11:09:15Z</updated>
    <published>2017-05-26T11:09:15Z</published>
    <title>Updated guidelines, updated curriculum: The GAISE College Report and
  introductory statistics for the modern student</title>
    <summary>  Since the 2005 American Statistical Association's (ASA) endorsement of the
Guidelines for Assessment and Instruction in Statistics Education (GAISE)
College Report, changes in the statistics field and statistics education have
had a major impact on the teaching and learning of statistics. We now live in a
world where "Statistics - the science of learning from data - is the
fastest-growing science, technology, engineering, and math (STEM) undergraduate
degree in the United States," according to the ASA, and where many jobs demand
an understanding of how to explore and make sense of data. In light of these
new reports and other changes and demands on the discipline, a group of
volunteers revised the 2005 GAISE College Report. The updated report was
endorsed by the Board of Directors of the American Statistical Association in
July 2016. To help shed additional light on the revision process and subsequent
changes in the report, we review the report and share insights into the
committee's thoughts and assumptions.
</summary>
    <author>
      <name>Beverly L. Wood</name>
    </author>
    <author>
      <name>Megan Mocko</name>
    </author>
    <author>
      <name>Michelle Everson</name>
    </author>
    <author>
      <name>Nicholas J. Horton</name>
    </author>
    <author>
      <name>Paul Velleman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in press, CHANCE</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.09530v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.09530v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62-01" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.01584v3</id>
    <updated>2017-07-19T10:30:49Z</updated>
    <published>2017-06-06T02:27:26Z</published>
    <title>Multiple Object Tracking in Unknown Backgrounds with Labeled Random
  Finite Sets</title>
    <summary>  This paper proposes an on-line multiple object tracking algorithm that can
operate in unknown background. In a majority of multiple object tracking
applications, model parameters for background processes such as clutter and
detection are unknown and vary with time, hence the ability of the algorithm to
adaptively learn the these parameters is essential in practice. In this work,
we detail how the Generalized Labeled Multi Bernouli (GLMB) filter a tractable
and provably Bayes optimal multi-object tracker can be tailored to learn
clutter and detection parameters on the fly while tracking. Provided that these
background model parameters do not fluctuate rapidly compared to the data rate,
the proposed algorithm can adapt to the unknown background yielding better
tracking performance.
</summary>
    <author>
      <name>Yuthika Punchihewa</name>
    </author>
    <author>
      <name>Ba-Tuong Vo</name>
    </author>
    <author>
      <name>Ba-Ngu Vo</name>
    </author>
    <author>
      <name>Du Yong Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.01584v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.01584v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.00944v1</id>
    <updated>2017-07-04T12:46:15Z</updated>
    <published>2017-07-04T12:46:15Z</published>
    <title>A novel entropy recurrence quantification analysis</title>
    <summary>  The growing study of time series, especially those related to nonlinear
systems, has challenged the methodologies to characterize and classify
dynamical structures of a signal. Here we conceive a new diagnostic tool for
time series based on the concept of information entropy, in which the
probabilities are associated to microstates defined from the recurrence phase
space. Recurrence properties can properly be studied using recurrence plots, a
methodology based on binary matrices where trajec- tories in phase space of
dynamical systems are evaluated against other embedded trajectory. Our novel
entropy methodology has several advantages compared to the traditional
recurrence entropy defined in the literature, namely, the correct evaluation of
the chaoticity level of the signal, the weak dependence on parameters, correct
evaluation of periodic time series properties and more sensitivity to noise
level of time series. Furthermore, the new entropy quantifier developed in this
manuscript also fixes inconsistent results of the traditional recurrence
entropy concept, reproducing classical results with novel insights.
</summary>
    <author>
      <name>G. Corso</name>
    </author>
    <author>
      <name>T. L. Prado</name>
    </author>
    <author>
      <name>G. Z. dos S. Lima</name>
    </author>
    <author>
      <name>S. R. Lopes</name>
    </author>
    <link href="http://arxiv.org/abs/1707.00944v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.00944v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.02748v1</id>
    <updated>2017-07-10T08:50:43Z</updated>
    <published>2017-07-10T08:50:43Z</published>
    <title>Confidence biases and learning among intuitive Bayesians</title>
    <summary>  We design a double-or-quits game to compare the speed of learning one's
specific ability with the speed of rising confidence as the task gets
increasingly difficult. We find that people on average learn to be
overconfident faster than they learn their true ability and we present an
intuitive-Bayesian model of confidence which integrates confidence biases and
learning. Uncertainty about one's true ability to perform a task in isolation
can be responsible for large and stable confidence biases, namely limited
discrimination, the hard--easy effect, the Dunning--Kruger effect, conservative
learning from experience and the overprecision phenomenon (without
underprecision) if subjects act as Bayesian learners who rely only on
sequentially perceived performance cues and contrarian illusory signals induced
by doubt. Moreover, these biases are likely to persist since the Bayesian
aggregation of past information consolidates the accumulation of errors and the
perception of contrarian illusory signals generates conservatism and
under-reaction to events. Taken together, these two features may explain why
intuitive Bayesians make systematically wrong predictions of their own
performance.
</summary>
    <author>
      <name>Louis Lévy-Garboua</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CIRANO, PSE, CES</arxiv:affiliation>
    </author>
    <author>
      <name>Muniza Askari</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CES</arxiv:affiliation>
    </author>
    <author>
      <name>Marco Gazel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PSE, CES</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Theory and Decision, Springer Verlag, 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.02748v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.02748v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.05769v1</id>
    <updated>2017-07-18T17:51:04Z</updated>
    <published>2017-07-18T17:51:04Z</published>
    <title>Estimation for Inverse Weibull Distribution Under Type-I Hybrid
  Censoring</title>
    <summary>  The mixture of Type I and Type II censoring schemes is called the hybrid
censoring. This paper presents the statistical inferences of the Inverse
Weibull distribution when the data are Type-I hybrid censored. First we
consider the maximum likelihood estimators of the unknown parameters. It is
observed that the maximum likelihood estimators can not be obtained in closed
form. We further obtain the Bayes estimators and the corresponding highest
posterior density credible intervals of the unknown parameters under the
assumption of independent gamma priors using the importance sampling procedure.
We also compute the approximate Bayes estimators using Lindley's approximation
technique. We have performed a simulation study in order to compare the
proposed Bayes estimators with the maximum likelihood estimators. A real life
data set is used to illustrate the results derived.
</summary>
    <author>
      <name>Mohammad Kazemi</name>
    </author>
    <author>
      <name>Mina Azizpour</name>
    </author>
    <link href="http://arxiv.org/abs/1707.05769v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.05769v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62N01, 62N02" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.05804v1</id>
    <updated>2017-07-18T18:16:31Z</updated>
    <published>2017-07-18T18:16:31Z</published>
    <title>Estimation of P(X &gt; Y ) for Weibull distribution based on hybrid
  censored samples</title>
    <summary>  A Hybrid censoring scheme is mixture of Type-I and Type-II censoring schemes.
Based on hybrid censored samples, this paper deals with the in- ference on R =
P(X &gt; Y ), when X and Y are two independent Weibull distributions with
different scale parameters, but having the same shape pa- rameter. The maximum
likelihood estimator (MLE), and the approximate MLE (AMLE) of R are obtained.
The asymptotic distribution of the maxi- mum likelihood estimator of R is
obtained. Based on the asymptotic distribu- tion, the confidence interval of R
can be derived. Two bootstrap confidence intervals are also proposed. We
consider the Bayesian estimate of R, and propose the corresponding credible
interval for R. Monte Carlo simulations are performed to compare the different
proposed methods. Analysis of a real data set has also been presented for
illustrative purposes.
</summary>
    <author>
      <name>Akbar Asgharzadeh</name>
    </author>
    <author>
      <name>Mohammad Kazemi</name>
    </author>
    <author>
      <name>Debasis Kundu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s13198-015-0390-2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s13198-015-0390-2" rel="related"/>
    <link href="http://arxiv.org/abs/1707.05804v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.05804v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.00060v2</id>
    <updated>2017-08-02T06:14:26Z</updated>
    <published>2017-07-20T09:07:46Z</published>
    <title>Application of Bayesian Networks for Estimation of Individual
  Psychological Characteristics</title>
    <summary>  In this paper we apply Bayesian networks for developing more accurate final
overall estimations of psychological characteristics of an individual, based on
psychological test results. Psychological tests which identify how much an
individual possesses a certain factor are very popular and quite common in the
modern world. We call this value for a given factor -- the final overall
estimation. Examples of factors could be stress resistance, the readiness to
take a risk, the ability to concentrate on certain complicated work and many
others. An accurate qualitative and comprehensive assessment of human potential
is one of the most important challenges in any company or collective. The most
common way of studying psychological characteristics of each single person is
testing. Psychologists and sociologists are constantly working on improvement
of the quality of their tests. Despite serious work, done by psychologists, the
questions in tests often do not produce enough feedback due to the use of
relatively poor estimation systems. The overall estimation is usually based on
personal experiences and the subjective perception of a psychologist or a group
of psychologists about the investigated psychological personality factors.
</summary>
    <author>
      <name>Alexander Litvinenko</name>
    </author>
    <author>
      <name>Natalya Litvinenko</name>
    </author>
    <author>
      <name>Orken Mamyrbayev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.00060v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.00060v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62C10, 62C12" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.2; G.3; G.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.06992v1</id>
    <updated>2017-07-26T21:12:42Z</updated>
    <published>2017-07-26T21:12:42Z</published>
    <title>Econométrie et Machine Learning</title>
    <summary>  Econometrics and machine learning seem to have one common goal: to construct
a predictive model, for a variable of interest, using explanatory variables (or
features). However, these two fields developed in parallel, thus creating two
different cultures, to paraphrase Breiman (2001). The first was to build
probabilistic models to describe economic phenomena. The second uses algorithms
that will learn from their mistakes, with the aim, most often to classify
(sounds, images, etc.). Recently, however, learning models have proven to be
more effective than traditional econometric techniques (with a price to pay
less explanatory power), and above all, they manage to manage much larger data.
In this context, it becomes necessary for econometricians to understand what
these two cultures are, what opposes them and especially what brings them
closer together, in order to appropriate tools developed by the statistical
learning community to integrate them into Econometric models.
</summary>
    <author>
      <name>Arthur Charpentier</name>
    </author>
    <author>
      <name>Emmanuel Flachaire</name>
    </author>
    <author>
      <name>Antoine Ly</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in French</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.06992v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.06992v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.01946v1</id>
    <updated>2017-10-05T10:10:56Z</updated>
    <published>2017-10-05T10:10:56Z</published>
    <title>Scientific progress despite irreproducibility: A seeming paradox</title>
    <summary>  It appears paradoxical that science is producing outstanding new results and
theories at a rapid rate at the same time that researchers are identifying
serious problems in the practice of science that cause many reports to be
irreproducible and invalid. Certainly the practice of science needs to be
improved and scientists are now pursuing this goal. However, in this
perspective we argue that this seeming paradox is not new, has always been part
of the way science works, and likely will remain so. We first introduce the
paradox. We then review a wide range of challenges that appear to make
scientific success difficult. Next, we describe the factors that make science
work-in the past, present, and presumably also in the future. We then suggest
that remedies for the present practice of science need to be applied
selectively so as not to slow progress, and illustrate with a few examples. We
conclude with arguments that communication of science needs to emphasize not
just problems but the enormous successes and benefits that science has brought
and is now bringing to all elements of modern society.
</summary>
    <author>
      <name>Richard M. Shiffrin</name>
    </author>
    <author>
      <name>Katy Borner</name>
    </author>
    <author>
      <name>Stephen M. Stigler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.01946v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.01946v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.2596v1</id>
    <updated>2010-12-12T22:35:47Z</updated>
    <published>2010-12-12T22:35:47Z</published>
    <title>A Unified MGF-Based Capacity Analysis of Diversity Combiners over
  Generalized Fading Channels</title>
    <summary>  Unified exact average capacity results for L-branch coherent diversity
receivers including equal-gain combining (EGC) and maximal-ratio combining
(MRC) are not known. This paper develops a novel generic framework for the
capacity analysis of $L$-branch EGC/MRC over generalized fading channels. The
framework is used to derive new results for the Gamma shadowed generalized
Nakagami-m fading model which can be a suitable model for the fading
environments encountered by high frequency (60 GHz and above) communications.
The mathematical formalism is illustrated with some selected numerical and
simulation results confirming the correctness of our newly proposed framework.
</summary>
    <author>
      <name>Ferkan Yilmaz</name>
    </author>
    <author>
      <name>Mohamed-Slim Alouini</name>
    </author>
    <link href="http://arxiv.org/abs/1012.2596v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.2596v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.3543v4</id>
    <updated>2011-04-10T13:40:57Z</updated>
    <published>2011-03-18T01:28:42Z</published>
    <title>Array Variate Elliptical Random Variables with Multiway Kronecker Delta
  Covariance Matrix Structure</title>
    <summary>  Standard statistical methods applied to matrix random variables often fail to
describe the underlying structure in multiway data sets. In this paper we will
discuss the concept of an array variate random variable and introduce a class
of elliptical array densities which have elliptical contours.
</summary>
    <author>
      <name>Deniz Akdemir</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A part of this paper appears in a Technical Report No: 11-02
  published by Department of Mathematics and Statistics at the Bowling Green
  State University</arxiv:comment>
    <link href="http://arxiv.org/abs/1103.3543v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.3543v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.4513v2</id>
    <updated>2011-10-03T13:55:40Z</updated>
    <published>2011-06-22T17:21:48Z</published>
    <title>A Markov Chain approach to determine the optimal performance period and
  bad definition for credit scorecard</title>
    <summary>  Performance period determination and bad definition for credit scorecard has
been a mix of fortune for the typical data modeler. The lack of literature on
these matters led to a proliferation of approaches and techniques to solve the
problems. However, the most commonly accepted approach involves subjective
interpretations of the performance period and bad definition as well as being
chicken and egg problem. These complications result in poorly developed credit
scorecard with minimal benefits to the banks. In this paper, we will be
recommending a simple and effective approach to resolve these issues.
</summary>
    <author>
      <name> Choy</name>
    </author>
    <author>
      <name> Ma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 Pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Research Journal of Social Science and Management,Vol 1, No. 6
  (2011) 227-234</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1106.4513v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.4513v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="91G40" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.3586v3</id>
    <updated>2011-10-25T01:37:08Z</updated>
    <published>2011-08-17T22:02:35Z</published>
    <title>Order preserving property of moment estimators</title>
    <summary>  Balakrishnan and Mi [1] considered order preserving property of maximum
likelihood estimators. In this paper there are given conditions under which the
moment estimators have the property of preserving stochastic orders. There is
considered property of preserving for usual stochastic order as well as for
likelihood ratio order. Mainly, sufficient conditions are given for one
parameter family of distributions and also for exponential family, location
family and scale family.
</summary>
    <author>
      <name>Piotr Nowak</name>
    </author>
    <link href="http://arxiv.org/abs/1108.3586v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.3586v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1112.3777v1</id>
    <updated>2011-12-16T12:22:29Z</updated>
    <published>2011-12-16T12:22:29Z</published>
    <title>Parameter estimation for the discretely observed fractional
  Ornstein-Uhlenbeck process and the Yuima R package</title>
    <summary>  This paper proposes consistent and asymptotically Gaussian estimators for the
drift, the diffusion coefficient and the Hurst exponent of the discretely
observed fractional Ornstein-Uhlenbeck process. For the estimation of the
drift, the results are obtained only in the case when 1/2 &lt; H &lt; 3/4. This paper
also provides ready-to-use software for the R statistical environment based on
the YUIMA package.
</summary>
    <author>
      <name>Alexandre Brouste</name>
    </author>
    <author>
      <name>Stefano M. Iacus</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1112.3777v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1112.3777v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62M" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.4743v1</id>
    <updated>2012-01-23T15:39:00Z</updated>
    <published>2012-01-23T15:39:00Z</published>
    <title>Voting Power : A Generalised Framework</title>
    <summary>  This paper examines an area of Game Theory called Voting Power Theory. With
the adoption of a measure theoretic framework it argues that the many different
indices and tools currently used for measuring voting power can be replaced by
just three simple probabilities. The framework is sufficiently general to be
applicable to every conceivable type of voting game, and every possible
decision rule.
</summary>
    <author>
      <name>Sreejith Das</name>
    </author>
    <author>
      <name>Iead Rezek</name>
    </author>
    <link href="http://arxiv.org/abs/1201.4743v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.4743v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.3468v1</id>
    <updated>2012-02-15T23:00:50Z</updated>
    <published>2012-02-15T23:00:50Z</published>
    <title>Partially-blind Estimation of Reciprocal Channels for AF Two-Way Relay
  Networks Employing M-PSK Modulation</title>
    <summary>  We consider the problem of channel estimation for amplify-and-forward two-way
relays assuming channel reciprocity and M-PSK modulation. In an earlier work, a
partially-blind maximum-likelihood estimator was derived by treating the data
as deterministic unknowns. We prove that this estimator approaches the true
channel with high probability at high signal-to-noise ratio (SNR) but is not
consistent. We then propose an alternative estimator which is consistent and
has similarly favorable high SNR performance. We also derive the Cramer-Rao
bound on the variance of unbiased estimators.
</summary>
    <author>
      <name>Saeed Abdallah</name>
    </author>
    <author>
      <name>Ioannis N. Psaromiligkos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1202.3468v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.3468v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.3880v1</id>
    <updated>2012-03-17T18:06:25Z</updated>
    <published>2012-03-17T18:06:25Z</published>
    <title>Parameter Estimation from Censored Samples using the
  Expectation-Maximization Algorithm</title>
    <summary>  This paper deals with parameter estimation when the data are randomly right
censored. The maximum likelihood estimates from censored samples are obtained
by using the expectation-maximization (EM) and Monte Carlo EM (MCEM)
algorithms. We introduce the concept of the EM and MCEM algorithms and develop
parameter estimation methods for a variety of distributions such as normal,
Laplace and Rayleigh distributions. These proposed methods are illustrated with
three examples.
</summary>
    <author>
      <name>Chanseok Park</name>
    </author>
    <author>
      <name>Seong Beom Lee</name>
    </author>
    <link href="http://arxiv.org/abs/1203.3880v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.3880v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.6140v1</id>
    <updated>2012-03-28T02:41:50Z</updated>
    <published>2012-03-28T02:41:50Z</published>
    <title>Why FARIMA Models are Brittle</title>
    <summary>  The FARIMA models, which have long-range-dependence (LRD), are widely used in
many areas. Through deriving a precise characterisation of the spectrum,
autocovariance function, and variance time function, we show that this family
is very atypical among LRD processes, being extremely close to the fractional
Gaussian noise in a precise sense. Furthermore, we show that this closeness
property is not robust to additive noise. We argue that the use of FARIMA, and
more generally fractionally differenced time series, should be reassessed in
some contexts, in particular when convergence rate under rescaling is important
and noise is expected.
</summary>
    <author>
      <name>Anders Gorst-Rasmussen</name>
    </author>
    <author>
      <name>Darryl Veitch</name>
    </author>
    <author>
      <name>András Gefferth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1203.6140v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.6140v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.3339v1</id>
    <updated>2012-04-16T00:54:45Z</updated>
    <published>2012-04-16T00:54:45Z</published>
    <title>Parameterization of Copulas and Covariance Decay of Stochastic Processes
  with Applications</title>
    <summary>  In this work we study the problem of constructing stochastic processes with a
predetermined covariance decay by parameterizing its marginals and a given
family of copulas. We present several examples to illustrate the theory,
including the important Gaussian and Euclidean families of copulas. We
associate the theory to common applied time series models and present a general
methodology to estimate a given parameter of interest identifiable through the
process' covariance decay. To exemplify the proposed methodology, we present
simple Monte Carlo applications to parameter estimation in time series. The
methodology is also applied to the S&amp;P500 US stock market index.
</summary>
    <author>
      <name>Guilherme Pumi</name>
    </author>
    <author>
      <name>Sílvia R. C. Lopes</name>
    </author>
    <link href="http://arxiv.org/abs/1204.3339v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.3339v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.5963v2</id>
    <updated>2013-06-26T22:34:16Z</updated>
    <published>2012-04-26T15:49:24Z</published>
    <title>On a Reliable Peer-Review Process</title>
    <summary>  We propose an enhanced peer-review process where the reviewers are encouraged
to truthfully disclose their reviews. We start by modelling that process using
a Bayesian model where the uncertainty regarding the quality of the manuscript
is taken into account. After that, we introduce a scoring function to evaluate
the reported reviews. Under mild assumptions, we show that reviewers strictly
maximize their expected scores by telling the truth. We also show how those
scores can be used in order to reach consensus.
</summary>
    <author>
      <name>Arthur Carvalho</name>
    </author>
    <author>
      <name>Kate Larson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by the author due to some errors in the
  basic model</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.5963v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.5963v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.0824v2</id>
    <updated>2013-05-22T17:15:33Z</updated>
    <published>2012-05-03T21:06:59Z</published>
    <title>A Generalization of a Gaussian Semiparametric Estimator on Multivariate
  Long-Range Dependent Processes</title>
    <summary>  In this paper we propose and study a general class of Gaussian Semiparametric
Estimators (GSE) of the fractional differencing parameter in the context of
long-range dependent multivariate time series. We establish large sample
properties of the estimator without assuming Gaussianity. The class of models
considered here satisfies simple conditions on the spectral density function,
restricted to a small neighborhood of the zero frequency and includes important
class of VARFIMA processes. We also present a simulation study to assess the
finite sample properties of the proposed estimator based on a smoothed version
of the GSE which supports its competitiveness.
</summary>
    <author>
      <name>Guilherme Pumi</name>
    </author>
    <author>
      <name>Sílvia R. C. Lopes</name>
    </author>
    <link href="http://arxiv.org/abs/1205.0824v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.0824v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.5478v2</id>
    <updated>2014-08-04T07:10:28Z</updated>
    <published>2012-06-24T10:40:45Z</published>
    <title>Developing methods for identifying the inflection point of a
  convex/concave curve</title>
    <summary>  We are introducing two methods for revealing the true inflection point of
data that contains or not error. The starting point is a set of geometrical
properties that follow the existence of an inflection point p for a smooth
function. These properties connect the concept of convexity/concavity before
and after p respectively with three chords defined properly. Finally a set of
experiments is presented for the class of sigmoid curves and for the third
order polynomials.
</summary>
    <author>
      <name>Demetris T. Christopoulos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 6 figures; 29 pages, 10 figures, 20 tables, new layout, new
  definitions in section 1, graphical explanation of ESE &amp; EDE methods,
  improved bibliography</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.5478v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.5478v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="Primary 65H99, Secondary 62F12" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.1371v1</id>
    <updated>2012-09-06T19:04:35Z</updated>
    <published>2012-09-06T19:04:35Z</published>
    <title>On the age-, time- and migration dependent dynamics of diseases</title>
    <summary>  This paper generalizes a previously published differential equation that
describes the relation between the age-specific incidence, remission, and
mortality of a disease with its prevalence. The underlying model is a simple
compartment model with three states (illness-death model). In contrast to the
former work, migration- and calendar time-effects are included. As an
application of the theoretical findings, a hypothetical example of an
irreversible disease is treated.
</summary>
    <author>
      <name>Ralph Brinks</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.1371v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.1371v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="92C60, 92D30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.4678v1</id>
    <updated>2012-09-20T22:34:57Z</updated>
    <published>2012-09-20T22:34:57Z</published>
    <title>On Control Charts for Monitoring the Variance of a Time Series</title>
    <summary>  In this paper we derive control charts for the variance of a Gaussian process
using the likelihood ratio approach, the generalized likelihood ratio approach,
the sequential probability ratio method and a generalized sequential
probability ratio procedure, the Shiryaev-Roberts procedure and a generalized
Shiryaev-Roberts ap- proach. Recursive presentations for the calculation of the
control statistics are given for autoregressive processes of order 1. In an
extensive simulation study these schemes are compared with existing control
charts for the variance. In order to asses the performance of the schemes both
the average run length and the average delay are used.
</summary>
    <author>
      <name>Taras Lazariv</name>
    </author>
    <author>
      <name>Wolfgang Schmid</name>
    </author>
    <author>
      <name>Svitlana Zabolotska</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 4 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.4678v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.4678v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.6617v1</id>
    <updated>2013-04-24T15:02:32Z</updated>
    <published>2013-04-24T15:02:32Z</published>
    <title>EM-based Semi-blind Channel Estimation in AF Two-Way Relay Networks</title>
    <summary>  We propose an expectation maximization (EM)-based algorithm for semi-blind
channel estimation of reciprocal channels in amplify-and-forward (AF) two-way
relay networks (TWRNs). By incorporating both data samples and pilots into the
estimation, the proposed algorithm provides substantially higher accuracy than
the conventional training-based approach. Furthermore, the proposed algorithm
has a linear computational complexity per iteration and converges after a small
number of iterations.
</summary>
    <author>
      <name>Saeed Abdallah</name>
    </author>
    <author>
      <name>Ioannis N. Psaromiligkos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1304.6617v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.6617v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.6307v1</id>
    <updated>2013-07-24T06:41:35Z</updated>
    <published>2013-07-24T06:41:35Z</published>
    <title>Is there currently a scientific revolution in scientometrics?</title>
    <summary>  The author of this letter to the editor would like to set forth the argument
that scientometrics is currently in a phase in which a taxonomic change, and
hence a revolution, is taking place. One of the key terms in scientometrics is
scientific impact which nowadays is understood to mean not only the impact on
science but the impact on every area of society.
</summary>
    <author>
      <name>Lutz Bornmann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in the Journal of the American Society for
  Information Science and Technology</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.6307v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.6307v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.6257v1</id>
    <updated>2013-11-25T10:44:53Z</updated>
    <published>2013-11-25T10:44:53Z</published>
    <title>Filters and smoothers for self-exciting Markov modulated counting
  processes</title>
    <summary>  We consider a self-exciting counting process, the parameters of which depend
on a hidden finite-state Markov chain. We derive the optimal filter and
smoother for the hidden chain based on observation of the jump process. This
filter is in closed form and is finite dimensional. We demonstrate the
performance of this filter both with simulated data, and by analysing the
`flash crash' of 6th May 2010 in this framework.
</summary>
    <author>
      <name>Samuel N. Cohen</name>
    </author>
    <author>
      <name>Robert J. Elliott</name>
    </author>
    <link href="http://arxiv.org/abs/1311.6257v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.6257v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.CP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.CP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.TR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62M05, 60G55, 60J28, 91G70" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.4471v2</id>
    <updated>2014-04-28T14:16:13Z</updated>
    <published>2014-03-18T14:23:02Z</published>
    <title>Principal bundles over statistical manifolds</title>
    <summary>  In this paper, we introduce the concept of principal bundles on statistical
manifolds. After necessary preliminaries on information geometry and principal
bundles on manifolds, we study the $\alpha$-structure of frame bundles over
statistical manifolds with respect to $\alpha$-connections, by giving geometric
structures. The manifold of one-dimensional normal distributions appears in the
end as an application and a concrete example.
</summary>
    <author>
      <name>Didong Li</name>
    </author>
    <author>
      <name>Huafei Sun</name>
    </author>
    <author>
      <name>Chen Tao</name>
    </author>
    <author>
      <name>Lin Jiu</name>
    </author>
    <link href="http://arxiv.org/abs/1403.4471v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.4471v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.DG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.2037v6</id>
    <updated>2014-07-28T13:53:26Z</updated>
    <published>2014-07-08T11:21:12Z</published>
    <title>Which of the world's institutions employ the most highly cited
  researchers? An analysis of the data from highlycited.com</title>
    <summary>  A few weeks ago, Thomson Reuters published a list of the highly cited
researchers worldwide (highlycited.com). Since the data is freely available for
downloading and includes the names of the researchers' institutions, we
produced a ranking of the institutions on the basis of the number of highly
cited researchers per institution. This ranking is intended to be a helpful
amendment of other available institutional rankings.
</summary>
    <author>
      <name>Lutz Bornmann</name>
    </author>
    <author>
      <name>Johann Bauer</name>
    </author>
    <link href="http://arxiv.org/abs/1407.2037v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.2037v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.7933v1</id>
    <updated>2014-09-28T17:10:20Z</updated>
    <published>2014-09-28T17:10:20Z</published>
    <title>Parametric Risk Parity</title>
    <summary>  Any optimization algorithm based on the risk parity approach requires the
formulation of portfolio total risk in terms of marginal contributions. In this
paper we use the independence of the underlying factors in the market to derive
the centered moments required in the risk decomposition process when the
modified versions of Value at Risk and Expected Shortfall are considered.
  The choice of the Mixed Tempered Stable distribution seems adequate for
fitting skewed and heavy tailed distributions. The ensuing detailed description
of the optimization procedure is due to the existence of analytical higher
order moments. Better results are achieved in terms of out of sample
performance and greater diversification.
</summary>
    <author>
      <name>Lorenzo Mercuri</name>
    </author>
    <author>
      <name>Edit Rroji</name>
    </author>
    <link href="http://arxiv.org/abs/1409.7933v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.7933v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.RM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.RM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.PM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.1802v1</id>
    <updated>2014-10-07T16:39:23Z</updated>
    <published>2014-10-07T16:39:23Z</published>
    <title>Piterbarg's max-discretisation theorem for stationary vector Gaussian
  processes observed on different grids</title>
    <summary>  In this paper we derive Piterbarg's max-discretisation theorem for two
different grids considering centered stationary vector Gaussian processes. So
far in the literature results in this direction have been derived for the joint
distribution of the maximum of Gaussian processes over $[0,T]$ and over a grid
$ \mathfrak{R}(\delta_1(T))=\{k\delta_1(T): k=0,1,\cdots\}$. In this paper we
extend recent findings by considering additionally the \bE{maximum} over
another grid $ \mathfrak{R}(\delta_2(T))$. We derive the joint limiting
distribution of maximum of stationary Gaussian vector processes for different
choices of such grids by letting $T\to \infty$.
</summary>
    <author>
      <name>E. Hashorva</name>
    </author>
    <author>
      <name>Z. Tan</name>
    </author>
    <link href="http://arxiv.org/abs/1410.1802v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.1802v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.2778v3</id>
    <updated>2015-06-30T08:52:05Z</updated>
    <published>2014-11-11T12:23:30Z</published>
    <title>Testing Order Constraints: Qualitative Differences Between Bayes Factors
  and Normalized Maximum Likelihood</title>
    <summary>  We compared Bayes factors to normalized maximum likelihood for the simple
case of selecting between an order-constrained versus a full binomial model.
This comparison revealed two qualitative differences in testing order
constraints regarding data dependence and model preference.
</summary>
    <author>
      <name>Daniel W. Heck</name>
    </author>
    <author>
      <name>Eric-Jan Wagenmakers</name>
    </author>
    <author>
      <name>Richard D. Morey</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.spl.2015.06.014</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.spl.2015.06.014" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Statistics &amp; Probability Letters 105 (2015) 157-162</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1411.2778v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.2778v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.7831v1</id>
    <updated>2014-12-25T15:49:41Z</updated>
    <published>2014-12-25T15:49:41Z</published>
    <title>Asymptotics of the convex hull of spherical samples</title>
    <summary>  In this paper we consider the convex hull of a spherically symmetric sample
in $R^d$. Our main contributions are some new asymptotic results for the
expectation of the number of vertices, number of facets, area and the volume of
the convex hull assuming that the marginal distributions are in the Gumbel
max-domain of attraction. Further, we briefly discuss two other models assuming
that the marginal distributions are regularly varying or $O$-regularly varying.
</summary>
    <author>
      <name>Enkelejd Hashorva</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.dam.2010.10.017</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.dam.2010.10.017" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proposition 3.2 is new in this version</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Discrete Applied Mathematics, 159(4), 201-211, 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1412.7831v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.7831v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.00318v1</id>
    <updated>2015-02-01T21:43:51Z</updated>
    <published>2015-02-01T21:43:51Z</published>
    <title>Setting the stage for data science: integration of data management
  skills in introductory and second courses in statistics</title>
    <summary>  Many have argued that statistics students need additional facility to express
statistical computations. By introducing students to commonplace tools for data
management, visualization, and reproducible analysis in data science and
applying these to real-world scenarios, we prepare them to think statistically.
In an era of increasingly big data, it is imperative that students develop
data-related capacities, beginning with the introductory course. We believe
that the integration of these precursors to data science into our
curricula-early and often-will help statisticians be part of the dialogue
regarding "Big Data" and "Big Questions".
</summary>
    <author>
      <name>Nicholas J. Horton</name>
    </author>
    <author>
      <name>Benjamin S. Baumer</name>
    </author>
    <author>
      <name>Hadley Wickham</name>
    </author>
    <link href="http://arxiv.org/abs/1502.00318v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.00318v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62-01" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.07902v1</id>
    <updated>2015-07-28T19:10:07Z</updated>
    <published>2015-07-28T19:10:07Z</published>
    <title>Robust Estimation in Stochastic Frontier Models</title>
    <summary>  This study proposes a robust estimator for stochastic frontier models by
integrating the idea of Basu et al. [1998, Biometrika 85, 549-559] into such
models. We verify that the suggested estimator is strongly consistent and
asymptotic normal under regularity conditions and investigate robust
properties. We use a simulation study to demonstrate that the estimator has
strong robust properties with little loss in asymptotic efficiency relative to
the maximum likelihood estimator. A real data analysis is performed for
illustrating the use of the estimator.
</summary>
    <author>
      <name>Junmo Song</name>
    </author>
    <author>
      <name>Dong-hyun Oh</name>
    </author>
    <author>
      <name>Jiwon Kang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">43 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.07902v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.07902v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.05307v1</id>
    <updated>2015-12-16T20:00:19Z</updated>
    <published>2015-12-16T20:00:19Z</published>
    <title>Implicit Regression: Detecting Constants and Inverse Relationships with
  Bivariate Random Error</title>
    <summary>  In 2011, Wooten introduced Non-Response Analysis the founding theory in
Implicit Regression where Implicit Regression treats the variables implicitly
as codependent variables and not as an explicit function with dependent or
independent variables as in standard regression. The motivation of this paper
is to introduce methods of implicit regression to determine the constant nature
of a variable or the interactive term, and address inverse relationship among
measured variables with random error present in both directions.
</summary>
    <author>
      <name>R. D. Wooten</name>
    </author>
    <author>
      <name>K. Baah</name>
    </author>
    <author>
      <name>J. D'Andrea</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 22 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.05307v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.05307v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.06696v1</id>
    <updated>2016-02-22T09:30:12Z</updated>
    <published>2016-02-22T09:30:12Z</published>
    <title>A note on basis dimension selection in generalized additive modelling</title>
    <summary>  Two new approaches for checking the dimension of the basis functions when
using penalized regression smoothers are presented. The first approach is a
test for adequacy of the basis dimension based on an estimate of the residual
variance calculated by differencing residuals that are neighbours according to
the smooth covariates. The second approach is based on estimated degrees of
freedom for a smooth of the model residuals with respect to the model
covariates. In comparison with basis dimension selection algorithms based on
smoothness selection criterion (GCV, AIC, REML) the above procedures are
computationally efficient enough for routine use as part of model checking.
</summary>
    <author>
      <name>Natalya Pya</name>
    </author>
    <author>
      <name>Simon N Wood</name>
    </author>
    <link href="http://arxiv.org/abs/1602.06696v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.06696v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.05814v1</id>
    <updated>2016-05-19T05:38:13Z</updated>
    <published>2016-05-19T05:38:13Z</published>
    <title>Some Mathematical Aspects of Price Optimisation</title>
    <summary>  Calculation of an optimal tariff is a principal challenge for pricing
actuaries. In this contribution we are concerned with the renewal insurance
business discussing various mathematical aspects of calculation of an optimal
renewal tariff. Our motivation comes from two important actuarial tasks, namely
a) construction of an optimal renewal tariff subject to business and technical
constraints, and b) determination of an optimal allocation of certain premium
loadings. We consider both continuous and discrete optimisation and then
present several algorithmic sub-optimal solutions. Additionally, we explore
some simulation techniques. Several illustrative examples show both the
complexity and the importance of the optimisation approach.
</summary>
    <author>
      <name>Y. Bai</name>
    </author>
    <author>
      <name>E. Hashorva</name>
    </author>
    <author>
      <name>G. Ratovomirija</name>
    </author>
    <author>
      <name>M. Tamraz</name>
    </author>
    <link href="http://arxiv.org/abs/1605.05814v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.05814v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.CP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.CP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.05150v1</id>
    <updated>2016-07-18T16:00:05Z</updated>
    <published>2016-07-18T16:00:05Z</published>
    <title>Statistical Methods in Topological Data Analysis for Complex,
  High-Dimensional Data</title>
    <summary>  The utilization of statistical methods an their applications within the new
field of study known as Topological Data Analysis has has tremendous potential
for broadening our exploration and understanding of complex, high-dimensional
data spaces. This paper provides an introductory overview of the mathematical
underpinnings of Topological Data Analysis, the workflow to convert samples of
data to topological summary statistics, and some of the statistical methods
developed for performing inference on these topological summary statistics. The
intention of this non-technical overview is to motivate statisticians who are
interested in learning more about the subject.
</summary>
    <author>
      <name>Patrick S. Medina</name>
    </author>
    <author>
      <name>R. W. Doerge</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 7 Figures, 27th Annual Conference on Applied Statistics in
  Agriculture</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.05150v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.05150v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.05551v2</id>
    <updated>2017-08-27T18:48:37Z</updated>
    <published>2016-09-18T21:21:39Z</published>
    <title>Graphical Models for Discrete and Continuous Data</title>
    <summary>  We introduce a general framework for undirected graphical models. It
generalizes Gaussian graphical models to a wide range of continuous, discrete,
and combinations of different types of data. We also show that the models in
the framework, called exponential trace models, are amenable to efficient
estimation and inference based on maximum likelihood. As a consequence, we
expect applications to a large variety of multivariate data that have
correlated coordinates.
</summary>
    <author>
      <name>Johannes Lederer</name>
    </author>
    <link href="http://arxiv.org/abs/1609.05551v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.05551v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.05292v1</id>
    <updated>2016-12-13T17:24:19Z</updated>
    <published>2016-12-13T17:24:19Z</published>
    <title>Probability, propensity and probabilities of propensities (and of
  probabilities)</title>
    <summary>  The process of doing Science in condition of uncertainty is illustrated with
a toy experiment in which the inferential and the forecasting aspects are both
present. The fundamental aspects of probabilistic reasoning, also relevant in
real life applications, arise quite naturally and the resulting discussion
among non-ideologized, free-minded people offers an opportunity for
clarifications.
</summary>
    <author>
      <name>Giulio D'Agostini</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1063/1.4985350</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1063/1.4985350" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Invited contribution to the proceedings MaxEnt 2016 based on the talk
  given at the workshop (Ghent, Belgium, 10-15 July 2016), supplemented by work
  done within the program Probability and Statistics in Forensic Science at the
  Isaac Newton Institute for Mathematical Sciences, Cambridge</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.05292v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.05292v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.HO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.HO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.hist-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.03043v2</id>
    <updated>2017-07-05T22:35:58Z</updated>
    <published>2017-03-08T21:40:57Z</published>
    <title>Bootstrap with Clustering in Two or More Dimensions</title>
    <summary>  We propose a bootstrap procedure for data that may exhibit clustering in two
or more dimensions. We use insights from the theory of generalized U-statistics
to analyze the large-sample properties of statistics that are sample averages
from the observations pooled across clusters. The asymptotic distribution of
these statistics may be non-standard if there is no clustering in means. We
show that the proposed bootstrap procedure is (a) point-wise consistent for any
fixed data-generating process (DGP), (b) uniformly consistent if we exclude the
case of clustering without clustering in means, and (c) provides refinements
for any DGP such that the limiting distribution is Gaussian.
</summary>
    <author>
      <name>Konrad Menzel</name>
    </author>
    <link href="http://arxiv.org/abs/1703.03043v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.03043v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.07825v1</id>
    <updated>2017-05-22T16:07:22Z</updated>
    <published>2017-05-22T16:07:22Z</published>
    <title>Comparing the Finite-Time Performance of Simulation-Optimization
  Algorithms</title>
    <summary>  We empirically evaluate the finite-time performance of several
simulation-optimization algorithms on a testbed of problems with the goal of
motivating further development of algorithms with strong finite-time
performance. We investigate if the observed performance of the algorithms can
be explained by properties of the problems, e.g., the number of decision
variables, the topology of the objective function, or the magnitude of the
simulation error.
</summary>
    <author>
      <name>Naijia Dong</name>
    </author>
    <author>
      <name>David J. Eckman</name>
    </author>
    <author>
      <name>Matthias Poloczek</name>
    </author>
    <author>
      <name>Xueqi Zhao</name>
    </author>
    <author>
      <name>Shane G. Henderson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The source codes of benchmarks and algorithms are available at
  https://bitbucket.org/poloczek/finitetimesimopt</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.07825v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.07825v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.00599v1</id>
    <updated>2017-06-02T09:17:54Z</updated>
    <published>2017-06-02T09:17:54Z</published>
    <title>On a Global Objective Prior from Score Rules</title>
    <summary>  This paper takes a novel look at the construction of objective prior
distributions. In particular we use calculus of variation methods to minimise
integrals of Langrangians. It is the Langrangian which therefore is the basic
construct for the objective prior. This will be based on combinations of
functions representing information for density functions. Another key idea is
that we think about information contained in a density function (the prior)
rather than thinking about the information a density holds for a parameter.
</summary>
    <author>
      <name>Fabrizio Leisen</name>
    </author>
    <author>
      <name>Cristiano Villa</name>
    </author>
    <author>
      <name>Stephen G. Walker</name>
    </author>
    <link href="http://arxiv.org/abs/1706.00599v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.00599v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.06354v1</id>
    <updated>2017-06-20T10:18:14Z</updated>
    <published>2017-06-20T10:18:14Z</published>
    <title>Consistency of the plug-in functional predictor of the
  Ornstein-Uhlenbeck process in Hilbert and Banach spaces</title>
    <summary>  New results on functional prediction of the Ornstein-Uhlenbeck process in an
autoregressive Hilbert-valued and Banach-valued frameworks are derived.
Specifically, consistency of the maximum likelihood estimator of the
autocorrelation operator, and of the associated plug-in predictor is obtained
in both frameworks.
</summary>
    <author>
      <name>J. Álvarez-Liébana</name>
    </author>
    <author>
      <name>D. Bosq</name>
    </author>
    <author>
      <name>M. D. Ruiz-Medina</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.spl.2016.04.023</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.spl.2016.04.023" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages with 10 figures. Supplementary material (8 pages) is also
  provided</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Statistics &amp; Probability Letters, 117, pp. 12-22 (2016)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1706.06354v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.06354v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.FA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60G10, 60G15, 60F99, 60J05, 65F15" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.05069v1</id>
    <updated>2017-08-05T05:47:33Z</updated>
    <published>2017-08-05T05:47:33Z</published>
    <title>A causation coefficient and taxonomy of correlation/causation
  relationships</title>
    <summary>  This paper introduces a causation coefficient which is defined in terms of
probabilistic causal models. This coefficient is suggested as the natural
causal analogue of the Pearson correlation coefficient and permits comparing
causation and correlation to each other in a simple, yet rigorous manner.
Together, these coefficients provide a natural way to classify the possible
correlation/causation relationships that can occur in practice and examples of
each relationship are provided. In addition, the typical relationship between
correlation and causation is analyzed to provide insight into why correlation
and causation are often conflated. Finally, example calculations of the
causation coefficient are shown on a real data set.
</summary>
    <author>
      <name>Joshua Brulé</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.05069v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.05069v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.03154v1</id>
    <updated>2017-09-10T18:59:00Z</updated>
    <published>2017-09-10T18:59:00Z</published>
    <title>Recent progress in log-concave density estimation</title>
    <summary>  In recent years, log-concave density estimation via maximum likelihood
estimation has emerged as a fascinating alternative to traditional
nonparametric smoothing techniques, such as kernel density estimation, which
require the choice of one or more bandwidths. The purpose of this article is to
describe some of the properties of the class of log-concave densities on
$\mathbb{R}^d$ which make it so attractive from a statistical perspective, and
to outline the latest methodological, theoretical and computational advances in
the area.
</summary>
    <author>
      <name>Richard J. Samworth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.03154v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.03154v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G05, 62G07" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1007.3210v1</id>
    <updated>2010-07-19T17:07:15Z</updated>
    <published>2010-07-19T17:07:15Z</published>
    <title>Development and Initial Validation of a Scale to Measure Instructors'
  Attitudes toward Concept-Based Teaching of Introductory Statistics in the
  Health and Behavioral Sciences</title>
    <summary>  Despite more than a decade of reform efforts, students continue to experience
difficulty understanding and applying statistical concepts. The predominant
focus of reform has been on content, pedagogy, technology and assessment, with
little attention to instructor characteristics. However, there is strong
theoretical and empirical evidence that instructors' attitudes impact the
quality of teaching and learning. The objective of this study was to develop
and initially validate a scale to measure instructors' attitudes toward
reform-oriented (or concept-based) teaching of introductory statistics in the
health and behavioral sciences, at the tertiary level. This scale will be
referred to as FATS (Faculty Attitudes Toward Statistics). Data were obtained
from 227 instructors (USA and international), and analyzed using factor
analysis, multidimensional scaling and hierarchical cluster analysis. The
overall scale consists of five sub-scales with a total of 25 items, and an
overall alpha of 0.89. Construct validity was established. Specifically, the
overall scale, and subscales (except perceived difficulty) plausibly
differentiated between low-reform and high-reform practice instructors.
Statistically significant differences in attitude were observed with respect to
age, but not gender, employment status, membership status in professional
organizations, ethnicity, highest academic qualification, and degree
concentration. This scale can be considered a reliable and valid measure of
instructors' attitudes toward reform-oriented (concept-based or constructivist)
teaching of introductory statistics in the health and behavioral sciences at
the tertiary level. These five dimensions influence instructors' attitudes.
Additional studies are required to confirm these structural and psychometric
properties.
</summary>
    <author>
      <name>Rossi A. Hassad</name>
    </author>
    <author>
      <name>Anthony P. M. Coxon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ISI (International Statistical Institute) Conference Publication</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Annual Session of the International Statistical Institute (56th,
  2007)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1007.3210v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1007.3210v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1008.0740v1</id>
    <updated>2010-08-04T10:44:30Z</updated>
    <published>2010-08-04T10:44:30Z</published>
    <title>$L_p$-nested symmetric distributions</title>
    <summary>  Tractable generalizations of the Gaussian distribution play an important role
for the analysis of high-dimensional data. One very general super-class of
Normal distributions is the class of $\nu$-spherical distributions whose random
variables can be represented as the product $\x = r\cdot \u$ of a uniformly
distribution random variable $\u$ on the $1$-level set of a positively
homogeneous function $\nu$ and arbitrary positive radial random variable $r$.
Prominent subclasses of $\nu$-spherical distributions are spherically symmetric
distributions ($\nu(\x)=\|\x\|_2$) which have been further generalized to the
class of $L_p$-spherically symmetric distributions ($\nu(\x)=\|\x\|_p$). Both
of these classes contain the Gaussian as a special case. In general, however,
$\nu$-spherical distributions are computationally intractable since, for
instance, the normalization constant or fast sampling algorithms are unknown
for an arbitrary $\nu$. In this paper we introduce a new subclass of
$\nu$-spherical distributions by choosing $\nu$ to be a nested cascade of
$L_p$-norms. This class is still computationally tractable, but includes all
the aforementioned subclasses as a special case. We derive a general expression
for $L_p$-nested symmetric distributions as well as the uniform distribution on
the $L_p$-nested unit sphere, including an explicit expression for the
normalization constant. We state several general properties of $L_p$-nested
symmetric distributions, investigate its marginals, maximum likelihood fitting
and discuss its tight links to well known machine learning methods such as
Independent Component Analysis (ICA), Independent Subspace Analysis (ISA) and
mixed norm regularizers. Finally, we derive a fast and exact sampling algorithm
for arbitrary $L_p$-nested symmetric distributions, and introduce the Nested
Radial Factorization algorithm (NRF), which is a form of non-linear ICA.
</summary>
    <author>
      <name>Fabian Sinz</name>
    </author>
    <author>
      <name>Matthias Bethge</name>
    </author>
    <link href="http://arxiv.org/abs/1008.0740v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1008.0740v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1010.2326v1</id>
    <updated>2010-10-12T09:15:45Z</updated>
    <published>2010-10-12T09:15:45Z</published>
    <title>A brief history of the Fail Safe Number in Applied Research</title>
    <summary>  Rosenthal's (1979) Fail-Safe-Number (FSN) is probably one of the best known
statistics in the context of meta-analysis aimed to estimate the number of
unpublished studies in meta-analyses required to bring the meta-analytic mean
effect size down to a statistically insignificant level. Already before
Scargle's (2000) and Schonemann &amp; Scargle's (2008) fundamental critique on the
claimed stability of the basic rationale of the FSN approach, objections
focusing on the basic assumption of the FSN which treats the number of studies
as unbiased with averaging null were expressed throughout the history of the
FSN by different authors (Elashoff, 1978; Iyengar &amp; Greenhouse, 1988a; 1988b;
see also Scargle, 2000). In particular, Elashoff's objection appears to be
important because it was the very first critique pointing directly to the
central problem of the FSN: "R &amp; R claim that the number of studies hidden in
the drawers would have to be 65,000 to achieve a mean effect size of zero when
combined with the 345 studies reviewed here. But surely, if we allowed the
hidden studies to be negative, on the average no more than 345 hidden studies
would be necessary to obtain a zero mean effect size" (p. 392). Thus, users of
meta-analysis could have been aware right from the beginning that something was
wrong with the statistical reasoning of the FSN. In particular, from an applied
research perspective, it is therefore of interest whether any of the
fundamental objections on the FSN are reflected in standard handbooks on
meta-analysis as well as -and of course even more importantly- in meta-analytic
studies itself.
</summary>
    <author>
      <name>Moritz Heene</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1010.2326v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1010.2326v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.1069v2</id>
    <updated>2011-12-13T08:36:32Z</updated>
    <published>2010-12-06T02:30:07Z</published>
    <title>Degrees of Equivalence in a Key Comparison</title>
    <summary>  In an interlaboratory key comparison, a data analysis procedure for this
comparison was proposed and recommended by CIPM [1, 2, 3], therein the degrees
of equivalence of measurement standards of the laboratories participated in the
comparison and the ones between each two laboratories were introduced but a
corresponding clear and plausible measurement model was not given. Authors in
[4] offered possible measurement models for a given comparison and a suitable
model was selected out after rigorous analyzing steps for expectation values of
these degrees of equivalence. The systematic laboratory-effects model was then
selected as a right one in this report. Those models were all based on the one
true value existence assumption. However in the year 2008, a new version of the
Vocabulary for International Metrology (VIM) [7] was issued where the true
value of a given measurement standard should be now perceived as multi true
values which following a given statistics distribution. Applying this
perception of true values of a measurement standard with combination of the
steps in [4], measurement models have been developed and degrees of equivalence
have been analyzed. The results show that although with new definition, the
systematic laboratory-effects model is still the reasonable one in a given key
comparison.
</summary>
    <author>
      <name>Thang H. Le</name>
    </author>
    <author>
      <name>Nguyen D. Do</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">multiple true values, key comparison</arxiv:comment>
    <link href="http://arxiv.org/abs/1012.1069v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.1069v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1107.5141v1</id>
    <updated>2011-07-26T09:22:46Z</updated>
    <published>2011-07-26T09:22:46Z</published>
    <title>Which are the best cities for psychology research worldwide? A map
  visualizing city ratios of observed and expected numbers of highly-cited
  papers</title>
    <summary>  We present scientometric results about world-wide centers of excellence in
psychology. Based on Web of Science data, domain-specific excellence can be
identified for cities where highly cited papers are published. Data refer to
all psychology articles published in 2007 which are documented in the Social
Science Citation Index and to their citation frequencies from 2007 to May 2011.
Visualized are 214 cities with an article output of at least 50 in 2007.
Statistical z tests are used for the evaluation of the degree to which an
observed number of top-cited papers (top-10%) for a city differs from the
number expected on the basis of randomness in the selection of papers. Map
visualizing city ratios on significant differences between observed and
expected numbers of highly-cited papers point at excellence centers in cities
at the East and West Coast of the United States as well as in Great Britain,
Germany, the Netherlands, Ireland, Belgium, Sweden, Finland, Australia, and
Taiwan. Furthermore, positive but non-significant differences in favor of high
citation rates are documented for some cities in the United States, Great
Britain, the Netherlands, the Scandinavian and the German-speaking countries,
Belgium, France, Spain, Israel, South Korea, and China. Scientometric results
show convincingly that highly-cited psychological research articles come from
the Anglo-American countries and some of the non-English European countries in
which the number of English-language publications has increased during the last
decades.
</summary>
    <author>
      <name>Lutz Bornmann</name>
    </author>
    <author>
      <name>Loet Leydesdorff</name>
    </author>
    <author>
      <name>Günter Krampen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, two figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1107.5141v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.5141v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62-XX" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.0828v2</id>
    <updated>2015-09-11T11:34:57Z</updated>
    <published>2011-09-05T08:31:00Z</published>
    <title>The Product Life Cycle of Durable Goods</title>
    <summary>  A dynamic model of the product lifecycle of (nearly) homogeneous durables in
polypoly markets is established. It describes the concurrent evolution of the
unit sales and price of durable goods. The theory is based on the idea that the
sales dynamics is determined by a meeting process of demanded with supplied
product units. Taking advantage from the Bass model for first purchase and a
logistic model for repurchase the entire product lifecycle of a durable can be
established. For the case of a fast growing supply the model suggests that the
mean price of the good decreases according to a logistic law. Both, the
established unit sales and price evolution are in agreement with the empirical
data studied in this paper. The presented approach discusses further the
interference of the diffusion process with the supply dynamics. The model
predicts the occurrence of lost sales in the initial stages of the lifecycle
due to supply constraints. They are the origin for a retarded market
penetration. The theory suggests that the imitation rate B indicating social
contagion in the Bass model has its maximum magnitude for the case of a large
amount of available units at introduction and a fast output increase. The
empirical data of the investigated samples are in qualitative agreement with
this prediction.
</summary>
    <author>
      <name>Joachim Kaldasch</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.9734/BJEMT/2015/20395</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.9734/BJEMT/2015/20395" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">British Journal of Economics, Management &amp; Trade 10(2): 1-17,
  2015, Article no.BJEMT.20395</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1109.0828v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.0828v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.4807v1</id>
    <updated>2012-05-22T05:36:49Z</updated>
    <published>2012-05-22T05:36:49Z</published>
    <title>A Conversation with Eugenio Regazzini</title>
    <summary>  Eugenio Regazzini was born on August 12, 1946 in Cremona (Italy), and took
his degree in 1969 at the University "L. Bocconi" of Milano. He has held
positions at the universities of Torino, Bologna and Milano, and at the
University "L. Bocconi" as assistant professor and lecturer from 1974 to 1980,
and then professor since 1980. He is currently professor in probability and
mathematical statistics at the University of Pavia. In the periods 1989-2001
and 2006-2009 he was head of the Institute for Applications of Mathematics and
Computer Science of the Italian National Research Council (C.N.R.) in Milano
and head of the Department of Mathematics at the University of Pavia,
respectively. For twelve years between 1989 and 2006, he served as a member of
the Scientific Board of the Italian Mathematical Union (U.M.I.). In 2007, he
was elected Fellow of the IMS and, in 2001, Fellow of the "Istituto
Lombardo---Accademia di Scienze e Lettere." His research activity in
probability and statistics has covered a wide spectrum of topics, including
finitely additive probabilities, foundations of the Bayesian paradigm,
exchangeability and partial exchangeability, distribution of functionals of
random probability measures, stochastic integration, history of probability and
statistics. Overall, he has been one of the most authoritative developers of de
Finetti's legacy. In the last five years, he has extended his scientific
interests to probabilistic methods in mathematical physics; in particular, he
has studied the asymptotic behavior of the solutions of equations, which are of
interest for the kinetic theory of gases. The present interview was taken in
occasion of his 65th birthday.
</summary>
    <author>
      <name>Antonio Lijoi</name>
    </author>
    <author>
      <name>Igor Prünster</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1214/11-STS362</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1214/11-STS362" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in at http://dx.doi.org/10.1214/11-STS362 the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Statistical Science 2011, Vol. 26, No. 4, 647-672</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1205.4807v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.4807v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.5597v1</id>
    <updated>2012-08-28T09:26:58Z</updated>
    <published>2012-08-28T09:26:58Z</published>
    <title>Biologists meet statisticians: A workshop for young scientists to foster
  interdisciplinary team work</title>
    <summary>  Life science and statistics have necessarily become essential partners. The
need to plan complex, structured experiments, involving elaborated designs, and
the need to analyse datasets in the era of systems biology and high throughput
technologies has to build upon professional statistical expertise. On the other
hand, conducting such analyses and also developing improved or new methods,
also for novel kinds of data, has to build upon solid biological understanding
and practise. However, the meeting of scientists of both fields is often
hampered by a variety of communicative hurdles - which are based on
field-specific working languages and cultural differences.
  As a step towards a better mutual understanding, we developed a workshop
concept bringing together young experimental biologists and statisticians, to
work as pairs and learn to value each others competences and practise
interdisciplinary communication in a casual atmosphere. The first
implementation of our concept was a cooperation of the German Region of the
International Biometrical Society and the Leibnitz Institute DSMZ-German
Collection of Microorganisms and Cell Cultures (short: DSMZ), Braunschweig,
Germany. We collected feedback in form of three questionnaires, oral comments,
and gathered experiences for the improvement of this concept. The long-term
challenge for both disciplines is the establishment of systematic schedules and
strategic partnerships which use the proposed workshop concept to foster mutual
understanding, to seed the necessary interdisciplinary cooperation network, and
to start training the indispensable communication skills at the earliest
possible phase of education.
</summary>
    <author>
      <name>Benjamin Hofner</name>
    </author>
    <author>
      <name>Lea Vaas</name>
    </author>
    <author>
      <name>John-Philip Lawo</name>
    </author>
    <author>
      <name>Tina Müller</name>
    </author>
    <author>
      <name>Johannes Sikorski</name>
    </author>
    <author>
      <name>Dirk Repsilber</name>
    </author>
    <link href="http://arxiv.org/abs/1208.5597v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.5597v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.3215v2</id>
    <updated>2012-12-04T16:27:01Z</updated>
    <published>2012-09-14T15:06:40Z</published>
    <title>Cramer-Rao-Induced Bounds for CANDECOMP/PARAFAC tensor decomposition</title>
    <summary>  This paper presents a Cramer-Rao lower bound (CRLB) on the variance of
unbiased estimates of factor matrices in Canonical Polyadic (CP) or
CANDECOMP/PARAFAC (CP) decompositions of a tensor from noisy observations,
(i.e., the tensor plus a random Gaussian i.i.d. tensor). A novel expression is
derived for a bound on the mean square angular error of factors along a
selected dimension of a tensor of an arbitrary dimension. The expression needs
less operations for computing the bound, O(NR^6), than the best existing
state-of-the art algorithm, O(N^3R^6) operations, where N and R are the tensor
order and the tensor rank. Insightful expressions are derived for tensors of
rank 1 and rank 2 of arbitrary dimension and for tensors of arbitrary dimension
and rank, where two factor matrices have orthogonal columns.
  The results can be used as a gauge of performance of different approximate CP
decomposition algorithms, prediction of their accuracy, and for checking
stability of a given decomposition of a tensor (condition whether the CRLB is
finite or not). A novel expression is derived for a Hessian matrix needed in
popular damped Gauss-Newton method for solving the CP decomposition of tensors
with missing elements. Beside computing the CRLB for these tensors the
expression may serve for design of damped Gauss-Newton algorithm for the
decomposition.
</summary>
    <author>
      <name>Petr Tichavsky</name>
    </author>
    <author>
      <name>Anh Huy Phan</name>
    </author>
    <author>
      <name>Zbynek Koldovsky</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TSP.2013.2245660</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TSP.2013.2245660" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">revised version with one new theorem and one new section</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions On Signal Processing, Vol 61, No. 8, April 15,
  2013, pp. 1986-1997</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1209.3215v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.3215v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.7474v1</id>
    <updated>2014-01-29T11:31:59Z</updated>
    <published>2014-01-29T11:31:59Z</published>
    <title>The phenotypic expansion and its boundaries</title>
    <summary>  The development of sport performances in the future is a subject of myth and
disagreement among experts. As arguments favoring and opposing such methodology
were discussed, other publications empirically showed that the past development
of performances followed a non linear trend. Other works, while deeply
exploring the conditions leading to world records, highlighted that performance
is tied to the economical and geopolitical context. Here we investigated the
following human boundaries: development of performances with time in Olympic
and non-Olympic events, development of sport performances with aging among
humans and others species (greyhounds, thoroughbreds, mice). Development of
performances from a broader point of view (demography &amp; lifespan) in a specific
sub-system centered on primary energy was also investigated. We show that the
physiological developments are limited with time. Three major and direct
determinants of sport performance are age, technology and climatic conditions
(temperature). However, all observed developments are related to the
international context including the efficient use of primary energies. This
last parameter is a major indirect propeller of performance development. We
show that when physiological and societal performance indicators such as
lifespan and population density depend on primary energies, the energy source,
competition and mobility are key parameters for achieving long term sustainable
trajectories. Otherwise, the vast majority (98.7%) of the studied trajectories
reaches 0 before 15 generations, due to the consumption of fossil energy and a
low mobility rate. This led us to consider that in the present turbulent
economical context and given the upcoming energy crisis, societal and physical
performances are not expected to grow continuously.
</summary>
    <author>
      <name>Geoffroy Berthelot</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Phd Thesis, 227 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.7474v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.7474v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.4287v1</id>
    <updated>2014-04-16T15:34:42Z</updated>
    <published>2014-04-16T15:34:42Z</published>
    <title>Network impact on persistence in a finite population dynamic diffusion
  model: application to an emergent seed exchange network</title>
    <summary>  Dynamic extinction colonisation models (also called contact processes) are
widely studied in epidemiology and in metapopulation theory. Contacts are
usually assumed to be possible only through a network of connected patches.
This network accounts for a spatial landscape or a social organisation of
interactions. Thanks to social network literature, heterogeneous networks of
contacts can be considered. A major issue is to assess the influence of the
network in the dynamic model. Most work with this common purpose uses
deterministic models or an approximation of a stochastic
Extinction-Colonisation model (sEC) which are relevant only for large networks.
When working with a limited size network, the induced stochasticity is
essential and has to be taken into account in the conclusions. Here, a rigorous
framework is proposed for limited size networks and the limitations of the
deterministic approximation are exhibited. This framework allows exact
computations when the number of patches is small. Otherwise, simulations are
used and enhanced by adapted simulation techniques when necessary. A
sensitivity analysis was conducted to compare four main topologies of networks
in contrasting settings to determine the role of the network. A challenging
case was studied in this context: seed exchange of crop species in the R\'eseau
Semences Paysannes (RSP), an emergent French farmers' organisation. A
stochastic Extinction-Colonisation model was used to characterize the
consequences of substantial changes in terms of RSP's social organisation on
the ability of the system to maintain crop varieties.
</summary>
    <author>
      <name>Pierre Barbillon</name>
    </author>
    <author>
      <name>Mathieu Thomas</name>
    </author>
    <author>
      <name>Isabelle Goldringer</name>
    </author>
    <author>
      <name>Frédéric Hospital</name>
    </author>
    <author>
      <name>Stéphane Robin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.jtbi.2014.10.032</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.jtbi.2014.10.032" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Theoretical Biology Volume 365, 21 January 2015, Pages
  365 376</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1404.4287v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.4287v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.7208v2</id>
    <updated>2014-05-07T03:59:13Z</updated>
    <published>2014-04-29T01:49:37Z</published>
    <title>Validating Sample Average Approximation Solutions with Negatively
  Dependent Batches</title>
    <summary>  Sample-average approximations (SAA) are a practical means of finding
approximate solutions of stochastic programming problems involving an extremely
large (or infinite) number of scenarios. SAA can also be used to find estimates
of a lower bound on the optimal objective value of the true problem which, when
coupled with an upper bound, provides confidence intervals for the true optimal
objective value and valuable information about the quality of the approximate
solutions. Specifically, the lower bound can be estimated by solving multiple
SAA problems (each obtained using a particular sampling method) and averaging
the obtained objective values. State-of-the-art methods for lower-bound
estimation generate batches of scenarios for the SAA problems independently. In
this paper, we describe sampling methods that produce negatively dependent
batches, thus reducing the variance of the sample-averaged lower bound
estimator and increasing its usefulness in defining a confidence interval for
the optimal objective value. We provide conditions under which the new sampling
methods can reduce the variance of the lower bound estimator, and present
computational results to verify that our scheme can reduce the variance
significantly, by comparison with the traditional Latin hypercube approach.
</summary>
    <author>
      <name>Jiajie Chen</name>
    </author>
    <author>
      <name>Cong Han Lim</name>
    </author>
    <author>
      <name>Peter Z. G. Qian</name>
    </author>
    <author>
      <name>Jeff Linderoth</name>
    </author>
    <author>
      <name>Stephen J. Wright</name>
    </author>
    <link href="http://arxiv.org/abs/1404.7208v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.7208v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.5501v2</id>
    <updated>2014-09-22T23:53:14Z</updated>
    <published>2014-09-19T02:15:43Z</published>
    <title>Tree Oriented Data Analysis</title>
    <summary>  Complex data objects arise in many areas of modern science including
evolutionary biology, nueroscience, dynamics of gene expression and medical
imaging. Object oriented data analysis (OODA) is the statistical analysis of
datasets of complex objects. Data analysis of tree data objects is an exciting
research area with interesting questions and challenging problems. This thesis
focuses on tree oriented statistical methodologies, and algorithms for solving
related mathematical optimization problems.
  This research is motivated by the goal of analyzing a data set of images of
human brain arteries. The approach we take here is to use a novel
representation of brain artery systems as points in phylogenetic treespace. The
treespace property of unique global geodesics leads to a notion of geometric
center called a Fr\'echet mean. For a sample of data points, the Fr\'echet
function is the sum of squared distances from a point to the data points, and
the Fr\'echet mean is the minimizer of the Fr\'echet function.
  In this thesis we use properties of the Fr\'echet function to develop an
algorithmic system for computing Fr\'echet means. Properties of the Fr\'echet
function are also used to show a sticky law of large numbers which describes a
surprising stability of the topological tree structure of sample Fr\'echet
means at that of the population Fr\'echet mean. We also introduce
non-parametric regression of brain artery tree structure as a response variable
to age based on weighted Fr\'echet means.
</summary>
    <author>
      <name>Sean Skwerer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PhD thesis, University of North Carolina, 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.5501v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.5501v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.5936v2</id>
    <updated>2015-10-13T16:22:27Z</updated>
    <published>2014-12-18T16:46:50Z</published>
    <title>Nonparametric estimation of the division rate of an age dependent
  branching process</title>
    <summary>  We study the nonparametric estimation of the branching rate $B(x)$ of a
supercritical Bellman-Harris population: a particle with age $x$ has a random
lifetime governed by $B(x)$; at its death time, it gives rise to $k \geq 2$
children with lifetimes governed by the same division rate and so on. We
observe in continuous time the process over $[0,T]$. Asymptotics are taken as
$T \rightarrow \infty$; the data are stochastically dependent and one has to
face simultaneously censoring, bias selection and non-ancillarity of the number
of observations. In this setting, under appropriate ergodicity properties, we
construct a kernel-based estimator of $B(x)$ that achieves the rate of
convergence $\exp(-\lambda_B \frac{\beta}{2\beta+1}T)$, where $\lambda_B$ is
the Malthus parameter and $\beta &gt;0$ is the smoothness of the function $B(x)$
in a vicinity of $x$. We prove that this rate is optimal in a minimax sense and
we relate it explicitly to classical nonparametric models such as density
estimation observed on an appropriate (parameter dependent) scale. We also shed
some light on the fact that estimation with kernel estimators based on data
alive at time $T$ only is not sufficient to obtain optimal rates of
convergence, a phenomenon which is specific to nonparametric estimation and
that has been observed in other related growth-fragmentation models.
</summary>
    <author>
      <name>Marc Hoffmann</name>
    </author>
    <author>
      <name>Adélaïde Olivier</name>
    </author>
    <link href="http://arxiv.org/abs/1412.5936v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.5936v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.04047v1</id>
    <updated>2015-02-08T18:32:06Z</updated>
    <published>2015-02-08T18:32:06Z</published>
    <title>On Statistical Analysis of the Pattern of Evolution of Perceived
  Emotions Induced by Hindustani Music- A Study Based on Listener Responses</title>
    <summary>  The objective of this study is to find the underlying pattern of how
perception of emotions has evolved in India. Here Hindustani Music has been
used as a reference frame for tracking the changing perception of emotions. It
has been found that different emotions perceived from Hindustani Music form a
particular sequential pattern when their corresponding pitch periods are
analyzed using the standard deviations and mean successive squared
differences.This sequential pattern of emotions coincides with their
corresponding sequential pattern of tempos or average number of steady states.
On the basis of this result we further found that the range of perception of
emotions has diminished significantly these days compared to what it was
before. The proportion of responses for the perceived emotions like Anger,
Serenity, Romantic and Sorrow has also decreased to a great extent than what it
was previously. The proportion of responses for the perceived emotion Anxiety
has increased phenomenally. Both standard deviation and mean successive squared
difference are two very good measures in tracking the changing perception of
emotions. The overall pattern of the change of perceived emotions has
corresponded to the psychological and sociological change of human life.
</summary>
    <author>
      <name>Vishal Midya</name>
    </author>
    <author>
      <name>Sneha Chakraborty</name>
    </author>
    <author>
      <name>Srijita Manna</name>
    </author>
    <author>
      <name>Ranjan Sengupta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Conference: Proceedings of 7th International Conference of IMBIC on
  "Mathematical Sciences for Advancement of Science and Technology" (MSAST
  2013)</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.04047v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.04047v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62-07" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.02780v3</id>
    <updated>2015-07-28T18:01:23Z</updated>
    <published>2015-03-10T06:17:46Z</published>
    <title>Replication, Communication, and the Population Dynamics of Scientific
  Discovery</title>
    <summary>  Many published research results are false, and controversy continues over the
roles of replication and publication policy in improving the reliability of
research. Addressing these problems is frustrated by the lack of a formal
framework that jointly represents hypothesis formation, replication,
publication bias, and variation in research quality. We develop a mathematical
model of scientific discovery that combines all of these elements. This model
provides both a dynamic model of research as well as a formal framework for
reasoning about the normative structure of science. We show that replication
may serve as a ratchet that gradually separates true hypotheses from false, but
the same factors that make initial findings unreliable also make replications
unreliable. The most important factors in improving the reliability of research
are the rate of false positives and the base rate of true hypotheses, and we
offer suggestions for addressing each. Our results also bring clarity to verbal
debates about the communication of research. Surprisingly, publication bias is
not always an obstacle, but instead may have positive impacts---suppression of
negative novel findings is often beneficial. We also find that communication of
negative replications may aid true discovery even when attempts to replicate
have diminished power. The model speaks constructively to ongoing debates about
the design and conduct of science, focusing analysis and discussion on precise,
internally consistent models, as well as highlighting the importance of
population dynamics.
</summary>
    <author>
      <name>Richard McElreath</name>
    </author>
    <author>
      <name>Paul E. Smaldino</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1371/journal.pone.0136088</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1371/journal.pone.0136088" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">PLOS ONE 10(8): e0136088 (2015)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1503.02780v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.02780v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62A01, 91D10, 92D25" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.01950v1</id>
    <updated>2015-04-08T13:17:43Z</updated>
    <published>2015-04-08T13:17:43Z</published>
    <title>Le Her and Other Problems in Probability Discussed by Bernoulli,
  Montmort and Waldegrave</title>
    <summary>  Part V of the second edition of Pierre R\'{e}mond de Montmort's Essay
d'analyse sur les jeux de hazard published in 1713 contains correspondence on
probability problems between Montmort and Nicolaus Bernoulli. This
correspondence begins in 1710. The last published letter, dated November 15,
1713, is from Montmort to Nicolaus Bernoulli. There is some discussion of the
strategy of play in the card game Le Her and a bit of news that Montmort's
friend Waldegrave in Paris was going to take care of the printing of the book.
From earlier correspondence between Bernoulli and Montmort, it is apparent that
Waldegrave had also analyzed Le Her and had come up with a mixed strategy as a
solution. He had also suggested working on the "problem of the pool," or what
is often called Waldegrave's problem. The Universit\"{a}tsbibliothek Basel
contains an additional forty-two letters between Bernoulli and Montmort written
after 1713, as well as two letters between Bernoulli and Waldegrave. The
letters are all in French, and here we provide translations of key passages.
The trio continued to discuss probability problems, particularly Le Her which
was still under discussion when the Essay d'analyse went to print. We describe
the probability content of this body of correspondence and put it in its
historical context. We also provide a proper identification of Waldegrave based
on manuscripts in the Archives nationales de France in Paris.
</summary>
    <author>
      <name>David R. Bellhouse</name>
    </author>
    <author>
      <name>Nicolas Fillion</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1214/14-STS469</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1214/14-STS469" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at http://dx.doi.org/10.1214/14-STS469 in the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Statistical Science 2015, Vol. 30, No. 1, 26-39</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1504.01950v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.01950v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.02129v1</id>
    <updated>2015-04-08T21:01:53Z</updated>
    <published>2015-04-08T21:01:53Z</published>
    <title>InSilicoVA: A Method to Automate Cause of Death Assignment for Verbal
  Autopsy</title>
    <summary>  Verbal autopsies (VA) are widely used to provide cause-specific mortality
estimates in developing world settings where vital registration does not
function well. VAs assign cause(s) to a death by using information describing
the events leading up to the death, provided by care givers. Typically
physicians read VA interviews and assign causes using their expert knowledge.
Physician coding is often slow, and individual physicians bring bias to the
coding process that results in non-comparable cause assignments. These problems
significantly limit the utility of physician-coded VAs. A solution to both is
to use an algorithmic approach that formalizes the cause-assignment process.
This ensures that assigned causes are comparable and requires many fewer
person-hours so that cause assignment can be conducted quickly without
disrupting the normal work of physicians. Peter Byass' InterVA method is the
most widely used algorithmic approach to VA coding and is aligned with the WHO
2012 standard VA questionnaire.
  The statistical model underpinning InterVA can be improved; uncertainty needs
to be quantified, and the link between the population-level CSMFs and the
individual-level cause assignments needs to be statistically rigorous.
Addressing these theoretical concerns provides an opportunity to create new
software using modern languages that can run on multiple platforms and will be
widely shared. Building on the overall framework pioneered by InterVA, our work
creates a statistical model for automated VA cause assignment.
</summary>
    <author>
      <name>Samuel J. Clark</name>
    </author>
    <author>
      <name>Tyler McCormick</name>
    </author>
    <author>
      <name>Zehang Li</name>
    </author>
    <author>
      <name>Jon Wakefield</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 4 figures, Center for Statistics and the Social Sciences
  (CSSS), University of Washington Working Paper No. 133</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.02129v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.02129v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.03089v1</id>
    <updated>2015-04-13T07:59:59Z</updated>
    <published>2015-04-13T07:59:59Z</published>
    <title>A Conversation with Nancy Flournoy</title>
    <summary>  Nancy Flournoy was born in Long Beach, California, on May 4, 1947. After
graduating from Polytechnic School in Pasadena in 1965, she earned a B.S.
(1969) and M.S. (1971) in biostatistics from UCLA. Between her bachelors and
masters degrees, she worked as a Statistician I for Regional Medical Programs
at UCLA. After receiving her master's degree, she spend three years at the
Southwest Laboratory for Education Research and Development in Seal Beach,
California. Flournoy joined the Seattle team pioneering bone marrow
transplantation in 1973. She moved with the transplant team into the newly
formed Fred Hutchinson Cancer Research Center in 1975 as Director of Clinical
Statistics, where she supervised a group responsible for the design and
analysis of about 80 simultaneous clinical trials. To support the Clinical
Division, she supervised the development of an interdisciplinary shared data
software system. She recruited Leonard B. Hearne to create this database
management system in 1975 (and married him in 1978). While at the Cancer
Center, she was also at the University of Washington, where she received her
doctorate in biomathematics in 1982. She became the first female director of
the program in statistics at the National Science Foundation (NSF) in 1986. She
received service awards from the NSF in 1988 and the National Institute of
Statistical Science in 2006 for facilitating interdisciplinary research.
Flournoy joined the Department of Mathematics and Statistics at American
University in 1988. She moved as department chair to the University of Missouri
in 2002, where she became Curators' Distinguished Professor in 2012.
</summary>
    <author>
      <name>William F. Rosenberger</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1214/14-STS495</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1214/14-STS495" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at http://dx.doi.org/10.1214/14-STS495 in the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Statistical Science 2015, Vol. 30, No. 1, 133-146</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1504.03089v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.03089v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.04570v1</id>
    <updated>2015-06-15T12:40:00Z</updated>
    <published>2015-06-15T12:40:00Z</published>
    <title>The Two-envelope Problem: An Informed Choice</title>
    <summary>  The host of a game presents two indistinguishable envelopes to an agent. One
of the envelopes is randomly selected and allocated to the agent. The agent is
informed that the monetary content of one of the envelopes is twice that of the
other. The dilemma is under which conditions it would be beneficial to switch
the allocated envelope for the complementary one. The objective of his or her
envelope-switching strategy is to determine the benefit of switching the
allocated envelope and its content for the expected content of the
complementary envelope.
  The agent, upon revealing the content of the allocated envelope, must
consider the events that are likely to have taken place as a result of the
host's activities. The preceding approach is in stark contrast to considering
the agent's reasoning for a particular outcome that seeks to derive a strategy
based on the relative contents of the presented envelopes. However, it is the
former reasoning that seeks to identify what the initial amounts could have
been, as a result of the observed amount, that facilitates the identification
of an appropriate switching strategy.
  Knowledge of the content and allocation process is essential for the agent to
derive a successful switching strategy, as is the distribution function from
which the host sampled the initial amount that is assigned to the first
envelope.
  For every play of the game, once the agent is afforded the opportunity of
sighting the content of the randomly allocated envelope, he or she can
determine the expected benefit of switching.
</summary>
    <author>
      <name>Jeffrey Brian Tyler</name>
    </author>
    <link href="http://arxiv.org/abs/1506.04570v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.04570v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.00543v1</id>
    <updated>2015-08-03T19:30:14Z</updated>
    <published>2015-08-03T19:30:14Z</published>
    <title>Combating anti-statistical thinking using simulation-based methods
  throughout the undergraduate curriculum</title>
    <summary>  The use of simulation-based methods for introducing inference is growing in
popularity for the Stat 101 course, due in part to increasing evidence of the
methods ability to improve students' statistical thinking. This impact comes
from simulation-based methods (a) clearly presenting the overarching logic of
inference, (b) strengthening ties between statistics and probability or
mathematical concepts, (c) encouraging a focus on the entire research process,
(d) facilitating student thinking about advanced statistical concepts, (e)
allowing more time to explore, do, and talk about real research and messy data,
and (f) acting as a firmer foundation on which to build statistical intuition.
Thus, we argue that simulation-based inference should be an entry point to an
undergraduate statistics program for all students, and that simulation-based
inference should be used throughout all undergraduate statistics courses. In
order to achieve this goal and fully recognize the benefits of simulation-based
inference on the undergraduate statistics program we will need to break free of
historical forces tying undergraduate statistics curricula to mathematics,
consider radical and innovative new pedagogical approaches in our courses,
fully implement assessment-driven content innovations, and embrace computation
throughout the curriculum.
</summary>
    <author>
      <name>Nathan Tintle</name>
    </author>
    <author>
      <name>Beth Chance</name>
    </author>
    <author>
      <name>George Cobb</name>
    </author>
    <author>
      <name>Soma Roy</name>
    </author>
    <author>
      <name>Todd Swanson</name>
    </author>
    <author>
      <name>Jill VanderStoep</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published in "The American Statistician"</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.00543v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.00543v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.03533v1</id>
    <updated>2015-12-11T05:53:11Z</updated>
    <published>2015-12-11T05:53:11Z</published>
    <title>A Conversation with Nan Laird</title>
    <summary>  Nan McKenzie Laird is the Harvey V. Fineberg Professor of Biostatistics at
the Harvard T. H. Chan School of Public Health. She has made fundamental
contributions to statistical methods for longitudinal data analysis, missing
data and meta-analysis. In addition, she is widely known for her work in
statistical genetics and in statistical methods for psychiatric epidemiology.
Her 1977 paper with Dempster and Rubin on the EM algorithm is among the top 100
most highly cited papers in science [Nature 524 (2014) 550-553]. Her applied
work on medical practice errors is widely cited among the medical malpractice
community. Nan was born in Gainesville, Florida, in 1943. Shortly thereafter,
her parents Angus McKenzie Laird and Myra Adelia Doyle, moved to Tallahassee,
Florida, with Nan and her sister Victoria Mell. Nan started college at Rice
University in 1961, but then transferred to the University of Georgia where she
received a B.S. in Statistics in 1969 and was elected to Phi Beta Kappa. After
graduation Nan worked at the Massachusetts Institute of Technology Draper
Laboratories where she worked on Kalman filtering for the Apollo Man to the
Moon Program. She enrolled in the Statistics Department at Harvard University
in 1971 and received her Ph.D. in 1975. She joined the faculty of Harvard
School of Public Health upon receiving her Ph.D., and remains there as research
professor, after her retirement in 2015. The interview was conducted in Boston,
Massachusetts, in July 2014. A link to Nan's full CV can be found at
\surlwww.hsph.harvard.edu/nan-laird/.
</summary>
    <author>
      <name>Louise Ryan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1214/15-STS528</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1214/15-STS528" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at http://dx.doi.org/10.1214/15-STS528 in the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org), Report No IMS-STS-STS528</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Statistical Science 2015, Vol. 30, No. 4, 582-596</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1512.03533v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.03533v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.02373v1</id>
    <updated>2016-03-08T03:59:53Z</updated>
    <published>2016-03-08T03:59:53Z</published>
    <title>On point processes in multitarget tracking</title>
    <summary>  The finite-set statistics (FISST) approach to multitarget tracking was
introduced in the mid-1990s. Its current extended form dates from 2001. In
2008, an "elementary" alternative to FISST was proposed, based on "finite point
processes" rather than RFS's. This was accompanied by single-sensor and
multisensor versions of a claimed generalization of the PHD filter, the
"iFilter." Then in 2013 in the Journal of Advances in Information Fusion (JAIF)
and elsewhere, the same author went on to claim that the FISST
p.g.fl./functional derivative approach is actually "due to" (a "corollary" of)
a 50-year-old pure-mathematics paper by Moyal; and described a "point process"
p.g.fl./functional derivative approach to multitarget tracking supposedly based
on it. In this paper it is shown that: (1)non-RFS point processes are a
phenomenologically erroneous foundation for multitarget tracking; (2) nearly
every equation, concept, discussion, derivation, and methodology in the JAIF
paper originally appeared in FISST publications, without being so attributed;
(3) FISST cannot possibly be "due to Moyal"; (4) the "point process" approach
described in JAIF differs from FISST only in regard to terminology and
notation, and thus in this sense appears to be an obscured, phenomenologically
erroneous, and improperly attributed copy of FISST. It is also shown that the
derivations of the single-sensor and multisensory iFilter appear to have had
major errors, as did a subsequent recasting of the multisensor iFilter as a
"traffic mapping filter."
</summary>
    <author>
      <name>Ronald Mahler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 0 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.02373v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.02373v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.01455v2</id>
    <updated>2016-07-06T13:50:00Z</updated>
    <published>2016-04-06T00:31:26Z</published>
    <title>Picking Winners Using Integer Programming</title>
    <summary>  We consider the problem of selecting a portfolio of entries of fixed
cardinality for a winner take all contest such that the probability of at least
one entry winning is maximized. This framework is very general and can be used
to model a variety of problems, such as movie studios selecting movies to
produce, drug companies choosing drugs to develop, or venture capital firms
picking start-up companies in which to invest. We model this as a combinatorial
optimization problem with a submodular objective function, which is the
probability of winning. We then show that the objective function can be
approximated using only pairwise marginal probabilities of the entries winning
when there is a certain structure on their joint distribution. We consider a
model where the entries are jointly Gaussian random variables and present a
closed form approximation to the objective function. We then consider a model
where the entries are given by sums of constrained resources and present a
greedy integer programming formulation to construct the entries. Our
formulation uses three principles to construct entries: maximize the expected
score of an entry, lower bound its variance, and upper bound its correlation
with previously constructed entries. To demonstrate the effectiveness of our
greedy integer programming formulation, we apply it to daily fantasy sports
contests that have top heavy payoff structures (i.e. most of the winnings go to
the top ranked entries). We find that the entries produced by our approach
perform well in practice and are even able to come in first place in contests
with thousands of entries. Our approach can easily be extended to other
problems with a winner take all type of payoff structure.
</summary>
    <author>
      <name>David Scott Hunter</name>
    </author>
    <author>
      <name>Juan Pablo Vielma</name>
    </author>
    <author>
      <name>Tauhid Zaman</name>
    </author>
    <link href="http://arxiv.org/abs/1604.01455v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.01455v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.05598v2</id>
    <updated>2016-07-29T19:43:56Z</updated>
    <published>2016-06-17T17:32:41Z</published>
    <title>Identifiability and testability in GRT with Individual Differences</title>
    <summary>  Silbert and Thomas (2013) showed that failures of decisional separability are
not, in general, identifiable in fully parameterized $2 \times 2$ Gaussian GRT
models. A recent extension of $2 \times 2$ GRT models (GRTwIND) was developed
to solve this problem and a conceptually similar problem with the simultaneous
identifiability of means and marginal variances in GRT models. Central to the
ability of GRTwIND to solve these problems is the assumption of universal
perception, which consists of shared perceptual distributions modified by
attentional and global scaling parameters (Soto et al., 2015). If universal
perception is valid, GRTwIND solves both issues. In this paper, we show that
GRTwIND with universal perception and subject-specific failures of decisional
separability is mathematically, and thereby empirically, equivalent to a model
with decisional separability and failure of universal perception. We then
provide a formal proof of the fact that means and marginal variances are not,
in general, simultaneously identifiable in $2 \times 2$ GRT models, including
GRTwIND. These results can be taken to delineate precisely what the assumption
of universal perception must consist of. Based on these results and related
recent mathematical developments in the GRT framework, we propose that, in
addition to requiring a fixed subset of parameters to determine the location
and scale of any given GRT model, some subset of parameters must be set in GRT
models to fix the orthogonality of the modeled perceptual dimensions, a central
conceptual underpinning of the GRT framework. We conclude with a discussion of
perceptual primacy and its relationship to universal perception.
</summary>
    <author>
      <name>Noah H. Silbert</name>
    </author>
    <author>
      <name>Robin D. Thomas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 5 figures, under review at the Journal of Mathematical
  Psychology</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.05598v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.05598v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.09017v1</id>
    <updated>2016-06-29T09:27:40Z</updated>
    <published>2016-06-29T09:27:40Z</published>
    <title>Consider avoiding the .05 significance level</title>
    <summary>  It is suggested that some shortcomings of Null Hypothesis Significance
Testing (NHST), viewed from the perspective of Bayesian statistics, turn benign
once the traditional threshold p value of .05 is substituted by a sufficiently
smaller value. To illustrate, the posterior probability of H0 stating P=.5,
given data that just render it rejected by NHST with a p value of .05 (and a
uniform prior), is shown here to be not much smaller than .50 for most values
of N below 100 (and even exceeds .50 for N&gt;=100); in contrast, with a p value
of .001 posterior probability does not exceed .06 for N&lt;=100 (neither .25 for
N&lt;9000). Yet more interesting, posterior probability becomes quite independent
of N with a p value of .0001, hence practically satisfying the alpha postulate
- set by Cornfield (1966) as the condition for p value being a measure of
evidence in itself. In view of the low prospect that most researchers will soon
convert to use Bayesian statistics in any form, we thus suggest that
researchers who elect the conservative option of resorting to NHST be
encouraged to avoid as much as possible using a p value of .05 as a threshold
for rejecting H0. The analysis presented here may be used to discuss afresh
which level of threshold p value seems to be a reasonable, practical
substitute.
</summary>
    <author>
      <name>David Navon</name>
    </author>
    <author>
      <name>Yoav Cohen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.09017v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.09017v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.04209v1</id>
    <updated>2016-07-14T17:03:24Z</updated>
    <published>2016-07-14T17:03:24Z</published>
    <title>Dynamic Question Ordering in Online Surveys</title>
    <summary>  Online surveys have the potential to support adaptive questions, where later
questions depend on earlier responses. Past work has taken a rule-based
approach, uniformly across all respondents. We envision a richer interpretation
of adaptive questions, which we call dynamic question ordering (DQO), where
question order is personalized. Such an approach could increase engagement, and
therefore response rate, as well as imputation quality. We present a DQO
framework to improve survey completion and imputation. In the general
survey-taking setting, we want to maximize survey completion, and so we focus
on ordering questions to engage the respondent and collect hopefully all
information, or at least the information that most characterizes the
respondent, for accurate imputations. In another scenario, our goal is to
provide a personalized prediction. Since it is possible to give reasonable
predictions with only a subset of questions, we are not concerned with
motivating users to answer all questions. Instead, we want to order questions
to get information that reduces prediction uncertainty, while not being too
burdensome. We illustrate this framework with an example of providing energy
estimates to prospective tenants. We also discuss DQO for national surveys and
consider connections between our statistics-based question-ordering approach
and cognitive survey methodology.
</summary>
    <author>
      <name>Kirstin Early</name>
    </author>
    <author>
      <name>Jennifer Mankoff</name>
    </author>
    <author>
      <name>Stephen E. Fienberg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In submission to the Journal of Official Statistics</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.04209v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.04209v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.04478v3</id>
    <updated>2017-04-13T19:18:38Z</updated>
    <published>2016-09-15T00:15:02Z</published>
    <title>Sterrett Procedure for the Generalized Group Testing Problem</title>
    <summary>  Group testing is a useful method that has broad applications in medicine,
engineering, and even in airport security control. Consider a finite population
of $N$ items, where item $i$ has a probability $p_i$ to be defective. The goal
is to identify all items by means of group testing. This is the generalized
group testing problem. The optimum procedure, with respect to the expected
total number of tests, is unknown even in case when all $p_i$ are equal.
\cite{H1975} proved that an ordered partition (with respect to $p_i$) is the
optimal for the Dorfman procedure (procedure $D$), and obtained an optimum
solution (i.e., found an optimal partition) by dynamic programming. In this
paper, we investigate the Sterrett procedure (procedure $S$). We provide close
form expression for the expected total number of tests, which allows us to find
the optimum arrangement of the items in the particular group. We also show that
an ordered partition is not optimal for the procedure $S$ or even for a
slightly modified Dorfman procedure (procedure $D^{\prime}$). This discovery
implies that finding an optimal procedure $S$ appears to be a hard
computational problem. However, by using an optimal ordered partition for all
procedures, we show that procedure $D^{\prime}$ is uniformly better than
procedure $D$, and based on numerical comparisons, procedure $S$ is uniformly
and significantly better than procedures $D$ and $D^{\prime}$.
</summary>
    <author>
      <name>Yaakov Malinovsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted for publication. Revised</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.04478v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.04478v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.07140v2</id>
    <updated>2017-05-15T11:28:19Z</updated>
    <published>2016-12-21T14:32:35Z</published>
    <title>A Guide to Teaching Data Science</title>
    <summary>  Demand for data science education is surging and traditional courses offered
by statistics departments are not meeting the needs of those seeking training.
This has led to a number of opinion pieces advocating for an update to the
Statistics curriculum. The unifying recommendation is computing should play a
more prominent role. We strongly agree with this recommendation, but advocate
the main priority is to bring applications to the forefront as proposed by
Nolan and Speed (1999). We also argue that the individuals tasked with
developing data science courses should not only have statistical training, but
also have experience analyzing data with the main objective of solving
real-world problems. Here, we share a set of general principles and offer a
detailed guide derived from our successful experience developing and teaching a
graduate-level, introductory data science course centered entirely on case
studies. We argue for the importance of statistical thinking, as defined by
Wild and Pfannkuck (1999) and describe how our approach teaches students three
key skills needed to succeed in data science, which we refer to as creating,
connecting, and computing. This guide can also be used for statisticians
wanting to gain more practical knowledge about data science before embarking on
teaching an introductory course.
</summary>
    <author>
      <name>Stephanie C. Hicks</name>
    </author>
    <author>
      <name>Rafael A. Irizarry</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 tables, 3 figures, 2 supplemental figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.07140v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.07140v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.02383v1</id>
    <updated>2017-01-09T22:58:05Z</updated>
    <published>2017-01-09T22:58:05Z</published>
    <title>SPEW: Synthetic Populations and Ecosystems of the World</title>
    <summary>  Agent-based models (ABMs) simulate interactions between autonomous agents in
constrained environments over time. ABMs are often used for modeling the spread
of infectious diseases. In order to simulate disease outbreaks or other
phenomena, ABMs rely on "synthetic ecosystems," or information about agents and
their environments that is representative of the real world. Previous
approaches for generating synthetic ecosystems have some limitations: they are
not open-source, cannot be adapted to new or updated input data sources, and do
not allow for alternative methods for sampling agent characteristics and
locations. We introduce a general framework for generating Synthetic
Populations and Ecosystems of the World (SPEW), implemented as an open-source R
package. SPEW allows researchers to choose from a variety of sampling methods
for agent characteristics and locations when generating synthetic ecosystems
for any geographic region. SPEW can produce synthetic ecosystems for any agent
(e.g. humans, mosquitoes, etc), provided that appropriate data is available. We
analyze the accuracy and computational efficiency of SPEW given different
sampling methods for agent characteristics and locations and provide a suite of
diagnostics to screen our synthetic ecosystems. SPEW has generated over five
billion human agents across approximately 100,000 geographic regions in about
70 countries, available online.
</summary>
    <author>
      <name>Shannon Gallagher</name>
    </author>
    <author>
      <name>Lee Richardson</name>
    </author>
    <author>
      <name>Samuel L. Ventura</name>
    </author>
    <author>
      <name>William F. Eddy</name>
    </author>
    <link href="http://arxiv.org/abs/1701.02383v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.02383v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.08290v1</id>
    <updated>2017-01-28T13:37:56Z</updated>
    <published>2017-01-28T13:37:56Z</published>
    <title>HyperTools: A Python toolbox for visualizing and manipulating
  high-dimensional data</title>
    <summary>  Data visualizations can reveal trends and patterns that are not otherwise
obvious from the raw data or summary statistics. While visualizing
low-dimensional data is relatively straightforward (for example, plotting the
change in a variable over time as (x,y) coordinates on a graph), it is not
always obvious how to visualize high-dimensional datasets in a similarly
intuitive way. Here we present HypeTools, a Python toolbox for visualizing and
manipulating large, high-dimensional datasets. Our primary approach is to use
dimensionality reduction techniques (Pearson, 1901; Tipping &amp; Bishop, 1999) to
embed high-dimensional datasets in a lower-dimensional space, and plot the data
using a simple (yet powerful) API with many options for data manipulation [e.g.
hyperalignment (Haxby et al., 2011), clustering, normalizing, etc.] and plot
styling. The toolbox is designed around the notion of data trajectories and
point clouds. Just as the position of an object moving through space can be
visualized as a 3D trajectory, HyperTools uses dimensionality reduction
algorithms to create similar 2D and 3D trajectories for time series of
high-dimensional observations. The trajectories may be plotted as interactive
static plots or visualized as animations. These same dimensionality reduction
and alignment algorithms can also reveal structure in static datasets (e.g.
collections of observations or attributes). We present several examples
showcasing how using our toolbox to explore data through trajectories and
low-dimensional embeddings can reveal deep insights into datasets across a wide
variety of domains.
</summary>
    <author>
      <name>Andrew C. Heusser</name>
    </author>
    <author>
      <name>Kirsten Ziman</name>
    </author>
    <author>
      <name>Lucy L. W. Owen</name>
    </author>
    <author>
      <name>Jeremy R. Manning</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 11 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.08290v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.08290v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.08438v1</id>
    <updated>2017-01-29T21:52:57Z</updated>
    <published>2017-01-29T21:52:57Z</published>
    <title>Using a "Study of Studies" to help statistics students assess research
  findings</title>
    <summary>  One learning goal of the introductory statistics course is to develop the
ability to make sense of research findings in published papers. The Atlantic
magazine regularly publishes a feature called "Study of Studies" that
summarizes multiple articles published in a particular domain. We describe a
classroom activity to develop this capacity using the "Study of Studies." In
this activity, students read capsule summaries of twelve research papers
related to restaurants and dining that was published in April 2015. The
selected papers report on topics such as how seating arrangement, server
posture, plate color and size, and the use of background music relate to
revenue, ambiance, and perceived food quality. The students are assigned one of
the twelve papers to read and critique as part of a small group. Their group
critiques are shared with the class and the instructor.
  A pilot study was conducted during the 2015-2016 academic year at Amherst
College. Students noted that key details were not included in the published
summary. They were generally skeptical of the published conclusions. The
students often provided additional summarization of information from the
journal articles that better describe the results. By independently assessing
and comparing the original study conclusions with the capsule summary in the
"Study of Studies," students can practice developing judgment and assessing the
validity of statistical results.
</summary>
    <author>
      <name>Azka Javaid</name>
    </author>
    <author>
      <name>Xiaofei Wang</name>
    </author>
    <author>
      <name>Nicholas J Horton</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in press, CHANCE</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.08438v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.08438v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62.01" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.07074v1</id>
    <updated>2017-02-23T02:39:59Z</updated>
    <published>2017-02-23T02:39:59Z</published>
    <title>Social Big Data Analytics of Consumer Choices: A Two Sided Online
  Platform Perspective</title>
    <summary>  This dissertation examines three distinct big data analytics problems related
to the social aspects of consumers' choices. The main goal of this line of
research is to help two sided platform firms to target their marketing policies
given the great heterogeneity among their customers. In three essays, I
combined structural modeling and machine learning approaches to first
understand customers' responses to intrinsic and extrinsic factors, using
unique data sets I scraped from the web, and then explore methods to optimize
two sided platforms' firms' reactions accordingly. The first essay examines
"social learning" in the mobile app store context, controlling for intrinsic
value of hedonic and utilitarian mobile apps, price, advertising, and number of
options available. The second essay investigates bidders' anticipated winner
and loser regret in the context of the eBay online auction platform. Using a
large data set from eBay and empirical Bayesian estimation method, I quantify
the bidders' anticipation of regret in various product categories, and
investigate the role of experience in explaining the bidders' regret and
learning behaviors. The third essay investigates the effects of Gamification
incentive mechanisms in an online platform for user generated content. I use an
ensemble method over LDA, mixed normal and k-mean clustering methods to segment
users into competitors, collaborators, achievers, explorers and uninterested
users. These findings help the Gamification platform to target its users. The
simulation counterfactual analysis suggests that a two sided platform can
increase the number of user contributions, by making earning badges more
difficult.
</summary>
    <author>
      <name>Meisam Hejazi Nia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PhD thesis, management science, UT Dallas, Aug 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.07074v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.07074v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.01171v1</id>
    <updated>2017-04-04T20:14:00Z</updated>
    <published>2017-04-04T20:14:00Z</published>
    <title>Rethinking probabilistic prediction in the wake of the 2016 U.S.
  presidential election</title>
    <summary>  To many statisticians and citizens, the outcome of the most recent U.S.
presidential election represents a failure of data-driven methods on the
grandest scale. This impression has led to much debate and discussion about how
the election predictions went awry -- Were the polls inaccurate? Were the
models wrong? Did we misinterpret the probabilities? -- and how they went right
-- Perhaps the analyses were correct even though the predictions were wrong,
that's just the nature of probabilistic forecasting. With this in mind, we
analyze the election outcome with respect to a core set of effectiveness
principles. Regardless of whether and how the election predictions were right
or wrong, we argue that they were ineffective in conveying the extent to which
the data was informative of the outcome and the level of uncertainty in making
these assessments. Among other things, our analysis sheds light on the
shortcomings of the classical interpretations of probability and its
communication to consumers in the form of predictions. We present here an
alternative approach, based on a notion of validity, which offers two immediate
insights for predictive inference. First, the predictions are more
conservative, arguably more realistic, and come with certain guarantees on the
probability of an erroneous prediction. Second, our approach easily and
naturally reflects the (possibly substantial) uncertainty about the model by
outputting plausibilities instead of probabilities. Had these simple steps been
taken by the popular prediction outlets, the election outcome may not have been
so shocking.
</summary>
    <author>
      <name>Harry Crane</name>
    </author>
    <author>
      <name>Ryan Martin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages; 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.01171v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.01171v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.06332v1</id>
    <updated>2017-05-17T20:14:18Z</updated>
    <published>2017-05-17T20:14:18Z</published>
    <title>Can rational choice guide us to correct {\em de se} beliefs?</title>
    <summary>  Significant controversy remains about what constitute correct self-locating
beliefs in scenarios such as the Sleeping Beauty problem, with proponents on
both the "halfer" and "thirder" sides. To attempt to settle the issue, one
natural approach consists in creating decision variants of the problem,
determining what actions the various candidate beliefs prescribe, and assessing
whether these actions are reasonable when we step back. Dutch book arguments
are a special case of this approach, but other Sleeping Beauty games have also
been constructed to make similar points. Building on a recent article (James
R.~Shaw. {\em De se} belief and rational choice. {\em Synthese},
190(3):491-508, 2013), I show that in general we should be wary of such
arguments, because unintuitive actions may result for reasons that are
unrelated to the beliefs. On the other hand, I show that, when we restrict our
attention to {\em additive} games, then a thirder will necessarily maximize her
{\em ex ante} expected payout, but a halfer in some cases will not (assuming
causal decision theory). I conclude that this does not necessarily settle the
issue and speculate about what might.
</summary>
    <author>
      <name>Vincent Conitzer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11229-015-0737-x</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11229-015-0737-x" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper appears in {\em Synthese}, Volume 192, Issue 12, pp.
  4107-4119, December 2015. The final publication is available at Springer via
  http://dx.doi.org/10.1007/s11229-015-0737-x</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">{\em Synthese}, Volume 192, Issue 12, pp. 4107-4119, December 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1705.06332v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.06332v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.08702v1</id>
    <updated>2017-06-27T07:58:22Z</updated>
    <published>2017-06-27T07:58:22Z</published>
    <title>A network flow approach to visualising the roles of covariates in random
  forests</title>
    <summary>  We propose novel applications of parallel coordinates plots and Sankey
diagrams to represent the hierarchies of interacting covariate effects in
random forests. Each visualisation summarises the frequencies of all of the
paths through all of the trees in a random forest. Visualisations of the roles
of covariates in random forests include: ranked bar or dot charts depicting
scalar metrics of the contributions of individual covariates to the predictive
accuracy of the random forest; line graphs depicting various summaries of the
effect of varying a particular covariate on the predictions from the random
forest; heatmaps of metrics of the strengths of interactions between all pairs
of covariates; and parallel coordinates plots for each response class depicting
the distributions of the values of all covariates among the observations most
representative of those predicted to belong that class. Together these
visualisations facilitate substantial insights into the roles of covariates in
a random forest but do not communicate the frequencies of the hierarchies of
covariates effects across the random forest or the orders in which covariates
occur in these hierarchies. Our visualisations address these gaps. We
demonstrate our visualisations using a random forest fitted to publicly
available data and provide a software implementation in the form of an R
package.
</summary>
    <author>
      <name>Benjamin R. Fitzpatrick</name>
    </author>
    <author>
      <name>Kerrie Mengersen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 4 Figures, 3 Supplementary Materials, Link to GitHub
  Repository for Software Implementation</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.08702v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.08702v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.02070v1</id>
    <updated>2017-07-07T08:16:52Z</updated>
    <published>2017-07-07T08:16:52Z</published>
    <title>Coherent combination of probabilistic outputs for group decision making:
  an algebraic approach</title>
    <summary>  Current decision support systems address domains that are heterogeneous in
nature and becoming progressively larger. Such systems often require the input
of expert judgement about a variety of different fields and an intensive
computational power to produce the scores necessary to rank the available
policies. Recently, integrating decision support systems have been introduced
to enable a formal Bayesian multi-agent decision analysis to be distributed and
consequently efficient. In such systems, where different panels of experts
oversee disjoint but correlated vectors of variables, each expert group needs
to deliver only certain summaries of the variables under their jurisdiction to
properly derive an overall score for the available policies. Here we present an
algebraic approach that makes this methodology feasible for a wide range of
modelling contexts and that enables us to identify the summaries needed for
such a combination of judgements. We are also able to demonstrate that
coherence, in a sense we formalize here, is still guaranteed when panels only
share a partial specification of their model with other panel members. We
illustrate this algebraic approach by applying it to a specific class of
Bayesian networks and demonstrate how we can use it to derive closed form
formulae for the computations of the joint moments of variables that determine
the score of different policies.
</summary>
    <author>
      <name>Manuele Leonelli</name>
    </author>
    <author>
      <name>Eva Riccomagno</name>
    </author>
    <author>
      <name>Jim Q. Smith</name>
    </author>
    <link href="http://arxiv.org/abs/1707.02070v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.02070v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.09344v1</id>
    <updated>2017-08-28T18:29:52Z</updated>
    <published>2017-08-28T18:29:52Z</published>
    <title>Stem-ming the Tide: Predicting STEM attrition using student transcript
  data</title>
    <summary>  Science, technology, engineering, and math (STEM) fields play growing roles
in national and international economies by driving innovation and generating
high salary jobs. Yet, the US is lagging behind other highly industrialized
nations in terms of STEM education and training. Furthermore, many economic
forecasts predict a rising shortage of domestic STEM-trained professions in the
US for years to come. One potential solution to this deficit is to decrease the
rates at which students leave STEM-related fields in higher education, as
currently over half of all students intending to graduate with a STEM degree
eventually attrite. However, little quantitative research at scale has looked
at causes of STEM attrition, let alone the use of machine learning to examine
how well this phenomenon can be predicted. In this paper, we detail our efforts
to model and predict dropout from STEM fields using one of the largest known
datasets used for research on students at a traditional campus setting. Our
results suggest that attrition from STEM fields can be accurately predicted
with data that is routinely collected at universities using only information on
students' first academic year. We also propose a method to model student STEM
intentions for each academic term to better understand the timing of STEM
attrition events. We believe these results show great promise in using machine
learning to improve STEM retention in traditional and non-traditional campus
settings.
</summary>
    <author>
      <name>Lovenoor Aulck</name>
    </author>
    <author>
      <name>Rohan Aras</name>
    </author>
    <author>
      <name>Lysia Li</name>
    </author>
    <author>
      <name>Coulter L'Heureux</name>
    </author>
    <author>
      <name>Peter Lu</name>
    </author>
    <author>
      <name>Jevin West</name>
    </author>
    <link href="http://arxiv.org/abs/1708.09344v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.09344v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ed-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.06400v1</id>
    <updated>2017-08-14T16:55:08Z</updated>
    <published>2017-08-14T16:55:08Z</published>
    <title>Distance Correlation: A New Tool for Detecting Association and Measuring
  Correlation Between Data Sets</title>
    <summary>  The difficulties of detecting association, measuring correlation, and
establishing cause and effect have fascinated mankind since time immemorial.
Democritus, the Greek philosopher, underscored well the importance and the
difficulty of proving causality when he wrote, "I would rather discover one
cause than gain the kingdom of Persia." To address the difficulties of relating
cause and effect, statisticians have developed many inferential techniques.
Perhaps the most well-known method stems from Karl Pearson's coefficient of
correlation, which Pearson introduced in the late 19th century based on ideas
of Francis Galton.
  I will describe in this lecture the recently-devised distance correlation
coefficient and describe its advantages over the Pearson and other classical
measures of correlation. We will examine an application of the distance
correlation coefficient to data drawn from large astrophysical databases, where
it is desired to classify galaxies according to various types. Further, the
lecture will analyze data arising in the ongoing national discussion of the
relationship between state-by-state homicide rates and the stringency of state
laws governing firearm ownership.
  The lecture will also describe a remarkable singular integral which lies at
the core of the theory of the distance correlation coefficient. We will see
that this singular integral admits generalizations to the truncated Maclaurin
expansions of the cosine function and to the theory of spherical functions on
symmetric cones.
</summary>
    <author>
      <name>Donald St. P. Richards</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages; 1 figure. This article is an expanded version of an
  announcement, published in the Notices of the American Mathematical Society,
  64 (2017), 16--18, of an invited lecture given at the 2017 Joint Mathematics
  Meeting</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Notices of the American Mathematical Society, 64 (2017), 16--18</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1709.06400v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.06400v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60E05, 62H20 (Primary), 33C05, 42C05, 60E10 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1011.1160v1</id>
    <updated>2010-11-04T13:52:00Z</updated>
    <published>2010-11-04T13:52:00Z</published>
    <title>A Conversation with James Hannan</title>
    <summary>  Jim Hannan is a professor who has lived an interesting life and one whose
fundamental research in repeated games was not fully appreciated until late in
his career. During his service as a meteorologist in the Army in World War II,
Jim played poker and made weather forecasts. It is curious that his later
research included strategies for repeated play that apply to selecting the best
forecaster. James Hannan was born in Holyoke, Massachusetts on September 14,
1922. He attended St. Jerome's High School and in January 1943 received the
Ph.B. from St. Michael's College in Colchester, Vermont. Jim enlisted in the US
Army Air Force to train and serve as a meteorologist. This took him to army
airbases in China by the close of the war. Following discharge from the army,
Jim studied mathematics at Harvard and graduated with the M.S. in June 1947. To
prepare for doctoral work in statistics at the University of North Carolina
that fall, Jim went to the University of Michigan in the summer of 1947. The
routine admissions' physical revealed a spot on the lung and the possibility of
tuberculosis. This caused Jim to stay at Ann Arbor through the fall of 1947 and
then at a Veterans Administration Hospital in Framingham, Massachusetts to have
his condition followed more closely. He was discharged from the hospital in the
spring and started his study at Chapel Hill in the fall of 1948. There he began
research in compound decision theory under Herbert Robbins. Feeling the need
for teaching experience, Jim left Chapel Hill after two years and short of
thesis to take a three year appointment as an instructor at Catholic University
in Washington, DC. When told that renewal was not coming, Jim felt pressure to
finish his degree.
</summary>
    <author>
      <name>Dennis Gilliland</name>
    </author>
    <author>
      <name>R. V. Ramamoorthi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1214/09-STS283</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1214/09-STS283" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in at http://dx.doi.org/10.1214/09-STS283 the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Statistical Science 2010, Vol. 25, No. 1, 126-144</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1011.1160v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.1160v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1011.6517v1</id>
    <updated>2010-11-30T11:44:26Z</updated>
    <published>2010-11-30T11:44:26Z</published>
    <title>A Conversation with Martin Bradbury Wilk</title>
    <summary>  Martin Bradbury Wilk was born on December 18, 1922, in Montr\'{e}al,
Qu\'{e}bec, Canada. He completed a B.Eng. degree in Chemical Engineering in
1945 at McGill University and worked as a Research Engineer on the Atomic
Energy Project for the National Research Council of Canada from 1945 to 1950.
He then went to Iowa State College, where he completed a M.Sc. and a Ph.D.
degree in Statistics in 1953 and 1955, respectively. After a one-year post-doc
with John Tukey, he became Assistant Director of the Statistical Techniques
Research Group at Princeton University in 1956--1957, and then served as
Professor and Director of Research in Statistics at Rutgers University from
1959 to 1963. In parallel, he also had a 14-year career at Bell Laboratories,
Murray Hill, New Jersey. From 1956 to 1969, he was in turn Member of Technical
Staff, Head of the Statistical Models and Methods Research Department, and
Statistical Director in Management Sciences Research. He wrote a number of
influential papers in statistical methodology during that period, notably
testing procedures for normality (the Shapiro--Wilk statistic) and probability
plotting techniques for multivariate data. In 1970, Martin moved into higher
management levels of the American Telephone and Telegraph (AT&amp;T) Company. He
occupied various positions culminating as Assistant Vice-President and Director
of Corporate Planning. In 1980, he returned to Canada and became the first
professional statistician to serve as Chief Statistician. His accomplishments
at Statistics Canada were numerous and contributed to a resurgence of the
institution's international standing. He played a crucial role in the
reinstatement of the Cabinet-cancelled 1986 Census.
</summary>
    <author>
      <name>Christian Genest</name>
    </author>
    <author>
      <name>Gordon Brackstone</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1214/08-STS272</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1214/08-STS272" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in at http://dx.doi.org/10.1214/08-STS272 the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Statistical Science 2010, Vol. 25, No. 2, 258-273</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1011.6517v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.6517v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1104.2434v1</id>
    <updated>2011-04-13T10:24:30Z</updated>
    <published>2011-04-13T10:24:30Z</published>
    <title>A Conversation with George G. Roussas</title>
    <summary>  George G. Roussas was born in the city of Marmara in central Greece, on June
29, 1933. He received a B.A. with high honors in Mathematics from the
University of Athens in 1956, and a Ph.D. in Statistics from the University of
California, Berkeley, in 1964. In 1964--1966, he served as Assistant Professor
of Mathematics at the California State University, San Jose, and he was a
faculty member of the Department of Statistics at the University of Wisconsin,
Madison, in 1966--1976, starting as an Assistant Professor in 1966, becoming a
Professor in 1972. He was a Professor of Applied Mathematics and Director of
the Laboratory of Applied Mathematics at the University of Patras, Greece, in
1972--1984. He was elected Dean of the School of Physical and Mathematical
Sciences at the University of Patras in 1978, and Chancellor of the university
in 1981. He served for about three years as Vice President-Academic Affairs of
the then new University of Crete, Greece, in 1981--1985. In 1984, he was a
Visiting Professor in the Intercollege Division of Statistics at the University
of California, Davis, and he was appointed Professor, Associate Dean and Chair
of the Graduate Group in Statistics in the same university in 1985; he served
in the two administrative capacities in 1985--1999. He is an elected member of
the International Statistical Institute since 1974, a Fellow of the Royal
Statistical Society since 1975, a Fellow of the Institute of Mathematical
Statistics since 1983, and a Fellow of the American Statistical Association
since 1986. He served as a member of the Council of the Hellenic Mathematical
Society, and as President of the Balkan Union of Mathematicians.
</summary>
    <author>
      <name>Debasis Bhattacharya</name>
    </author>
    <author>
      <name>Francisco J. Samaniego</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1214/09-STS299A</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1214/09-STS299A" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in at http://dx.doi.org/10.1214/09-STS299A the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Statistical Science 2010, Vol. 25, No. 4, 566-587</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1104.2434v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.2434v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.02124v2</id>
    <updated>2017-05-26T01:37:58Z</updated>
    <published>2015-04-08T20:47:12Z</published>
    <title>Hyak Mortality Monitoring System: Innovative Sampling and Estimation
  Methods - Proof of Concept by Simulation</title>
    <summary>  Traditionally health statistics are derived from civil and/or vital
registration. Civil registration in low-income countries varies from partial
coverage to essentially nothing at all. Consequently the state of the art for
public health information in low-income countries is efforts to combine or
triangulate data from different sources to produce a more complete picture
across both time and space - data amalgamation. Data sources amenable to this
approach include sample surveys, sample registration systems, health and
demographic surveillance systems, administrative records, census records,
health facility records and others.
  We propose a new statistical framework for gathering health and population
data - Hyak - that leverages the benefits of sampling and longitudinal,
prospective surveillance to create a cheap, accurate, sustainable monitoring
platform. Hyak has three fundamental components:
  1) Data Amalgamation: a sampling and surveillance component that organizes
two or more data collection systems to work together: a) data from HDSS with
frequent, intense, linked, prospective follow-up and b) data from sample
surveys conducted in large areas surrounding the Health and Demographic
Surveillance System sites using informed sampling so as to capture as many
events as possible;
  2) Cause of Death: verbal autopsy to characterize the distribution of deaths
by cause at the population level; and
  3) SES: measurement of socioeconomic status in order to characterize poverty
and wealth.
  We conduct a simulation study of the informed sampling component of Hyak
based on the Agincourt HDSS site in South Africa. Compared to traditional
cluster sampling, Hyak's informed sampling captures more deaths, and when
combined with an estimation model that includes spatial smoothing, produces
estimates mortality that have lower variance and small bias.
</summary>
    <author>
      <name>Samuel J. Clark</name>
    </author>
    <author>
      <name>Jon Wakefield</name>
    </author>
    <author>
      <name>Tyler McCormick</name>
    </author>
    <author>
      <name>Michelle Ross</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Updated version including new simulation study with two-stage cluster
  and optimum allocation sampling</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.02124v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.02124v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.03131v1</id>
    <updated>2015-04-13T11:14:57Z</updated>
    <published>2015-04-13T11:14:57Z</published>
    <title>A Conversation with Richard A. Olshen</title>
    <summary>  Richard Olshen was born in Portland, Oregon, on May 17, 1942. Richard spent
his early years in Chevy Chase, Maryland, but has lived most of his life in
California. He received an A.B. in Statistics at the University of California,
Berkeley, in 1963, and a Ph.D. in Statistics from Yale University in 1966,
writing his dissertation under the direction of Jimmie Savage and Frank
Anscombe. He served as Research Staff Statistician and Lecturer at Yale in
1966-1967. Richard accepted a faculty appointment at Stanford University in
1967, and has held tenured faculty positions at the University of Michigan
(1972-1975), the University of California, San Diego (1975-1989), and Stanford
University (since 1989). At Stanford, he is Professor of Health Research and
Policy (Biostatistics), Chief of the Division of Biostatistics (since 1998) and
Professor (by courtesy) of Electrical Engineering and of Statistics. At various
times, he has had visiting faculty positions at Columbia, Harvard, MIT,
Stanford and the Hebrew University. Richard's research interests are in
statistics and mathematics and their applications to medicine and biology. Much
of his work has concerned binary tree-structured algorithms for classification,
regression, survival analysis and clustering. Those for classification and
survival analysis have been used with success in computer-aided diagnosis and
prognosis, especially in cardiology, oncology and toxicology. He coauthored the
1984 book Classification and Regression Trees (with Leo Brieman, Jerome
Friedman and Charles Stone) which gives motivation, algorithms, various
examples and mathematical theory for what have come to be known as CART
algorithms. The approaches to tree-structured clustering have been applied to
problems in digital radiography (with Stanford EE Professor Robert Gray) and to
HIV genetics, the latter work including studies on single nucleotide
polymorphisms, which has helped to shed light on the presence of hypertension
in certain subpopulations of women.
</summary>
    <author>
      <name>John A. Rice</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1214/14-STS492</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1214/14-STS492" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at http://dx.doi.org/10.1214/14-STS492 in the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Statistical Science 2015, Vol. 30, No. 1, 118-132</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1504.03131v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.03131v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.08500v1</id>
    <updated>2015-07-29T09:05:14Z</updated>
    <published>2015-07-29T09:05:14Z</published>
    <title>A Conversation with Robert C. Elston</title>
    <summary>  Robert C. Elston was born on February 4, 1932, in London, England. He went to
Cambridge University to study natural science from 1952-1956 and obtained B.A.,
M.A. and Diploma in Agriculture (Dip Ag). He came to the US at age 24 to study
animal breeding at Cornell University and received his Ph.D. in 1959. From
1959-1960, he was a post-doctoral fellow in biostatistics at University of
North Carolina (UNC), Chapel Hill, where he studied mathematical statistics. He
then rose through the academic ranks in the department of biostatistics at UNC,
becoming a full professor in 1969. From 1979-1995, he was a professor and head
of the Department of Biometry and Genetics at Louisiana State University
Medical Center in New Orleans. In 1995, he moved to Case Western Reserve
University where he is a professor of epidemiology and biostatistics and served
as chairman from 2008 to 2014. Between 1966 and 2013, he directed 42 Ph.D.
students and mentored over 40 post-doctoral fellows. If one regards him as a
founder of a pedigree in research in genetic epidemiology, it was estimated in
2007 that there were more than 500 progeny. Among his many honors are a NIH
Research Career Development Award (1966-1976), the Leadership Award from
International Society of Human Genetics (1995), William Allan Award from
American Society of Human Genetics (1996), NIH MERIT Award (1998) and the
Marvin Zelen Leadership Award, Harvard University (2004). He is a Fellow of the
American Statistical Association and the Institute of Mathematical Statistics
as well as a Fellow of the Ohio Academy of Science. A leader in research in
genetic epidemiology for over 40 years, he has published over 600 research
articles in biostatistics, genetic epidemiology and applications. He has also
coauthored and edited 9 books in biostatistics, population genetics and methods
for the analysis of genetic data.
</summary>
    <author>
      <name>Gang Zheng</name>
    </author>
    <author>
      <name>Zhaohai Li</name>
    </author>
    <author>
      <name>Nancy L. Geller</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1214/14-STS497</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1214/14-STS497" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at http://dx.doi.org/10.1214/14-STS497 in the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Statistical Science 2015, Vol. 30, No. 2, 258-267</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1507.08500v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.08500v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.08502v1</id>
    <updated>2015-07-29T09:39:28Z</updated>
    <published>2015-07-29T09:39:28Z</published>
    <title>A Conversation with Jerry Friedman</title>
    <summary>  Jerome H. Friedman was born in Yreka, California, USA, on December 29, 1939.
He received his high school education at Yreka High School, then spent two
years at Chico State College before transferring to the University of
California at Berkeley in 1959. He completed an undergraduate degree in physics
in 1962 and a Ph.D. in high-energy particle physics in 1968 and was a
post-doctoral research physicist at the Lawrence Berkeley Laboratory during
1968-1972. In 1972, he moved to Stanford Linear Accelerator Center (SLAC) as
head of the Computation Research Group, retaining this position until 2006. In
1981, he was appointed half time as Professor in the Department of Statistics,
Stanford University, remaining half time with his SLAC appointment. He has held
visiting appointments at CSIRO in Sydney, CERN and the Department of Statistics
at Berkeley, and has had a very active career as a commercial consultant. Jerry
became Professor Emeritus in the Department of Statistics in 2007. Apart from
some 30 publications in high-energy physics early in his career, Jerry has
published over 70 research articles and books in statistics and computer
science, including co-authoring the pioneering books Classification and
Regression Trees and The Elements of Statistical Learning. Many of his
publications have hundreds if not thousands of citations (e.g., the CART book
has over 21,000). Much of his software is incorporated in commercial products,
including at least one popular search engine. Many of his methods and
algorithms are essential inclusions in modern statistical and data mining
packages. Honors include the following: the Rietz Lecture (1999) and the Wald
Lectures (2009); election to the American Academy of Arts and Sciences (2005)
and the US National Academy of Sciences (2010); a Fellow of the American
Statistical Association; Paper of the Year (JASA 1980, 1985; Technometrics
1998, 1992); Statistician of the Year (ASA, Chicago Chapter, 1999); ACM Data
Mining Lifetime Innovation Award (2002), Emanuel &amp; Carol Parzen Award for
Statistical Innovation (2004); Noether Senior Lecturer (American Statistical
Association, 2010); and the IEEE Computer Society Data Mining Research
Contribution Award (2012).
</summary>
    <author>
      <name>N. I. Fisher</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1214/14-STS509</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1214/14-STS509" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at http://dx.doi.org/10.1214/14-STS509 in the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Statistical Science 2015, Vol. 30, No. 2, 268-295</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1507.08502v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.08502v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.03056v1</id>
    <updated>2015-09-10T09:07:14Z</updated>
    <published>2015-09-10T09:07:14Z</published>
    <title>A Conversation with Professor Tadeusz Caliński</title>
    <summary>  Tadeusz Cali\'{n}ski was born in Pozna\'{n}, Poland in 1928. Despite the
absence of formal secondary eduction for Poles during the Second World War, he
entered the University of Pozna\'{n} in 1948, initially studying agronomy and
in later years mathematics. From 1953 to 1988 he taught statistics, biometry
and experimental design at the Agricultural University of Pozna\'{n}. During
this period he founded and developed the Pozna\'{n} inter-university school of
mathematical statistics and biometry, which has become one of the most
important schools of this type in Poland and beyond. He has supervised 24 Ph.D.
students, many of whom are currently professors at a variety of universities.
He is now Professor Emeritus. Among many awards, in 1995 Professor Cali\'{n}ski
received the Order of Polonia Restituta for his outstanding achievements in the
fields of Education and Science. In 2012 the Polish Statistical Society awarded
him The Jerzy Sp{\l}awa-Neyman Medal for his contribution to the development of
research in statistics in Poland. Professor Cali\'{n}ski in addition has
Doctoral Degrees honoris causa from the Agricultural University of Pozna\'{n}
and the Warsaw University of Life Sciences. His research interests include
mathematical statistics and biometry, with applications to agriculture, natural
sciences, biology and genetics. He has published over 140 articles in
scientific journals as well as, with Sanpei Kageyama, two important books on
the randomization approach to the design and analysis of experiments. He has
been extremely active and successful in initiating and contributing to fruitful
international research cooperation between Polish statisticians and
biometricians and their colleagues in various countries, particularly in the
Netherlands, France, Italy, Great Britain, Germany, Japan and Portugal. The
conversations in addition cover the history of biometry and experimental design
in Poland and the early influence of British statisticians.
</summary>
    <author>
      <name>Anthony C. Atkinson</name>
    </author>
    <author>
      <name>Barbara Bogacka</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1214/15-STS522</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1214/15-STS522" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at http://dx.doi.org/10.1214/15-STS522 in the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Statistical Science 2015, Vol. 30, No. 3, 423-442</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1509.03056v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.03056v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.03068v1</id>
    <updated>2015-09-10T09:34:03Z</updated>
    <published>2015-09-10T09:34:03Z</published>
    <title>A Conversation with Alan Gelfand</title>
    <summary>  Alan E. Gelfand was born April 17, 1945, in the Bronx, New York. He attended
public grade schools and did his undergraduate work at what was then called
City College of New York (CCNY, now CUNY), excelling at mathematics. He then
surprised and saddened his mother by going all the way across the country to
Stanford to graduate school, where he completed his dissertation in 1969 under
the direction of Professor Herbert Solomon, making him an academic grandson of
Herman Rubin and Harold Hotelling. Alan then accepted a faculty position at the
University of Connecticut (UConn) where he was promoted to tenured associate
professor in 1975 and to full professor in 1980. A few years later he became
interested in decision theory, then empirical Bayes, which eventually led to
the publication of Gelfand and Smith [J. Amer. Statist. Assoc. 85 (1990)
398-409], the paper that introduced the Gibbs sampler to most statisticians and
revolutionized Bayesian computing. In the mid-1990s, Alan's interests turned
strongly to spatial statistics, leading to fundamental contributions in
spatially-varying coefficient models, coregionalization, and spatial boundary
analysis (wombling). He spent 33 years on the faculty at UConn, retiring in
2002 to become the James B. Duke Professor of Statistics and Decision Sciences
at Duke University, serving as chair from 2007-2012. At Duke, he has continued
his work in spatial methodology while increasing his impact in the
environmental sciences. To date, he has published over 260 papers and 6 books;
he has also supervised 36 Ph.D. dissertations and 10 postdocs. This interview
was done just prior to a conference of his family, academic descendants, and
colleagues to celebrate his 70th birthday and his contributions to statistics
which took place on April 19-22, 2015 at Duke University.
</summary>
    <author>
      <name>Bradley P. Carlin</name>
    </author>
    <author>
      <name>Amy H. Herring</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1214/15-STS521</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1214/15-STS521" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at http://dx.doi.org/10.1214/15-STS521 in the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Statistical Science 2015, Vol. 30, No. 3, 413-422</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1509.03068v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.03068v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0807.1750v2</id>
    <updated>2012-07-13T13:21:09Z</updated>
    <published>2008-07-10T22:00:46Z</published>
    <title>A Markovian growth dynamics on rooted binary trees evolving according to
  the Gompertz curve</title>
    <summary>  Inspired by biological dynamics, we consider a growth Markov process taking
values on the space of rooted binary trees, similar to the Aldous-Shields
model. Fix $n\ge 1$ and $\beta&gt;0$. We start at time 0 with the tree composed of
a root only. At any time, each node with no descendants, independently from the
other nodes, produces two successors at rate $\beta(n-k)/n$, where $k$ is the
distance from the node to the root. Denote by $Z_n(t)$ the number of nodes with
no descendants at time $t$ and let $T_n = \beta^{-1} n \ln(n /\ln 4) + (\ln
2)/(2 \beta)$. We prove that $2^{-n} Z_n(T_n + n \tau)$, $\tau\in\bb R$,
converges to the Gompertz curve $\exp (- (\ln 2) e^{-\beta \tau})$. We also
prove a central limit theorem for the martingale associated to $Z_n(t)$.
</summary>
    <author>
      <name>C. Landim</name>
    </author>
    <author>
      <name>R. D. Portugal</name>
    </author>
    <author>
      <name>B. F. Svaiter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0807.1750v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0807.1750v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.CB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.CB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.MP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.5366v5</id>
    <updated>2012-06-09T05:26:24Z</updated>
    <published>2010-06-28T14:40:00Z</published>
    <title>"Not only defended but also applied": The perceived absurdity of
  Bayesian inference</title>
    <summary>  The missionary zeal of many Bayesians of old has been matched, in the other
direction, by a view among some theoreticians that Bayesian methods are
absurd-not merely misguided but obviously wrong in principle. We consider
several examples, beginning with Feller's classic text on probability theory
and continuing with more recent cases such as the perceived Bayesian nature of
the so-called doomsday argument. We analyze in this note the intellectual
background behind various misconceptions about Bayesian statistics, without
aiming at a complete historical coverage of the reasons for this dismissal.
</summary>
    <author>
      <name>Andrew Gelman</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Columbia University</arxiv:affiliation>
    </author>
    <author>
      <name>Christian P. Robert</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Universite Paris-Dauphine, IuF, and CREST</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, to appear in The American Statistician (with discussion)</arxiv:comment>
    <link href="http://arxiv.org/abs/1006.5366v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.5366v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.4217v2</id>
    <updated>2012-08-20T18:40:48Z</updated>
    <published>2010-09-21T21:07:03Z</published>
    <title>Measurement error and deconvolution in spaces of generalized functions</title>
    <summary>  This paper considers convolution equations that arise from problems such as
measurement error and non-parametric regression with errors in variables with
independence conditions. The equations are examined in spaces of generalized
functions to account for possible singularities; this makes it possible to
consider densities for arbitrary and not only absolutely continuous
distributions, and to operate with Fourier transforms for polynomially growing
regression functions. Results are derived for identification and well-posedness
in the topology of generalized functions for the deconvolution problem and for
some regression models. Conditions for consistency of plug-in estimation for
these models are derived.
</summary>
    <author>
      <name>Victoria Zinde-Walsh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is a revised version of the paper "Measurement Error and
  Deconvolution in Generalized Functions Spaces". The revision maintains the
  focus on the two types of equations, the convolution equation and the system
  of equations; this version does not include Theorems 6 and 7</arxiv:comment>
    <link href="http://arxiv.org/abs/1009.4217v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.4217v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="46F99, 62G99" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.2184v2</id>
    <updated>2011-11-06T16:42:19Z</updated>
    <published>2010-12-10T05:29:18Z</published>
    <title>Inherent Difficulties of Non-Bayesian Likelihood-based Inference, as
  Revealed by an Examination of a Recent Book by Aitkin</title>
    <summary>  For many decades, statisticians have made attempts to prepare the Bayesian
omelette without breaking the Bayesian eggs; that is, to obtain probabilistic
likelihood-based inferences without relying on informative prior distributions.
A recent example is Murray Aitkin's recent book, {\em Statistical Inference},
which presents an approach to statistical hypothesis testing based on
comparisons of posterior distributions of likelihoods under competing models.
Aitkin develops and illustrates his method using some simple examples of
inference from iid data and two-way tests of independence. We analyze in this
note some consequences of the inferential paradigm adopted therein, discussing
why the approach is incompatible with a Bayesian perspective and why we do not
find it relevant for applied work.
</summary>
    <author>
      <name>Andrew Gelman</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Columbia University</arxiv:affiliation>
    </author>
    <author>
      <name>Christian P. Robert</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Universite Paris-Dauphine, IUF, and CREST</arxiv:affiliation>
    </author>
    <author>
      <name>Judith Rousseau</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CREST-ENSAE, and Universite Paris-Dauphine</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 3 figures, revised version</arxiv:comment>
    <link href="http://arxiv.org/abs/1012.2184v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.2184v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1102.1128v2</id>
    <updated>2011-02-20T08:02:09Z</updated>
    <published>2011-02-06T06:01:34Z</published>
    <title>Simultaneous concentration of order statistics</title>
    <summary>  Let $\mu$ be a probability measure on $\mathbb{R}$ with cumulative
distribution function $F$, $(x_{i})_{1}^{n}$ a large i.i.d. sample from $\mu$,
and $F_{n}$ the associated empirical distribution function. The
Glivenko-Cantelli theorem states that with probability 1, $F_{n}$ converges
uniformly to $F$. In so doing it describes the macroscopic structure of
$\{x_{i}\}_{1}^{n}$, however it is insensitive to the position of individual
points. Indeed any subset of $o(n)$ points can be perturbed at will without
disturbing the convergence. We provide several refinements of the
Glivenko-Cantelli theorem which are sensitive not only to the global structure
of the sample but also to individual points. Our main result provides
conditions that guarantee simultaneous concentration of all order statistics.
The example of main interest is the normal distribution.
</summary>
    <author>
      <name>Daniel Fresen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1102.1128v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1102.1128v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G30, 60G55" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1102.2146v3</id>
    <updated>2012-10-25T18:48:26Z</updated>
    <published>2011-02-10T15:34:27Z</published>
    <title>Coexistence of cooperators and defectors in well mixed populations
  mediated by limiting resources</title>
    <summary>  Traditionally, resource limitation in evolutionary game theory is assumed
just to impose a constant population size. Here we show that resource
limitations may generate dynamical payoffs able to alter an original prisoner's
dilemma, and to allow for the stable coexistence between unconditional
cooperators and defectors in well-mixed populations. This is a consequence of a
self-organizing process that turns the interaction payoff matrix into
evolutionary neutral, and represents a resource-based control mechanism
preventing the spread of defectors. To our knowledge, this is the first example
of coexistence in well-mixed populations with a game structure different from a
snowdrift game.
</summary>
    <author>
      <name>Rubén J. Requejo</name>
    </author>
    <author>
      <name>Juan Camacho</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevLett.108.038701</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevLett.108.038701" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 7 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. Lett. 108, 038701 (2012)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1102.2146v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1102.2146v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1104.0061v1</id>
    <updated>2011-03-31T03:23:31Z</updated>
    <published>2011-03-31T03:23:31Z</published>
    <title>Böögg Bang drives global climate change</title>
    <summary>  The B\"o\"ogg is a large model of a snowman, constructed of inflammable
materials and filled with explosives. During the traditional festival of
Sechsel\"auten, which takes place each spring in Zurich, Switzerland, the
B\"o\"ogg is placed atop a wooden pyre, which is set alight. According to
popular legend, the time that elapses until the B\"o\"ogg's head explodes (the
"head-bang" time) is said to give a rough forecast of local weather conditions
prevailing during the following summer. However, recent research has questioned
the validity of this prediction. To study the B\"o\"ogg's predictive powers, we
analyzed the B\"o\"ogg head-bang time record from 1965-2010 within the context
of global climate change. Our analysis shows that the B\"o\"ogg head-bang time
is a good predictor not of short-term local weather, as might be expected from
the legend, but of the behavior of the entire global climate system.
</summary>
    <author>
      <name>M. S. Brennwald</name>
    </author>
    <author>
      <name>D. M. Livingstone</name>
    </author>
    <author>
      <name>R. Kipfer</name>
    </author>
    <link href="http://arxiv.org/abs/1104.0061v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.0061v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.pop-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.pop-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ao-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.5038v2</id>
    <updated>2014-09-02T14:28:39Z</updated>
    <published>2011-05-25T14:32:11Z</published>
    <title>Uniform bias study and Bahadur representation for local polynomial
  estimators of the conditional quantile function</title>
    <summary>  This paper investigates the bias and the weak Bahadur representation of a
local polynomial estimator of the conditional quantile function and its
derivatives. The bias and Bahadur remainder term are studied uniformly with
respect to the quantile level, the covariates and the smoothing parameter. The
order of the local polynomial estimator can be higher than the
differentiability order of the conditional quantile function. Applications of
the results deal with global optimal consistency rates of the local polynomial
quantile estimator, performance of random bandwidths and estimation of the
conditional quantile density function. The latter allows to obtain a simple
estimator of the conditional quantile function of the private values in a first
price sealed bids auctions under the independent private values paradigm and
risk neutrality.
</summary>
    <author>
      <name>Emmanuel Guerre</name>
    </author>
    <author>
      <name>Camille Sabbah</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1017/S0266466611000132</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1017/S0266466611000132" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This version corrects an error in the proof of Lemma B.2 which was
  pointed out by Zhongjun Qu but does not change the results of the published
  version, Econometric Theory, 28, 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1105.5038v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.5038v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.1615v1</id>
    <updated>2011-10-07T19:26:21Z</updated>
    <published>2011-10-07T19:26:21Z</published>
    <title>Estimation of time-delayed mutual information and bias for irregularly
  and sparsely sampled time-series</title>
    <summary>  A method to estimate the time-dependent correlation via an empirical bias
estimate of the time-delayed mutual information for a time-series is proposed.
In particular, the bias of the time-delayed mutual information is shown to
often be equivalent to the mutual information between two distributions of
points from the same system separated by infinite time. Thus intuitively,
estimation of the bias is reduced to estimation of the mutual information
between distributions of data points separated by large time intervals. The
proposed bias estimation techniques are shown to work for Lorenz equations data
and glucose time series data of three patients from the Columbia University
Medical Center database.
</summary>
    <author>
      <name>DJ Albers</name>
    </author>
    <author>
      <name>George Hripcsak</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.chaos.2012.03.003</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.chaos.2012.03.003" rel="related"/>
    <link href="http://arxiv.org/abs/1110.1615v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.1615v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.1892v6</id>
    <updated>2015-07-05T13:58:53Z</updated>
    <published>2011-10-09T22:47:47Z</published>
    <title>Confidence-based Reasoning in Stochastic Constraint Programming</title>
    <summary>  In this work we introduce a novel approach, based on sampling, for finding
assignments that are likely to be solutions to stochastic constraint
satisfaction problems and constraint optimisation problems. Our approach
reduces the size of the original problem being analysed; by solving this
reduced problem, with a given confidence probability, we obtain assignments
that satisfy the chance constraints in the original model within prescribed
error tolerance thresholds. To achieve this, we blend concepts from stochastic
constraint programming and statistics. We discuss both exact and approximate
variants of our method. The framework we introduce can be immediately employed
in concert with existing approaches for solving stochastic constraint programs.
A thorough computational study on a number of stochastic combinatorial
optimisation problems demonstrates the effectiveness of our approach.
</summary>
    <author>
      <name>Roberto Rossi</name>
    </author>
    <author>
      <name>Brahim Hnich</name>
    </author>
    <author>
      <name>S. Armagan Tarim</name>
    </author>
    <author>
      <name>Steven Prestwich</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.artint.2015.07.004</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.artint.2015.07.004" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">53 pages, working draft</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Artificial Intelligence, Elsevier, 228(1):129-152, 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1110.1892v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.1892v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.3860v1</id>
    <updated>2011-10-18T01:35:32Z</updated>
    <published>2011-10-18T01:35:32Z</published>
    <title>Contending Parties: A Logistic Choice Analysis of Inter- and Intra-group
  Blog Citation Dynamics in the 2004 US Presidential Election</title>
    <summary>  The 2004 US Presidential Election cycle marked the debut of Internet-based
media such as blogs and social networking websites as institutionally
recognized features of the American political landscape. Using a longitudinal
sample of all DNC/RNC-designated blog-citation networks we are able to test the
influence of various strategic, institutional, and balance-theoretic mechanisms
and exogenous factors such as seasonality and political events on the
propensity of blogs to cite one another over time. Capitalizing on the temporal
resolution of our data, we utilize an autoregressive network regression
framework to carry out inference for a logistic choice process. Using a
combination of deviance-based model selection criteria and simulation-based
model adequacy tests, we identify the combination of processes that best
characterizes the choice behavior of the contending blogs.
</summary>
    <author>
      <name>Zack W. Almquist</name>
    </author>
    <author>
      <name>Carter T. Butts</name>
    </author>
    <link href="http://arxiv.org/abs/1110.3860v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.3860v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.5971v1</id>
    <updated>2011-10-27T03:17:04Z</updated>
    <published>2011-10-27T03:17:04Z</published>
    <title>Effects of dimers on cooperation in the spatial prisoner's dilemma game</title>
    <summary>  We investigate the evolutionary prisoner's dilemma game in structured
populations by introducing dimers, which are defined as that two players in
each dimer always hold a same strategy. We find that influences of dimers on
cooperation depend on the type of dimers and the population structure. For
those dimers in which players interact with each other, the cooperation level
increases with the number of dimers though the cooperation improvement level
depends on the type of network structures. On the other hand, the dimers, in
which there are not mutual interactions, will not do any good to the
cooperation level in a single community, but interestingly, will improve the
cooperation level in a population with two communities. We explore the
relationship between dimers and self-interactions and find that the effects of
dimers are similar to that of self-interactions. Also, we find that the dimers,
which are established over two communities in a multi-community network, act as
one type of interaction through which information between communities is
communicated by the requirement that two players in a dimer hold a same
strategy.
</summary>
    <author>
      <name>Haihong Li</name>
    </author>
    <author>
      <name>Hongyan Cheng</name>
    </author>
    <author>
      <name>Qionglin Dai</name>
    </author>
    <author>
      <name>Ping Ju</name>
    </author>
    <author>
      <name>Mei Zhang</name>
    </author>
    <author>
      <name>Junzhong Yang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/0253-6102/56/5/04</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/0253-6102/56/5/04" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages and 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1110.5971v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.5971v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.2091v2</id>
    <updated>2012-03-26T22:32:43Z</updated>
    <published>2011-11-09T02:37:12Z</published>
    <title>Performance-based regularization in mean-CVaR portfolio optimization</title>
    <summary>  We introduce performance-based regularization (PBR), a new approach to
addressing estimation risk in data-driven optimization, to mean-CVaR portfolio
optimization. We assume the available log-return data is iid, and detail the
approach for two cases: nonparametric and parametric (the log-return
distribution belongs in the elliptical family). The nonparametric PBR method
penalizes portfolios with large variability in mean and CVaR estimations. The
parametric PBR method solves the empirical Markowitz problem instead of the
empirical mean-CVaR problem, as the solutions of the Markowitz and mean-CVaR
problems are equivalent when the log-return distribution is elliptical. We
derive the asymptotic behavior of the nonparametric PBR solution, which leads
to insight into the effect of penalization, and justification of the parametric
PBR method. We also show via simulations that the PBR methods produce efficient
frontiers that are, on average, closer to the population efficient frontier
than the empirical approach to the mean-CVaR problem, with less variability.
</summary>
    <author>
      <name>Noureddine El Karoui</name>
    </author>
    <author>
      <name>Andrew E. B. Lim</name>
    </author>
    <author>
      <name>Gah-Yi Vahn</name>
    </author>
    <link href="http://arxiv.org/abs/1111.2091v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.2091v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.PM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.PM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.RM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="90C20, 62P05 (Primary) 90C90, 91B30 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.1436v2</id>
    <updated>2012-07-20T20:04:16Z</updated>
    <published>2012-02-07T14:38:36Z</published>
    <title>Linear regression for numeric symbolic variables: an ordinary least
  squares approach based on Wasserstein Distance</title>
    <summary>  In this paper we present a linear regression model for modal symbolic data.
The observed variables are histogram variables according to the definition
given in the framework of Symbolic Data Analysis and the parameters of the
model are estimated using the classic Least Squares method. An appropriate
metric is introduced in order to measure the error between the observed and the
predicted distributions. In particular, the Wasserstein distance is proposed.
Some properties of such metric are exploited to predict the response variable
as direct linear combination of other independent histogram variables. Measures
of goodness of fit are discussed. An application on real data corroborates the
proposed method.
</summary>
    <author>
      <name>Antonio Irpino</name>
    </author>
    <author>
      <name>Rosanna Verde</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11634-015-0197-7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11634-015-0197-7" rel="related"/>
    <link href="http://arxiv.org/abs/1202.1436v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.1436v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.4810v1</id>
    <updated>2012-03-21T19:59:24Z</updated>
    <published>2012-03-21T19:59:24Z</published>
    <title>Estimating a Random Walk First-Passage Time from Noisy or Delayed
  Observations</title>
    <summary>  A random walk (or a Wiener process), possibly with drift, is observed in a
noisy or delayed fashion. The problem considered in this paper is to estimate
the first time \tau the random walk reaches a given level. Specifically, the
p-moment (p\geq 1) optimization problem \inf_\eta \ex|\eta-\tau|^p is
investigated where the infimum is taken over the set of stopping times that are
defined on the observation process.
  When there is no drift, optimal stopping rules are characterized for both
types of observations. When there is a drift, upper and lower bounds on
\inf_\eta \ex|\eta-\tau|^p are established for both types of observations. The
bounds are tight in the large-level regime for noisy observations and in the
large-level-large-delay regime for delayed observations. Noteworthy, for noisy
observations there exists an asymptotically optimal stopping rule that is a
function of a single observation.
  Simulation results are provided that corroborate the validity of the results
for non-asymptotic settings.
</summary>
    <author>
      <name>Marat V. Burnashev</name>
    </author>
    <author>
      <name>Aslan Tchamkerten</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in the IEEE Transactions on Information Theory</arxiv:comment>
    <link href="http://arxiv.org/abs/1203.4810v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.4810v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.0165v3</id>
    <updated>2015-02-27T01:36:58Z</updated>
    <published>2012-04-01T05:03:25Z</published>
    <title>Analytical Models for Power Networks: The case of the Western US and
  ERCOT grids</title>
    <summary>  The topological structure of the power grid plays a key role in the reliable
delivery of electricity and price settlement in the electricity market.
Incorporation of new energy sources and loads into the grid over time has led
to its structural and geographical expansion and can affect its stable
operation. This paper presents an intuitive analytical model for the temporal
evolution of large grids and uses it to understand common structural features
observed in grids across America. In particular, key graph parameters like
degree distribution, graph diameter, betweenness centralities, eigen-spread and
clustering coefficients, as well as graph processes like infection propagation
are used to quantify the model's benefits through comparison with the Western
US and ERCOT power grids. The most significant contribution of the developed
model is its analytical tractability, that provides a closed form expression
for the nodal degree distribution observed in large grids. The discussed model
can be used to generate realistic test cases to analyze topological effects on
grid functioning and new grid technologies.
</summary>
    <author>
      <name>Deepjyoti Deka</name>
    </author>
    <author>
      <name>Sriram Vishwanath</name>
    </author>
    <link href="http://arxiv.org/abs/1204.0165v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.0165v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.2410v2</id>
    <updated>2012-10-28T22:35:06Z</updated>
    <published>2012-04-11T10:54:22Z</published>
    <title>Densities of nested Archimedean copulas</title>
    <summary>  Nested Archimedean copulas recently gained interest since they generalize the
well-known class of Archimedean copulas to allow for partial asymmetry.
Sampling algorithms and strategies have been well investigated for nested
Archimedean copulas. However, for likelihood based inference it is important to
have the density. The present work fills this gap. A general formula for the
derivatives of the nodes and inner generators appearing in nested Archimedean
copulas is developed. This leads to a tractable formula for the density of
nested Archimedean copulas in arbitrary dimensions if the number of nesting
levels is not too large. Various examples including famous Archimedean families
and transformations of such are given. Furthermore, a numerically efficient way
to evaluate the log-density is presented.
</summary>
    <author>
      <name>Marius Hofert</name>
    </author>
    <author>
      <name>David Pham</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.2410v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.2410v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62H99, 65C60, 62H12, 62F10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.1107v1</id>
    <updated>2012-05-05T06:47:58Z</updated>
    <published>2012-05-05T06:47:58Z</published>
    <title>Estimation of spatial max-stable models using threshold exceedances</title>
    <summary>  Parametric inference for spatial max-stable processes is difficult since the
related likelihoods are unavailable. A composite likelihood approach based on
the bivariate distribution of block maxima has been recently proposed in the
literature. However modeling block maxima is a wasteful approach provided that
other information is available. Moreover an approach based on block, typically
annual, maxima is unable to take into account the fact that maxima occur or not
simultaneously. If time series of, say, daily data are available, then
estimation procedures based on exceedances of a high threshold could mitigate
such problems. In this paper we focus on two approaches for composing
likelihoods based on pairs of exceedances. The first one comes from the tail
approximation for bivariate distribution proposed by Ledford and Tawn (1996)
when both pairs of observations exceed the fixed threshold. The second one uses
the bivariate extension (Rootzen and Tajvidi, 2006) of the generalized Pareto
distribution which allows to model exceedances when at least one of the
components is over the threshold. The two approaches are compared through a
simulation study according to different degrees of spatial dependency. Results
show that both the strength of the spatial dependencies and the threshold
choice play a fundamental role in determining which is the best estimating
procedure.
</summary>
    <author>
      <name>Jean-Noel Bacro</name>
    </author>
    <author>
      <name>Carlo Gaetan</name>
    </author>
    <link href="http://arxiv.org/abs/1205.1107v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.1107v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.3845v1</id>
    <updated>2012-05-17T04:06:29Z</updated>
    <published>2012-05-17T04:06:29Z</published>
    <title>Forecasting with Historical Data or Process Knowledge under
  Misspecification: A Comparison</title>
    <summary>  When faced with the task of forecasting a dynamic system, practitioners often
have available historical data, knowledge of the system, or a combination of
both. While intuition dictates that perfect knowledge of the system should in
theory yield perfect forecasting, often knowledge of the system is only
partially known, known up to parameters, or known incorrectly. In contrast,
forecasting using previous data without any process knowledge might result in
accurate prediction for simple systems, but will fail for highly nonlinear and
chaotic systems. In this paper, the authors demonstrate how even in chaotic
systems, forecasting with historical data is preferable to using process
knowledge if this knowledge exhibits certain forms of misspecification. Through
an extensive simulation study, a range of misspecification and forecasting
scenarios are examined with the goal of gaining an improved understanding of
the circumstances under which forecasting from historical data is to be
preferred over using process knowledge.
</summary>
    <author>
      <name>Luke Bornn</name>
    </author>
    <author>
      <name>Marian Anghel</name>
    </author>
    <author>
      <name>Ingo Steinwart</name>
    </author>
    <link href="http://arxiv.org/abs/1205.3845v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.3845v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.0407v1</id>
    <updated>2012-06-24T02:18:08Z</updated>
    <published>2012-06-24T02:18:08Z</published>
    <title>What is Statistics?; The Answer by Quantum Language</title>
    <summary>  Since the problem: "What is statistics?" is most fundamental in sceince, in
order to solve this problem, there is every reason to believe that we have to
start from the proposal of a worldview. Recently we proposed measurement theory
(i.e., quantum language, or the linguistic interpretation of quantum
mechanics), which is characterized as the linguistic turn of the Copenhagen
interpretation of quantum mechanics. This turn from physics to language does
not only extend quantum theory to classical theory but also yield the quantum
mechanical world view (i.e., the (quantum) linguistic world view, and thus, a
form of quantum thinking, in other words, quantum philosophy). Thus, we believe
that the quantum lingistic formulation of statistics gives an answer to the
question: "What is statistics?". In this paper, this will be done through the
studies of inference interval, statistical hypothesis testing, Fisher maximum
likelihood method, Bayes method and regression analysis in meaurement theory.
</summary>
    <author>
      <name>Shiro Ishikawa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages. arXiv admin note: substantial text overlap with
  arXiv:1204.3892</arxiv:comment>
    <link href="http://arxiv.org/abs/1207.0407v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.0407v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62A01" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.1708v2</id>
    <updated>2012-11-02T14:59:15Z</updated>
    <published>2012-07-06T19:05:59Z</published>
    <title>Estimators for Archimedean copulas in high dimensions</title>
    <summary>  The performance of known and new parametric estimators for Archimedean
copulas is investigated, with special focus on large dimensions and numerical
difficulties. In particular, method-of-moments-like estimators based on
pairwise Kendall's tau, a multivariate extension of Blomqvist's beta, minimum
distance estimators, the maximum-likelihood estimator, a simulated
maximum-likelihood estimator, and a maximum-likelihood estimator based on the
copula diagonal are studied. Their performance is compared in a large-scale
simulation study both under known and unknown margins (pseudo-observations), in
small and high dimensions, under small and large dependencies, various
different Archimedean families and sample sizes. High dimensions up to one
hundred are considered for the first time and computational problems arising
from such large dimensions are addressed in detail. All methods are implemented
in the open source \R{} package \pkg{copula} and can thus be easily accessed
and studied.
</summary>
    <author>
      <name>Marius Hofert</name>
    </author>
    <author>
      <name>Martin Maechler</name>
    </author>
    <author>
      <name>Alexander J. McNeil</name>
    </author>
    <link href="http://arxiv.org/abs/1207.1708v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.1708v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62H12, 62F10, 62H99, 62H20, 65C60" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.2296v6</id>
    <updated>2013-03-25T10:47:41Z</updated>
    <published>2012-07-10T10:25:00Z</published>
    <title>Extremal t processes: Elliptical domain of attraction and a spectral
  representation</title>
    <summary>  The extremal t process was proposed in the literature for modeling spatial
extremes within a copula framework based on the extreme value limit of
elliptical t distributions (Davison, Padoan and Ribatet (2012)). A major
drawback of this max-stable model was the lack of a spectral representation
such that for instance direct simulation was infeasible. The main contribution
of this note is to propose such a spectral construction for the extremal t
process. Interestingly, the extremal Gaussian process introduced by Schlather
(2002) appears as a special case. We further highlight the role of the extremal
t process as the maximum attractor for processes with finite-dimensional
elliptical distributions. All results naturally also hold within the
multivariate domain.
</summary>
    <author>
      <name>Thomas Opitz</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.jmva.2013.08.008</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.jmva.2013.08.008" rel="related"/>
    <link href="http://arxiv.org/abs/1207.2296v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.2296v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.2719v1</id>
    <updated>2012-08-13T22:04:30Z</updated>
    <published>2012-08-13T22:04:30Z</published>
    <title>Unified Analysis of Transmit Antenna Selection/Space-Time Block Coding
  with Receive Selection and Combining over Nakagami-m Fading Channels in the
  Presence of Feedback Errors</title>
    <summary>  Examining the effect of imperfect transmit antenna selection (TAS) caused by
the feedback link errors on the performance of hybrid TAS/space-time block
coding (STBC) with selection combining (SC) (i.e., joint transmit and receive
antenna selection (TRAS)/STBC) and TAS/STBC (with receive maximal-ratio
combining (MRC)-like combining structure) over Nakagami-m fading channels is
the main objective of this paper. Under ideal channel estimation and delay-free
feedback assumptions, statistical expressions and several performance metrics
related to the post-processing signal-to-noise ratio (SNR) are derived for a
unified system model concerning both joint TRAS/STBC and TAS/STBC schemes.
Exact analytical expressions for outage probability and bit/symbol error rates
(BER/SER) of binary and M-ary modulations are presented in order to provide an
extensive examination on the capacity and error performance of the unified
system that experiences feedback errors. Also, the asymptotic diversity order
analysis, which shows that the diversity order of the investigated schemes is
lower bounded by the diversity order provided by STBC transmission itself, is
included in the paper. Moreover, all theoretical results are validated by
performing Monte Carlo simulations.
</summary>
    <author>
      <name>Ahmet Faruk Coskun</name>
    </author>
    <author>
      <name>Oguz Kucur</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1208.2719v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.2719v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.3121v5</id>
    <updated>2014-06-23T12:47:29Z</updated>
    <published>2012-08-15T14:12:01Z</published>
    <title>The Multivariate $S_n$ Estimator</title>
    <summary>  In this note we introduce the M$S_n$ estimator (for Multivariate $S_n$) a new
robust estimator of multivariate ranking. Like MVE and MCD it searches for an
$h$-subset which minimizes a criterion. The difference is that the new
criterion measures the degree of overlap between univariate projections of the
data. A primary advantage of this new criterion lies in its relative
independence from the configuration of the outliers. A second advantage is that
it easily lends itself to so-called "symmetricizing" transformations whereby
the observations only enter the objective function through their pairwise
differences: this makes our proposal well suited for models with an asymmetric
distribution. M$S_n$ is, therefore, more generally applicable than either MVE,
MCD or SDE. We also construct a fast algorithm for the M$S_n$ estimator, and
simulate its bias under various adversary configurations of outliers.
</summary>
    <author>
      <name>Kaveh Vakili</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 6 figures This paper has been withdrawn by the author due
  to a crucial error in equation 7 (said equation did not correspond with
  computer code used in the simulation, the computer code being the correct
  one.)</arxiv:comment>
    <link href="http://arxiv.org/abs/1208.3121v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.3121v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.1350v1</id>
    <updated>2012-09-06T17:44:47Z</updated>
    <published>2012-09-06T17:44:47Z</published>
    <title>Probability Distribution of the Quality Factor of a Mode-Stirred
  Reverberation Chamber</title>
    <summary>  We derive a probability distribution, confidence intervals and statistics of
the quality (Q) factor of an arbitrarily shaped mode-stirred reverberation
chamber, based on ensemble distributions of the idealized random cavity field
with assumed perfect stir efficiency. It is shown that Q exhibits a
Fisher-Snedecor F-distribution whose degrees of freedom are governed by the
number of simultaneously excited cavity modes per stir state. The most probable
value of Q is between a fraction 2/9 and 1 of its mean value, and between a
fraction 4/9 and 1 of its asymptotic (composite Q) value. The arithmetic mean
value is found to always exceed the values of all other theoretical metrics for
centrality of Q. For a rectangular cavity, we retrieve the known asymptotic Q
in the limit of highly overmoded regime.
</summary>
    <author>
      <name>L. R. Arnaut</name>
    </author>
    <author>
      <name>G. Gradoni</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TEMC.2012.2213257</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TEMC.2012.2213257" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted for publication in IEEE Trans. Electromagn. Compat., 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.1350v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.1350v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.class-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.4065v2</id>
    <updated>2012-09-19T12:18:01Z</updated>
    <published>2012-09-18T19:14:13Z</published>
    <title>On the Performance of Transmit Antenna Selection Based on Shadowing Side
  Information</title>
    <summary>  In this paper, a transmit antenna selection scheme, which is based on
shadowing side information, is investigated. In this scheme, the selected
single transmit antenna provides the highest shadowing coefficient between
transmitter and receiver. By the proposed technique, the frequency of the usage
of the feedback channel from the receiver to the transmitter and also channel
estimation complexity at the receiver can be reduced. We study the performance
of our proposed technique and in the analysis, we consider an independent but
not identically distributed Generalized-K composite fading model. More
specifically exact and closed-form expressions for the outage probability, the
moment generating function, the moments of signal-to-noise ratio, and the
average symbol error probability are derived. In addition, asymptotic outage
probability and symbol error probability expressions are also presented in
order to investigate the diversity order and the array gain. Finally, our
theoretical performance results are validated by Monte Carlo simulations.
</summary>
    <author>
      <name>Ahmet Yilmaz</name>
    </author>
    <author>
      <name>Ferkan Yilmaz</name>
    </author>
    <author>
      <name>Mohamed-Slim Alouini</name>
    </author>
    <author>
      <name>Oğuz Kucur</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 5 figures, journal</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.4065v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.4065v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.0756v1</id>
    <updated>2012-10-02T12:57:46Z</updated>
    <published>2012-10-02T12:57:46Z</published>
    <title>Stochastic dynamical model of a growing network based on self-exciting
  point process</title>
    <summary>  We perform experimental verification of the preferential attachment model
that is commonly accepted as a generating mechanism of the scale-free complex
networks. To this end we chose citation network of Physics papers and traced
citation history of 40,195 papers published in one year. Contrary to common
belief, we found that citation dynamics of the individual papers follows the
\emph{superlinear} preferential attachment, with the exponent $\alpha=
1.25-1.3$. Moreover, we showed that the citation process cannot be described as
a memoryless Markov chain since there is substantial correlation between the
present and recent citation rates of a paper. Basing on our findings we
constructed a stochastic growth model of the citation network, performed
numerical simulations based on this model and achieved an excellent agreement
with the measured citation distributions.
</summary>
    <author>
      <name>Michael Golosovsky</name>
    </author>
    <author>
      <name>Sorin Solomon</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevLett.109.098701</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevLett.109.098701" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 9 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. Lett. 109, 098701 (2012)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1210.0756v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.0756v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.1773v1</id>
    <updated>2012-10-05T14:38:00Z</updated>
    <published>2012-10-05T14:38:00Z</published>
    <title>Efficient Forward Simulation of Fisher-Wright Populations with
  Stochastic Population Size and Neutral Single Step Mutations in Haplotypes</title>
    <summary>  In both population genetics and forensic genetics it is important to know how
haplotypes are distributed in a population. Simulation of population dynamics
helps facilitating research on the distribution of haplotypes. In forensic
genetics, the haplotypes can for example consist of lineage markers such as
short tandem repeat loci on the Y chromosome (Y-STR). A dominating model for
describing population dynamics is the simple, yet powerful, Fisher-Wright
model. We describe an efficient algorithm for exact forward simulation of exact
Fisher-Wright populations (and not approximative such as the coalescent model).
The efficiency comes from convenient data structures by changing the
traditional view from individuals to haplotypes. The algorithm is implemented
in the open-source R package 'fwsim' and is able to simulate very large
populations. We focus on a haploid model and assume stochastic population size
with flexible growth specification, no selection, a neutral single step
mutation process, and self-reproducing individuals. These assumptions make the
algorithm ideal for studying lineage markers such as Y-STR.
</summary>
    <author>
      <name>Mikkel Meyer Andersen</name>
    </author>
    <author>
      <name>Poul Svante Eriksen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1210.1773v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.1773v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62-04" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.1183v2</id>
    <updated>2014-04-15T09:56:38Z</updated>
    <published>2012-11-06T11:26:30Z</published>
    <title>KernSmoothIRT: An R Package for Kernel Smoothing in Item Response Theory</title>
    <summary>  Item response theory (IRT) models are a class of statistical models used to
describe the response behaviors of individuals to a set of items having a
certain number of options. They are adopted by researchers in social science,
particularly in the analysis of performance or attitudinal data, in psychology,
education, medicine, marketing and other fields where the aim is to measure
latent constructs. Most IRT analyses use parametric models that rely on
assumptions that often are not satisfied. In such cases, a nonparametric
approach might be preferable; nevertheless, there are not many software
applications allowing to use that. To address this gap, this paper presents the
R package KernSmoothIRT. It implements kernel smoothing for the estimation of
option characteristic curves, and adds several plotting and analytical tools to
evaluate the whole test/questionnaire, the items, and the subjects. In order to
show the package's capabilities, two real datasets are used, one employing
multiple-choice responses, and the other scaled responses.
</summary>
    <author>
      <name>Angelo Mazza</name>
    </author>
    <author>
      <name>Antonio Punzo</name>
    </author>
    <author>
      <name>Brian McGuire</name>
    </author>
    <link href="http://arxiv.org/abs/1211.1183v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.1183v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.1277v1</id>
    <updated>2013-01-07T17:37:50Z</updated>
    <published>2013-01-07T17:37:50Z</published>
    <title>The generalized lognormal distribution and the Stieltjes moment problem</title>
    <summary>  This paper studies a Stieltjes-type moment problem defined by the generalized
lognormal distribution, a heavy-tailed distribution with applications in
economics, finance and related fields. It arises as the distribution of the
exponential of a random variable following a generalized error distribution,
and hence figures prominently in the EGARCH model of asset price volatility.
Compared to the classical lognormal distribution it has an additional shape
parameter. It emerges that moment (in)determinacy depends on the value of this
parameter: for some values, the distribution does not have finite moments of
all orders, hence the moment problem is not of interest in these cases. For
other values, the distribution has moments of all orders, yet it is
moment-indeterminate. Finally, a limiting case is supported on a bounded
interval, and hence determined by its moments. For those generalized lognormal
distributions that are moment-indeterminate Stieltjes classes of
moment-equivalent distributions are presented.
</summary>
    <author>
      <name>Christian Kleiber</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10959-013-0477-0</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10959-013-0477-0" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 1 figure</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Theoretical Probability, 2014, Vol. 27, No. 4,
  1167-1177</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1301.1277v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.1277v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="Primary 60E05, Secondary 44A60" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.2525v3</id>
    <updated>2015-08-30T15:15:20Z</updated>
    <published>2013-02-11T16:32:55Z</published>
    <title>Foundations of Descriptive and Inferential Statistics</title>
    <summary>  These lecture notes were written with the aim to provide an accessible though
technically solid introduction to the logic of systematical analyses of
statistical data to undergraduate and to postgraduate students, in particular
in the Social Sciences and in Economics. They may also serve as a general
reference for the application of quantitative--empirical research methods. In
an attempt to encourage the adoption of an interdisciplinary perspective on
quantitative problems arising in practice, the notes cover the four broad
topics (i) descriptive statistical processing of raw data, (ii) elementary
probability theory, mainly as seen from a frequentist's viewpoint, (iii) the
operationalisation of one-dimensional latent statistical variables according to
Likert's widely used scaling approach, and (iv) the standard statistical tests
of hypotheses concerning (a) distributional differences of variables between
subgroups of a target population, and (b) statistical associations between two
variables. The lecture notes are fully hyperlinked, thus providing a direct
route to original scientific papers as well as to interesting biographical
information. They also list many commands for activating statistical functions
and data analysis routines in the software packages SPSS, R, EXCEL and
OpenOffice.
</summary>
    <author>
      <name>Henk van Elst</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Karlshochschule International University</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">163 pages, 15 *.eps figures, LaTeX2e, hyperlinked references. Second
  thorough revision, extended list of references</arxiv:comment>
    <link href="http://arxiv.org/abs/1302.2525v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.2525v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.7498v1</id>
    <updated>2013-07-29T08:42:07Z</updated>
    <published>2013-07-29T08:42:07Z</published>
    <title>The normalization of citation counts based on classification systems</title>
    <summary>  If we want to assess whether the paper in question has had a particularly
high or low citation impact compared to other papers, the standard practice in
bibliometrics is to normalize citations in respect of the subject category and
publication year. A number of proposals for an improved procedure in the
normalization of citation impact have been put forward in recent years. Against
the background of these proposals this study describes an ideal solution for
the normalization of citation impact: in a first step, the reference set for
the publication in question is collated by means of a classification scheme,
where every publication is associated with a single principal research field or
subfield entry (e. g. via Chemical Abstracts sections) and a publication year.
In a second step, percentiles of citation counts are calculated for this set
and used to assign the normalized citation impact score to the publications
(and also to the publication in question).
</summary>
    <author>
      <name>Lutz Bornmann</name>
    </author>
    <author>
      <name>Werner Marx</name>
    </author>
    <author>
      <name>Andreas Barth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in the journal publications</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.7498v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.7498v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.4178v1</id>
    <updated>2013-08-19T21:29:55Z</updated>
    <published>2013-08-19T21:29:55Z</published>
    <title>The factor paradox: Common factors can be correlated with the variance
  not accounted for by the common factors!</title>
    <summary>  The case that the factor model does not account for all the covariances of
the observed variables is considered. This is a quite realistic condition
because some model error as well as some sampling error should usually occur
with empirical data. It is shown that principal components representing
covariances not accounted for by the factors of the model can have a non-zero
correlation with the common factors of the factor model. Non-zero correlations
of components representing variance not accounted for by the factor model with
common factors were also found in a simulation study. Based on these results it
should be concluded that common factors can be correlated with variance
components representing model error as well as sampling error. In consequence,
even when researchers decide not to represent some small or trivial variance by
means of a common factor, these excluded variances can still be part of the
model.
</summary>
    <author>
      <name>Andre Beauducel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 2 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1308.4178v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.4178v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.7163v1</id>
    <updated>2013-10-27T06:29:55Z</updated>
    <published>2013-10-27T06:29:55Z</published>
    <title>Generalized Thompson Sampling for Contextual Bandits</title>
    <summary>  Thompson Sampling, one of the oldest heuristics for solving multi-armed
bandits, has recently been shown to demonstrate state-of-the-art performance.
The empirical success has led to great interests in theoretical understanding
of this heuristic. In this paper, we approach this problem in a way very
different from existing efforts. In particular, motivated by the connection
between Thompson Sampling and exponentiated updates, we propose a new family of
algorithms called Generalized Thompson Sampling in the expert-learning
framework, which includes Thompson Sampling as a special case. Similar to most
expert-learning algorithms, Generalized Thompson Sampling uses a loss function
to adjust the experts' weights. General regret bounds are derived, which are
also instantiated to two important loss functions: square loss and logarithmic
loss. In contrast to existing bounds, our results apply to quite general
contextual bandits. More importantly, they quantify the effect of the "prior"
distribution on the regret bounds.
</summary>
    <author>
      <name>Lihong Li</name>
    </author>
    <link href="http://arxiv.org/abs/1310.7163v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.7163v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62L05" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.1670v1</id>
    <updated>2013-12-05T20:12:05Z</updated>
    <published>2013-12-05T20:12:05Z</published>
    <title>An agent-based epidemiological model of incarceration</title>
    <summary>  We build an agent-based model of incarceration based on the SIS model of
infectious disease propagation. Our central hypothesis is that the observed
racial disparities in incarceration rates between Black and White Americans can
be explained as the result of differential sentencing between the two
demographic groups. We demonstrate that if incarceration can be spread through
a social influence network, then even relatively small differences in
sentencing can result in the large disparities in incarceration rates.
Controlling for effects of transmissibility, susceptibility, and influence
network structure, our model reproduces the observed large disparities in
incarceration rates given the differences in sentence lengths for White and
Black drug offenders in the United States without extensive parameter tuning.
We further establish the suitability of the SIS model as applied to
incarceration, as the observed structural patterns of recidivism are an
emergent property of the model. In fact, our model shows a remarkably close
correspondence with California incarceration data, without requiring any
parameter tuning. This work advances efforts to combine the theories and
methods of epidemiology and criminology.
</summary>
    <author>
      <name>Kristian Lum</name>
    </author>
    <author>
      <name>Samarth Swarup</name>
    </author>
    <author>
      <name>Stephen Eubank</name>
    </author>
    <author>
      <name>James Hawdon</name>
    </author>
    <link href="http://arxiv.org/abs/1312.1670v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.1670v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.2211v1</id>
    <updated>2013-12-08T13:41:23Z</updated>
    <published>2013-12-08T13:41:23Z</published>
    <title>Index Distribution of Cauchy Random Matrices</title>
    <summary>  Using a Coulomb gas technique, we compute analytically the probability
$\mathcal{P}_\beta^{(C)}(N_+,N)$ that a large $N\times N$ Cauchy random matrix
has $N_+$ positive eigenvalues, where $N_+$ is called the index of the
ensemble. We show that this probability scales for large $N$ as
$\mathcal{P}_\beta^{(C)}(N_+,N)\approx \exp\left[-\beta N^2
\psi_C(N_+/N)\right]$, where $\beta$ is the Dyson index of the ensemble. The
rate function $\psi_C(\kappa)$ is computed in terms of single integrals that
are easily evaluated numerically and amenable to an asymptotic analysis. We
find that the rate function, around its minimum at $\kappa=1/2$, has a
quadratic behavior modulated by a logarithmic singularity. As a consequence,
the variance of the index scales for large $N$ as $\mathrm{Var}(N_+)\sim
\sigma_C\ln N$, where $\sigma_C=2/(\beta\pi^2)$ is twice as large as the
corresponding prefactor in the Gaussian and Wishart cases. The analytical
results are checked by numerical simulations and against an exact finite $N$
formula which, for $\beta=2$, can be derived using orthogonal polynomials.
</summary>
    <author>
      <name>Ricardo Marino</name>
    </author>
    <author>
      <name>Satya N. Majumdar</name>
    </author>
    <author>
      <name>Grégory Schehr</name>
    </author>
    <author>
      <name>Pierpaolo Vivo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/1751-8113/47/5/055001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/1751-8113/47/5/055001" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">J. Phys. A: Math. Theor. 47, 055001 (2014)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1312.2211v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.2211v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.MP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.3269v1</id>
    <updated>2014-01-14T17:43:11Z</updated>
    <published>2014-01-14T17:43:11Z</published>
    <title>Teaching precursors to data science in introductory and second courses
  in statistics</title>
    <summary>  Statistics students need to develop the capacity to make sense of the
staggering amount of information collected in our increasingly data-centered
world. Data science is an important part of modern statistics, but our
introductory and second statistics courses often neglect this fact. This paper
discusses ways to provide a practical foundation for students to learn to
"compute with data" as defined by Nolan and Temple Lang (2010), as well as
develop "data habits of mind" (Finzer, 2013). We describe how introductory and
second courses can integrate two key precursors to data science: the use of
reproducible analysis tools and access to large databases. By introducing
students to commonplace tools for data management, visualization, and
reproducible analysis in data science and applying these to real-world
scenarios, we prepare them to think statistically in the era of big data.
</summary>
    <author>
      <name>Nicholas J Horton</name>
    </author>
    <author>
      <name>Benjamin S Baumer</name>
    </author>
    <author>
      <name>Hadley Wickham</name>
    </author>
    <link href="http://arxiv.org/abs/1401.3269v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.3269v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62-07" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.4787v3</id>
    <updated>2015-08-16T14:48:15Z</updated>
    <published>2014-01-20T03:55:15Z</published>
    <title>On the Measurement of Economic Tail Risk</title>
    <summary>  This paper attempts to provide a decision-theoretic foundation for the
measurement of economic tail risk, which is not only closely related to utility
theory but also relevant to statistical model uncertainty. The main result is
that the only risk measures that satisfy a set of economic axioms for the
Choquet expected utility and the statistical property of elicitability (i.e.
there exists an objective function such that minimizing the expected objective
function yields the risk measure) are the mean functional and the median
shortfall, which is the median of tail loss distribution. Elicitability is
important for backtesting. We also extend the result to address model
uncertainty by incorporating multiple scenarios. As an application, we argue
that median shortfall is a better alternative than expected shortfall for
setting capital requirements in Basel Accords.
</summary>
    <author>
      <name>Steven Kou</name>
    </author>
    <author>
      <name>Xianhua Peng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">51 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.4787v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.4787v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.RM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.RM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="91Gxx, 91B30, 62G35, 46N30, 47N30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.6849v4</id>
    <updated>2017-09-26T16:45:27Z</updated>
    <published>2014-01-27T13:50:51Z</published>
    <title>A New Approach to Inference in Multi-Survey Studies with Unknown
  Population Size</title>
    <summary>  We investigate a Poisson sampling design in the presence of unknown selection
probabilities when applied to a population of unknown size for multiple
sampling occasions. The fixed-population model is adopted and extended upon for
inference. The complete minimal sufficient statistic is derived for the
sampling model parameters and fixed-population parameter vector. The
Rao-Blackwell version of population quantity estimators is detailed. An
application is applied to an emprical population. The extended inferential
framework is found to have much potential and utility for empirical studies.
</summary>
    <author>
      <name>Kyle Vincent</name>
    </author>
    <author>
      <name>Saman Muthukumarana</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper has been updated and changed to Rao-Blackwellization to give
  Improved Estimates in Multi-List Studies</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.6849v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.6849v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.1673v3</id>
    <updated>2016-07-01T17:18:59Z</updated>
    <published>2014-02-07T15:50:21Z</published>
    <title>Non-Orthogonal Tensor Diagonalization</title>
    <summary>  Tensor diagonalization means transforming a given tensor to an exactly or
nearly diagonal form through multiplying the tensor by non-orthogonal
invertible matrices along selected dimensions of the tensor. It is
generalization of approximate joint diagonalization (AJD) of a set of matrices.
In particular, we derive (1) a new algorithm for symmetric AJD, which is called
two-sided symmetric diagonalization of order-three tensor, (2) a similar
algorithm for non-symmetric AJD, also called general two-sided diagonalization
of an order-3 tensor, and (3) an algorithm for three-sided diagonalization of
order-3 or order-4 tensors. The latter two algorithms may serve for canonical
polyadic (CP) tensor decomposition, and they can outperform other CP tensor
decomposition methods in terms of computational speed under the restriction
that the tensor rank does not exceed the tensor multilinear rank. Finally, we
propose (4) similar algorithms for tensor block diagonalization, which is
related to the tensor block-term decomposition.
</summary>
    <author>
      <name>Petr Tichavsky</name>
    </author>
    <author>
      <name>Anh Huy Phan</name>
    </author>
    <author>
      <name>Andrzej Cichocki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The manuscript was revised deeply, but the main idea is the same. The
  algorithm has changed significantly</arxiv:comment>
    <link href="http://arxiv.org/abs/1402.1673v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.1673v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.7354v2</id>
    <updated>2014-08-06T16:25:38Z</updated>
    <published>2014-03-28T12:10:34Z</published>
    <title>Extremes of Order Statistics of Stationary Processes</title>
    <summary>  Let $\{X_i(t),t\ge0\}, 1\le i\le n$ be independent copies of a stationary
process $\{X(t), t\ge0\}$. For given positive constants $u,T$, define the set
of $r$th conjunctions $ C_{r,T,u}:= \{t\in [0,T]: X_{r:n}(t) &gt; u\}$ with
$X_{r:n}(t)$ the $r$th largest order statistics of $X_1(t), \ldots , X_n(t),
t\ge 0$. In numerous applications such as brain mapping and digital
communication systems, of interest is the approximation of the probability that
the set of conjunctions $C_{r,T,u}$ is not empty. Imposing the Albin's
conditions on $X$, in this paper we obtain an exact asymptotic expansion of
this probability as $u$ tends to infinity. Further, we establish the tail
asymptotics of the supremum of a generalized skew-Gaussian process and a Gumbel
limit theorem for the minimum order statistics of stationary Gaussian
processes. As a by-product we derive a version of Li and Shao's normal
comparison lemma for the minimum and the maximum of Gaussian random vectors.
</summary>
    <author>
      <name>Krzysztof Debicki</name>
    </author>
    <author>
      <name>Enkelejd Hashorva</name>
    </author>
    <author>
      <name>Lanpeng Ji</name>
    </author>
    <author>
      <name>Chengxiu Ling</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, revised version</arxiv:comment>
    <link href="http://arxiv.org/abs/1403.7354v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.7354v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.6085v1</id>
    <updated>2014-06-23T20:46:26Z</updated>
    <published>2014-06-23T20:46:26Z</published>
    <title>Spectrum Estimation: A Unified Framework for Covariance Matrix
  Estimation and PCA in Large Dimensions</title>
    <summary>  Covariance matrix estimation and principal component analysis (PCA) are two
cornerstones of multivariate analysis. Classic textbook solutions perform
poorly when the dimension of the data is of a magnitude similar to the sample
size, or even larger. In such settings, there is a common remedy for both
statistical problems: nonlinear shrinkage of the eigenvalues of the sample
covariance matrix. The optimal nonlinear shrinkage formula depends on unknown
population quantities and is thus not available. It is, however, possible to
consistently estimate an oracle nonlinear shrinkage, which is motivated on
asymptotic grounds. A key tool to this end is consistent estimation of the set
of eigenvalues of the population covariance matrix (also known as the
spectrum), an interesting and challenging problem in its own right. Extensive
Monte Carlo simulations demonstrate that our methods have desirable
finite-sample properties and outperform previous proposals.
</summary>
    <author>
      <name>Olivier Ledoit</name>
    </author>
    <author>
      <name>Michael Wolf</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">40 pages, 8 figures, 5 tables, University of Zurich, Department of
  Economics, Working Paper No. 105, Revised version, July 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.6085v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.6085v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62H12" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.7172v1</id>
    <updated>2014-07-27T00:30:58Z</updated>
    <published>2014-07-27T00:30:58Z</published>
    <title>Tractable Measure of Component Overlap for Gaussian Mixture Models</title>
    <summary>  The ability to quantify distinctness of a cluster structure is fundamental
for certain simulation studies, in particular for those comparing performance
of different classification algorithms. The intrinsic integral measure based on
the overlap of corresponding mixture components is often analytically
intractable. This is also the case for Gaussian mixture models with unequal
covariance matrices when space dimension $d &gt; 1$. In this work we focus on
Gaussian mixture models and at the sample level we assume the class assignments
to be known. We derive a measure of component overlap based on eigenvalues of a
generalized eigenproblem that represents Fisher's discriminant task. We explain
rationale behind it and present simulation results that show how well it can
reflect the behavior of the integral measure in its linear approximation. The
analyzed coefficient possesses the advantage of being analytically tractable
and numerically computable even in complex setups.
</summary>
    <author>
      <name>Ewa Nowakowska</name>
    </author>
    <author>
      <name>Jacek Koronacki</name>
    </author>
    <author>
      <name>Stan Lipovetsky</name>
    </author>
    <link href="http://arxiv.org/abs/1407.7172v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.7172v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.1809v2</id>
    <updated>2015-08-21T17:17:36Z</updated>
    <published>2014-08-08T10:25:15Z</published>
    <title>Graphs for margins of Bayesian networks</title>
    <summary>  Directed acyclic graph (DAG) models, also called Bayesian networks, impose
conditional independence constraints on a multivariate probability
distribution, and are widely used in probabilistic reasoning, machine learning
and causal inference. If latent variables are included in such a model, then
the set of possible marginal distributions over the remaining (observed)
variables is generally complex, and not represented by any DAG. Larger classes
of mixed graphical models, which use multiple edge types, have been introduced
to overcome this; however, these classes do not represent all the models which
can arise as margins of DAGs. In this paper we show that this is because
ordinary mixed graphs are fundamentally insufficiently rich to capture the
variety of marginal models.
  We introduce a new class of hyper-graphs, called mDAGs, and a latent
projection operation to obtain an mDAG from the margin of a DAG. We show that
each distinct marginal of a DAG model is represented by at least one mDAG, and
provide graphical results towards characterizing when two such marginal models
are the same. Finally we show that mDAGs correctly capture the marginal
structure of causally-interpreted DAGs under interventions on the observed
variables.
</summary>
    <author>
      <name>Robin J. Evans</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1111/sjos.12194</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1111/sjos.12194" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Scandinavian Journal of Statistics, Volume 43, Issue 3, Pages
  625-920, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1408.1809v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.1809v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.0403v1</id>
    <updated>2014-10-01T22:37:08Z</updated>
    <published>2014-10-01T22:37:08Z</published>
    <title>Computer experiments with functional inputs and scalar outputs by a
  norm-based approach</title>
    <summary>  A framework for designing and analyzing computer experiments is presented,
which is constructed for dealing with functional and real number inputs and
real number outputs. For designing experiments with both functional and real
number inputs a two stage approach is suggested. The first stage consists of
constructing a candidate set for each functional input and during the second
stage an optimal combination of the found candidate sets and a Latin hypercube
for the real number inputs is searched for. The resulting designs can be
considered to be generalizations of Latin hypercubes. GP models are explored as
metamodel. The functional inputs are incorporated into the kriging model by
applying norms in order to define distances between two functional inputs. In
order to make the calculation of these norms computationally feasible, the use
of B-splines is promoted.
</summary>
    <author>
      <name>Thomas Muehlenstaedt</name>
    </author>
    <author>
      <name>Jana Fruth</name>
    </author>
    <author>
      <name>Olivier Roustant</name>
    </author>
    <link href="http://arxiv.org/abs/1410.0403v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.0403v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.7967v1</id>
    <updated>2014-10-29T13:20:14Z</updated>
    <published>2014-10-29T13:20:14Z</published>
    <title>Technical Report: Compressive Temporal Higher Order Cyclostationary
  Statistics</title>
    <summary>  The application of nonlinear transformations to a cyclostationary signal for
the purpose of revealing hidden periodicities has proven to be useful for
applications requiring signal selectivity and noise tolerance. The fact that
the hidden periodicities, referred to as cyclic moments, are often compressible
in the Fourier domain motivates the use of compressive sensing (CS) as an
efficient acquisition protocol for capturing such signals. In this work, we
consider the class of Temporal Higher Order Cyclostationary Statistics (THOCS)
estimators when CS is used to acquire the cyclostationary signal assuming
compressible cyclic moments in the Fourier domain. We develop a theoretical
framework for estimating THOCS using the low-rate nonuniform sampling protocol
from CS and illustrate the performance of this framework using simulated data.
</summary>
    <author>
      <name>Chia Wei Lim</name>
    </author>
    <author>
      <name>Michael B. Wakin</name>
    </author>
    <link href="http://arxiv.org/abs/1410.7967v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.7967v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.2571v1</id>
    <updated>2014-11-10T20:25:04Z</updated>
    <published>2014-11-10T20:25:04Z</published>
    <title>Parametric Order Constraints in Multinomial Processing Tree Models: An
  Extension of Knapp and Batchelder (2004)</title>
    <summary>  Multinomial processing tree (MPT) models are tools for disentangling the
contributions of latent cognitive processes in a given experimental paradigm.
The present note analyzes MPT models subject to order constraints on subsets of
its parameters. The constraints that we consider frequently arise in cases
where the response categories are ordered in some sense such as in
confidence-rating data, Likert scale data, where graded guessing tendencies or
response biases are created via base-rate or payoff manipulations, in the
analysis of contingency tables with order constraints, and in many other cases.
We show how to construct an MPT model without order constraints that is
statistically equivalent to the MPT model with order constraints. This new
closure result extends the mathematical analysis of the MPT class, and it
offers an approach to order-restricted inference that extends the approaches
discussed by Knapp and Batchelder (2004). The usefulness of the method is
illustrated by means of an analysis of an order-constrained version of the
two-high-threshold model for confidence ratings.
</summary>
    <author>
      <name>Karl Christoph Klauer</name>
    </author>
    <author>
      <name>Henrik Singmann</name>
    </author>
    <author>
      <name>David Kellen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">author submitted version accepted for publication in "Journal of
  Mathematical Psychology"</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.2571v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.2571v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62F30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.4750v3</id>
    <updated>2016-01-15T15:16:30Z</updated>
    <published>2014-11-18T07:21:39Z</published>
    <title>Convergence rates of maximal deviation distribution for projection
  estimates of Lévy densities</title>
    <summary>  In this paper, we consider projection estimates for L\'evy densities in
high-frequency setup. We give a unified treatment for different sets of basis
functions and focus on the asymptotic properties of the maximal deviation
distribution for these estimates. Our results are based on the idea to
reformulate the problems in terms of Gaussian processes of some special type
and to further analyze these Gaussian processes. In particular, we construct a
sequence of excursion sets, which guarantees the convergence of the deviation
distribution to the Gumbel distribution. We show that the rates of convergence
presented in previous articles on this topic are logarithmic and construct the
sequences of accompanying laws, which approximate the deviation distribution
with polynomial rate.
</summary>
    <author>
      <name>Valentin Konakov</name>
    </author>
    <author>
      <name>Vladimir Panov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">42 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.4750v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.4750v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60G51, 62M99, 62G05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.00599v1</id>
    <updated>2015-01-03T20:15:05Z</updated>
    <published>2015-01-03T20:15:05Z</published>
    <title>On testing More IFRA Ordering-II</title>
    <summary>  Suppose F and G are two life distribution functions. It is said that F is
more IFRA than G (written by F&lt;_* G) if G^(-1) F(x) is starshaped on (0,infty).
In this paper, the problem of testing H_0:F=_* G against H_1:F&lt;_* G and F
\neq_* G is considered in both cases when G is known and when G is unknown. We
propose a new test based on U-statistics and obtain the asymptotic distribution
of the test statistics. The new test is compared with some well known tests in
the literature. In addition, we apply our test to a real data set in the
context of reliability.
</summary>
    <author>
      <name>Muhyiddin Izadi</name>
    </author>
    <author>
      <name>Baha-Eldin Khaledi</name>
    </author>
    <author>
      <name>Chin-Diew Lai</name>
    </author>
    <link href="http://arxiv.org/abs/1501.00599v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.00599v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.04620v1</id>
    <updated>2015-03-16T12:26:04Z</updated>
    <published>2015-03-16T12:26:04Z</published>
    <title>Two symmetry breaking mechanisms for the development of orientation
  selectivity in a neural system</title>
    <summary>  Orientation selectivity is a remarkable feature of the neurons located in the
primary visual cortex. Provided that the visual neurons acquire orientation
selectivity through activity-dependent Hebbian learning, the development
process could be understood as a kind of symmetry breaking phenomenon in the
view of physics. The key mechanisms of the development process are examined
here in a neural system. Found is that there are at least two different
mechanisms which lead to the development of orientation selectivity through
breaking the radial symmetry in receptive fields. The first, a simultaneous
symmetry breaking mechanism, bases on the competition between neighboring
neurons, and the second, a spontaneous one, bases on the nonlinearity in
interactions. It turns out that only the second mechanism leads to the
formation of a columnar pattern which characteristics accord with those
observed in an animal experiment.
</summary>
    <author>
      <name>Myoung Won Cho</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3938/jkps.67.1661</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3938/jkps.67.1661" rel="related"/>
    <link href="http://arxiv.org/abs/1503.04620v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.04620v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.06201v1</id>
    <updated>2015-03-20T19:31:09Z</updated>
    <published>2015-03-20T19:31:09Z</published>
    <title>Data Science as a New Frontier for Design</title>
    <summary>  The purpose of this paper is to contribute to the challenge of transferring
know-how, theories and methods from design research to the design processes in
information science and technologies. More specifically, we shall consider a
domain, namely data-science, that is becoming rapidly a globally invested
research and development axis with strong imperatives for innovation given the
data deluge we are currently facing. We argue that, in order to rise to the
data-related challenges that the society is facing, data-science initiatives
should ensure a renewal of traditional research methodologies that are still
largely based on trial-error processes depending on the talent and insights of
a single (or a restricted group of) researchers. It is our claim that design
theories and methods can provide, at least to some extent, the much-needed
framework. We will use a worldwide data-science challenge organized to study a
technical problem in physics, namely the detection of Higgs boson, as a use
case to demonstrate some of the ways in which design theory and methods can
help in analyzing and shaping the innovation dynamics in such projects.
</summary>
    <author>
      <name>Akin Osman</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CGS</arxiv:affiliation>
    </author>
    <author>
      <name>Kazakçi Mines</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CGS</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Engineering Design, Jul 2015, Milan,
  Italy</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.06201v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.06201v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.08019v2</id>
    <updated>2016-03-24T04:37:16Z</updated>
    <published>2015-03-27T10:52:40Z</published>
    <title>Optimality of Fast Matching Algorithms for Random Networks with
  Applications to Structural Controllability</title>
    <summary>  Network control refers to a very large and diverse set of problems including
controllability of linear time-invariant dynamical systems, where the objective
is to select an appropriate input to steer the network to a desired state.
There are many notions of controllability, one of them being structural
controllability, which is intimately connected to finding maximum matchings on
the underlying network topology. In this work, we study fast, scalable
algorithms for finding maximum matchings for a large class of random networks.
First, we illustrate that degree distribution random networks are realistic
models for real networks in terms of structural controllability. Subsequently,
we analyze a popular, fast and practical heuristic due to Karp and Sipser as
well as a simplification of it. For both heuristics, we establish asymptotic
optimality and provide results concerning the asymptotic size of maximum
matchings for an extensive class of random networks.
</summary>
    <author>
      <name>Mohamad Kazem Shirani Faradonbeh</name>
    </author>
    <author>
      <name>Ambuj Tewari</name>
    </author>
    <author>
      <name>George Michailidis</name>
    </author>
    <link href="http://arxiv.org/abs/1503.08019v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.08019v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.08278v1</id>
    <updated>2015-03-28T07:41:16Z</updated>
    <published>2015-03-28T07:41:16Z</published>
    <title>Modeling population structure under hierarchical Dirichlet processes</title>
    <summary>  We propose a Bayesian nonparametric model to infer population admixture,
extending the Hierarchical Dirichlet Process to allow for correlation between
loci due to Linkage Disequilibrium. Given multilocus genotype data from a
sample of individuals, the model allows inferring classifying individuals as
unadmixed or admixed, inferring the number of subpopulations ancestral to an
admixed population and the population of origin of chromosomal regions. Our
model does not assume any specific mutation process and can be applied to most
of the commonly used genetic markers. We present a MCMC algorithm to perform
posterior inference from the model and discuss methods to summarise the MCMC
output for the analysis of population admixture. We demonstrate the performance
of the proposed model in simulations and in a real application, using genetic
data from the EDAR gene, which is considered to be ancestry-informative due to
well-known variations in allele frequency as well as phenotypic effects across
ancestry. The structure analysis of this dataset leads to the identification of
a rare haplotype in Europeans.
</summary>
    <author>
      <name>M. De Iorio</name>
    </author>
    <author>
      <name>L. T. Elliott</name>
    </author>
    <author>
      <name>S. Favaro</name>
    </author>
    <author>
      <name>K. Adhikari</name>
    </author>
    <author>
      <name>Y. W. Teh</name>
    </author>
    <link href="http://arxiv.org/abs/1503.08278v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.08278v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.07336v1</id>
    <updated>2015-04-28T03:09:52Z</updated>
    <published>2015-04-28T03:09:52Z</published>
    <title>Information content of partially rank-ordered set samples</title>
    <summary>  Partially rank-ordered set (PROS) sampling is a generalization of ranked set
sampling in which rankers are not required to fully rank the sampling units in
each set, hence having more flexibility to perform the necessary judgemental
ranking process. The PROS sampling has a wide range of applications in
different fields ranging from environmental and ecological studies to medical
research and it has been shown to be superior over ranked set sampling and
simple random sampling for estimating the population mean. In this paper, we
study the Fisher information content and uncertainty structure of the PROS
samples and compare them with those of simple random sample (SRS) and ranked
set sample (RSS) counterparts of the same size from the underlying population.
We study the uncertainty structure in terms of the Shannon entropy, Renyi
entropy and Kullback-Leibler (KL) discrimination measures. Several examples
including the FI of PROS samples from the location-scale family of
distributions as well as a regression model are discussed.
</summary>
    <author>
      <name>Armin Hatefi</name>
    </author>
    <author>
      <name>Mohammad Jafari Jozani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.07336v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.07336v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62B10, 62D05, 62E15, 62F99, 62J99" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.06354v1</id>
    <updated>2015-05-23T17:34:15Z</updated>
    <published>2015-05-23T17:34:15Z</published>
    <title>Online Updating of Statistical Inference in the Big Data Setting</title>
    <summary>  We present statistical methods for big data arising from online analytical
processing, where large amounts of data arrive in streams and require fast
analysis without storage/access to the historical data. In particular, we
develop iterative estimating algorithms and statistical inferences for linear
models and estimating equations that update as new data arrive. These
algorithms are computationally efficient, minimally storage-intensive, and
allow for possible rank deficiencies in the subset design matrices due to
rare-event covariates. Within the linear model setting, the proposed
online-updating framework leads to predictive residual tests that can be used
to assess the goodness-of-fit of the hypothesized model. We also propose a new
online-updating estimator under the estimating equation setting. Theoretical
properties of the goodness-of-fit tests and proposed estimators are examined in
detail. In simulation studies and real data applications, our estimator
compares favorably with competing approaches under the estimating equation
setting.
</summary>
    <author>
      <name>Elizabeth D. Schifano</name>
    </author>
    <author>
      <name>Jing Wu</name>
    </author>
    <author>
      <name>Chun Wang</name>
    </author>
    <author>
      <name>Jun Yan</name>
    </author>
    <author>
      <name>Ming-Hui Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Technometrics</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.06354v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.06354v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.04131v1</id>
    <updated>2015-06-12T19:50:33Z</updated>
    <published>2015-06-12T19:50:33Z</published>
    <title>Two Challenges of Stealthy Hypervisors Detection: Time Cheating and Data
  Fluctuations</title>
    <summary>  Hardware virtualization technologies play a significant role in cyber
security. On the one hand these technologies enhance security levels, by
designing a trusted operating system. On the other hand these technologies can
be taken up into modern malware which is rather hard to detect. None of the
existing methods is able to efficiently detect a hypervisor in the face of
countermeasures such as time cheating, temporary self uninstalling, memory
hiding etc. New hypervisor detection methods which will be described in this
paper can detect a hypervisor under these countermeasures and even count
several nested ones. These novel approaches rely on the new statistical
analysis of time discrepancies by examination of a set of instructions, which
are unconditionally intercepted by a hypervisor. Reliability was achieved
through the comprehensive analysis of the collected data despite its
fluctuation. These offered methods were comprehensively assessed in both Intel
and AMD CPUs.
</summary>
    <author>
      <name>Igor Korkin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 7 figures, 8 tables. Paper presented at the Proceedings of
  the 10th Annual Conference on Digital Forensics, Security and Law (CDFSL),
  33-57, Daytona Beach, Florida, USA (2015, May 18-21)</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.04131v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.04131v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.05557v2</id>
    <updated>2015-11-04T06:23:35Z</updated>
    <published>2015-06-18T06:40:57Z</published>
    <title>Exponential Quantum Tsallis Havrda Charvat Entropy of Type Alpha</title>
    <summary>  Entropy is a key measure in studies related to information theory and its
many applications. Campbell of the first time recognized that exponential of
Shannons entropy is just the size of the sample space when the distribution is
uniform. In this paper, we introduce a quantity which is called exponential
Tsallis Havrda Charvat entropy and discuss its some properties. Further, we
gave the application of exponential Tsallis Havrda Charvat entropy in quantum
information theory which is called exponential quantum Tsallis Havrda Charvat
entropy with its some major properties such as non-negative, concavity and
continuity. It is found that projective measurement will not decrease the
quantum entropy of a quantum state and at the end of the paper gave an upper
bound on the quantum exponential entropy in terms of ensembles of pure state.
</summary>
    <author>
      <name>Dhanesh Garg</name>
    </author>
    <author>
      <name>Staish Kumar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, no figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.05557v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.05557v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.MP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="94A15, 94A24, 26D15, 46N50, 46L30, 47L90" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.01404v1</id>
    <updated>2015-07-06T11:56:18Z</updated>
    <published>2015-07-06T11:56:18Z</published>
    <title>A new Universal Resample Stable Bootstrap-based Stopping Criterion in
  PLS Components Construction</title>
    <summary>  We develop a new robust stopping criterion in Partial Least Squares
Regressions (PLSR) components construction characterised by a high level of
stability. This new criterion is defined as a universal one since it is
suitable both for PLSR and its extension to Generalized Linear Regressions
(PLSGLR). This criterion is based on a non-parametric bootstrap process and has
to be computed algorithmically. It allows to test each successive components on
a preset significant level alpha. In order to assess its performances and
robustness with respect to different noise levels, we perform intensive
datasets simulations, with a preset and known number of components to extract,
both in the case n&gt;p (n being the number of subjects and p the number of
original predictors), and for datasets with n&lt;p. We then use t-tests to compare
the performance of our approach to some others classical criteria. The property
of robustness is particularly tested through resampling processes on a real
allelotyping dataset. Our conclusion is that our criterion presents also better
global predictive performances, both in the PLSR and PLSGLR (Logistic and
Poisson) frameworks.
</summary>
    <author>
      <name>Jérémy Magnanensi</name>
    </author>
    <author>
      <name>Frédéric Bertrand</name>
    </author>
    <author>
      <name>Myriam Maumy-Bertrand</name>
    </author>
    <author>
      <name>Nicolas Meyer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages, 20 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.01404v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.01404v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62F40, 62F35" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.02537v2</id>
    <updated>2016-03-08T10:12:28Z</updated>
    <published>2015-07-09T14:52:02Z</published>
    <title>Modeling asymptotically independent spatial extremes based on Laplace
  random fields</title>
    <summary>  We tackle the modeling of threshold exceedances in asymptotically independent
stochastic processes by constructions based on Laplace random fields. These are
defined as Gaussian random fields scaled with a stochastic variable following
an exponential distribution. This framework yields useful asymptotic properties
while remaining statistically convenient. Univariate distribution tails are of
the half exponential type and are part of the limiting generalized Pareto
distributions for threshold exceedances. After normalizing marginal tail
distributions in data, a standard Laplace field can be used to capture spatial
dependence among extremes. Asymptotic properties of Laplace fields are explored
and compared to the classical framework of asymptotic dependence. Multivariate
joint tail decay rates for Laplace fields are slower than for Gaussian fields
with the same covariance structure; hence they provide more conservative
estimates of very extreme joint risks while maintaining asymptotic
independence. Statistical inference is illustrated on extreme wind gusts in the
Netherlands where a comparison to the Gaussian dependence model shows a better
goodness-of-fit in terms of Akaike's criterion. In this specific application we
fit the well-adapted Weibull distribution as univariate tail model, such that
the normalization of univariate tail distributions can be done through a simple
power transformation of data.
</summary>
    <author>
      <name>Thomas Opitz</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.spasta.2016.01.001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.spasta.2016.01.001" rel="related"/>
    <link href="http://arxiv.org/abs/1507.02537v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.02537v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.01365v1</id>
    <updated>2015-09-04T08:24:43Z</updated>
    <published>2015-09-04T08:24:43Z</published>
    <title>Publication Bias in Meta-Analysis: Confidence Intervals for Rosenthal's
  Fail-Safe Number</title>
    <summary>  The purpose of the present paper is to assess the efficacy of confidence
intervals for Rosenthal's fail-safe number. Although Rosenthal's estimator is
highly used by researchers, its statistical properties are largely unexplored.
First of all, we developed statistical theory which allowed us to produce
confidence intervals for Rosenthal's fail-safe number.This was produced by
discerning whether the number of studies analysed in a meta-analysis is fixed
or random. Each case produces different variance estimators. For a given number
of studies and a given distribution, we provided five variance estimators.
Confidence intervals are examined with a normal approximation and a
nonparametric bootstrap. The accuracy of the different confidence interval
estimates was then tested by methods of simulation under different
distributional assumptions. The half normal distribution variance estimator has
the best probability coverage. Finally, we provide a table of lower confidence
intervals for Rosenthal's estimator.
</summary>
    <author>
      <name>Konstantinos C. Fragkos</name>
    </author>
    <author>
      <name>Michail Tsagris</name>
    </author>
    <author>
      <name>Christos C. Frangos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in the International Scholarly Research Notices in December
  2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.01365v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.01365v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="91E99" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.01993v1</id>
    <updated>2015-10-07T15:40:48Z</updated>
    <published>2015-10-07T15:40:48Z</published>
    <title>Sensor Selection for Target Tracking in Wireless Sensor Networks with
  Uncertainty</title>
    <summary>  In this paper, we propose a multiobjective optimization framework for the
sensor selection problem in uncertain Wireless Sensor Networks (WSNs). The
uncertainties of the WSNs result in a set of sensor observations with
insufficient information about the target. We propose a novel mutual
information upper bound (MIUB) based sensor selection scheme, which has low
computational complexity, same as the Fisher information (FI) based sensor
selection scheme, and gives estimation performance similar to the mutual
information (MI) based sensor selection scheme. Without knowing the number of
sensors to be selected a priori, the multiobjective optimization problem (MOP)
gives a set of sensor selection strategies that reveal different trade-offs
between two conflicting objectives: minimization of the number of selected
sensors and minimization of the gap between the performance metric (MIUB and
FI) when all the sensors transmit measurements and when only the selected
sensors transmit their measurements based on the sensor selection strategy.
Illustrative numerical results that provide valuable insights are presented.
</summary>
    <author>
      <name>Nianxia Cao</name>
    </author>
    <author>
      <name>Sora Choi</name>
    </author>
    <author>
      <name>Engin Masazade</name>
    </author>
    <author>
      <name>Pramod K. Varshney</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TSP.2016.2595500</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TSP.2016.2595500" rel="related"/>
    <link href="http://arxiv.org/abs/1510.01993v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.01993v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.00634v3</id>
    <updated>2016-07-12T20:01:31Z</updated>
    <published>2015-11-02T19:12:05Z</published>
    <title>A Simple and Adaptive Dispersion Regression Model for Count Data</title>
    <summary>  Regression for count data is widely performed by models such as Poisson,
negative Binomial and zero-inflated regression models. A challenge often faced
by practitioners is the selection of the right model to take into account
dispersion and excessive zeros, both of which typically occur in count data
sets. It is highly desirable to have a unified model that can automatically
adapt to the underlying dispersion and that can be easily implemented in
practice. In this paper, a discrete Weibull regression model is shown to be
able to adapt to different types of dispersions in a simple way. In particular,
this model has the ability to model data that are both over and under dispersed
relative to Poisson. Additionally, this simple model is shown to be capable of
modelling highly skewed count data with excessive zeros, without the need for
introducing zero-inflated and hurdle components. Maximum likelihood can be used
for efficient parameter estimation. Both simulated examples and real data
analyses illustrate the model and its adaptation to modelling over-dispersion,
under-dispersion and excessive zeros, in comparison to classical regression
approaches for count data.
</summary>
    <author>
      <name>Hadeel S. Kalktawi</name>
    </author>
    <author>
      <name>Veronica Vinciotti</name>
    </author>
    <author>
      <name>Keming Yu</name>
    </author>
    <link href="http://arxiv.org/abs/1511.00634v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.00634v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.02688v1</id>
    <updated>2015-12-08T22:57:58Z</updated>
    <published>2015-12-08T22:57:58Z</published>
    <title>Modelling Hospital length of stay using convolutive mixtures
  distributions</title>
    <summary>  Length of hospital stay (LOS) is an important indicator of the hospital
activity and management of health care. The skewness in the distribution of LOS
poses problems in statistical modelling because it fails to adequately follow
the usual traditional distribution such as the log-normal distribution. The aim
of this work is to model the variable LOS using the convolution of two
distributions; a technique well known in the signal processing community. The
specificity of that model is that the variable of interest is considered to be
the resulting sum of two random variables with different distributions. One of
the variables will feature the patient-related factors in terms their need to
recover from their admission condition, while the other models the hospital
management process such as the discharging process. Two estimation procedures
are proposed. One is the classical maximum likelihood, while the other relates
to the expectation maximisation algorithm. We will present some results
obtained by applying this model to a set of real data from a group of hospitals
in Victoria (Australia).
</summary>
    <author>
      <name>Adrien Ickowicz</name>
    </author>
    <author>
      <name>Ross Sparks</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.02688v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.02688v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.08773v1</id>
    <updated>2015-12-29T20:15:23Z</updated>
    <published>2015-12-29T20:15:23Z</published>
    <title>Hot Hands, Streaks and Coin-flips: Numerical Nonsense in the New York
  Times</title>
    <summary>  The existence of "Hot Hands" and "Streaks" in sports and gambling is hotly
debated, but there is no uncertainty about the recent batting-average of the
New York Times: it is now two-for-two in mangling and misunderstanding
elementary concepts in probability and statistics; and mixing up the key points
in a recent paper that re-examines earlier work on the statistics of streaks.
In so doing, it's high-visibility articles have added to the general-public's
confusion about probability, making it seem mysterious and paradoxical when it
needn't be. However, those articles make excellent case studies on how to get
it wrong, and for discussions in high-school and college classes focusing on
quantitative reasoning, data analysis, probability and statistics. What I have
written here is intended for that audience.
</summary>
    <author>
      <name>Dan Gusfield</name>
    </author>
    <link href="http://arxiv.org/abs/1512.08773v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.08773v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.HO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.HO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60-01 62-01 97A40" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.09016v2</id>
    <updated>2017-02-02T14:55:25Z</updated>
    <published>2015-12-30T17:05:31Z</published>
    <title>Pairwise Markov properties for regression graphs</title>
    <summary>  With a sequence of regressions, one may generate joint probability
distributions. One starts with a joint, marginal distribution of context
variables having possibly a concentration graph structure and continues with an
ordered sequence of conditional distributions, named regressions in joint
responses. The involved random variables may be discrete, continuous or of both
types. Such a generating process specifies for each response a conditioning set
which contains just its regressor variables and it leads to at least one valid
ordering of all nodes in the corresponding regression graph which has three
types of edge; one for undirected dependences among context variables, another
for undirected dependences among joint responses and one for any directed
dependence of a response on a regressor variable. For this regression graph,
there are several definitions of pairwise Markov properties, where each
interprets the conditional independence associated with a missing edge in the
graph in a different way. We explain how these properties arise, prove their
equivalence for compositional graphoids and point at the equivalence of each
one of them to the global Markov property.
</summary>
    <author>
      <name>Kayvan Sadeghi</name>
    </author>
    <author>
      <name>Nanny Wermuth</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1002/sta4.122</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1002/sta4.122" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 1 figure</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">STAT, 5, (2016), 286-294</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1512.09016v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.09016v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.08099v2</id>
    <updated>2016-02-12T13:10:11Z</updated>
    <published>2016-01-29T13:31:27Z</published>
    <title>Chaos in Fractionally Integrated Generalized Autoregressive Conditional
  Heteroskedastic Processes</title>
    <summary>  Fractionally integrated generalized autoregressive conditional
heteroskedasticity (FIGARCH) arises in modeling of financial time series.
FIGARCH is essentially governed by a system of nonlinear stochastic difference
equations ${u_t}$ = ${z_t}$ $(1-\sum\limits_{j=1}^q \beta_j L^j)\sigma_{t}^2 =
\omega+(1-\sum\limits_{j=1}^q \beta_j L^j - (\sum\limits_{k=1}^p \varphi_k L^k)
(1-L)^d) u_t^2$, where $\omega\in$ R, and $\beta_j\in$ R are constant
parameters, $\{u_t\}_{{t\in}^+}$ and $\{\sigma_t\}_{{t\in}^+}$ are the discrete
time real valued stochastic processes which represent FIGARCH (p,d,q) and
stochastic volatility, respectively. Moreover, L is the backward shift
operator, i.e. $L^d u_t \equiv u_{t-d}$ (d is the fractional differencing
parameter 0$&lt;$d$&lt;$1).
  In this work, we have studied the chaoticity properties of FIGARCH (p,d,q)
processes by computing mutual information, correlation dimensions, FNNs (False
Nearest Neighbour), the Lyapunov exponents, and for both the stochastic
difference equation given above and for the financial time series. We have
observed that maximal Lyapunov exponents are negative, therefore, it can be
suggested that FIGARCH (p,d,q) is not deterministic chaotic process.
</summary>
    <author>
      <name>Adil Yilmaz</name>
    </author>
    <author>
      <name>Gazanfer Unal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 8 figures, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.08099v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.08099v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.MF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.MF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="39A50, 37D45, 91G80, 62M10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.01370v1</id>
    <updated>2016-02-03T17:19:13Z</updated>
    <published>2016-02-03T17:19:13Z</published>
    <title>Sensory evaluation of commercial coffee brands in Colombia</title>
    <summary>  Colombian coffee farmers have traditionally focused their efforts on
activities including seeding, planting and drying. Strategic issues to
successfully compete in the industry, such as branding, marketing and consumer
research, have been neglected. In this research, we apply a type of sensory
analysis, based on several statistical techniques used to investigate the key
features of ten different brands of Colombian coffee. A panel composed of 32
judges investigated nine different attributes related to flavour, fragrance,
sweetness and acidity, among others. The last section presents the conclusions
reached regarding customer preference and brands profiles.
</summary>
    <author>
      <name>Edis Mauricio Sanmiguel JaimesRelated</name>
    </author>
    <author>
      <name>Igor Barahona Torres</name>
    </author>
    <author>
      <name>Héctor Hugo Pérez-Villarreal</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1504/IJBSR.2015.071831</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1504/IJBSR.2015.071831" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is a revised and expanded version of a paper entitled.
  Evaluacion sensorial de marcas comerciales de cafe en Colombia. Presented at
  the Sexto Coloquio Interdisciplinario de Doctorado. Universidad Popular
  Autonoma del Estado de Puebla, Puebla City, Mexico, 25 June 2014</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Business and Systems Research (2015)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1602.01370v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.01370v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.07836v2</id>
    <updated>2016-02-27T17:27:49Z</updated>
    <published>2016-02-25T07:57:07Z</published>
    <title>A Bayesian baseline for belief in uncommon events</title>
    <summary>  The plausibility of uncommon events and miracles based on testimony of such
an event has been much discussed. When analyzing the probabilities involved, it
has mostly been assumed that the common events can be taken as data in the
calculations. However, we usually have only testimonies for the common events.
While this difference does not have a significant effect on the inductive part
of the inference, it has a large influence on how one should view the
reliability of testimonies. In this work, a full Bayesian solution is given for
the more realistic case, where one has a large number of testimonies for a
common event and one testimony for an uncommon event. It is seen that, in order
for there to be a large amount of testimonies for a common event, the
testimonies will probably be quite reliable. For this reason, because the
testimonies are quite reliable based on the testimonies for the common events,
the probability for the uncommon event, given a testimony for it, is also
higher. Hence, one should be more open-minded when considering the plausibility
of uncommon events.
</summary>
    <author>
      <name>V. Palonen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.07836v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.07836v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.01844v1</id>
    <updated>2016-04-07T01:28:28Z</updated>
    <published>2016-04-07T01:28:28Z</published>
    <title>Statistical sensitiveness for science</title>
    <summary>  Research often necessitates of samples, yet obtaining large enough samples is
not always possible. When it is, the researcher may use one of two methods for
deciding upon the required sample size: rules-of-thumb, quick yet uncertain,
and estimations for power, mathematically precise yet with the potential to
overestimate or underestimate sample sizes when effect sizes are unknown.
Misestimated sample sizes have negative repercussions in the form of increased
costs, abandoned projects or abandoned publication of non-significant results.
Here I describe a procedure for estimating sample sizes adequate for the
testing approach which is most common in the behavioural, social, and
biomedical sciences, that of tests of significance developed by Fisher. The
procedure focuses on a desired minimum effect size for the research at hand and
finds the minimum sample size required for capturing such effect size as a
statistically significant result. In a similar fashion than power analyses,
sensitiveness analyses can also be extended to finding the minimum effect for a
given sample size a priori as well as to calculating sensitiveness a
posteriori. The article provides a full tutorial for carrying out a
sensitiveness analysis, as well as empirical support via simulation
</summary>
    <author>
      <name>Jose D. Perezgonzalez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 1 figure, 2 tables, 3 supplemental materials</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.01844v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.01844v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07391v1</id>
    <updated>2016-04-23T16:33:08Z</updated>
    <published>2016-04-23T16:33:08Z</published>
    <title>Ubiquity of Benfords law and emergence of the reciprocal distribution</title>
    <summary>  We apply the Law of Total Probability to the construction of scale-invariant
probability distribution functions (pdfs), and require that probability
measures be dimensionless and unitless under a continuous change of scales. If
the scale-change distribution function is scale invariant then the constructed
distribution will also be scale invariant. Repeated application of this
construction on an arbitrary set of (normalizable) pdfs results again in
scale-invariant distributions. The invariant function of this procedure is
given uniquely by the reciprocal distribution, suggesting a kind of
universality. We separately demonstrate that the reciprocal distribution
results uniquely from requiring maximum entropy for size-class distributions
with uniform bin sizes.
</summary>
    <author>
      <name>J. L. Friar</name>
    </author>
    <author>
      <name>T. Goldman</name>
    </author>
    <author>
      <name>J. Perez-Mercader</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.physleta.2016.03.045</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.physleta.2016.03.045" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Physics Letters A 380, 1895 (2016)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1604.07391v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07391v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.05069v1</id>
    <updated>2016-05-17T09:20:49Z</updated>
    <published>2016-05-17T09:20:49Z</published>
    <title>Sobol' indices for problems defined in non-rectangular domains</title>
    <summary>  A novel theoretical and numerical framework for the estimation of Sobol
sensitivity indices for models in which inputs are confined to a
non-rectangular domain (e.g., in presence of inequality constraints) is
developed. Two numerical methods, namely the quadrature integration method
which may be very efficient for problems of low and medium dimensionality and
the MC/QMC estimators based on the acceptance-rejection sampling method are
proposed for the numerical estimation of Sobol sensitivity indices. Several
model test functions with constraints are considered for which analytical
solutions for Sobol sensitivity indices were found. These solutions were used
as benchmarks for verifying numerical estimates. The method is shown to be
general and efficient.
</summary>
    <author>
      <name>S. Kucherenko</name>
    </author>
    <author>
      <name>O. V. Klymenko</name>
    </author>
    <author>
      <name>N. Shah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 16 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.05069v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.05069v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.08753v1</id>
    <updated>2016-05-27T18:46:13Z</updated>
    <published>2016-05-27T18:46:13Z</published>
    <title>Fairly Random: The Impact of Winning the Toss on the Probability of
  Winning</title>
    <summary>  In a competitive sport, every little thing matters. Yet, many sports leave
some large levers out of the reach of the teams, and in the hands of fate. In
cricket, world's second most popular sport by some measures, one such
lever---the toss---has been subject to much recent attention. Using a large
novel dataset of 44,224 cricket matches, we estimate the impact of winning the
toss on the probability of winning. The data suggest that winning the toss
increases the chance of winning by a small ($\sim$ 2.8\%) but significant
margin. The advantage varies heftily and systematically, by how closely matched
the competing teams are, and by playing conditions---tautologically, winning
the toss in conditions where the toss grants a greater advantage, for e.g., in
day and night matches, has a larger impact on the probability of winning.
</summary>
    <author>
      <name>Gaurav Sood</name>
    </author>
    <author>
      <name>Derek Willis</name>
    </author>
    <link href="http://arxiv.org/abs/1605.08753v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.08753v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.00546v1</id>
    <updated>2016-06-02T06:01:14Z</updated>
    <published>2016-06-02T06:01:14Z</published>
    <title>Forecasting wind power - Modeling periodic and non-linear effects under
  conditional heteroscedasticity</title>
    <summary>  In this article we present an approach that enables joint wind speed and wind
power forecasts for a wind park. We combine a multivariate seasonal time
varying threshold autoregressive moving average (TVARMA) model with a power
threshold generalized autoregressive conditional heteroscedastic (power-TGARCH)
model. The modeling framework incorporates diurnal and annual periodicity
modeling by periodic B-splines, conditional heteroscedasticity and a complex
autoregressive structure with non-linear impacts. In contrast to usually
time-consuming estimation approaches as likelihood estimation, we apply a
high-dimensional shrinkage technique. We utilize an iteratively re-weighted
least absolute shrinkage and selection operator (lasso) technique. It allows
for conditional heteroscedasticity, provides fast computing times and
guarantees a parsimonious and regularized specification, even though the
parameter space may be vast. We are able to show that our approach provides
accurate forecasts of wind power at a turbine-specific level for forecasting
horizons of up to 48 h (short- to medium-term forecasts).
</summary>
    <author>
      <name>Florian Ziel</name>
    </author>
    <author>
      <name>Carsten Croonenbroeck</name>
    </author>
    <author>
      <name>Daniel Ambach</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.apenergy.2016.05.111</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.apenergy.2016.05.111" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Applied Energy, 177 (2016) 285-297</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1606.00546v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.00546v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62P12, 62M10, 62J07" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.00770v1</id>
    <updated>2016-06-02T17:11:38Z</updated>
    <published>2016-06-02T17:11:38Z</published>
    <title>Different numerical estimators for main effect global sensitivity
  indices</title>
    <summary>  The variance-based method of global sensitivity indices based on Sobol
sensitivity indices became very popular among practitioners due to its easiness
of interpretation. For complex practical problems computation of Sobol indices
generally requires a large number of function evaluations to achieve reasonable
convergence. Four different direct formulas for computing Sobol main effect
sensitivity indices are compared on a set of test problems for which there are
analytical results. These formulas are based on high-dimensional integrals
which are evaluated using MC and QMC techniques. Direct formulas are also
compared with a different approach based on the so-called double loop
reordering formula. It is found that the double loop reordering (DLR) approach
shows a superior performance among all methods both for models with independent
and dependent variables.
</summary>
    <author>
      <name>Sergei Kucherenko</name>
    </author>
    <author>
      <name>Shufang Song</name>
    </author>
    <link href="http://arxiv.org/abs/1606.00770v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.00770v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.01202v1</id>
    <updated>2016-06-03T17:53:33Z</updated>
    <published>2016-06-03T17:53:33Z</published>
    <title>When Does a Boltzmannian Equilibrium Exist?</title>
    <summary>  The received wisdom in statistical mechanics is that isolated systems, when
left to themselves, approach equilibrium. But under what circumstances does an
equilibrium state exist and an approach to equilibrium take place? In this
paper we address these questions from the vantage point of the long-run
fraction of time definition of Boltzmannian equilibrium that we developed in
two recent papers (Werndl and Frigg 2015a, 2015b). After a short summary of
Boltzmannian statistical mechanics and our definition of equilibrium, we state
an existence theorem which provides general criteria for the existence of an
equilibrium state. We first illustrate how the theorem works with a toy
example, which allows us to illustrate the various elements of the theorem in a
simple setting. After commenting on the ergodic programme, we discuss
equilibria in a number of different gas systems: the ideal gas, the dilute gas,
the Kac gas, the stadium gas, the mushroom gas and the multi-mushroom gas. In
the conclusion we briefly summarise the main points and highlight open
questions.
</summary>
    <author>
      <name>Charlotte Werndl</name>
    </author>
    <author>
      <name>Roman Frigg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Forthcoming in: Daniel Bedingham, Owen Maroney and Christopher
  Timpson (eds.): Quantum Foundations of Statistical Mechanics, Oxford: Oxford
  University Press. arXiv admin note: text overlap with arXiv:1510.02260</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.01202v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.01202v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.MP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.00021v1</id>
    <updated>2016-06-30T20:04:45Z</updated>
    <published>2016-06-30T20:04:45Z</published>
    <title>The Simulator: An Engine to Streamline Simulations</title>
    <summary>  The simulator is an R package that streamlines the process of performing
simulations by creating a common infrastructure that can be easily used and
reused across projects. Methodological statisticians routinely write
simulations to compare their methods to preexisting ones. While developing
ideas, there is a temptation to write "quick and dirty" simulations to try out
ideas. This approach of rapid prototyping is useful but can sometimes backfire
if bugs are introduced. Using the simulator allows one to remove the "dirty"
without sacrificing the "quick." Coding is quick because the statistician
focuses exclusively on those aspects of the simulation that are specific to the
particular paper being written. Code written with the simulator is succinct,
highly readable, and easily shared with others. The modular nature of
simulations written with the simulator promotes code reusability, which saves
time and facilitates reproducibility. The syntax of the simulator leads to
simulation code that is easily human-readable. Other benefits of using the
simulator include the ability to "step in" to a simulation and change one
aspect without having to rerun the entire simulation from scratch, the
straightforward integration of parallel computing into simulations, and the
ability to rapidly generate plots, tables, and reports with minimal effort.
</summary>
    <author>
      <name>Jacob Bien</name>
    </author>
    <link href="http://arxiv.org/abs/1607.00021v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.00021v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.05810v4</id>
    <updated>2017-07-11T17:38:11Z</updated>
    <published>2016-08-20T11:25:08Z</published>
    <title>Unifying Markov Properties for Graphical Models</title>
    <summary>  Several types of graphs with different conditional independence
interpretations --- also known as Markov properties --- have been proposed and
used in graphical models. In this paper we unify these Markov properties by
introducing a class of graphs with four types of edges --- lines, arrows, arcs,
and dotted lines --- and a single separation criterion. We show that
independence structures defined by this class specialize to each of the
previously defined cases, when suitable subclasses of graphs are considered. In
addition, we define a pairwise Markov property for the subclass of chain mixed
graphs which includes chain graphs with the LWF interpretation, as well as
summary graphs (and consequently ancestral graphs). We prove the equivalence of
this pairwise Markov property to the global Markov property for compositional
graphoid independence models.
</summary>
    <author>
      <name>Steffen Lauritzen</name>
    </author>
    <author>
      <name>Kayvan Sadeghi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 Pages, 6 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.05810v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.05810v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.00711v1</id>
    <updated>2016-09-02T19:38:23Z</updated>
    <published>2016-09-02T19:38:23Z</published>
    <title>Generalized Spatial and Spatiotemporal Autoregressive Conditional
  Heteroscedasticity</title>
    <summary>  In this paper, we introduce a new spatial model that incorporates
heteroscedastic variance depending on neighboring locations. The proposed
process is regarded as the spatial equivalent to the temporal autoregressive
conditional heteroscedasticity (ARCH) model. We show additionally how the
introduced spatial ARCH model can be used in spatiotemporal settings. In
contrast to the temporal ARCH model, in which the distribution is known given
the full information set of the prior periods, the distribution is not
straightforward in the spatial and spatiotemporal setting. However, it is
possible to estimate the parameters of the model using the maximum-likelihood
approach. Via Monte Carlo simulations, we demonstrate the performance of the
estimator for a specific spatial weighting matrix. Moreover, we combine the
known spatial autoregressive model with the spatial ARCH model assuming
heteroscedastic errors. Eventually, the proposed autoregressive process is
illustrated using an empirical example. Specifically, we model lung cancer
mortality in 3108 U.S. counties and compare the introduced model with two
benchmark approaches.
</summary>
    <author>
      <name>Philipp Otto</name>
    </author>
    <author>
      <name>Wolfgang Schmid</name>
    </author>
    <author>
      <name>Robert Garthoff</name>
    </author>
    <link href="http://arxiv.org/abs/1609.00711v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.00711v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.09184v2</id>
    <updated>2017-02-01T18:14:43Z</updated>
    <published>2016-10-28T12:19:47Z</published>
    <title>Eigenvector statistics of the product of Ginibre matrices</title>
    <summary>  We develop a method to calculate left-right eigenvector correlations of the
product of $m$ independent $N\times N$ complex Ginibre matrices. For
illustration, we present explicit analytical results for the vector overlap for
a couple of examples for small $m$ and $N$. We conjecture that the integrated
overlap between left and right eigenvectors is given by the formula $O = 1 +
(m/2)(N-1)$ and support this conjecture by analytical and numerical
calculations. We derive an analytical expression for the limiting correlation
density as $N\rightarrow \infty$ for the product of Ginibre matrices as well as
for the product of elliptic matrices. In the latter case, we find that the
correlation function is independent of the eccentricities of the elliptic laws.
</summary>
    <author>
      <name>Zdzisław Burda</name>
    </author>
    <author>
      <name>Bartłomiej J. Spisak</name>
    </author>
    <author>
      <name>Pierpaolo Vivo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.95.022134</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.95.022134" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pag., 8 fig - Typos fixed, minor improvements in presentation</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 95, 022134 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1610.09184v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.09184v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.MP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.08942v5</id>
    <updated>2016-12-14T21:26:10Z</updated>
    <published>2016-11-28T00:23:46Z</published>
    <title>The BIN_COUNTS Constraint: Filtering and Applications</title>
    <summary>  We introduce the BIN_COUNTS constraint, which deals with the problem of
counting the number of decision variables in a set which are assigned values
that lie in given bins. We illustrate a decomposition and a filtering algorithm
that achieves generalised arc consistency. We contrast the filtering power of
these two approaches and we discuss a number of applications. We show that
BIN_COUNTS can be employed to develop a decomposition for the $\chi^2$ test
constraint, a new statistical constraint that we introduce in this work. We
also show how this new constraint can be employed in the context of the
Balanced Academic Curriculum Problem and of the Balanced Nursing Workload
Problem. For both these problems we carry out numerical studies involving our
reformulations. Finally, we present a further application of the $\chi^2$ test
constraint in the context of confidence interval analysis.
</summary>
    <author>
      <name>Roberto Rossi</name>
    </author>
    <author>
      <name>Özgür Akgün</name>
    </author>
    <author>
      <name>Steven Prestwich</name>
    </author>
    <author>
      <name>Armagan Tarim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, working draft</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.08942v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.08942v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.08366v1</id>
    <updated>2017-01-29T11:49:54Z</updated>
    <published>2017-01-29T11:49:54Z</published>
    <title>Faithfulness of Probability Distributions and Graphs</title>
    <summary>  A main question in graphical models and causal inference is whether, given a
probability distribution $P$ (which is usually an underlying distribution of
data), there is a graph (or graphs) to which $P$ is faithful. The main goal of
this paper is to provide a theoretical answer to this problem. We work with
general independence models, which contain probabilistic independence models as
a special case. We exploit a generalization of ordering, called preordering, of
the nodes of (mixed) graphs. This allows us to provide sufficient conditions
for a given independence model to be Markov to a graph with the minimum
possible number of edges, and more importantly, necessary and sufficient
conditions for a given probability distribution to be faithful to a graph. We
present our results for the general case of mixed graphs, but specialize the
definitions and results to the better-known subclasses of undirected
(concentration) and bidirected (covariance) graphs as well as directed acyclic
graphs.
</summary>
    <author>
      <name>Kayvan Sadeghi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.08366v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.08366v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.00308v2</id>
    <updated>2017-09-20T18:30:58Z</updated>
    <published>2017-02-01T15:21:42Z</published>
    <title>Approximate Variational Estimation for a Model of Network Formation</title>
    <summary>  We study an equilibrium model of sequential network formation with
heterogeneous players. The payoffs depend on the number and composition of
direct connections, but also the number of indirect links. We show that the
network formation process is a potential game and in the long run the model
converges to an exponential random graph (ERGM). Since standard
simulation-based inference methods for ERGMs could have exponentially slow
convergence, we propose an alternative deterministic method, based on a
variational approximation of the likelihood. We compute bounds for the
approximation error for a given network size and we prove that our variational
method is asymptotically exact, extending results from the large deviations and
graph limits literature to allow for covariates in the ERGM. A simple Monte
Carlo shows that our deterministic method provides more robust estimates than
standard simulation based inference.
</summary>
    <author>
      <name>Angelo Mele</name>
    </author>
    <author>
      <name>Lingjiong Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">44 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.00308v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.00308v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.05340v1</id>
    <updated>2017-02-17T13:53:15Z</updated>
    <published>2017-02-17T13:53:15Z</published>
    <title>Combinatorics of Distance Covariance: Inclusion-Minimal Maximizers of
  Quasi-Concave Set Functions for Diverse Variable Selection</title>
    <summary>  In this paper we show that the negative sample distance covariance function
is a quasi-concave set function of samples of random variables that are not
statistically independent. We use these properties to propose greedy algorithms
to combinatorially optimize some diversity (low statistical dependence)
promoting functions of distance covariance. Our greedy algorithm obtains all
the inclusion-minimal maximizers of this diversity promoting objective.
Inclusion-minimal maximizers are multiple solution sets of globally optimal
maximizers that are not a proper subset of any other maximizing set in the
solution set. We present results upon applying this approach to obtain diverse
features (covariates/variables/predictors) in a feature selection setting for
regression (or classification) problems. We also combine our diverse feature
selection algorithm with a distance covariance based relevant feature selection
algorithm of [7] to produce subsets of covariates that are both relevant yet
ordered in non-increasing levels of diversity of these subsets.
</summary>
    <author>
      <name>Praneeth Vepakomma</name>
    </author>
    <author>
      <name>Yulia Kempner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.05340v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.05340v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.06176v3</id>
    <updated>2017-05-31T19:34:47Z</updated>
    <published>2017-02-20T21:09:44Z</published>
    <title>MOLIERE: Automatic Biomedical Hypothesis Generation System</title>
    <summary>  Hypothesis generation is becoming a crucial time-saving technique which
allows biomedical researchers to quickly discover implicit connections between
important concepts. Typically, these systems operate on domain-specific
fractions of public medical data. MOLIERE, in contrast, utilizes information
from over 24.5 million documents. At the heart of our approach lies a
multi-modal and multi-relational network of biomedical objects extracted from
several heterogeneous datasets from the National Center for Biotechnology
Information (NCBI). These objects include but are not limited to scientific
papers, keywords, genes, proteins, diseases, and diagnoses. We model hypotheses
using Latent Dirichlet Allocation applied on abstracts found near shortest
paths discovered within this network, and demonstrate the effectiveness of
MOLIERE by performing hypothesis generation on historical data. Our network,
implementation, and resulting data are all publicly available for the broad
scientific community.
</summary>
    <author>
      <name>Justin Sybrandt</name>
    </author>
    <author>
      <name>Michael Shtutman</name>
    </author>
    <author>
      <name>Ilya Safro</name>
    </author>
    <link href="http://arxiv.org/abs/1702.06176v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.06176v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8; J.3; H.3; I.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.03924v2</id>
    <updated>2017-09-12T13:40:54Z</updated>
    <published>2017-04-12T20:45:38Z</published>
    <title>A Tutorial on Kernel Density Estimation and Recent Advances</title>
    <summary>  This tutorial provides a gentle introduction to kernel density estimation
(KDE) and recent advances regarding confidence bands and geometric/topological
features. We begin with a discussion of basic properties of KDE: the
convergence rate under various metrics, density derivative estimation, and
bandwidth selection. Then, we introduce common approaches to the construction
of confidence intervals/bands, and we discuss how to handle bias. Next, we talk
about recent advances in the inference of geometric and topological features of
a density function using KDE. Finally, we illustrate how one can use KDE to
estimate a cumulative distribution function and a receiver operating
characteristic curve. We provide R implementations related to this tutorial at
the end.
</summary>
    <author>
      <name>Yen-Chi Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A tutorial paper; accepted to Biostatistics &amp; Epidemiology. Main
  article: 26 pages, 8 figures. R implementations: 11 pages, generated by
  Rmarkdown</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.03924v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.03924v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.05630v1</id>
    <updated>2017-04-19T06:56:54Z</updated>
    <published>2017-04-19T06:56:54Z</published>
    <title>Classical and bayesian componentwise predictors for non-compact
  correlated ARH(1) processes</title>
    <summary>  A special class of standard Gaussian Autoregressive Hilbertian processes of
order one (Gaussian ARH(1) processes), with bounded linear autocorrelation
operator, which does not satisfy the usual Hilbert-Schmidt assumption, is
considered. To compensate the slow decay of the diagonal coefficients of the
autocorrelation operator, a faster decay velocity of the eigenvalues of the
trace autocovariance operator of the innovation process is assumed. As usual,
the eigenvectors of the autocovariance operator of the ARH(1) process are
considered for projection, since, here, they are assumed to be known. Diagonal
componentwise classical and bayesian estimation of the autocorrelation operator
is studied for prediction. The asymptotic efficiency and equivalence of both
estimators is proved, as well as of their associated componentwise ARH(1)
plugin predictors. A simulation study is undertaken to illustrate the
theoretical results derived.
</summary>
    <author>
      <name>M. Dolores Ruiz-Medina</name>
    </author>
    <author>
      <name>J. Álvarez-Liébana</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted. 31 pages: 6 figures are included</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.05630v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.05630v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60G10, 60G15, 60F99, 60J05, 65F15" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.06828v2</id>
    <updated>2017-07-02T01:43:40Z</updated>
    <published>2017-05-18T23:10:37Z</published>
    <title>Agent-based simulation of the learning dissemination on a Project-Based
  Learning context considering the human aspects</title>
    <summary>  This work presents an agent-based simulation (ABS) of the active learning
process in an Electrical Engineering course. In order to generate input data to
the simulation, an active learning methodology developed especially for
part-time degree courses, called Project-Based Learning Agile (PBLA), has been
proposed and implemented at the Regional University of Blumenau (FURB), Brazil.
Through the analysis of survey responses obtained over five consecutive
semesters, using partial least squares path modeling (PLS-PM), it was possible
to generate data parameters to use as an input in a hybrid kind of agent-based
simulation known as PLS agent. The simulation of the scenario suggests that the
learning occur faster when the student has higher levels of humanist's aspects
as self-esteem, self-realization and cooperation.
</summary>
    <author>
      <name>Laio Oriel Seman</name>
    </author>
    <author>
      <name>Romeu Hausmann</name>
    </author>
    <author>
      <name>Eduardo Augusto Bezerra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 6 figures, minor corrections</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.06828v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.06828v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.08082v1</id>
    <updated>2017-05-23T05:26:26Z</updated>
    <published>2017-05-23T05:26:26Z</published>
    <title>An Investigation of the Different Levels of Poverty and the
  Corresponding Variance in Student Academic Prosperity</title>
    <summary>  Underprivileged students, especially in primary school, have shown to have
less access to educational materials often resulting in general dissatisfaction
in the school system and lower academic performance (Saatcioglu and Rury, 2012,
p.23). The relationship between family socioeconomic status and student
interest in academic endeavors, level of classroom engagement, and
participation in extracurricular programs were analyzed. Socioeconomic status
was categorized as below poverty level, at or above poverty level, 100 to 199
percent of poverty, and 200 percent of poverty or higher (United States Census
Bureau). Student interest, engagement, and persistence were measured as a
scalar quantity of three variables: never, sometimes, and often. The
participation of students in extracurricular activities was also compared based
on the same categories of socioeconomic status. After running the multivariate
analysis of variance, it was found that there was a statistically significant
variance of student academic prosperity and poverty level.
</summary>
    <author>
      <name>Sebastian Del Barco</name>
    </author>
    <author>
      <name>Erast Davidjuk</name>
    </author>
    <link href="http://arxiv.org/abs/1705.08082v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.08082v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.06498v1</id>
    <updated>2017-06-20T14:50:55Z</updated>
    <published>2017-06-20T14:50:55Z</published>
    <title>Asymptotic properties of a componentwise ARH(1) plug-in predictor</title>
    <summary>  This paper presents new results on prediction of linear processes in function
spaces. The autoregressive Hilbertian process framework of order one (ARH(1)
process framework) is adopted. A componentwise estimator of the autocorrelation
operator is formulated, from the moment-based estimation of its diagonal
coefficients, with respect to the orthogonal eigenvectors of the
auto-covariance operator, which are assumed to be known. Mean-square
convergence to the theoretical autocorrelation operator, in the space of
Hilbert-Schmidt operators, is proved. Consistency then follows in that space.
For the associated ARH(1) plug-in predictor, mean absolute convergence to the
corresponding conditional expectation, in the considered Hilbert space, is
obtained. Hence, consistency in that space also holds. A simulation study is
undertaken to illustrate the finite-large sample behavior of the formulated
componentwise estimator and predictor. The performance of the presented
approach is compared with alternative approaches in the previous and current
ARH(1) framework literature, including the case of unknown eigenvectors.
</summary>
    <author>
      <name>J. Álvarez-Liébana</name>
    </author>
    <author>
      <name>D. Bosq</name>
    </author>
    <author>
      <name>M. Dolores Ruiz-Medina</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.jmva.2016.11.009</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.jmva.2016.11.009" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">46 pages (with 4 figures)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Multivariate Analysis, 155, pp. 12-34 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1706.06498v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.06498v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.FA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="11C20, 11H55, 11M50, 15A24, 15A63, 15A69, 34L05, 60B12, 60G10,&#10;  60G15, 60G25, 60H25, 62M10, 62M15, 62M20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.09076v1</id>
    <updated>2017-07-28T00:06:44Z</updated>
    <published>2017-07-28T00:06:44Z</published>
    <title>Sensitivity Analysis for Unmeasured Confounding in Meta-Analyses</title>
    <summary>  Random-effects meta-analyses of observational studies can produce biased
estimates if the synthesized studies are subject to unmeasured confounding. We
propose sensitivity analyses quantifying the extent to which unmeasured
confounding of specified magnitude could reduce to below a certain threshold
the proportion of true effect sizes that are scientifically meaningful. We also
develop converse methods to estimate the strength of confounding capable of
reducing the proportion of scientifically meaningful true effects to below a
chosen threshold. These methods apply when a "bias factor" is assumed to be
normally distributed across studies or is assessed across a range of fixed
values. Our estimators are derived using recently proposed sharp bounds on
confounding bias within a single study that do not make assumptions regarding
the unmeasured confounders themselves or the functional form of their
relationships to the exposure and outcome of interest. We provide an R package,
ConfoundedMeta, and a freely available online graphical user interface that
compute point estimates and inference and produce plots for conducting such
sensitivity analyses. These methods facilitate principled use of random-effects
meta-analyses of observational studies to assess the strength of causal
evidence for a hypothesis.
</summary>
    <author>
      <name>Maya B. Mathur</name>
    </author>
    <author>
      <name>Tyler J. VanderWeele</name>
    </author>
    <link href="http://arxiv.org/abs/1707.09076v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.09076v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.00190v1</id>
    <updated>2017-09-30T12:36:17Z</updated>
    <published>2017-09-30T12:36:17Z</published>
    <title>Power analysis for a linear regression model when regressors are matrix
  sampled</title>
    <summary>  Multiple matrix sampling is a survey methodology technique that randomly
chooses a relatively small subset of items to be presented to survey
respondents for the purpose of reducing respondent burden. The data produced
are missing completely at random (MCAR), and special missing data techniques
should be used in linear regression and other multivariate statistical
analysis. We derive asymptotic variances of regression parameter estimates that
allow us to conduct power analysis for linear regression models fit to the data
obtained via a multiple matrix sampling design. The ideas are demonstrated with
a variation of the Big Five Inventory of psychological traits. An exploration
of the regression parameter space demonstrates instability of the sample size
requirements, and substantial losses of precision with matrix-sampled
regressors. A simulation with non-normal data demonstrates the advantages of a
semi-parametric multiple imputation scheme.
</summary>
    <author>
      <name>Stanislav Kolenikov</name>
    </author>
    <author>
      <name>Heather Hammer</name>
    </author>
    <link href="http://arxiv.org/abs/1710.00190v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.00190v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62D05, 91CXX" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.5831v3</id>
    <updated>2013-11-26T16:54:55Z</updated>
    <published>2010-06-30T11:10:09Z</published>
    <title>Statistical Inference in Dynamic Treatment Regimes</title>
    <summary>  Dynamic treatment regimes are of growing interest across the clinical
sciences as these regimes provide one way to operationalize and thus inform
sequential personalized clinical decision making. A dynamic treatment regime is
a sequence of decision rules, with a decision rule per stage of clinical
intervention; each decision rule maps up-to-date patient information to a
recommended treatment. We briefly review a variety of approaches for using data
to construct the decision rules. We then review an interesting challenge, that
of nonregularity that often arises in this area. By nonregularity, we mean the
parameters indexing the optimal dynamic treatment regime are nonsmooth
functionals of the underlying generative distribution.
  A consequence is that no regular or asymptotically unbiased estimator of
these parameters exists. Nonregularity arises in inference for parameters in
the optimal dynamic treatment regime; we illustrate the effect of nonregularity
on asymptotic bias and via sensitivity of asymptotic, limiting, distributions
to local perturbations. We propose and evaluate a locally consistent Adaptive
Confidence Interval (ACI) for the parameters of the optimal dynamic treatment
regime. We use data from the Adaptive Interventions for Children with ADHD
study as an illustrative example. We conclude by highlighting and discussing
emerging theoretical problems in this area.
</summary>
    <author>
      <name>Eric B. Laber</name>
    </author>
    <author>
      <name>Min Qian</name>
    </author>
    <author>
      <name>Dan J. Lizotte</name>
    </author>
    <author>
      <name>William E. Pelham</name>
    </author>
    <author>
      <name>Susan A. Murphy</name>
    </author>
    <link href="http://arxiv.org/abs/1006.5831v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.5831v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="47N30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1101.4207v2</id>
    <updated>2012-03-02T02:27:28Z</updated>
    <published>2011-01-21T19:21:40Z</published>
    <title>Blind Channel Estimation for Amplify-and-Forward Two-Way Relay Networks
  Employing M-PSK Modulation</title>
    <summary>  We consider the problem of channel estimation for amplify-and-forward (AF)
two-way relay networks (TWRNs). Most works on this problem focus on pilot-based
approaches which impose a significant training overhead that reduces the
spectral efficiency of the system. To avoid such losses, this work proposes
blind channel estimation algorithms for AF TWRNs that employ constant-modulus
(CM) signaling. Our main algorithm is based on the deterministic maximum
likelihood (DML) approach. Assuming M-PSK modulation, we show that the
resulting estimator is consistent and approaches the true channel with high
probability at high SNR for modulation orders higher than 2. For BPSK, however,
the DML performs poorly and we propose an alternative algorithm that performs
much better by taking into account the BPSK structure of the data symbols. For
comparative purposes, we also investigate the Gaussian maximum-likelihood (GML)
approach which treats the data symbols as Gaussian-distributed nuisance
parameters. We derive the Cramer-Rao bound and use Monte-Carlo simulations to
investigate the mean squared error (MSE) performance of the proposed
algorithms. We also compare the symbol-error rate (SER) performance of the DML
algorithm with that of the training-based least-squares (LS) algorithm and
demonstrate that the DML offers a superior tradeoff between accuracy and
spectral efficiency.
</summary>
    <author>
      <name>Saeed Abdallah</name>
    </author>
    <author>
      <name>Ioannis N. Psaromiligkos</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TSP.2012.2193577</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TSP.2012.2193577" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1101.4207v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1101.4207v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.2994v1</id>
    <updated>2011-06-15T15:20:44Z</updated>
    <published>2011-06-15T15:20:44Z</published>
    <title>Widely Linear vs. Conventional Subspace-Based Estimation of SIMO
  Flat-Fading Channels: Mean-Squared Error Analysis</title>
    <summary>  We analyze the mean-squared error (MSE) performance of widely linear (WL) and
conventional subspace-based channel estimation for single-input multiple-output
(SIMO) flat-fading channels employing binary phase-shift-keying (BPSK)
modulation when the covariance matrix is estimated using a finite number of
samples. The conventional estimator suffers from a phase ambiguity that reduces
to a sign ambiguity for the WL estimator. We derive closed-form expressions for
the MSE of the two estimators under four different ambiguity resolution
scenarios. The first scenario is optimal resolution, which minimizes the
Euclidean distance between the channel estimate and the actual channel. The
second scenario assumes that a randomly chosen coefficient of the actual
channel is known and the third assumes that the one with the largest magnitude
is known. The fourth scenario is the more realistic case where pilot symbols
are used to resolve the ambiguities. Our work demonstrates that there is a
strong relationship between the accuracy of ambiguity resolution and the
relative performance of WL and conventional subspace-based estimators, and
shows that the less information available about the actual channel for
ambiguity resolution, or the lower the accuracy of this information, the higher
the performance gap in favor of the WL estimator.
</summary>
    <author>
      <name>Saeed Abdallah</name>
    </author>
    <author>
      <name>Ioannis N. Psaromiligkos</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TSP.2011.2177261</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TSP.2011.2177261" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1106.2994v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.2994v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.0294v1</id>
    <updated>2011-09-01T20:18:59Z</updated>
    <published>2011-09-01T20:18:59Z</published>
    <title>Extreme value and record statistics in heavy-tailed processes with
  long-range memory</title>
    <summary>  Extreme events are an important theme in various areas of science because of
their typically devastating effects on society and their scientific
complexities. The latter is particularly true if the underlying dynamics does
not lead to independent extreme events as often observed in natural systems.
Here, we focus on this case and consider stationary stochastic processes that
are characterized by long-range memory and heavy-tailed distributions, often
called fractional L\'evy noise. While the size distribution of extreme events
is not affected by the long-range memory in the asymptotic limit and remains a
Fr\'echet distribution, there are strong finite-size effects if the memory
leads to persistence in the underlying dynamics. Moreover, we show that this
persistence is also present in the extreme events, which allows one to make a
time-dependent hazard assessment of future extreme events based on events
observed in the past. This has direct applications in the field of space
weather as we discuss specifically for the case of the solar power influx into
the magnetosphere. Finally, we show how the statistics of records, or
record-breaking extreme events, is affected by the presence of long-range
memory.
</summary>
    <author>
      <name>Aicko Yves Schumann</name>
    </author>
    <author>
      <name>Nicholas R. Moloney</name>
    </author>
    <author>
      <name>Jörn Davidsen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1029/2011GM001088</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1029/2011GM001088" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 20 figures, accepted for publication in AGU Monographs:
  Complexity and Extreme Events in Geoscience</arxiv:comment>
    <link href="http://arxiv.org/abs/1109.0294v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.0294v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.geo-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.space-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.4371v5</id>
    <updated>2015-03-06T01:32:13Z</updated>
    <published>2011-09-20T17:27:39Z</published>
    <title>High dimensional Bayesian inference for Gaussian directed acyclic graph
  models</title>
    <summary>  In this paper, we consider Gaussian models Markov with respect to an
arbitrary DAG. We first construct a family of conjugate priors for the Cholesky
parametrization of the covariance matrix of such models. This family has as
many shape parameters as the DAG has vertices, and naturally extends the work
of Geiger and Heckerman [8]. From these distributions, we derive prior
distributions for the covariance and precision parameters of the Gaussian DAG
Markov models. Our works thus extends the work of Dawid and Lauritzen [5] and
Letac and Massam [16] for Gaussian models Markov with respect to a decomposable
graph to arbitrary DAGs. For this reason, we call our distributions DAG-Wishart
distributions. An advantage of these distributions is that they possess strong
hyper Markov properties and thus allow for explicit estimation of the
covariance and precision parameters, regardless of the dimension of the
problem. They also allow us to develop methodology for model selection and
covariance estimation in the space of DAG-Markov models. We demonstrate via
several numerical examples that the proposed method scales well to
high-dimensions.
</summary>
    <author>
      <name>Emanuel Ben-David</name>
    </author>
    <author>
      <name>Tianxi Li</name>
    </author>
    <author>
      <name>Helene Massam</name>
    </author>
    <author>
      <name>Bala Rajaratnam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">55 pages, 8 figures, 12 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1109.4371v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.4371v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62-09, 62E10, 62J05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.1561v1</id>
    <updated>2012-04-06T20:45:19Z</updated>
    <published>2012-04-06T20:45:19Z</published>
    <title>The macroeconomic effect of the information and communication technology
  in Hungary</title>
    <summary>  It was not until the beginning of the 1990s that the effects of information
and communication technology on economic growth as well as on the profitability
of enterprises raised the interest of researchers. After giving a general
description on the relationship between a more intense use of ICT devices and
dynamic economic growth, the author identified and explained those four
channels that had a robust influence on economic growth and productivity. When
comparing the use of information technonology devices in developed as well as
in developing countries, the author highlighted the importance of the available
additional human capital and the elimination of organizational inflexibilities
in the attempt of narrowing the productivity gap between the developed and
developing nations. By processing a large quantitiy of information gained from
Hungarian enterprises operating in several economic sectors, the author made an
attempt to find a strong correlation between the development level of using ICT
devices and profitability together with total factor productivity. Although the
impact of using ICT devices cannot be measured unequivocally at the
microeconomic level because of certain statistical and methodological
imperfections, by applying such analytical methods as cluster analysis and
correlation and regression calculation, the author managed to prove that both
the correlation coefficient and the gradient of the regression trend line
showed a positive relationship between the extensive use of information and
communication technology and the profitability of enterprises.
</summary>
    <author>
      <name>Peter Sasvari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Advanced Computer Science and
  Applications (IJACSA), Vol. 2, No. 12, 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1204.1561v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.1561v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.5635v2</id>
    <updated>2012-12-05T12:20:17Z</updated>
    <published>2012-04-25T12:11:40Z</published>
    <title>Locally Most Powerful Invariant Tests for Correlation and Sphericity of
  Gaussian Vectors</title>
    <summary>  In this paper we study the existence of locally most powerful invariant tests
(LMPIT) for the problem of testing the covariance structure of a set of
Gaussian random vectors. The LMPIT is the optimal test for the case of close
hypotheses, among those satisfying the invariances of the problem, and in
practical scenarios can provide better performance than the typically used
generalized likelihood ratio test (GLRT). The derivation of the LMPIT usually
requires one to find the maximal invariant statistic for the detection problem
and then derive its distribution under both hypotheses, which in general is a
rather involved procedure. As an alternative, Wijsman's theorem provides the
ratio of the maximal invariant densities without even finding an explicit
expression for the maximal invariant. We first consider the problem of testing
whether a set of $N$-dimensional Gaussian random vectors are uncorrelated or
not, and show that the LMPIT is given by the Frobenius norm of the sample
coherence matrix. Second, we study the case in which the vectors under the null
hypothesis are uncorrelated and identically distributed, that is, the
sphericity test for Gaussian vectors, for which we show that the LMPIT is given
by the Frobenius norm of a normalized version of the sample covariance matrix.
Finally, some numerical examples illustrate the performance of the proposed
tests, which provide better results than their GLRT counterparts.
</summary>
    <author>
      <name>D. Ramírez</name>
    </author>
    <author>
      <name>J. Vía</name>
    </author>
    <author>
      <name>I. Santamaría</name>
    </author>
    <author>
      <name>L. L. Scharf</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TIT.2012.2232705</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TIT.2012.2232705" rel="related"/>
    <link href="http://arxiv.org/abs/1204.5635v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.5635v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.3217v2</id>
    <updated>2013-02-06T18:58:00Z</updated>
    <published>2012-05-14T23:01:28Z</published>
    <title>A Generalized Fellegi-Sunter Framework for Multiple Record Linkage With
  Application to Homicide Record Systems</title>
    <summary>  We present a probabilistic method for linking multiple datafiles. This task
is not trivial in the absence of unique identifiers for the individuals
recorded. This is a common scenario when linking census data to coverage
measurement surveys for census coverage evaluation, and in general when
multiple record-systems need to be integrated for posterior analysis. Our
method generalizes the Fellegi-Sunter theory for linking records from two
datafiles and its modern implementations. The multiple record linkage goal is
to classify the record K-tuples coming from K datafiles according to the
different matching patterns. Our method incorporates the transitivity of
agreement in the computation of the data used to model matching probabilities.
We use a mixture model to fit matching probabilities via maximum likelihood
using the EM algorithm. We present a method to decide the record K-tuples
membership to the subsets of matching patterns and we prove its optimality. We
apply our method to the integration of three Colombian homicide record systems
and we perform a simulation study in order to explore the performance of the
method under measurement error and different scenarios. The proposed method
works well and opens some directions for future research.
</summary>
    <author>
      <name>Mauricio Sadinle</name>
    </author>
    <author>
      <name>Stephen E. Fienberg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Several changes with respect to previous version. Accepted in the
  Journal of the American Statistical Association</arxiv:comment>
    <link href="http://arxiv.org/abs/1205.3217v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.3217v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.5781v4</id>
    <updated>2013-09-28T16:46:31Z</updated>
    <published>2012-07-24T19:32:00Z</published>
    <title>Confidence-based Optimization for the Newsvendor Problem</title>
    <summary>  We introduce a novel strategy to address the issue of demand estimation in
single-item single-period stochastic inventory optimisation problems. Our
strategy analytically combines confidence interval analysis and inventory
optimisation. We assume that the decision maker is given a set of past demand
samples and we employ confidence interval analysis in order to identify a range
of candidate order quantities that, with prescribed confidence probability,
includes the real optimal order quantity for the underlying stochastic demand
process with unknown stationary parameter(s). In addition, for each candidate
order quantity that is identified, our approach can produce an upper and a
lower bound for the associated cost. We apply our novel approach to three
demand distribution in the exponential family: binomial, Poisson, and
exponential. For two of these distributions we also discuss the extension to
the case of unobserved lost sales. Numerical examples are presented in which we
show how our approach complements existing frequentist - e.g. based on maximum
likelihood estimators - or Bayesian strategies.
</summary>
    <author>
      <name>Roberto Rossi</name>
    </author>
    <author>
      <name>Steven Prestwich</name>
    </author>
    <author>
      <name>S. Armagan Tarim</name>
    </author>
    <author>
      <name>Brahim Hnich</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.ejor.2014.06.007</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.ejor.2014.06.007" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Working draft</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">European Journal of Operational Research, Elsevier, Vol.
  239(3):674-684, 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1207.5781v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.5781v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.1184v1</id>
    <updated>2012-11-06T11:27:08Z</updated>
    <published>2012-11-06T11:27:08Z</published>
    <title>DBKGrad: An R Package for Mortality Rates Graduation by Fixed and
  Adaptive Discrete Beta Kernel Techniques</title>
    <summary>  Kernel smoothing represents a useful approach in the graduation of mortality
rates. Though there exist several options for performing kernel smoothing in
statistical software packages, there have been very few contributions to date
that have focused on applications of these techniques in the graduation
context. Also, although it has been shown that the use of a variable or
adaptive smoothing parameter, based on the further information provided by the
exposed to the risk of death, provides additional benefits, specific
computational tools for this approach are essentially absent. Furthermore,
little attention has been given to providing methods in available software for
any kind of subsequent analysis with respect to the graduated mortality rates.
To facilitate analyses in the field, the R package DBKGrad is introduced. Among
the available kernel approaches, it considers a recent discrete beta kernel
estimator, in both its fixed and adaptive variants. In this approach, boundary
bias is automatically reduced and age is pragmatically considered as a discrete
variable. The bandwidth, fixed or adaptive, is allowed to be manually given by
the user or selected by cross-validation. Pointwise confidence intervals, for
each considered age, are also provided. An application to mortality rates from
the Sicily Region (Italy) for the year 2008 is also presented to exemplify the
use of the package.
</summary>
    <author>
      <name>Angelo Mazza</name>
    </author>
    <author>
      <name>Antonio Punzo</name>
    </author>
    <link href="http://arxiv.org/abs/1211.1184v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.1184v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.3748v3</id>
    <updated>2013-05-10T12:10:06Z</updated>
    <published>2013-01-16T16:32:52Z</published>
    <title>Deep Impact: Unintended consequences of journal rank</title>
    <summary>  Most researchers acknowledge an intrinsic hierarchy in the scholarly journals
('journal rank') that they submit their work to, and adjust not only their
submission but also their reading strategies accordingly. On the other hand,
much has been written about the negative effects of institutionalizing journal
rank as an impact measure. So far, contributions to the debate concerning the
limitations of journal rank as a scientific impact assessment tool have either
lacked data, or relied on only a few studies. In this review, we present the
most recent and pertinent data on the consequences of our current scholarly
communication system with respect to various measures of scientific quality
(such as utility/citations, methodological soundness, expert ratings or
retractions). These data corroborate previous hypotheses: using journal rank as
an assessment tool is bad scientific practice. Moreover, the data lead us to
argue that any journal rank (not only the currently-favored Impact Factor)
would have this negative impact. Therefore, we suggest that abandoning journals
altogether, in favor of a library-based scholarly communication system, will
ultimately be necessary. This new system will use modern information technology
to vastly improve the filter, sort and discovery functions of the current
journal system.
</summary>
    <author>
      <name>Björn Brembs</name>
    </author>
    <author>
      <name>Marcus Munafò</name>
    </author>
    <link href="http://arxiv.org/abs/1301.3748v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.3748v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.4628v1</id>
    <updated>2013-01-20T05:11:40Z</updated>
    <published>2013-01-20T05:11:40Z</published>
    <title>Intrinsic posterior regret gamma-minimax estimation for the exponential
  family of distributions</title>
    <summary>  In practice, it is desired to have estimates that are invariant under
reparameterization. The invariance property of the estimators helps to
formulate a unified solution to the underlying estimation problem. In robust
Bayesian analysis, a frequent criticism is that the optimal estimators are not
invariant under smooth reparameterizations. This paper considers the problem of
posterior regret gamma-minimax (PRGM) estimation of the natural parameter of
the exponential family of distributions under intrinsic loss functions. We show
that under the class of Jeffrey's Conjugate Prior (JCP) distributions, PRGM
estimators are invariant to smooth one-to-one reparameterizations. We apply our
results to several distributions and different classes of JCP, as well as the
usual conjugate prior distributions. We observe that, in many cases, invariant
PRGM estimators in the class of JCP distributions can be obtained by some
modifications of PRGM estimators in the usual class of conjugate priors.
  Moreover, when the class of priors are convex or dependant on a
hyper-parameter belonging to a connected set, we show that the PRGM estimator
under the intrinsic loss function could be Bayes with respect to a prior
distribution in the original prior class. Theoretical results are supplemented
with several examples and illustrations.
</summary>
    <author>
      <name>Mohammad Jafari Jozani</name>
    </author>
    <author>
      <name>Nahid Jafari Tabrizi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1214/13-EJS828</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1214/13-EJS828" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.4628v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.4628v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.1232v1</id>
    <updated>2013-05-06T16:03:48Z</updated>
    <published>2013-05-06T16:03:48Z</published>
    <title>Bayesian Modeling and MCMC Computation in Linear Logistic Regression for
  Presence-only Data</title>
    <summary>  Presence-only data are referred to situations in which, given a censoring
mechanism, a binary response can be observed only with respect to on outcome,
usually called \textit{presence}. In this work we present a Bayesian approach
to the problem of presence-only data based on a two levels scheme. A
probability law and a case-control design are combined to handle the double
source of uncertainty: one due to the censoring and one due to the sampling. We
propose a new formalization for the logistic model with presence-only data that
allows further insight into inferential issues related to the model. We
concentrate on the case of the linear logistic regression and, in order to make
inference on the parameters of interest, we present a Markov Chain Monte Carlo
algorithm with data augmentation that does not require the a priori knowledge
of the population prevalence. A simulation study concerning 24,000 simulated
datasets related to different scenarios is presented comparing our proposal to
optimal benchmarks.
</summary>
    <author>
      <name>Fabio Divino</name>
    </author>
    <author>
      <name>Natalia Golini</name>
    </author>
    <author>
      <name>Giovanna Jona Lasinio</name>
    </author>
    <author>
      <name>Antti Penttinen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Affiliations: Fabio Divino - Division of Physics, Computer Science
  and Mathematics, University of Molise Giovanna jona Lasinio and Natalia
  Golini - Department of Statistical Sciences, University of Rome "La Sapienza"
  Antti Penttinen - Department of Mathematics and Statistics, University of
  Jyv\"{a}skyl\"{a} CONTACT: fabio.divino@unimol.it,
  giovanna.jonalasinio@uniroma1.it</arxiv:comment>
    <link href="http://arxiv.org/abs/1305.1232v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.1232v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.3395v1</id>
    <updated>2013-06-14T13:43:40Z</updated>
    <published>2013-06-14T13:43:40Z</published>
    <title>Evolutionary Model of a Anonymous Consumer Durable Market</title>
    <summary>  An analytic model is presented that considers the evolution of a market of
durable goods. The model suggests that after introduction goods spread always
according to a Bass diffusion. However, this phase will be followed by a
diffusion process for durable consumer goods governed by a
variation-selection-reproduction mechanism and the growth dynamics can be
described by a replicator equation. Describing the aggregate sales as the sum
of first, multiple and replacement purchase the product life cycle can be
derived. Replacement purchase causes periodic variations of the sales
determined by the finite lifetime of the good (Juglar cycles). The model
suggests that both, Bass- and Gompertz diffusion may contribute to the product
life cycle of a consumer durable. The theory contains the standard equilibrium
view of a market as a special case. It depends on the time scale, whether an
equilibrium or evolutionary description is more appropriate. The evolutionary
framework is used to derive also the size, growth rate and price distribution
of manufacturing business units. It predicts that the size distribution of the
business units (products) is lognormal, while the growth rates exhibit a
Laplace distribution. Large price deviations from the mean price are also
governed by a Laplace distribution (fat tails). These results are in agreement
with empirical findings. The explicit comparison of the time evolution of
consumer durables with empirical investigations confirms the close relationship
between price decline and Gompertz diffusion, while the product life cycle can
be described qualitatively for a long time period.
</summary>
    <author>
      <name>Joachim Kaldasch</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">preprint</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Physica A 390(2011)2692 2715</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1306.3395v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.3395v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.1185v1</id>
    <updated>2014-03-05T16:42:29Z</updated>
    <published>2014-03-05T16:42:29Z</published>
    <title>Phase transitions in the condition number distribution of Gaussian
  random matrices</title>
    <summary>  We study the statistics of the condition number
$\kappa=\lambda_{\mathrm{max}}/\lambda_{\mathrm{min}}$ (the ratio between
largest and smallest squared singular values) of $N\times M$ Gaussian random
matrices. Using a Coulomb fluid technique, we derive analytically and for large
$N$ the cumulative $\mathcal{P}[\kappa&lt;x]$ and tail-cumulative
$\mathcal{P}[\kappa&gt;x]$ distributions of $\kappa$. We find that these
distributions decay as $\mathcal{P}[\kappa&lt;x]\approx\exp\left(-\beta N^2
\Phi_{-}(x)\right)$ and $\mathcal{P}[\kappa&gt;x]\approx\exp\left(-\beta N
\Phi_{+}(x)\right)$, where $\beta$ is the Dyson index of the ensemble. The left
and right rate functions $\Phi_{\pm}(x)$ are independent of $\beta$ and
calculated exactly for any choice of the rectangularity parameter
$\alpha=M/N-1&gt;0$. Interestingly, they show a weak non-analytic behavior at
their minimum $\langle\kappa\rangle$ (corresponding to the average condition
number), a direct consequence of a phase transition in the associated Coulomb
fluid problem. Matching the behavior of the rate functions around
$\langle\kappa\rangle$, we determine exactly the scale of typical fluctuations
$\sim\mathcal{O}(N^{-2/3})$ and the tails of the limiting distribution of
$\kappa$. The analytical results are in excellent agreement with numerical
simulations.
</summary>
    <author>
      <name>Isaac Pérez Castillo</name>
    </author>
    <author>
      <name>Eytan Katzav</name>
    </author>
    <author>
      <name>Pierpaolo Vivo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.90.050103</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.90.050103" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pag. + 7 pag. Suppl. Material. 3 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1403.1185v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.1185v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.MP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.7091v1</id>
    <updated>2014-05-27T23:49:48Z</updated>
    <published>2014-05-27T23:49:48Z</published>
    <title>Bayesian hierarchical modelling for inferring genetic interactions in
  yeast</title>
    <summary>  Identifying genetic interactions for a given microorganism such as yeast is
difficult. Quantitative Fitness Analysis (QFA) is a high-throughput
experimental and computational methodology for quantifying the fitness of
microbial cultures. QFA can be used to compare between fitness observations for
different genotypes and thereby infer genetic interaction strengths. Current
"naive" frequentist statistical approaches used in QFA do not model
between-genotype variation or difference in genotype variation under different
conditions. In this thesis, a Bayesian approach is introduced to evaluate
hierarchical models that better reflect the structure or design of QFA
experiments. First, a two-stage approach is presented: a hierarchical logistic
model is fitted to microbial culture growth curves and then a hierarchical
interaction model is fitted to fitness summaries inferred for each genotype.
Next, a one-stage Bayesian approach is presented: a joint hierarchical model
which does not require a univariate summary of fitness, used to pass
information between models. The new hierarchical approaches are then compared
using a dataset examining the effect of telomere defects on yeast. By better
describing the experimental structure, new evidence is found for genes and
complexes which interact with the telomere cap. Various extensions of these
models, including models for data transformation, batch effects, and
intrinsically stochastic growth models are also considered.
</summary>
    <author>
      <name>Jonathan Heydari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">All images rasterized. Contact author for non-rasterized and
  searchable Figures. 158 pages, PhD thesis, Newcastle University (2014),
  Institute for Cell &amp; Molecular Biosciences</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.7091v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.7091v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.0758v1</id>
    <updated>2014-05-29T04:33:52Z</updated>
    <published>2014-05-29T04:33:52Z</published>
    <title>Pythagoras at the Bat</title>
    <summary>  The Pythagorean formula is one of the most popular ways to measure the true
ability of a team. It is very easy to use, estimating a team's winning
percentage from the runs they score and allow. This data is readily available
on standings pages; no computationally intensive simulations are needed.
Normally accurate to within a few games per season, it allows teams to
determine how much a run is worth in different situations. This determination
helps solve some of the most important economic decisions a team faces: How
much is a player worth, which players should be pursued, and how much should
they be offered. We discuss the formula and these applications in detail, and
provide a theoretical justification, both for the formula as well as simpler
linear estimators of a team's winning percentage. The calculations and modeling
are discussed in detail, and when possible multiple proofs are given. We
analyze the 2012 season in detail, and see that the data for that and other
recent years support our modeling conjectures. We conclude with a discussion of
work in progress to generalize the formula and increase its predictive power
\emph{without} needing expensive simulations, though at the cost of requiring
play-by-play data.
</summary>
    <author>
      <name>Steven J. Miller</name>
    </author>
    <author>
      <name>Taylor Corcoran</name>
    </author>
    <author>
      <name>Jennifer Gossels</name>
    </author>
    <author>
      <name>Victor Luo</name>
    </author>
    <author>
      <name>Jaclyn Porfilio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Version 1.0, 25 pages, 6 images. This is an older version; a slightly
  updated version will appear in "Social Networks and the Economics of Sports",
  to be published by Springer-Verlag</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.0758v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.0758v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.HO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.HO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.7419v1</id>
    <updated>2014-09-25T21:03:00Z</updated>
    <published>2014-09-25T21:03:00Z</published>
    <title>Identifying the number of clusters in discrete mixture models</title>
    <summary>  Research on cluster analysis for categorical data continues to develop, with
new clustering algorithms being proposed. However, in this context, the
determination of the number of clusters is rarely addressed. In this paper, we
propose a new approach in which clustering of categorical data and the
estimation of the number of clusters is carried out simultaneously. Assuming
that the data originate from a finite mixture of multinomial distributions, we
develop a method to select the number of mixture components based on a minimum
message length (MML) criterion and implement a new expectation-maximization
(EM) algorithm to estimate all the model parameters. The proposed EM-MML
approach, rather than selecting one among a set of pre-estimated candidate
models (which requires running EM several times), seamlessly integrates
estimation and model selection in a single algorithm. The performance of the
proposed approach is compared with other well-known criteria (such as the
Bayesian information criterion-BIC), resorting to synthetic data and to two
real applications from the European Social Survey. The EM-MML computation time
is a clear advantage of the proposed method. Also, the real data solutions are
much more parsimonious than the solutions provided by competing methods, which
reduces the risk of model order overestimation and increases interpretability.
</summary>
    <author>
      <name>Cláudia Silvestre</name>
    </author>
    <author>
      <name>Margarida G. M. S. Cardoso</name>
    </author>
    <author>
      <name>Mário A. T. Figueiredo</name>
    </author>
    <link href="http://arxiv.org/abs/1409.7419v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.7419v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.5772v2</id>
    <updated>2014-12-09T10:58:06Z</updated>
    <published>2014-10-20T07:57:32Z</published>
    <title>Methods for the generation of normalized citation impact scores in
  bibliometrics: Which method best reflects the judgements of experts?</title>
    <summary>  Evaluative bibliometrics compares the citation impact of researchers,
research groups and institutions with each other across time scales and
disciplines. Both factors - discipline and period - have an influence on the
citation count which is independent of the quality of the publication.
Normalizing the citation impact of papers for these two factors started in the
mid-1980s. Since then, a range of different methods have been presented for
producing normalized citation impact scores. The current study uses a data set
of over 50,000 records to test which of the methods so far presented correlate
better with the assessment of papers by peers. The peer assessments come from
F1000Prime - a post-publication peer review system of the biomedical
literature. Of the normalized indicators, the current study involves not only
cited-side indicators, such as the mean normalized citation score, but also
citing-side indicators. As the results show, the correlations of the indicators
with the peer assessments all turn out to be very similar. Since F1000 focuses
on biomedicine, it is important that the results of this study are validated by
other studies based on datasets from other disciplines or (ideally) based on
multi-disciplinary datasets.
</summary>
    <author>
      <name>Lutz Bornmann</name>
    </author>
    <author>
      <name>Werner Marx</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in the Journal of Informetrics</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.5772v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.5772v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.02708v2</id>
    <updated>2015-08-11T13:36:19Z</updated>
    <published>2015-02-09T22:10:19Z</published>
    <title>A comparative review of generalizations of the Gumbel extreme value
  distribution with an application to wind speed data</title>
    <summary>  The generalized extreme value distribution and its particular case, the
Gumbel extreme value distribution, are widely applied for extreme value
analysis. The Gumbel distribution has certain drawbacks because it is a
non-heavy-tailed distribution and is characterized by constant skewness and
kurtosis. The generalized extreme value distribution is frequently used in this
context because it encompasses the three possible limiting distributions for a
normalized maximum of infinite samples of independent and identically
distributed observations. However, the generalized extreme value distribution
might not be a suitable model when each observed maximum does not come from a
large number of observations. Hence, other forms of generalizations of the
Gumbel distribution might be preferable. Our goal is to collect in the present
literature the distributions that contain the Gumbel distribution embedded in
them and to identify those that have flexible skewness and kurtosis, are
heavy-tailed and could be competitive with the generalized extreme value
distribution. The generalizations of the Gumbel distribution are described and
compared using an application to a wind speed data set and Monte Carlo
simulations. We show that some distributions suffer from overparameterization
and coincide with other generalized Gumbel distributions with a smaller number
of parameters, i.e., are non-identifiable. Our study suggests that the
generalized extreme value distribution and a mixture of two extreme value
distributions should be considered in practical applications.
</summary>
    <author>
      <name>E. C. Pinheiro</name>
    </author>
    <author>
      <name>S. L. P. Ferrari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages and 18 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.02708v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.02708v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.00481v1</id>
    <updated>2015-03-02T10:54:56Z</updated>
    <published>2015-03-02T10:54:56Z</published>
    <title>A Reputation Economy: Results from an Empirical Survey on Academic Data
  Sharing</title>
    <summary>  Academic data sharing is a way for researchers to collaborate and thereby
meet the needs of an increasingly complex research landscape. It enables
researchers to verify results and to pursuit new research questions with "old"
data. It is therefore not surprising that data sharing is advocated by funding
agencies, journals, and researchers alike. We surveyed 2661 individual academic
researchers across all disciplines on their dealings with data, their
publication practices, and motives for sharing or withholding research data.
The results for 1564 valid responses show that researchers across disciplines
recognise the benefit of secondary research data for their own work and for
scientific progress as a whole-still they only practice it in moderation. An
explanation for this evidence could be an academic system that is not driven by
monetary incentives, nor the desire for scientific progress, but by individual
reputation-expressed in (high ranked journal) publications. We label this
system a Reputation Economy. This special economy explains our findings that
show that researchers have a nuanced idea how to provide adequate formal
recognition for making data available to others-namely data citations. We
conclude that data sharing will only be widely adopted among research
professionals if sharing pays in form of reputation. Thus, policy measures that
intend to foster research collaboration need to understand academia as a
reputation economy. Successful measures must value intermediate products, such
as research data, more highly than it is the case now.
</summary>
    <author>
      <name>Benedikt Fecher</name>
    </author>
    <author>
      <name>Sascha Friesike</name>
    </author>
    <author>
      <name>Marcel Hebing</name>
    </author>
    <author>
      <name>Stephanie Linek</name>
    </author>
    <author>
      <name>Armin Sauermann</name>
    </author>
    <link href="http://arxiv.org/abs/1503.00481v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.00481v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.02057v1</id>
    <updated>2015-04-08T18:08:57Z</updated>
    <published>2015-04-08T18:08:57Z</published>
    <title>A Singular Value Decomposition-based Factorization and Parsimonious
  Component Model of Demographic Quantities Correlated by Age: Predicting
  Complete Demographic Age Schedules with Few Parameters</title>
    <summary>  BACKGROUND. Formal demography has a long history of building simple models of
age schedules of demographic quantities, e.g. mortality and fertility rates.
These are widely used in demographic methods to manipulate whole age schedules
using few parameters.
  OBJECTIVE. The Singular Value Decomposition (SVD) factorizes a matrix into
three matrices with useful properties including the ability to reconstruct the
original matrix using many fewer, simple matrices. This work demonstrates how
these properties can be exploited to build parsimonious models of whole age
schedules of demographic quantities that can be further parameterized in terms
of arbitrary covariates.
  METHODS. The SVD is presented and explained in detail with attention to
developing an intuitive understanding. The SVD is used to construct a general,
component model of demographic age schedules, and that model is demonstrated
with age-specific mortality and fertility rates. Finally, the model is used (1)
to predict age-specific mortality using HIV indicators and summary measures of
age-specific mortality, and (2) to predict age-specific fertility using the
total fertility rate (TFR).
  RESULTS. The component model of age-specific mortality and fertility rates
succeeds in reproducing the data with two inputs, and acting through those two
inputs, various covariates are able to accurately predict full age schedules.
  CONCLUSIONS. The SVD is potentially useful as a way to summarize, smooth and
model age-specific demographic quantities. The component model is a general
method of relating covariates to whole age schedules.
  COMMENTS. The focus of this work is the SVD and the component model. The
applications are for illustrative purposes only.
</summary>
    <author>
      <name>Samuel J. Clark</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">65 pages, 40 figures, University of Washington Center for Statistics
  and the Social Sciences (CSSS) Working Paper No. 143</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.02057v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.02057v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.03808v2</id>
    <updated>2016-03-18T12:46:43Z</updated>
    <published>2015-08-16T10:30:57Z</published>
    <title>Quantifying information transfer and mediation along causal pathways in
  complex systems</title>
    <summary>  Measures of information transfer have become a popular approach to analyze
interactions in complex systems such as the Earth or the human brain from
measured time series. Recent work has focused on causal definitions of
information transfer excluding effects of common drivers and indirect
influences. While the former clearly constitutes a spurious causality, the aim
of the present article is to develop measures quantifying different notions of
the strength of information transfer along indirect causal paths, based on
first reconstructing the multivariate causal network (\emph{Tigramite}
approach). Another class of novel measures quantifies to what extent different
intermediate processes on causal paths contribute to an interaction mechanism
to determine pathways of causal information transfer. A rigorous mathematical
framework allows for a clear information-theoretic interpretation that can also
be related to the underlying dynamics as proven for certain classes of
processes. Generally, however, estimates of information transfer remain hard to
interpret for nonlinearly intertwined complex systems. But, if experiments or
mathematical models are not available, measuring pathways of information
transfer within the causal dependency structure allows at least for an
abstraction of the dynamics. The measures are illustrated on a climatological
example to disentangle pathways of atmospheric flow over Europe.
</summary>
    <author>
      <name>Jakob Runge</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.92.062829</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.92.062829" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 6 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 92, 062829 (2015)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1508.03808v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.03808v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.05973v2</id>
    <updated>2015-11-04T23:25:37Z</updated>
    <published>2015-08-24T21:05:29Z</published>
    <title>A Review of Nonparametric Hypothesis Tests of Isotropy Properties in
  Spatial Data</title>
    <summary>  An important aspect of modeling spatially-referenced data is appropriately
specifying the covariance function of the random field. A practitioner working
with spatial data is presented a number of choices regarding the structure of
the dependence between observations. One of these choices is determining
whether or not an isotropic covariance function is appropriate. Isotropy
implies that spatial dependence does not depend on the direction of the spatial
separation between sampling locations. Misspecification of isotropy properties
(directional dependence) can lead to misleading inferences, e.g., inaccurate
predictions and parameter estimates. A researcher may use graphical
diagnostics, such as directional sample variograms, to decide whether the
assumption of isotropy is reasonable. These graphical techniques can be
difficult to assess, open to subjective interpretations, and misleading.
Hypothesis tests of the assumption of isotropy may be more desirable. To this
end, a number of tests of directional dependence have been developed using both
the spatial and spectral representations of random fields. We provide an
overview of nonparametric methods available to test the hypotheses of isotropy
and symmetry in spatial data. We summarize test properties, discuss important
considerations and recommendations in choosing and implementing a test, compare
several of the methods via a simulation study, and propose a number of open
research questions. Several of the reviewed methods can be implemented in R
using our package spTest, available on CRAN.
</summary>
    <author>
      <name>Zachary D. Weller</name>
    </author>
    <author>
      <name>Jennifer A. Hoeting</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 6 figures, 9 tables, appendix</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.05973v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.05973v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.06721v1</id>
    <updated>2015-09-22T18:41:43Z</updated>
    <published>2015-09-22T18:41:43Z</published>
    <title>Designed Sampling from Large Databases for Controlled Trials</title>
    <summary>  The increasing prevalence of rich sources of data and the availability of
electronic medical record databases and electronic registries opens tremendous
opportunities for enhancing medical research. For example, controlled trials
are ubiquitously used to investigate the effect of a medical treatment, perhaps
dependent on a set of patient covariates, and traditional approaches have
relied primarily on randomized patient sampling and allocation to treatment and
control group. However, when covariate data for a large cohort group of
patients have already been collected and are available in a database, one can
potentially design a treatment/control sample and allocation that provides far
better estimates of the covariate-dependent effects of the treatment. In this
paper, we develop a new approach that uses optimal design of experiments (DOE)
concepts to accomplish this objective. The approach selects the patients for
the treatment and control samples upfront, based on their covariate values, in
a manner that optimizes the information content in the data. For the optimal
sample selection, we develop simple guidelines and an optimization algorithm
that provides solutions that are substantially better than random sampling.
Moreover, our approach causes no sampling bias in the estimated effects, for
the same reason that DOE principles do not bias estimated effects. We test our
method with a simulation study based on a testbed data set containing
information on the effect of statins on low-density lipoprotein (LDL)
cholesterol.
</summary>
    <author>
      <name>Liwen Ouyang</name>
    </author>
    <author>
      <name>Daniel W. Apley</name>
    </author>
    <author>
      <name>Sanjay Mehrotra</name>
    </author>
    <link href="http://arxiv.org/abs/1509.06721v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.06721v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.04351v2</id>
    <updated>2016-01-10T06:01:34Z</updated>
    <published>2015-11-13T16:40:28Z</published>
    <title>A Scalable Framework for NBA Player and Team Comparisons Using Player
  Tracking Data</title>
    <summary>  The release of NBA player tracking data greatly enhances the granularity and
dimensionality of basketball statistics used to evaluate and compare player
performance. However, the high dimensionality of this new data source can be
troublesome as it demands more computational resources and reduces the ability
to easily interpret findings. Therefore, we must find a way to reduce the
dimensionality of the data while retaining the ability to differentiate and
compare player performance.
  In this paper, Principal Component Analysis (PCA) is used to identify four
principal components that account for 68% of the variation in player tracking
data from the 2013-2014 regular season and intuitive interpretations of these
new dimensions are developed by examining the statistics that influence them
the most. In this new high variance, low dimensional space, you can easily
compare statistical profiles across any or all of the principal component
dimensions to evaluate characteristics that make certain players and teams
similar or unique. A simple measure of similarity between two player or team
statistical profiles based on the four principal component scores is also
constructed. The Statistical Diversity Index (SDI) allows for quick and
intuitive comparisons using the entirety of the player tracking data. As new
statistics emerge, this framework is scalable as it can incorporate existing
and new data sources by reconstructing the principal component dimensions and
SDI for improved comparisons. Using principal component scores and SDI, several
use cases are presented for improved personnel management.
</summary>
    <author>
      <name>Scott Bruce</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3233/JSA-160022</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3233/JSA-160022" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages including figures and appendices</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Sports Analytics 2 (2016) 107-119</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1511.04351v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.04351v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62H25, 62P99, 62-07" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.08819v1</id>
    <updated>2015-12-30T00:30:19Z</updated>
    <published>2015-12-30T00:30:19Z</published>
    <title>Joint limiting laws for high-dimensional independence tests</title>
    <summary>  Testing independence is of significant interest in many important areas of
large-scale inference. Using extreme-value form statistics to test against
sparse alternatives and using quadratic form statistics to test against dense
alternatives are two important testing procedures for high-dimensional
independence. However, quadratic form statistics suffer from low power against
sparse alternatives, and extreme-value form statistics suffer from low power
against dense alternatives with small disturbances and may have size
distortions due to its slow convergence. For real-world applications, it is
important to derive powerful testing procedures against more general
alternatives. Based on intermediate limiting distributions, we derive
(model-free) joint limiting laws of extreme-value form and quadratic form
statistics, and surprisingly, we prove that they are asymptotically
independent. Given such asymptotic independencies, we propose (model-free)
testing procedures to boost the power against general alternatives and also
retain the correct asymptotic size. Under the high-dimensional setting, we
derive the closed-form limiting null distributions, and obtain their explicit
rates of uniform convergence. We prove their consistent statistical powers
against general alternatives. We demonstrate the performance of our proposed
test statistics in simulation studies. Our work provides very helpful insights
to high-dimensional independence tests, and fills an important gap.
</summary>
    <author>
      <name>Danning Li</name>
    </author>
    <author>
      <name>Lingzhou Xue</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.08819v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.08819v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62H12, 60F05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.03434v1</id>
    <updated>2016-02-10T16:33:33Z</updated>
    <published>2016-02-10T16:33:33Z</published>
    <title>Impact of degree truncation on the spread of a contagious process on
  networks</title>
    <summary>  Understanding how person-to-person contagious processes spread through a
population requires accurate information on connections between population
members. However, such connectivity data, when collected via interview, is
often incomplete due to partial recall, respondent fatigue or study design,
e.g., fixed choice designs (FCD) truncate out-degree by limiting the number of
contacts each respondent can report. Past research has shown how FCD truncation
affects network properties, but its implications for predicted speed and size
of spreading processes remain largely unexplored. To study the impact of degree
truncation on spreading processes, we generated collections of synthetic
networks containing specific properties (degree distribution,
degree-assortativity, clustering), and also used empirical social network data
from 75 villages in Karnataka, India. We simulated FCD using various truncation
thresholds and ran a susceptible-infectious-recovered (SIR) process on each
network. We found that spreading processes propagated on truncated networks
resulted in slower and smaller epidemics, with a sudden decrease in prediction
accuracy at a level of truncation that varied by network type. Our results have
implications beyond FCD to truncation due to any limited sampling from a larger
network. We conclude that knowledge of network structure is important for
understanding the accuracy of predictions of process spread on degree truncated
networks.
</summary>
    <author>
      <name>Guy Harling</name>
    </author>
    <author>
      <name>Jukka-Pekka Onnela</name>
    </author>
    <link href="http://arxiv.org/abs/1602.03434v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.03434v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.03926v1</id>
    <updated>2016-02-11T23:02:10Z</updated>
    <published>2016-02-11T23:02:10Z</published>
    <title>Modelling the level of adoption of analytical tools; An implementation
  of multi-criteria evidential reasoning</title>
    <summary>  In the future, competitive advantages will be given to organisations that can
extract valuable information from massive data and make better decisions. In
most cases, this data comes from multiple sources. Therefore, the challenge is
to aggregate them into a common framework in order to make them meaningful and
useful. This paper will first review the most important multi-criteria decision
analysis methods (MCDA) existing in current literature. We will offer a novel,
practical and consistent methodology based on a type of MCDA, to aggregate data
from two different sources into a common framework. Two datasets that are
different in nature but related to the same topic are aggregated to a common
scale by implementing a set of transformation rules. This allows us to generate
appropriate evidence for assessing and finally prioritising the level of
adoption of analytical tools in four types of companies. A numerical example is
provided to clarify the form for implementing this methodology. A six-step
process is offered as a guideline to assist engineers, researchers or
practitioners interested in replicating this methodology in any situation where
there is a need to aggregate and transform multiple source data.
</summary>
    <author>
      <name>Igor Barahona</name>
    </author>
    <author>
      <name>Judith Cavazos</name>
    </author>
    <author>
      <name>Jian-Bo Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Keywords: MCDA methods; evidential reasoning; analytical tools;
  multiple source data</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Supply and Operations Management. (2014)
  Vol.1, Issue 2, pp 129-151</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1602.03926v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.03926v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.00544v2</id>
    <updated>2016-05-30T05:10:20Z</updated>
    <published>2016-03-02T01:34:00Z</published>
    <title>On the capacity of information processing systems</title>
    <summary>  We propose and analyze a family of information processing systems, where a
finite set of experts or servers are employed to extract information about a
stream of incoming jobs. Each job is associated with a hidden label drawn from
some prior distribution. An inspection by an expert produces a noisy outcome
that depends both on the job's hidden label and the type of the expert, and
occupies the expert for a finite time duration. A decision maker's task is to
dynamically assign inspections so that the resulting outcomes can be used to
accurately recover the labels of all jobs, while keeping the system stable.
Among our chief motivations are applications in crowd-sourcing, diagnostics,
and experiment designs, where one wishes to efficiently learn the nature of a
large number of items, using a finite pool of computational resources or human
agents.
  We focus on the capacity of such an information processing system. Given a
level of accuracy guarantee, we ask how many experts are needed in order to
stabilize the system, and through what inspection architecture. Our main result
provides an adaptive inspection policy that is asymptotically optimal in the
following sense: the ratio between the required number of experts under our
policy and the theoretical optimal converges to one, as the probability of
error in label recovery tends to zero.
</summary>
    <author>
      <name>Laurent Massoulie</name>
    </author>
    <author>
      <name>Kuang Xu</name>
    </author>
    <link href="http://arxiv.org/abs/1603.00544v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.00544v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.09406v2</id>
    <updated>2017-04-25T00:57:52Z</updated>
    <published>2016-03-30T22:43:19Z</published>
    <title>Risk contagion under regular variation and asymptotic tail independence</title>
    <summary>  Risk contagion concerns any entity dealing with large scale risks. Suppose
(X,Y) denotes a risk vector pertaining to two components in some system. A
relevant measurement of risk contagion would be to quantify the amount of
influence of high values of Y on X. This can be measured in a variety of ways.
In this paper, we study two such measures: the quantity E[max(X-t,0)|Y &gt; t]
called Marginal Mean Excess (MME) as well as the related quantity E[X|Y &gt; t]
called Marginal Expected Shortfall (MES). Both quantities are indicators of
risk contagion and useful in various applications ranging from finance,
insurance and systemic risk to environmental and climate risk. We work under
the assumptions of multivariate regular variation, hidden regular variation and
asymptotic tail independence for the risk vector (X,Y). Many broad and useful
model classes satisfy these assumptions. We present several examples and derive
the asymptotic behavior of both MME and MES as the threshold t tends to
infinity. We observe that although we assume asymptotic tail independence in
the models, MME and MES converge to 1 under very general conditions; this
reflects that the underlying weak dependence in the model still remains
significant. Besides the consistency of the empirical estimators, we introduce
an extrapolation method based on extreme value theory to estimate both MME and
MES for high thresholds t where little data are available. We show that these
estimators are consistent and illustrate our methodology in both simulated and
real data sets.
</summary>
    <author>
      <name>Bikramjit Das</name>
    </author>
    <author>
      <name>Vicky Fasen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages; 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.09406v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.09406v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.RM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="Primary 62G32, 62-09, 60G70, secondary 62G10, 62G15, 60F05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.04391v3</id>
    <updated>2017-07-11T13:12:23Z</updated>
    <published>2016-05-14T08:31:57Z</published>
    <title>Bayesian Lower Bounds for Dense or Sparse (Outlier) Noise in the RMT
  Framework</title>
    <summary>  Robust estimation is an important and timely research subject. In this paper,
we investigate performance lower bounds on the mean-square-error (MSE) of any
estimator for the Bayesian linear model, corrupted by a noise distributed
according to an i.i.d. Student's t-distribution. This class of prior
parametrized by its degree of freedom is relevant to modelize either dense or
sparse (accounting for outliers) noise. Using the hierarchical Normal-Gamma
representation of the Student's t-distribution, the Van Trees' Bayesian
Cram\'er-Rao bound (BCRB) on the amplitude parameters is derived. Furthermore,
the random matrix theory (RMT) framework is assumed, i.e., the number of
measurements and the number of unknown parameters grow jointly to infinity with
an asymptotic finite ratio. Using some powerful results from the RMT,
closed-form expressions of the BCRB are derived and studied. Finally, we
propose a framework to fairly compare two models corrupted by noises with
different degrees of freedom for a fixed common target signal-to-noise ratio
(SNR). In particular, we focus our effort on the comparison of the BCRBs
associated with two models corrupted by a sparse noise promoting outliers and a
dense (Gaussian) noise, respectively.
</summary>
    <author>
      <name>Virginie Ollier</name>
    </author>
    <author>
      <name>Rémy Boyer</name>
    </author>
    <author>
      <name>Mohammed Nabil El Korso</name>
    </author>
    <author>
      <name>Pascal Larzabal</name>
    </author>
    <link href="http://arxiv.org/abs/1605.04391v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.04391v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.05952v2</id>
    <updated>2017-04-29T18:51:08Z</updated>
    <published>2016-07-16T11:54:27Z</published>
    <title>Modelling spatio-temporal routines in human mobility</title>
    <summary>  Human mobility modelling is of fundamental importance in a wide range of
applications, such as the developing of protocols for mobile ad-hoc networks or
what-if analysis in urban ecosystems. Current generative models fail in
accurately reproducing the individuals' recurrent schedules and at the same
time in accounting for the possibility that individuals may break the routine
during periods of variable duration. In this article we present DITRAS
(DIary-based TRAjectory Simulator), a framework to simulate the spatio-temporal
patterns of human mobility. DITRAS operates in two steps: the generation of a
mobility diary and the translation of the mobility diary into a mobility
trajectory. We propose a data-driven algorithm which constructs a diary
generator from real data, capturing the tendency of individuals to follow or
break their routine. We also propose a trajectory generator based on the
concept of preferential exploration and preferential return. We instantiate
DITRAS with the proposed diary and trajectory generators and compare the
resulting spatio-temporal model with real data and synthetic data produced by
other spatio-temporal mobility models, built by instantiating DITRAS with
several combinations of diary and trajectory generators. We show that the
proposed model reproduces the statistical properties of real trajectories in
the most accurate way, making a step forward the understanding of the origin of
the spatio-temporal patterns of human mobility.
</summary>
    <author>
      <name>Luca Pappalardo</name>
    </author>
    <author>
      <name>Filippo Simini</name>
    </author>
    <link href="http://arxiv.org/abs/1607.05952v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.05952v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8; I.6.5; I.6.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.00494v2</id>
    <updated>2016-11-20T23:46:18Z</updated>
    <published>2016-09-02T08:02:43Z</published>
    <title>Publication bias and the canonization of false facts</title>
    <summary>  In the process of scientific inquiry, certain claims accumulate enough
support to be established as facts. Unfortunately, not every claim accorded the
status of fact turns out to be true. In this paper, we model the dynamic
process by which claims are canonized as fact through repeated experimental
confirmation. The community's confidence in a claim constitutes a Markov
process: each successive published result shifts the degree of belief, until
sufficient evidence accumulates to accept the claim as fact or to reject it as
false. In our model, publication bias --- in which positive results are
published preferentially over negative ones --- influences the distribution of
published results. We find that when readers do not know the degree of
publication bias and thus cannot condition on it, false claims often can be
canonized as facts. Unless a sufficient fraction of negative results are
published, the scientific process will do a poor job at discriminating false
from true claims. This problem is exacerbated when scientists engage in
p-hacking, data dredging, and other behaviors that increase the rate at which
false positives are published. If negative results become easier to publish as
a claim approaches acceptance as a fact, however, true and false claims can be
more readily distinguished. To the degree that the model accurately represents
current scholarly practice, there will be serious concern about the validity of
purported facts in some areas of scientific research.
</summary>
    <author>
      <name>Silas B. Nissen</name>
    </author>
    <author>
      <name>Tali Magidson</name>
    </author>
    <author>
      <name>Kevin Gross</name>
    </author>
    <author>
      <name>Carl T. Bergstrom</name>
    </author>
    <link href="http://arxiv.org/abs/1609.00494v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.00494v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.04383v1</id>
    <updated>2016-09-14T19:11:03Z</updated>
    <published>2016-09-14T19:11:03Z</published>
    <title>Probabilistic Population Projections for Countries with Generalized
  HIV/AIDS Epidemics</title>
    <summary>  The United Nations (UN) issued official probabilistic population projections
for all countries to 2100 in July 2015. This was done by simulating future
levels of total fertility and life expectancy from Bayesian hierarchical
models, and combining the results using a standard cohort-component projection
method. The 40 countries with generalized HIV/AIDS epidemics were treated
differently from others, in that the projections used the highly multistate
Spectrum/EPP model, a complex 15-compartment model that was designed for
short-term projections of quantities relevant to policy for the epidemic. Here
we propose a simpler approach that is more compatible with the existing UN
probabilistic projection methodology for other countries. Changes in life
expectancy are projected probabilistically using a simple time series
regression model on current life expectancy, HIV prevalence and ART coverage.
These are then converted to age- and sex-specific mortality rates using a new
family of model life tables designed for countries with HIV/AIDS epidemics that
reproduces the characteristic hump in middle adult mortality. These are then
input to the standard cohort-component method, as for other countries. The
method performed well in an out-of-sample cross-validation experiment. It gives
similar population projections to Spectrum/EPP in the short run, while being
simpler and avoiding multistate modeling.
</summary>
    <author>
      <name>David J. Sharrow</name>
    </author>
    <author>
      <name>Jessica Godwin</name>
    </author>
    <author>
      <name>Yanjun He</name>
    </author>
    <author>
      <name>Samuel J. Clark</name>
    </author>
    <author>
      <name>Adrian E. Raftery</name>
    </author>
    <link href="http://arxiv.org/abs/1609.04383v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.04383v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.01297v1</id>
    <updated>2017-04-05T08:00:14Z</updated>
    <published>2017-04-05T08:00:14Z</published>
    <title>Automated Diagnosis of Epilepsy Employing Multifractal Detrended
  Fluctuation Analysis Based Features</title>
    <summary>  This contribution reports an application of MultiFractal Detrended
Fluctuation Analysis, MFDFA based novel feature extraction technique for
automated detection of epilepsy. In fractal geometry, Multifractal Detrended
Fluctuation Analysis MFDFA is a popular technique to examine the
self-similarity of a nonlinear, chaotic and noisy time series. In the present
research work, EEG signals representing healthy, interictal (seizure free) and
ictal activities (seizure) are acquired from an existing available database.
The acquired EEG signals of different states are at first analyzed using MFDFA.
To requisite the time series singularity quantification at local and global
scales, a novel set of fourteen different features. Suitable feature ranking
employing students t-test has been done to select the most statistically
significant features which are henceforth being used as inputs to a support
vector machines (SVM) classifier for the classification of different EEG
signals. Eight different classification problems have been presented in this
paper and it has been observed that the overall classification accuracy using
MFDFA based features are reasonably satisfactory for all classification
problems. The performance of the proposed method are also found to be quite
commensurable and in some cases even better when compared with the results
published in existing literature studied on the similar data set.
</summary>
    <author>
      <name>S Pratiher</name>
    </author>
    <author>
      <name>S Chatterjee</name>
    </author>
    <author>
      <name>R Bose</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.01297v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.01297v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="92B25 92F99" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.08248v1</id>
    <updated>2017-04-26T10:57:42Z</updated>
    <published>2017-04-26T10:57:42Z</published>
    <title>Modeling and replicating statistical topology, and evidence for CMB
  non-homogeneity</title>
    <summary>  Under the banner of `Big Data', the detection and classification of structure
in extremely large, high dimensional, data sets, is, one of the central
statistical challenges of our times. Among the most intriguing approaches to
this challenge is `TDA', or `Topological Data Analysis', one of the primary
aims of which is providing non-metric, but topologically informative,
pre-analyses of data sets which make later, more quantitative analyses
feasible. While TDA rests on strong mathematical foundations from Topology, in
applications it has faced challenges due to an inability to handle issues of
statistical reliability and robustness and, most importantly, in an inability
to make scientific claims with verifiable levels of statistical confidence. We
propose a methodology for the parametric representation, estimation, and
replication of persistence diagrams, the main diagnostic tool of TDA. The power
of the methodology lies in the fact that even if only one persistence diagram
is available for analysis -- the typical case for big data applications --
replications can be generated to allow for conventional statistical hypothesis
testing. The methodology is conceptually simple and computationally practical,
and provides a broadly effective statistical procedure for persistence diagram
TDA analysis. We demonstrate the basic ideas on a toy example, and the power of
the approach in a novel and revealing analysis of CMB non-homogeneity.
</summary>
    <author>
      <name>Robert J. Adler</name>
    </author>
    <author>
      <name>Sarit Agami</name>
    </author>
    <author>
      <name>Pratyush Pranav</name>
    </author>
    <link href="http://arxiv.org/abs/1704.08248v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.08248v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.01031v2</id>
    <updated>2017-08-15T19:23:07Z</updated>
    <published>2017-07-04T15:17:38Z</published>
    <title>Decision Making and Biases in Cybersecurity Capability Development:
  Evidence from a Simulation Game Experiment</title>
    <summary>  Despite the rise in the frequency and intensity of cyber-attacks, many
organizations are still negligent in their management of cybersecurity
practices. To address this shortcoming, we developed a simulation game to
understand and improve how managers make investment decisions in building
cybersecurity capabilities. The simulation game focuses on how managers'
decisions may impact the profits of their business, considering the costs of
cybersecurity capability development, the unpredictability of cyber-attacks,
and potential delays in building capabilities. In an experiment with 67
individuals, we recorded and analyzed 1,479 simulation runs. We compared the
performances of a group of experienced cybersecurity professionals with diverse
industry backgrounds to an inexperienced control group. Both groups exhibited
similar systematic errors in decision-making, indicative of erroneous
heuristics when dealing with uncertainty. Experienced subjects did not
understand the mechanisms of delays any better than inexperienced subjects, and
in fact, performed worse in a less uncertain environment, suggesting more
developed heuristics. Our findings highlight the importance of training and
education for decision-makers and professionals in cybersecurity, and lay the
groundwork for future research in uncovering mental biases about the
complexities of cybersecurity capability development.
</summary>
    <author>
      <name>M. S. Jalali</name>
    </author>
    <author>
      <name>M. Siegel</name>
    </author>
    <author>
      <name>S. Madnick</name>
    </author>
    <link href="http://arxiv.org/abs/1707.01031v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.01031v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1007.0566v3</id>
    <updated>2011-05-17T20:52:49Z</updated>
    <published>2010-07-04T16:11:15Z</published>
    <title>Organisation of signal flow in directed networks</title>
    <summary>  Confining an answer to the question whether and how the coherent operation of
network elements is determined by the the network structure is the topic of our
work. We map the structure of signal flow in directed networks by analysing the
degree of edge convergence and the overlap between the in- and output sets of
an edge. Definitions of convergence degree and overlap are based on the
shortest paths, thus they encapsulate global network properties. Using the
defining notions of convergence degree and overlapping set we clarify the
meaning of network causality and demonstrate the crucial role of chordless
circles. In real-world networks the flow representation distinguishes nodes
according to their signal transmitting, processing and control properties. The
analysis of real-world networks in terms of flow representation was in
accordance with the known functional properties of the network nodes. It is
shown that nodes with different signal processing, transmitting and control
properties are randomly connected at the global scale, while local connectivity
patterns depart from randomness. Grouping network nodes according to their
signal flow properties was unrelated to the network's community structure. We
present evidence that signal flow properties of small-world-like, real-world
networks can not be reconstructed by algorithms used to generate small-world
networks. Convergence degree values were calculated for regular oriented trees,
and its probability density function for networks grown with the preferential
attachment mechanism. For Erd\H{o}s-R\'enyi graphs we calculated both the
probability density function of convergence degrees and of overlaps.
</summary>
    <author>
      <name>M. Bányai</name>
    </author>
    <author>
      <name>L. Négyessy</name>
    </author>
    <author>
      <name>F. Bazsó</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/1742-5468/2011/06/P06001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/1742-5468/2011/06/P06001" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 10 figures, 3 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Statistical Mechanics: Theory and Experiment, (2011)
  P06001</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1007.0566v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1007.0566v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.1462v1</id>
    <updated>2011-10-07T09:11:35Z</updated>
    <published>2011-10-07T09:11:35Z</published>
    <title>Dynamic Clustering of Histogram Data Based on Adaptive Squared
  Wasserstein Distances</title>
    <summary>  This paper deals with clustering methods based on adaptive distances for
histogram data using a dynamic clustering algorithm. Histogram data describes
individuals in terms of empirical distributions. These kind of data can be
considered as complex descriptions of phenomena observed on complex objects:
images, groups of individuals, spatial or temporal variant data, results of
queries, environmental data, and so on. The Wasserstein distance is used to
compare two histograms. The Wasserstein distance between histograms is
constituted by two components: the first based on the means, and the second, to
internal dispersions (standard deviation, skewness, kurtosis, and so on) of the
histograms. To cluster sets of histogram data, we propose to use Dynamic
Clustering Algorithm, (based on adaptive squared Wasserstein distances) that is
a k-means-like algorithm for clustering a set of individuals into $K$ classes
that are apriori fixed.
  The main aim of this research is to provide a tool for clustering histograms,
emphasizing the different contributions of the histogram variables, and their
components, to the definition of the clusters. We demonstrate that this can be
achieved using adaptive distances. Two kind of adaptive distances are
considered: the first takes into account the variability of each component of
each descriptor for the whole set of individuals; the second takes into account
the variability of each component of each descriptor in each cluster. We
furnish interpretative tools of the obtained partition based on an extension of
the classical measures (indexes) to the use of adaptive distances in the
clustering criterion function. Applications on synthetic and real-world data
corroborate the proposed procedure.
</summary>
    <author>
      <name>Antonio Irpino</name>
    </author>
    <author>
      <name>Rosanna Verde</name>
    </author>
    <author>
      <name>Francisco de AT De Carvalho</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.eswa.2013.12.001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.eswa.2013.12.001" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Expert Systems with Applications, vol. 41, p. 3351-3366, 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1110.1462v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.1462v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.7006v6</id>
    <updated>2014-02-25T17:35:36Z</updated>
    <published>2013-01-29T18:02:44Z</published>
    <title>A Unified Community Detection, Visualization and Analysis method</title>
    <summary>  Community detection in social graphs has attracted researchers' interest for
a long time. With the widespread of social networks on the Internet it has
recently become an important research domain. Most contributions focus upon the
definition of algorithms for optimizing the so-called modularity function. In
the first place interest was limited to unipartite graph inputs and partitioned
community outputs. Recently bipartite graphs, directed graphs and overlapping
communities have been investigated. Few contributions embrace at the same time
the three types of nodes. In this paper we present a method which unifies
commmunity detection for the three types of nodes and at the same time merges
partitionned and overlapping communities. Moreover results are visualized in
such a way that they can be analyzed and semantically interpreted. For
validation we experiment this method on well known simple benchmarks. It is
then applied to real data in three cases. In two examples of photos sets with
tagged people we reveal social networks. A second type of application is of
particularly interest. After applying our method to Human Brain Tractography
Data provided by a team of neurologists, we produce clusters of white fibers in
accordance with other well known clustering methods. Moreover our approach for
visualizing overlapping clusters allows better understanding of the results by
the neurologist team. These last results open up the possibility of applying
community detection methods in other domains such as data analysis with
original enhanced performances.
</summary>
    <author>
      <name>Michel Crampes</name>
    </author>
    <author>
      <name>Michel Plantié</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Advances in Complex Systems</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.7006v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.7006v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.5720v3</id>
    <updated>2015-07-20T11:00:37Z</updated>
    <published>2014-08-25T11:04:25Z</published>
    <title>Invariant sums of random matrices and the onset of level repulsion</title>
    <summary>  We compute analytically the joint probability density of eigenvalues and the
level spacing statistics for an ensemble of random matrices with interesting
features. It is invariant under the standard symmetry groups (orthogonal and
unitary) and yet the interaction between eigenvalues is not Vandermondian. The
ensemble contains real symmetric or complex hermitian matrices $\mathbf{S}$ of
the form $\mathbf{S}=\sum_{i=1}^M \langle \mathbf{O}_i
\mathbf{D}_i\mathbf{O}_i^{\mathrm{T}}\rangle$ or $\mathbf{S}=\sum_{i=1}^M
\langle \mathbf{U}_i \mathbf{D}_i\mathbf{U}_i^\dagger\rangle$ respectively. The
diagonal matrices
$\mathbf{D}_i=\mathrm{diag}\{\lambda_1^{(i)},\ldots,\lambda_N^{(i)}\}$ are
constructed from real eigenvalues drawn \emph{independently} from distributions
$p^{(i)}(x)$, while the matrices $\mathbf{O}_i$ and $\mathbf{U}_i$ are all
orthogonal or unitary. The average $\langle\cdot\rangle$ is simultaneously
performed over the symmetry group and the joint distribution of
$\{\lambda_j^{(i)}\}$. We focus on the limits i.) $N\to\infty$ and ii.)
$M\to\infty$, with $N=2$. In the limit i.), the resulting sum $\mathbf{S}$
develops level repulsion even though the original matrices do not feature it,
and classical RMT universality is restored asymptotically. In the limit ii.)
the spacing distribution attains scaling forms that are computed exactly: for
the orthogonal case, we recover the $\beta=1$ Wigner's surmise, while for the
unitary case an entirely new universal distribution is obtained. Our results
allow to probe analytically the microscopic statistics of the sum of random
matrices that become asymptotically free. We also give an interpretation of
this model in terms of radial random walks in a matrix space. The analytical
results are corroborated by numerical simulations.
</summary>
    <author>
      <name>Zdzisław Burda</name>
    </author>
    <author>
      <name>Giacomo Livan</name>
    </author>
    <author>
      <name>Pierpaolo Vivo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/1742-5468/2015/06/P06024</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/1742-5468/2015/06/P06024" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pag., 6 fig. - published version</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">J. Stat. Mech. 2015 P06024</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1408.5720v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.5720v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.MP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.3773v3</id>
    <updated>2015-12-24T11:37:57Z</updated>
    <published>2014-12-11T19:34:39Z</published>
    <title>Distinguishing cause from effect using observational data: methods and
  benchmarks</title>
    <summary>  The discovery of causal relationships from purely observational data is a
fundamental problem in science. The most elementary form of such a causal
discovery problem is to decide whether X causes Y or, alternatively, Y causes
X, given joint observations of two variables X, Y. An example is to decide
whether altitude causes temperature, or vice versa, given only joint
measurements of both variables. Even under the simplifying assumptions of no
confounding, no feedback loops, and no selection bias, such bivariate causal
discovery problems are challenging. Nevertheless, several approaches for
addressing those problems have been proposed in recent years. We review two
families of such methods: Additive Noise Methods (ANM) and Information
Geometric Causal Inference (IGCI). We present the benchmark CauseEffectPairs
that consists of data for 100 different cause-effect pairs selected from 37
datasets from various domains (e.g., meteorology, biology, medicine,
engineering, economy, etc.) and motivate our decisions regarding the "ground
truth" causal directions of all pairs. We evaluate the performance of several
bivariate causal discovery methods on these real-world benchmark data and in
addition on artificially simulated data. Our empirical results on real-world
data indicate that certain methods are indeed able to distinguish cause from
effect using only purely observational data, although more benchmark data would
be needed to obtain statistically significant conclusions. One of the best
performing methods overall is the additive-noise method originally proposed by
Hoyer et al. (2009), which obtains an accuracy of 63+-10 % and an AUC of
0.74+-0.05 on the real-world benchmark. As the main theoretical contribution of
this work we prove the consistency of that method.
</summary>
    <author>
      <name>Joris M. Mooij</name>
    </author>
    <author>
      <name>Jonas Peters</name>
    </author>
    <author>
      <name>Dominik Janzing</name>
    </author>
    <author>
      <name>Jakob Zscheischler</name>
    </author>
    <author>
      <name>Bernhard Schölkopf</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">101 pages, second revision submitted to Journal of Machine Learning
  Research</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Machine Learning Research 17(32):1-102, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1412.3773v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.3773v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.00292v2</id>
    <updated>2016-03-18T14:01:16Z</updated>
    <published>2015-10-01T15:57:16Z</published>
    <title>On Classification Issues within Ensemble-Based Complex System Simulation
  Tasks</title>
    <summary>  Contemporary tasks of complex system simulation are often related to the
issue of uncertainty management. It comes from the lack of information or
knowledge about the simulated system as well as from restrictions of the model
set being used. One of the powerful tools for the uncertainty management is
ensemble-based simulation, which uses variation in input or output data, model
parameters, or available versions of models to improve the simulation
performance. Furthermore the system of models for complex system simulation
(especially in case of hiring ensemble-based approach) can be considered as a
complex system. As a result, the identification of the complex model's
structure and parameters provide additional sources of uncertainty to be
managed. Within the presented work we are developing a conceptual and
technological approach to manage the ensemble-based simulation taking into
account changing states of both simulated system and system of models within
the ensemble-based approach. The states of these systems are considered as a
subject of classification with consequent inference of better strategies for
ensemble evolution over the simulation time and ensemble aggregation. Here the
ensemble evolution enables implementation of dynamic reactive solutions which
can automatically conform to the changing states of both systems. The ensemble
aggregation can be considered within a scope of averaging (regression way) or
selection (classification way, which complement the classification mentioned
earlier) approach. The technological basis for such approach includes
ensemble-based simulation techniques using domain-specific software combined
within a composite application; data science approaches for analysis of
available datasets (simulation data, observations, situation assessment etc.);
and machine learning algorithms for classes identification, ensemble management
and knowledge acquisition.
</summary>
    <author>
      <name>Sergey V. Kovalchuk</name>
    </author>
    <author>
      <name>Aleksey V. Krikunov</name>
    </author>
    <author>
      <name>Konstantin V. Knyazkov</name>
    </author>
    <author>
      <name>Sergey S. Kosukhin</name>
    </author>
    <author>
      <name>Alexander V. Boukhanovsky</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s00477-016-1324-5</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s00477-016-1324-5" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be presented at CCS'15 (http://www.ccs2015.org/)</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.00292v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.00292v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.01815v2</id>
    <updated>2016-05-18T10:20:09Z</updated>
    <published>2016-02-03T13:28:08Z</published>
    <title>Exploiting plume structure to decode gas source distance using
  metal-oxide gas sensors</title>
    <summary>  Estimating the distance of a gas source is important in many applications of
chemical sensing, like e.g. environmental monitoring, or chemically-guided
robot navigation. If an estimation of the gas concentration at the source is
available, source proximity can be estimated from the time-averaged gas
concentration at the sensing site. However, in turbulent environments, where
fast concentration fluctuations dominate, comparably long measurements are
required to obtain a reliable estimate. A lesser known feature that can be
exploited for distance estimation in a turbulent environment lies in the
relationship between source proximity and the temporal variance of the local
gas concentration - the farther the source, the more intermittent are gas
encounters. However, exploiting this feature requires measurement of changes in
gas concentration on a comparably fast time scale, that have up to now only
been achieved using photo-ionisation detectors. Here, we demonstrate that by
appropriate signal processing, off-the-shelf metal-oxide sensors are capable of
extracting rapidly fluctuating features of gas plumes that strongly correlate
with source distance. We show that with a straightforward analysis method it is
possible to decode events of large, consistent changes in the measured signal,
so-called 'bouts'. The frequency of these bouts predicts the distance of a gas
source in wind-tunnel experiments with good accuracy. In addition, we found
that the variance of bout counts indicates cross-wind offset to the centreline
of the gas plume. Our results offer an alternative approach to estimating gas
source proximity that is largely independent of gas concentration, using
off-the-shelf metal-oxide sensors. The analysis method we employ demands very
few computational resources and is suitable for low-power microcontrollers.
</summary>
    <author>
      <name>Michael Schmuker</name>
    </author>
    <author>
      <name>Viktor Bahr</name>
    </author>
    <author>
      <name>Ramón Huerta</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.snb.2016.05.098</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.snb.2016.05.098" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">41 pages, 20 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Sensors and Actuators B: Chemical 235:636-646 (2016)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1602.01815v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.01815v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.01344v1</id>
    <updated>2017-10-03T18:42:36Z</updated>
    <published>2017-10-03T18:42:36Z</published>
    <title>Development of models for predicting Torsade de Pointes cardiac
  arrhythmias using perceptron neural networks</title>
    <summary>  Blockage of some ion channels and in particular, the hERG cardiac potassium
channel delays cardiac repolarization and can induce arrhythmia. In some cases
it leads to a potentially life-threatening arrhythmia known as Torsade de
Pointes (TdP). Therefore recognizing drugs with TdP risk is essential.
Candidate drugs that are determined not to cause cardiac ion channel blockage
are more likely to pass successfully through clinical phases II and III trials
(and preclinical work) and not be withdrawn even later from the marketplace due
to cardiotoxic effects. The objective of the present study is to develop an SAR
model that can be used as an early screen for torsadogenic (causing TdP
arrhythmias) potential in drug candidates. The method is performed using
descriptors comprised of atomic NMR chemical shifts and corresponding
interatomic distances which are combined into a 3D abstract space matrix. The
method is called 3D-SDAR (3 dimensional spectral data-activity relationship)
and can be interrogated to identify molecular features responsible for the
activity, which can in turn yield simplified hERG toxicophores. A dataset of 55
hERG potassium channel inhibitors collected from Kramer et al. consisting of 32
drugs with TdP risk and 23 with no TdP risk was used for training the 3D-SDAR
model.An ANN model with multilayer perceptron was used to define collinearities
among the independent 3D-SDAR features. A composite model from 200 random
iterations with 25% of the molecules in each case yielded the following figures
of merit: training, 99.2 %; internal test sets, 66.7%; external (blind
validation) test set, 68.4%. In the external test set, 70.3% of positive TdP
drugs were correctly predicted. Moreover, toxicophores were generated from TdP
drugs. A 3D-SDAR was successfully used to build a predictive model for
drug-induced torsadogenic and non-torsadogenic drugs.
</summary>
    <author>
      <name>Mohsen Sharifi</name>
    </author>
    <author>
      <name>Dan Buzatu</name>
    </author>
    <author>
      <name>Stephen Harris</name>
    </author>
    <author>
      <name>Jon Wilkes</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in BMC Bioinformatics (Springer) July 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.01344v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.01344v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.BM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.BM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.MN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.02608v3</id>
    <updated>2016-10-17T13:35:40Z</updated>
    <published>2016-10-09T01:03:58Z</published>
    <title>Research and Education in Computational Science and Engineering</title>
    <summary>  Over the past two decades the field of computational science and engineering
(CSE) has penetrated both basic and applied research in academia, industry, and
laboratories to advance discovery, optimize systems, support decision-makers,
and educate the scientific and engineering workforce. Informed by centuries of
theory and experiment, CSE performs computational experiments to answer
questions that neither theory nor experiment alone is equipped to answer. CSE
provides scientists and engineers of all persuasions with algorithmic
inventions and software systems that transcend disciplines and scales. Carried
on a wave of digital technology, CSE brings the power of parallelism to bear on
troves of data. Mathematics-based advanced computing has become a prevalent
means of discovery and innovation in essentially all areas of science,
engineering, technology, and society; and the CSE community is at the core of
this transformation. However, a combination of disruptive
developments---including the architectural complexity of extreme-scale
computing, the data revolution that engulfs the planet, and the specialization
required to follow the applications to new frontiers---is redefining the scope
and reach of the CSE endeavor. This report describes the rapid expansion of CSE
and the challenges to sustaining its bold advances. The report also presents
strategies and directions for CSE research and education for the next decade.
</summary>
    <author>
      <name>Ulrich Rüde</name>
    </author>
    <author>
      <name>Karen Willcox</name>
    </author>
    <author>
      <name>Lois Curfman McInnes</name>
    </author>
    <author>
      <name>Hans De Sterck</name>
    </author>
    <author>
      <name>George Biros</name>
    </author>
    <author>
      <name>Hans Bungartz</name>
    </author>
    <author>
      <name>James Corones</name>
    </author>
    <author>
      <name>Evin Cramer</name>
    </author>
    <author>
      <name>James Crowley</name>
    </author>
    <author>
      <name>Omar Ghattas</name>
    </author>
    <author>
      <name>Max Gunzburger</name>
    </author>
    <author>
      <name>Michael Hanke</name>
    </author>
    <author>
      <name>Robert Harrison</name>
    </author>
    <author>
      <name>Michael Heroux</name>
    </author>
    <author>
      <name>Jan Hesthaven</name>
    </author>
    <author>
      <name>Peter Jimack</name>
    </author>
    <author>
      <name>Chris Johnson</name>
    </author>
    <author>
      <name>Kirk E. Jordan</name>
    </author>
    <author>
      <name>David E. Keyes</name>
    </author>
    <author>
      <name>Rolf Krause</name>
    </author>
    <author>
      <name>Vipin Kumar</name>
    </author>
    <author>
      <name>Stefan Mayer</name>
    </author>
    <author>
      <name>Juan Meza</name>
    </author>
    <author>
      <name>Knut Martin Mørken</name>
    </author>
    <author>
      <name>J. Tinsley Oden</name>
    </author>
    <author>
      <name>Linda Petzold</name>
    </author>
    <author>
      <name>Padma Raghavan</name>
    </author>
    <author>
      <name>Suzanne M. Shontz</name>
    </author>
    <author>
      <name>Anne Trefethen</name>
    </author>
    <author>
      <name>Peter Turner</name>
    </author>
    <author>
      <name>Vladimir Voevodin</name>
    </author>
    <author>
      <name>Barbara Wohlmuth</name>
    </author>
    <author>
      <name>Carol S. Woodward</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to SIAM Review</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.02608v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.02608v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.HO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="00A72, 62-07, 68U20, 68W01, 68W10, 97A99, 97M10, 97N80, 97R20, 97R30" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.0; G.4; I.6; J.0; J.2; J.3; J.4; J.6; J.7; K.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
